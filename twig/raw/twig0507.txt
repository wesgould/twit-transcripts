;FFMETADATA1
title=Machine Learning is the New Chicken Sexer
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=507
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.240]   It's time for Twig. This week in Google, Kevin Tofol joins us, filling in for Stacey Higginbotham.
[00:00:05.240 --> 00:00:10.040]   Jeff Jarvis is in the studio. He was at Google I/O. We'll get the highlights.
[00:00:10.040 --> 00:00:14.440]   We'll also unbox a little something he got at the Google I/O conference.
[00:00:14.440 --> 00:00:18.640]   And we'll take a walk. I'll actually make Jeff go outside to take a walk with Google Maps.
[00:00:18.640 --> 00:00:20.720]   It's all coming up next on Twig.
[00:00:20.720 --> 00:00:27.560]   Netcast, you love from people you trust.
[00:00:27.880 --> 00:00:29.560]   This is Twig.
[00:00:29.560 --> 00:00:33.560]   This is Twig.
[00:00:33.560 --> 00:00:41.560]   This is Twig. This week in Google, episode 507 recorded Wednesday, May 8th, 2019.
[00:00:41.560 --> 00:00:45.560]   Machine learning is the new chicken sexer.
[00:00:45.560 --> 00:00:50.560]   This week in Google is brought to you by Casper, a sleep brand that continues to revolutionize
[00:00:50.560 --> 00:00:55.560]   its line of products to create an exceptionally comfortable sleep experience one night at a time.
[00:00:55.560 --> 00:01:01.560]   Get $100 towards select mattresses by visiting casper.com/twig.
[00:01:01.560 --> 00:01:03.560]   And using the promo code Twig at checkout.
[00:01:03.560 --> 00:01:08.560]   And by ExpressVPN, protect your online privacy with one click.
[00:01:08.560 --> 00:01:16.560]   It's that easy. For three extra months free with a one-year package, go to expressvpn.com/twig.
[00:01:16.560 --> 00:01:22.560]   It's time for Twig. This week in Google, we cover the latest news from the Googleverse.
[00:01:22.560 --> 00:01:27.560]   Which this week will be all from the Googleverse. We're not going to talk about Facebook or Twitter or anything else.
[00:01:27.560 --> 00:01:30.560]   Except maybe the times pick you.
[00:01:30.560 --> 00:01:34.560]   Joining me right now in studio, he was at Google I/O yesterday.
[00:01:34.560 --> 00:01:40.560]   Jeff Jarvis has taken the chance of driving up the Golden Gate Bridge and joining us here in studio.
[00:01:40.560 --> 00:01:41.560]   It's great to have you.
[00:01:41.560 --> 00:01:42.560]   It's good to be here.
[00:01:42.560 --> 00:01:44.560]   From the New Heart School.
[00:01:44.560 --> 00:01:45.560]   New Heart?
[00:01:45.560 --> 00:01:47.560]   The Bob New Heart.
[00:01:47.560 --> 00:01:49.560]   New Man!
[00:01:50.560 --> 00:01:52.560]   From the Bob New Heart School of Journalism.
[00:01:52.560 --> 00:01:53.560]   It's not the name?
[00:01:53.560 --> 00:01:54.560]   No.
[00:01:54.560 --> 00:01:55.560]   It's the Craig Newmark.
[00:01:55.560 --> 00:01:56.560]   It's not the name.
[00:01:56.560 --> 00:01:57.560]   It's not the name.
[00:01:57.560 --> 00:01:58.560]   It's the New Mark.
[00:01:58.560 --> 00:02:00.560]   It's the Craig Newmark.
[00:02:00.560 --> 00:02:04.560]   Graduate School of Journalism at the City University of New York.
[00:02:04.560 --> 00:02:08.560]   And we love having you in studio, so it's nice to see you.
[00:02:08.560 --> 00:02:10.560]   You're colorless so much better in real life.
[00:02:10.560 --> 00:02:14.560]   I did it was. It's Wednesday. It's smell like soy sauce day here at Twitch.
[00:02:14.560 --> 00:02:16.560]   Well, we don't always have Chinese, but we did today.
[00:02:16.560 --> 00:02:17.560]   We did today.
[00:02:17.560 --> 00:02:21.560]   And I hope you partook of our festive feast for all of our.
[00:02:21.560 --> 00:02:22.560]   Leo was so good.
[00:02:22.560 --> 00:02:23.560]   He just ate broccoli.
[00:02:23.560 --> 00:02:25.560]   I had some beef with the broccoli.
[00:02:25.560 --> 00:02:27.560]   Kevin Tuffles back too.
[00:02:27.560 --> 00:02:28.560]   He was here.
[00:02:28.560 --> 00:02:29.560]   It was so great to have you, Kevin.
[00:02:29.560 --> 00:02:31.560]   Well, you weren't really filling in.
[00:02:31.560 --> 00:02:34.560]   You were joining me for the keynote yesterday of Google I/O.
[00:02:34.560 --> 00:02:39.560]   And then you took off because Liverpool Barcelona was about to start.
[00:02:39.560 --> 00:02:41.560]   Yes, yes, yes.
[00:02:41.560 --> 00:02:44.560]   And somehow, some way we needed four goals and got them.
[00:02:44.560 --> 00:02:45.560]   One, we have them.
[00:02:45.560 --> 00:02:46.560]   Very happy.
[00:02:46.560 --> 00:02:47.560]   Very happy.
[00:02:47.560 --> 00:02:50.560]   But you did something very I thought impressive.
[00:02:50.560 --> 00:02:56.560]   At the same time as you're watching this most important football game, you also watched the developer keynote.
[00:02:56.560 --> 00:03:00.560]   Yes, I did. I had to mute the game.
[00:03:00.560 --> 00:03:04.560]   And I was shouting and screaming and jumping around every time we scored.
[00:03:04.560 --> 00:03:08.560]   But on the other screen, I was listening to and watching the developer keynote for promo
[00:03:08.560 --> 00:03:11.560]   S news because there wasn't any in the main keynote.
[00:03:11.560 --> 00:03:15.560]   Kevin is, of course, the guy behind the bell.
[00:03:15.560 --> 00:03:17.560]   The guy behind about crumbooks.com.
[00:03:17.560 --> 00:03:18.560]   So it is kind of a professional.
[00:03:18.560 --> 00:03:19.560]   My beloved about crumb-- his beloved.
[00:03:19.560 --> 00:03:20.560]   I love it.
[00:03:20.560 --> 00:03:21.560]   I love it.
[00:03:21.560 --> 00:03:22.560]   We all love it.
[00:03:22.560 --> 00:03:23.560]   Yes, we love it about crumbooks.
[00:03:23.560 --> 00:03:24.560]   And my Bible.
[00:03:24.560 --> 00:03:26.560]   Co-host was Stacy Hagenbotham on the I of T podcast.
[00:03:26.560 --> 00:03:29.560]   So I saw Rick Osterlo after yesterday.
[00:03:29.560 --> 00:03:30.560]   He was on stage.
[00:03:30.560 --> 00:03:35.560]   People remember from the old Motorola days when Google bought Motorola.
[00:03:35.560 --> 00:03:38.560]   He came over and introduced the Moto X and now is in charge of all Google hardware.
[00:03:38.560 --> 00:03:42.560]   So after-- they said he was doing his Oprah moment.
[00:03:42.560 --> 00:03:45.560]   We hope that would be having a self-driving car under the seat.
[00:03:45.560 --> 00:03:46.560]   But we didn't get that.
[00:03:46.560 --> 00:03:47.560]   But we did get phones.
[00:03:47.560 --> 00:03:50.560]   But I went up to him and I said, I have one thing to say.
[00:03:50.560 --> 00:03:51.560]   It's not a question.
[00:03:51.560 --> 00:03:52.560]   It's begging.
[00:03:52.560 --> 00:03:53.560]   I love my Chromebook.
[00:03:53.560 --> 00:03:54.560]   Please.
[00:03:54.560 --> 00:03:56.560]   And he said we're committed to Chromebooks.
[00:03:56.560 --> 00:03:57.560]   Good.
[00:03:57.560 --> 00:03:58.560]   So there may be a new Pixelbook.
[00:03:58.560 --> 00:04:03.560]   And we have four teachers from Sacramento who are fist pumping right now.
[00:04:03.560 --> 00:04:06.560]   I guess you guys use Chromebooks in the schools.
[00:04:06.560 --> 00:04:07.560]   Yeah, nice.
[00:04:07.560 --> 00:04:08.560]   Yeah, I love Chromebooks.
[00:04:08.560 --> 00:04:09.560]   Love them.
[00:04:09.560 --> 00:04:11.560]   And they should be committed to Chromebooks.
[00:04:11.560 --> 00:04:13.560]   But it's doing so well.
[00:04:13.560 --> 00:04:17.560]   It's causing Microsoft to kind of shift its direction with its hardware.
[00:04:17.560 --> 00:04:23.560]   They're going to do a version of Windows Lite designed to be basically a Chromebook for schools.
[00:04:23.560 --> 00:04:26.560]   I don't know if Apple's changed its direction.
[00:04:26.560 --> 00:04:28.560]   But I think it's an important product.
[00:04:28.560 --> 00:04:36.560]   But let's start by talking about this because the Pixel 3A was the big announcement at Google I/O.
[00:04:36.560 --> 00:04:41.560]   And I authorized Carsten to run out to the Verizon store.
[00:04:41.560 --> 00:04:44.560]   He's been still using, like a really old phone.
[00:04:44.560 --> 00:04:46.560]   So we thought we'd get him a new phone.
[00:04:46.560 --> 00:04:48.560]   I've still got the OG Pixel.
[00:04:48.560 --> 00:04:49.560]   The OG?
[00:04:49.560 --> 00:04:50.560]   Whoa!
[00:04:50.560 --> 00:04:51.560]   That's a long time ago.
[00:04:51.560 --> 00:04:52.560]   Oh, that is.
[00:04:52.560 --> 00:04:53.560]   Yeah.
[00:04:53.560 --> 00:04:54.560]   So you deserve a new one.
[00:04:54.560 --> 00:04:59.560]   Jeff and I were both puzzled by, can you get a close up at the back of this box?
[00:04:59.560 --> 00:05:02.560]   Because we're both puzzled by the graphic on the back of the box.
[00:05:02.560 --> 00:05:03.560]   There's the phone.
[00:05:03.560 --> 00:05:04.560]   There's a chair.
[00:05:04.560 --> 00:05:06.560]   There's a Manila envelope.
[00:05:06.560 --> 00:05:08.560]   There's a piece of concrete.
[00:05:08.560 --> 00:05:11.560]   I don't understand.
[00:05:11.560 --> 00:05:12.560]   It's terrible.
[00:05:12.560 --> 00:05:14.560]   What is that?
[00:05:14.560 --> 00:05:16.560]   Why did it get me into the L's?
[00:05:16.560 --> 00:05:17.560]   It's concrete.
[00:05:17.560 --> 00:05:23.560]   Why would you put that in the box to show how tough it is?
[00:05:23.560 --> 00:05:24.560]   But it's not.
[00:05:24.560 --> 00:05:25.560]   It's plastic.
[00:05:25.560 --> 00:05:26.560]   Yeah, it's not.
[00:05:26.560 --> 00:05:28.560]   It's better than glass.
[00:05:28.560 --> 00:05:29.560]   True.
[00:05:29.560 --> 00:05:31.560]   So let's open it up.
[00:05:31.560 --> 00:05:34.560]   You might not, Carson, if I open your new phone?
[00:05:34.560 --> 00:05:35.560]   Do it at the same time.
[00:05:35.560 --> 00:05:36.560]   Double unboxing.
[00:05:36.560 --> 00:05:37.560]   Double unboxing.
[00:05:37.560 --> 00:05:41.560]   So a nice little plastic strip here that we were able to easily peel off.
[00:05:41.560 --> 00:05:43.560]   Oh, there's two of them.
[00:05:43.560 --> 00:05:46.560]   That's a lot better than trying to slit it with your thumb.
[00:05:46.560 --> 00:05:48.560]   How long has this been unboxing?
[00:05:48.560 --> 00:05:50.560]   It's quite a while.
[00:05:50.560 --> 00:05:52.560]   I mean, it's kind of boring.
[00:05:52.560 --> 00:05:53.560]   It's very cool.
[00:05:53.560 --> 00:05:55.560]   Why did we ever think this was interesting?
[00:05:55.560 --> 00:05:56.560]   Right, this is...
[00:05:56.560 --> 00:05:58.560]   Now it's a new thing on YouTube for kids.
[00:05:58.560 --> 00:06:00.560]   It's like a 20 minute video on it.
[00:06:00.560 --> 00:06:02.560]   So this really feels pretty nice, actually.
[00:06:02.560 --> 00:06:03.560]   It does.
[00:06:03.560 --> 00:06:05.560]   I mean, plastic's a fine material.
[00:06:05.560 --> 00:06:06.560]   It's not a...
[00:06:06.560 --> 00:06:08.560]   Well, they gave us cases, too.
[00:06:08.560 --> 00:06:09.560]   Oh, that's good.
[00:06:09.560 --> 00:06:11.560]   Where they made out of concrete.
[00:06:11.560 --> 00:06:15.560]   So the sacrifice you're making here, there are a few.
[00:06:15.560 --> 00:06:21.560]   It doesn't have the depth sensing camera on the front, just the standard 8 megapixel front facing camera.
[00:06:21.560 --> 00:06:25.560]   The back does, though, have the same 12.2 megapixel sensor.
[00:06:25.560 --> 00:06:29.560]   And they emphasize that the software means this is as good as the pixel.
[00:06:29.560 --> 00:06:36.560]   Well, but I'm wondering about that, because it doesn't have the imaging DSP that the pixel, the big brother pixel is.
[00:06:36.560 --> 00:06:37.560]   Oh.
[00:06:37.560 --> 00:06:41.560]   So it doesn't have the hardware support, but of course that just means probably it does the same things a little bit slower.
[00:06:41.560 --> 00:06:46.560]   Deeter Bone and his Verge review said that the camera was exactly, almost exactly as good.
[00:06:46.560 --> 00:06:51.560]   And that's really what people are buying these pixels for, the same fingerprint reader on the back.
[00:06:51.560 --> 00:06:53.560]   This is a six inch screen.
[00:06:53.560 --> 00:06:55.560]   This is the Pixel 3A XL.
[00:06:55.560 --> 00:06:56.560]   They also have a smaller 5.
[00:06:56.560 --> 00:07:00.560]   So you can see here, oh, yeah, fairly sizable bezel.
[00:07:00.560 --> 00:07:03.560]   Yeah, but you know what you get with that bezel?
[00:07:03.560 --> 00:07:05.560]   Two things, no notch.
[00:07:05.560 --> 00:07:06.560]   Yay!
[00:07:06.560 --> 00:07:07.560]   And a headphone jack.
[00:07:07.560 --> 00:07:08.560]   Yay!
[00:07:08.560 --> 00:07:11.560]   So, to be honest, I think where's the headphone jack?
[00:07:11.560 --> 00:07:12.560]   Oh, that's good.
[00:07:12.560 --> 00:07:13.560]   They're on top.
[00:07:13.560 --> 00:07:14.560]   It's on the top?
[00:07:14.560 --> 00:07:15.560]   Remember where they used to be there?
[00:07:15.560 --> 00:07:16.560]   Wow, they used to be there.
[00:07:16.560 --> 00:07:17.560]   How they are again.
[00:07:17.560 --> 00:07:20.560]   It does make sense to me, not coming out of the bottom, but coming out of the top.
[00:07:20.560 --> 00:07:21.560]   Yeah.
[00:07:21.560 --> 00:07:25.560]   I'm really, actually, please, I wonder, do you think Kevin, this is kind of a retreat?
[00:07:25.560 --> 00:07:30.560]   For Google saying, well, maybe that notch thing in the head, lack of the jack was a mistake.
[00:07:30.560 --> 00:07:32.560]   You follow Apple into your...
[00:07:32.560 --> 00:07:33.560]   At your peril.
[00:07:33.560 --> 00:07:34.560]   Kevin, you're muted.
[00:07:34.560 --> 00:07:35.560]   Oh, you're...
[00:07:35.560 --> 00:07:36.560]   I think they...
[00:07:36.560 --> 00:07:38.560]   I don't think it's a retreat.
[00:07:38.560 --> 00:07:42.560]   I think it's an easy way for them to cut costs in terms of design and special parts.
[00:07:42.560 --> 00:07:44.560]   I mean, this starts at $3.99.
[00:07:44.560 --> 00:07:45.560]   I think you guys have the XLs.
[00:07:45.560 --> 00:07:47.560]   That's $4.79.
[00:07:47.560 --> 00:07:49.560]   I don't think it's a retreat at all.
[00:07:49.560 --> 00:07:54.560]   I think they're just trying to get their stock Android devices in people's hands because
[00:07:54.560 --> 00:07:56.560]   they just aren't selling.
[00:07:56.560 --> 00:07:59.560]   They only had one carrier for the last couple of years that were selling in stores.
[00:07:59.560 --> 00:08:03.560]   Now that we've got four or five carriers and I don't think it's a retreat.
[00:08:03.560 --> 00:08:05.560]   I think it's a smart expansion strategy.
[00:08:05.560 --> 00:08:07.560]   Yeah, Kevin, you're right.
[00:08:07.560 --> 00:08:11.560]   You go back, I remember when they introduced Android low many years ago.
[00:08:11.560 --> 00:08:15.560]   The whole strategy was get them in as many hands as possible.
[00:08:15.560 --> 00:08:16.560]   Yeah.
[00:08:16.560 --> 00:08:17.560]   That's why we go open source.
[00:08:17.560 --> 00:08:18.560]   We get them everywhere.
[00:08:18.560 --> 00:08:19.560]   It is a little weird.
[00:08:19.560 --> 00:08:23.560]   I remember we were all a little bit upset that the Nexus phone program, which was a less expensive
[00:08:23.560 --> 00:08:28.560]   pure Google experience, got replaced by the Pixel phones, which is still a pure Google
[00:08:28.560 --> 00:08:31.560]   experience, but a feature phone price.
[00:08:31.560 --> 00:08:36.560]   I thought that maybe was kind of contrary to the whole spirit of the Nexus.
[00:08:36.560 --> 00:08:37.560]   Maybe they're going back to that.
[00:08:37.560 --> 00:08:39.560]   Maybe they will continue to have a low cost.
[00:08:39.560 --> 00:08:47.560]   I think it struck me most about the keynote yesterday and the day was shrinking the machine
[00:08:47.560 --> 00:08:54.560]   learning model from 100 gigs down to half a gig and making that low.
[00:08:54.560 --> 00:08:55.560]   That was impressive.
[00:08:55.560 --> 00:08:56.560]   It was extremely impressive.
[00:08:56.560 --> 00:09:02.560]   By making that local, what's possible in this device is awe inspiring.
[00:09:02.560 --> 00:09:09.560]   That was in general, if I were going to pick one thing about the keynote yesterday, it
[00:09:09.560 --> 00:09:15.560]   seemed to me that the main point Google was making is, well, yes, we collect a lot of
[00:09:15.560 --> 00:09:16.560]   information about you.
[00:09:16.560 --> 00:09:18.560]   There's a benefit to that.
[00:09:18.560 --> 00:09:21.560]   Ben Thompson talks a little bit about it in this trajectory column today.
[00:09:21.560 --> 00:09:22.560]   Google fights back.
[00:09:22.560 --> 00:09:24.560]   There's a benefit for you.
[00:09:24.560 --> 00:09:28.560]   Then I loved it that they also said, but we're going to do more and more on device.
[00:09:28.560 --> 00:09:32.560]   We're going to give you incognito mode, not just in the browser, but in maps.
[00:09:32.560 --> 00:09:36.560]   I think it's an acknowledgement that privacy is an important feature.
[00:09:36.560 --> 00:09:44.560]   I think that for Google to say, we think we can do a great job for you by knowing your
[00:09:44.560 --> 00:09:47.560]   information, but we also want to give you the chance to protect yourself.
[00:09:47.560 --> 00:09:50.560]   It's not so different from the way Google has always been.
[00:09:50.560 --> 00:09:54.560]   They pointed out they showed the data liberation front and the Google takeout and all that
[00:09:54.560 --> 00:09:55.560]   stuff.
[00:09:55.560 --> 00:09:59.560]   It's the obligatory mom and apple pie thing you have to do at every keynote now.
[00:09:59.560 --> 00:10:00.560]   Yeah.
[00:10:00.560 --> 00:10:01.560]   Privacy.
[00:10:01.560 --> 00:10:02.560]   Privacy and ethics.
[00:10:02.560 --> 00:10:08.560]   Ben says to put it more succinctly, Pachaya effectively said, yes, we collect a lot of data,
[00:10:08.560 --> 00:10:10.560]   but that data makes amazing things possible.
[00:10:10.560 --> 00:10:16.360]   I think there's a little more to it though, even privacy was one of the main features and
[00:10:16.360 --> 00:10:19.200]   factors of the keynote without a doubt.
[00:10:19.200 --> 00:10:25.120]   I don't know that Google was technically capable to deliver what it wanted to deliver
[00:10:25.120 --> 00:10:29.240]   and yet still offer more privacy until recently.
[00:10:29.240 --> 00:10:32.880]   That machine learning model that Jeff brought up is a great example.
[00:10:32.880 --> 00:10:36.800]   So I see Kevin, because they had to send the data up before.
[00:10:36.800 --> 00:10:40.200]   Now that the machine learning is local, they don't have to send the data up.
[00:10:40.200 --> 00:10:41.200]   That's correct.
[00:10:41.200 --> 00:10:42.360]   It's on device.
[00:10:42.360 --> 00:10:47.000]   Now the models themselves will go back up in small bits and pieces without data.
[00:10:47.000 --> 00:10:51.640]   Again, I don't know that they were ready to deliver that, say, two, three, four years
[00:10:51.640 --> 00:10:52.640]   ago.
[00:10:52.640 --> 00:10:56.960]   It's taken them a long time to get here, but now it's like, maybe we're over the hump
[00:10:56.960 --> 00:11:02.920]   of the mountain here with privacy and everything happens on devices and they just learn from
[00:11:02.920 --> 00:11:06.360]   those models that are generated on the devices, not the data.
[00:11:06.360 --> 00:11:10.320]   So Kevin explains something to me, but you just said, because I missed it as it was going
[00:11:10.320 --> 00:11:12.640]   by in the keynote.
[00:11:12.640 --> 00:11:17.840]   When there's a machine learning model is down on the phone, it learns and does things.
[00:11:17.840 --> 00:11:23.720]   The lessons that it learns, what you're saying is it's the conclusion that it comes to about
[00:11:23.720 --> 00:11:28.840]   a best method that goes back up to improve the machine learning federated, but not your
[00:11:28.840 --> 00:11:29.840]   data that goes back up.
[00:11:29.840 --> 00:11:30.840]   Is that what you're saying?
[00:11:30.840 --> 00:11:31.840]   That's what they say.
[00:11:31.840 --> 00:11:32.840]   Exactly.
[00:11:32.840 --> 00:11:37.960]   They said that yesterday they showed a little graphic where they kind of flipped the model,
[00:11:37.960 --> 00:11:41.560]   Jeff, instead of having all of the machine learning happening in the cloud, which is
[00:11:41.560 --> 00:11:43.840]   why your data had to go to the cloud.
[00:11:43.840 --> 00:11:48.240]   Now the machine learning can happen on the phone, your data stays on the phone, and what
[00:11:48.240 --> 00:11:52.000]   it's learned, the conclusion, as you said, from the modeling goes back to the cloud and
[00:11:52.000 --> 00:11:55.720]   gets aggregated as a giant, we'll call it a giant model.
[00:11:55.720 --> 00:12:00.160]   That's usually the word federated in that sense, which means that they have the ability
[00:12:00.160 --> 00:12:02.400]   of learning going on in all of these places.
[00:12:02.400 --> 00:12:03.400]   It's so impressive.
[00:12:03.400 --> 00:12:08.480]   I think that was, I tweeted yesterday that I think we moved from an era of Intel inside
[00:12:08.480 --> 00:12:14.760]   to AI inside, that when all of that power is in your device, what it can do.
[00:12:14.760 --> 00:12:18.240]   I mean, it was so impressive just with the speed of a conversation of multitasking and
[00:12:18.240 --> 00:12:22.600]   all that, but it goes so far beyond that that it was really, really impressive.
[00:12:22.600 --> 00:12:24.160]   I thought that was huge yesterday.
[00:12:24.160 --> 00:12:30.440]   I do wonder though, and I'm not technical enough to know this, but it sounds as if you,
[00:12:30.440 --> 00:12:36.120]   we talked a little bit about this during the keynote, you have public data and your machine
[00:12:36.120 --> 00:12:43.120]   learning occurring on powerful processors, TPUs, perhaps in the cloud, perhaps on machine,
[00:12:43.120 --> 00:12:48.240]   generating this data set, which you can then put on the phone in a much reduced size.
[00:12:48.240 --> 00:12:50.040]   I think at that point it's static.
[00:12:50.040 --> 00:12:52.040]   What do you think, Kevin?
[00:12:52.040 --> 00:12:55.440]   I don't think you're going to be able to add to that data set.
[00:12:55.440 --> 00:12:58.000]   You've now created a machine learning.
[00:12:58.000 --> 00:13:01.800]   For instance, they showed and it was really impressive, an illiterate Indian woman who
[00:13:01.800 --> 00:13:08.680]   was using Google, basically Google Lens, I guess, to read signs, to tell, to buy railroad
[00:13:08.680 --> 00:13:09.920]   tickets, she says, "This gives me such freedom.
[00:13:09.920 --> 00:13:12.320]   I don't have to ask for somebody to read this for me."
[00:13:12.320 --> 00:13:17.120]   It's changed my life, but the technology, that lens technology on that phone, I don't
[00:13:17.120 --> 00:13:18.120]   think that could be updated.
[00:13:18.120 --> 00:13:23.680]   I have a feeling it could be updated maybe as a big mass download, but it's not learning
[00:13:23.680 --> 00:13:24.680]   anymore.
[00:13:24.680 --> 00:13:25.680]   Well, I don't know.
[00:13:25.680 --> 00:13:26.680]   That's what they're going to say.
[00:13:26.680 --> 00:13:30.600]   And what Kevin just says, why I caught on this, is that it does learn some things there and
[00:13:30.600 --> 00:13:32.440]   federates that back up to the...
[00:13:32.440 --> 00:13:35.920]   Yeah, they showed that mechanism, but I'm not sure if that's the same thing.
[00:13:35.920 --> 00:13:38.240]   What does it mean for a machine learning to learn something?
[00:13:38.240 --> 00:13:40.080]   Kevin, what do you think?
[00:13:40.080 --> 00:13:42.080]   Right.
[00:13:42.080 --> 00:13:44.560]   I can't say I'm technical enough to fully understand it either.
[00:13:44.560 --> 00:13:49.160]   My thought process is that the models themselves could be updated.
[00:13:49.160 --> 00:13:50.160]   Think about it.
[00:13:50.160 --> 00:13:53.560]   They said that they had shrunken down that one machine learning model from 100 gig to
[00:13:53.560 --> 00:14:00.480]   half a gig is certainly downloadable and updates to something that's a half a gig are
[00:14:00.480 --> 00:14:02.480]   easily downloadable.
[00:14:02.480 --> 00:14:03.960]   So I don't know what it is.
[00:14:03.960 --> 00:14:04.960]   But it's not...
[00:14:04.960 --> 00:14:05.960]   Here's my point.
[00:14:05.960 --> 00:14:08.440]   It's not updateable with the information it's gleaning from your phone at this time.
[00:14:08.440 --> 00:14:10.080]   So unlike...
[00:14:10.080 --> 00:14:13.920]   We often hear of assistants that are getting smarter because they're learning your patterns.
[00:14:13.920 --> 00:14:18.600]   If you have a canned data set, that's not going to be changed in any way.
[00:14:18.600 --> 00:14:20.080]   That's a static data set.
[00:14:20.080 --> 00:14:21.080]   Oh, but right.
[00:14:21.080 --> 00:14:24.600]   But you have... You can have generic stuff uploaded to the cloud.
[00:14:24.600 --> 00:14:28.000]   That data set can get better generically, but it's not learning from you at that point.
[00:14:28.000 --> 00:14:29.000]   I understand.
[00:14:29.000 --> 00:14:33.240]   But it can send response data from you.
[00:14:33.240 --> 00:14:35.200]   Yeah, but I think that's entirely separate.
[00:14:35.200 --> 00:14:36.200]   Okay.
[00:14:36.200 --> 00:14:38.840]   Because they want to decouple it from your identity.
[00:14:38.840 --> 00:14:41.120]   So I think that's an entirely separate function.
[00:14:41.120 --> 00:14:43.240]   So what I think we're saying this...
[00:14:43.240 --> 00:14:44.240]   Yeah, yeah.
[00:14:44.240 --> 00:14:47.840]   But what we're saying is it's true in both cases, which is that that canned data set is
[00:14:47.840 --> 00:14:48.840]   now immutable.
[00:14:48.840 --> 00:14:49.840]   It's the way it is.
[00:14:49.840 --> 00:14:54.040]   You could download a new one, but it's not going to be starting from you particularly.
[00:14:54.040 --> 00:14:57.280]   The Kevin's point is still, I think, very valid.
[00:14:57.280 --> 00:15:03.880]   Is the shrinkage is what enabled them to do the privacy that they did?
[00:15:03.880 --> 00:15:04.880]   Yes.
[00:15:04.880 --> 00:15:09.240]   Yeah, no, the shrinkage is huge because also these data sets normally are gigantic.
[00:15:09.240 --> 00:15:10.440]   And so you couldn't put them on a phone.
[00:15:10.440 --> 00:15:12.720]   You couldn't reasonably download them and you couldn't put them on a phone and they wouldn't
[00:15:12.720 --> 00:15:13.720]   be useful.
[00:15:13.720 --> 00:15:15.360]   So I'm not sure what they're doing with that.
[00:15:15.360 --> 00:15:19.520]   It's because I keep thinking of the Seinfeld episode when you guys say shrinkage.
[00:15:19.520 --> 00:15:20.520]   Sorry.
[00:15:20.520 --> 00:15:22.880]   It just keeps coming to mind.
[00:15:22.880 --> 00:15:24.880]   It was a little cold yesterday.
[00:15:24.880 --> 00:15:25.880]   How about I hope?
[00:15:25.880 --> 00:15:26.880]   Was it?
[00:15:26.880 --> 00:15:27.880]   Yeah.
[00:15:27.880 --> 00:15:28.880]   Did they ever hand out that sunscreen?
[00:15:28.880 --> 00:15:29.880]   No, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we,
[00:15:29.880 --> 00:15:30.880]   journalists didn't get it.
[00:15:30.880 --> 00:15:31.880]   So you see, I got a, oh, you did get a burn.
[00:15:31.880 --> 00:15:32.880]   I got a burn.
[00:15:32.880 --> 00:15:34.880]   And, and so I think we've all learned something here.
[00:15:34.880 --> 00:15:38.880]   You can see the, um, the stripe of the, of the, uh, the lanyard.
[00:15:38.880 --> 00:15:39.880]   Even during overcast weather.
[00:15:39.880 --> 00:15:40.880]   You still get a sunburn.
[00:15:40.880 --> 00:15:42.760]   I want to go back to Ben Thompson's things.
[00:15:42.760 --> 00:15:44.000]   I think it's important.
[00:15:44.000 --> 00:15:45.000]   I tweeted yesterday.
[00:15:45.000 --> 00:15:47.520]   There were two great goose bump moments.
[00:15:47.520 --> 00:15:49.000]   And you just mentioned, which is the illiterate.
[00:15:49.000 --> 00:15:50.000]   Incredible.
[00:15:50.000 --> 00:15:51.000]   And he leads with that as well.
[00:15:51.000 --> 00:15:56.360]   And then the other one was, uh, gathering data for those who have trouble with their
[00:15:56.360 --> 00:15:57.360]   language.
[00:15:57.360 --> 00:16:00.960]   Wasn't that amazing to me, tree, who was, I think both a Russian native Russian speaker
[00:16:00.960 --> 00:16:05.000]   and had a speech impediment of some cons who was, you could kind of understand them very
[00:16:05.000 --> 00:16:11.080]   difficult, but they kind of glossed over this a little bit, but he had us provide 15,000
[00:16:11.080 --> 00:16:13.560]   individual speech samples for this.
[00:16:13.560 --> 00:16:14.560]   So it's not.
[00:16:14.560 --> 00:16:15.560]   Well, he's a researcher at Google.
[00:16:15.560 --> 00:16:16.560]   So I think he did it to get.
[00:16:16.560 --> 00:16:17.560]   Yeah.
[00:16:17.560 --> 00:16:19.520]   So, and the effect, this is the euphonia project.
[00:16:19.520 --> 00:16:20.520]   They solicited.
[00:16:20.520 --> 00:16:25.440]   They said, if anybody has difficulty communicating with our assistants, g.co/uphonia.
[00:16:25.440 --> 00:16:29.320]   We'd love to get more volunteers, but it points up kind of a problem with this, which is it
[00:16:29.320 --> 00:16:31.000]   can't be generalized.
[00:16:31.000 --> 00:16:33.880]   It's at least specific to that speaker.
[00:16:33.880 --> 00:16:38.840]   You could, yes, but you could say people who have who stutter or people who have MS.
[00:16:38.840 --> 00:16:39.840]   I don't know.
[00:16:39.840 --> 00:16:43.080]   You might make it make it better at non standard speech.
[00:16:43.080 --> 00:16:48.440]   But of course, the worst, of course, which is people from Scotland.
[00:16:48.440 --> 00:16:49.960]   I don't think we'll ever do it.
[00:16:49.960 --> 00:16:50.960]   Do it.
[00:16:50.960 --> 00:16:51.960]   No, not good.
[00:16:51.960 --> 00:16:54.120]   I already insulted Scotland enough.
[00:16:54.120 --> 00:17:00.080]   But I think Ben's point here is one I was thinking of yesterday too is it's not just
[00:17:00.080 --> 00:17:01.560]   Google fights back.
[00:17:01.560 --> 00:17:02.680]   It's the internet fights back.
[00:17:02.680 --> 00:17:03.880]   The technologist fight back.
[00:17:03.880 --> 00:17:08.720]   I mean yesterday was all was filled, not all about, but it's filled with moments of look
[00:17:08.720 --> 00:17:11.720]   at the good things we can do because we have this technology.
[00:17:11.720 --> 00:17:12.720]   People has always done that.
[00:17:12.720 --> 00:17:13.960]   Apple's always done that.
[00:17:13.960 --> 00:17:15.400]   They did it more than every yesterday.
[00:17:15.400 --> 00:17:17.600]   They certainly had heartwarming moments.
[00:17:17.600 --> 00:17:19.400]   Both those videos were like, wow.
[00:17:19.400 --> 00:17:23.360]   Now, on the other hand, not once did they use the word advertising.
[00:17:23.360 --> 00:17:25.760]   The core business model thing that's higher corporation.
[00:17:25.760 --> 00:17:29.560]   The entire enterprise didn't get in there at all.
[00:17:29.560 --> 00:17:30.720]   And I found that kind of interesting.
[00:17:30.720 --> 00:17:34.960]   But that aside, it was technology saying technology is still okay.
[00:17:34.960 --> 00:17:35.960]   It's not evil.
[00:17:35.960 --> 00:17:36.960]   It's not ruining the world.
[00:17:36.960 --> 00:17:37.960]   It's all right, folks.
[00:17:37.960 --> 00:17:38.960]   Yeah.
[00:17:38.960 --> 00:17:39.960]   And I was glad to see you.
[00:17:39.960 --> 00:17:40.960]   You like that.
[00:17:40.960 --> 00:17:43.200]   Jeff, do you think this is the start of a shift?
[00:17:43.200 --> 00:17:47.520]   Because they can't just, they've been looking to expand beyond advertising for years.
[00:17:47.520 --> 00:17:50.520]   Other bets is all about that, obviously.
[00:17:50.520 --> 00:17:56.320]   When they start getting machine learning models on the device, think about it.
[00:17:56.320 --> 00:17:57.520]   And maybe this is a stretch.
[00:17:57.520 --> 00:18:01.320]   But what they showed in the videos is because Gmail knows so much about you.
[00:18:01.320 --> 00:18:06.400]   That's why it can help you get a car on your next trip because it's got emails that show
[00:18:06.400 --> 00:18:08.400]   you're going somewhere, et cetera.
[00:18:08.400 --> 00:18:10.600]   That's all happening through the cloud as well.
[00:18:10.600 --> 00:18:15.760]   But if that all moves down to the lowest level of the device, and advertising, in a sense,
[00:18:15.760 --> 00:18:21.000]   it starts to not be Google's number one priority anymore.
[00:18:21.000 --> 00:18:22.000]   It's so much of their revenue.
[00:18:22.000 --> 00:18:23.360]   I don't know what the latest number is.
[00:18:23.360 --> 00:18:25.760]   And it's hard to imagine them shrinking that much.
[00:18:25.760 --> 00:18:26.760]   Yes.
[00:18:26.760 --> 00:18:30.160]   Here's what Ben writes about this, which I think is right on.
[00:18:30.160 --> 00:18:34.160]   There should be to be sure concerns about Google believing their own hype.
[00:18:34.160 --> 00:18:38.560]   Many of the problems with YouTube, for example, stem from the Pollyanna-ish assumption that
[00:18:38.560 --> 00:18:43.720]   treats technology as an inherent good instead of an amoral force that makes everything both
[00:18:43.720 --> 00:18:47.160]   positive outcomes and negative ones easier and more efficient to achieve.
[00:18:47.160 --> 00:18:52.200]   At the same time, from a purely strategic perspective, the positive message makes sense.
[00:18:52.200 --> 00:18:56.000]   Presuming that everything about technology is bad is just as mistaken as the opposite.
[00:18:56.000 --> 00:19:00.640]   And the fact of the matter is lots of people like Google products and reminding them of
[00:19:00.640 --> 00:19:02.920]   the fact is Google's long-term benefit.
[00:19:02.920 --> 00:19:06.320]   Moreover, and this is the paragraph that's really, I think, seminal, more over a world
[00:19:06.320 --> 00:19:10.920]   of assistance in machine learning based products is very much to Google's advantage.
[00:19:10.920 --> 00:19:15.280]   The argument to not simply tolerate Google's collection of data, but to actually give them
[00:19:15.280 --> 00:19:21.640]   more is less about some lame case about better targeted ads, but about making actually useful
[00:19:21.640 --> 00:19:22.840]   products better.
[00:19:22.840 --> 00:19:29.240]   The better targeted ads are a strategy credit, which is what Ben Thompson is.
[00:19:29.240 --> 00:19:33.920]   Yeah, the whole thing yesterday was that we wanted a, what was the word, a useful home?
[00:19:33.920 --> 00:19:35.720]   No, a helpful home.
[00:19:35.720 --> 00:19:36.720]   So Google's your help, Ben.
[00:19:36.720 --> 00:19:37.720]   Google's well aware.
[00:19:37.720 --> 00:19:43.960]   Google's well aware that all of this aids its bottom line, but they're smart to focus
[00:19:43.960 --> 00:19:46.400]   on the fact that it's also helpful to you.
[00:19:46.400 --> 00:19:48.560]   And this is a tradeoff you and I've talked about a lot.
[00:19:48.560 --> 00:19:52.640]   This is a tradeoff I and you and I suspect Kevin are willing to make.
[00:19:52.640 --> 00:19:53.960]   We get a lot of benefit.
[00:19:53.960 --> 00:19:55.840]   It started with Google Now cards.
[00:19:55.840 --> 00:19:58.720]   Remember that it would know when you got to the airport that you had a flight and it
[00:19:58.720 --> 00:20:00.720]   would pop up that card?
[00:20:00.720 --> 00:20:03.960]   That required looking at your email, knowing your travel plans.
[00:20:03.960 --> 00:20:05.320]   Yeah, and now it was worth it.
[00:20:05.320 --> 00:20:09.640]   You can say make a car reservation and it pulls that data in and pre-fills it.
[00:20:09.640 --> 00:20:10.840]   It goes way beyond the now.
[00:20:10.840 --> 00:20:17.080]   It's really interesting though that Google deprecated now, which perhaps crossed the
[00:20:17.080 --> 00:20:20.520]   creepy line in a way that this doesn't, which does the same thing.
[00:20:20.520 --> 00:20:21.520]   This is transactional.
[00:20:21.520 --> 00:20:26.400]   Does the same exact thing, but it's less creepy because you're asking it.
[00:20:26.400 --> 00:20:27.400]   Yes.
[00:20:27.400 --> 00:20:29.120]   Well, it just happens to know.
[00:20:29.120 --> 00:20:36.800]   I also liked that Google showed how when in the restaurant recommendations or the recipe
[00:20:36.800 --> 00:20:40.200]   recommendations where you said, what should I make for dinner?
[00:20:40.200 --> 00:20:44.600]   It showed a meal to make, but it showed you below it why Google thought that where it
[00:20:44.600 --> 00:20:48.720]   got that information based on searches you've made or other recipes.
[00:20:48.720 --> 00:20:50.040]   Amazon used to do.
[00:20:50.040 --> 00:20:51.360]   They stopped doing it, but they used to do to.
[00:20:51.360 --> 00:20:54.200]   I think that helps with the creepy factor too.
[00:20:54.200 --> 00:20:55.200]   Yeah.
[00:20:55.200 --> 00:21:02.400]   So next week my friend David Weinberger's book comes out, Everyday Chaos.
[00:21:02.400 --> 00:21:06.600]   I think the part of the problem we have here is that machine learning is going to be less
[00:21:06.600 --> 00:21:08.400]   and less explicable.
[00:21:08.400 --> 00:21:11.360]   We're not going to know why it comes to the conclusions it does about us or about the
[00:21:11.360 --> 00:21:12.360]   world.
[00:21:12.360 --> 00:21:13.360]   Right.
[00:21:13.360 --> 00:21:17.040]   In fact, it's reductionist to say under that beef bourbon recipe, oh, this is based on
[00:21:17.040 --> 00:21:18.040]   your searches.
[00:21:18.040 --> 00:21:23.240]   It's based on a hell of a lot more than that, but it makes us feel a little better.
[00:21:23.240 --> 00:21:26.720]   So, you know, maybe that's just a little benign white lie.
[00:21:26.720 --> 00:21:27.720]   Yeah.
[00:21:27.720 --> 00:21:32.160]   It could be people who don't care about the climate like you eat a lot of beef.
[00:21:32.160 --> 00:21:35.760]   The truth is we know you have heart disease, buddy, and there's only one possible way.
[00:21:35.760 --> 00:21:37.480]   It's from eating a lot of beef bourbon.
[00:21:37.480 --> 00:21:39.000]   So have some more.
[00:21:39.000 --> 00:21:45.000]   We'll be passing this along to all state later.
[00:21:45.000 --> 00:21:51.560]   It's so hard with these kinds of events because companies, besides Microsoft, which somehow
[00:21:51.560 --> 00:21:54.280]   just completely fumbled its keynote Monday.
[00:21:54.280 --> 00:21:59.520]   But companies now know how to tug the right heart strings, how to position their message
[00:21:59.520 --> 00:22:00.880]   in a positive way.
[00:22:00.880 --> 00:22:02.080]   Apple led the way with this.
[00:22:02.080 --> 00:22:08.600]   And Apple showed how a keynote can be used to create an incredible halo effect.
[00:22:08.600 --> 00:22:13.920]   Sometimes people call that the reality distortion field that Steve Jobs created.
[00:22:13.920 --> 00:22:19.400]   And so there is definitely around modern keynotes, this sense as you come out of this glow of
[00:22:19.400 --> 00:22:22.480]   a man at Google, they're doing great stuff, aren't they?
[00:22:22.480 --> 00:22:24.320]   That sometimes takes a while to wear off.
[00:22:24.320 --> 00:22:28.680]   Last year was Google duplex and it took a day or two before people said, wait a minute.
[00:22:28.680 --> 00:22:29.680]   Are you?
[00:22:29.680 --> 00:22:31.520]   But duplex was back this time.
[00:22:31.520 --> 00:22:34.440]   And a much less heartwarming use.
[00:22:34.440 --> 00:22:35.440]   Yes.
[00:22:35.440 --> 00:22:36.440]   So they've learned.
[00:22:36.440 --> 00:22:39.960]   They've got a good machine learning model.
[00:22:39.960 --> 00:22:43.280]   So maybe we haven't, it hasn't quite worn off yet.
[00:22:43.280 --> 00:22:49.760]   But let's attempt to look beyond the reality distortion field in the glow.
[00:22:49.760 --> 00:22:53.000]   It's also nice refreshing once in a while to look beyond the dystopia.
[00:22:53.000 --> 00:22:54.000]   Yeah.
[00:22:54.000 --> 00:22:55.000]   I know.
[00:22:55.000 --> 00:23:01.600]   I mean, I'm very happily living in the Google universe and go ahead and try not to.
[00:23:01.600 --> 00:23:05.280]   So in a way, I think I'm embracing as most of us do the inevitable.
[00:23:05.280 --> 00:23:08.000]   Kevin, did you hear much yesterday at all?
[00:23:08.000 --> 00:23:11.200]   Anything about Chrome OS?
[00:23:11.200 --> 00:23:16.960]   So in the developer keynote, they did talk about developing Android apps.
[00:23:16.960 --> 00:23:21.720]   And I wrote up a story afterwards because I installed the new Android studio on my Pixel
[00:23:21.720 --> 00:23:23.360]   Slate.
[00:23:23.360 --> 00:23:26.640]   That's pretty cool that you can run Android Studio on a Chromebook.
[00:23:26.640 --> 00:23:28.800]   Well, I have been doing that.
[00:23:28.800 --> 00:23:29.800]   Oh, you have Linux.
[00:23:29.800 --> 00:23:30.800]   Yeah.
[00:23:30.800 --> 00:23:34.480]   As soon as Christina came out and they said all new Chromebooks coming out this year will
[00:23:34.480 --> 00:23:40.760]   have Linux out of the box, which is so because that's a big feature right now.
[00:23:40.760 --> 00:23:45.720]   So I've been running lots of different coding development environments on Christina.
[00:23:45.720 --> 00:23:51.160]   Is this new way that they're talking about different from what you've been doing?
[00:23:51.160 --> 00:23:56.120]   They created a one-click install for it, which is just fine.
[00:23:56.120 --> 00:23:58.360]   I mean, I've got a screenshot of it.
[00:23:58.360 --> 00:24:03.160]   It downloads a Debian package, which using the files app in Chrome OS, you can just say
[00:24:03.160 --> 00:24:04.320]   install with Linux.
[00:24:04.320 --> 00:24:05.320]   And that's okay.
[00:24:05.320 --> 00:24:07.720]   You don't have to have Linux set up already.
[00:24:07.720 --> 00:24:09.680]   You don't have to, Christina, set up already, or do you?
[00:24:09.680 --> 00:24:11.200]   It has to be enabled.
[00:24:11.200 --> 00:24:12.920]   It does have to be enabled or over.
[00:24:12.920 --> 00:24:13.920]   That's not going to work.
[00:24:13.920 --> 00:24:15.800]   But the difference is you don't have to go into these days.
[00:24:15.800 --> 00:24:18.240]   You don't go into developer mode to do that.
[00:24:18.240 --> 00:24:21.000]   You don't have to downgrade your security or any of that.
[00:24:21.000 --> 00:24:24.960]   Yeah, they really, I mean, they made it slightly easier and quite honestly, if you're developing
[00:24:24.960 --> 00:24:27.000]   Android apps, they all certainly wasn't hard.
[00:24:27.000 --> 00:24:28.440]   You should know how to do it.
[00:24:28.440 --> 00:24:29.440]   Yeah.
[00:24:29.440 --> 00:24:30.840]   You should know how to do that and a little more.
[00:24:30.840 --> 00:24:32.400]   But here's what kind of irks me.
[00:24:32.400 --> 00:24:36.520]   In fact, I'm going to share something a little personal here because this afternoon I was
[00:24:36.520 --> 00:24:43.840]   quite upset and thinking about selling my Pixel Slate because with this new Android
[00:24:43.840 --> 00:24:51.240]   Studio preview, specifically for Chrome OS, they list recommended Chromebooks and the
[00:24:51.240 --> 00:24:54.320]   Pixelbook and the Pixel Slate are not on there.
[00:24:54.320 --> 00:24:56.880]   And that's because of the CPUs inside.
[00:24:56.880 --> 00:24:58.480]   It's a Y series CPU.
[00:24:58.480 --> 00:25:00.600]   Or the Y series CPU's.
[00:25:00.600 --> 00:25:05.480]   And that's really disheartening for me, even though I'm not an Android developer, I'm
[00:25:05.480 --> 00:25:06.840]   thinking ahead here.
[00:25:06.840 --> 00:25:11.320]   I'm taking programming classes and I assume I'm going to move on to Android development
[00:25:11.320 --> 00:25:12.480]   at some point.
[00:25:12.480 --> 00:25:14.680]   So I'm a little distraught about this.
[00:25:14.680 --> 00:25:18.920]   A couple of Chrome boxes are recommended and two Chrome books right now.
[00:25:18.920 --> 00:25:23.240]   Does the older Pixelbook like the one Jeff's staring at, does that support it?
[00:25:23.240 --> 00:25:25.080]   Is that a U processor?
[00:25:25.080 --> 00:25:26.440]   That's a Y series as well.
[00:25:26.440 --> 00:25:27.440]   That's a seventh gen.
[00:25:27.440 --> 00:25:33.320]   So your $1,000 plus Pixelbook cannot be used.
[00:25:33.320 --> 00:25:34.720]   It's not that it can't be used.
[00:25:34.720 --> 00:25:35.720]   It's not recommended.
[00:25:35.720 --> 00:25:36.720]   Oh, OK.
[00:25:36.720 --> 00:25:37.720]   So is it too slow?
[00:25:37.720 --> 00:25:38.720]   I got it.
[00:25:38.720 --> 00:25:40.160]   That's the thing so far.
[00:25:40.160 --> 00:25:43.600]   And I don't know if it's just because it's a preview version of Android Studio or it's
[00:25:43.600 --> 00:25:48.240]   because I'm using a Y series processor in my Chromebook, but it was pretty laggy.
[00:25:48.240 --> 00:25:49.240]   Oh, well.
[00:25:49.240 --> 00:25:54.880]   And so the minimum they require 8 gigs of RAM, 4 gigs of space on the disk, 1280 by 800
[00:25:54.880 --> 00:25:55.880]   screen resolution.
[00:25:55.880 --> 00:25:59.720]   The real key here is the i5 U series or higher.
[00:25:59.720 --> 00:26:05.200]   And right now there's only a five Chromebooks that recommend the Yoga 630, the Acer Spin
[00:26:05.200 --> 00:26:11.400]   13, the HP X30 and G2 and the Acer Chrome box, the i3.
[00:26:11.400 --> 00:26:14.000]   So those are two boxes, not even Chromebooks.
[00:26:14.000 --> 00:26:18.280]   So well, it is a development environment, honestly.
[00:26:18.280 --> 00:26:21.160]   I mean, how many people are doing it, but still?
[00:26:21.160 --> 00:26:27.600]   I think why I'm slightly upset was when they said back in November that Android Studio
[00:26:27.600 --> 00:26:30.960]   would be officially supported on Chromebooks, you know, of course they were showing it on
[00:26:30.960 --> 00:26:31.960]   the Pixelbook.
[00:26:31.960 --> 00:26:36.680]   So I think many people took away like, yeah, it's the flagship device.
[00:26:36.680 --> 00:26:37.680]   It's going to support this.
[00:26:37.680 --> 00:26:42.520]   Lots of Android or Google people have these Pixelbooks, but it's not on there for understandable
[00:26:42.520 --> 00:26:43.520]   reason.
[00:26:43.520 --> 00:26:45.520]   But yeah, I'm thinking.
[00:26:45.520 --> 00:26:48.040]   If you sell it, what would you get?
[00:26:48.040 --> 00:26:50.760]   Oh, not what I paid.
[00:26:50.760 --> 00:26:51.760]   No, no.
[00:26:51.760 --> 00:26:53.640]   I mean, what would you buy in its place?
[00:26:53.640 --> 00:26:54.640]   I understand.
[00:26:54.640 --> 00:26:56.040]   He's so bitter, Kevin is so bitter.
[00:26:56.040 --> 00:26:57.280]   It's not what I paid for.
[00:26:57.280 --> 00:26:58.280]   That's for sure.
[00:26:58.280 --> 00:26:59.280]   Well, that's true.
[00:26:59.280 --> 00:27:00.280]   I know that.
[00:27:00.280 --> 00:27:04.080]   So I looked at some of the recommended ones and I'm well aware of those devices.
[00:27:04.080 --> 00:27:07.880]   You know, I write about them and all and, you know, they're going to cost me anywhere
[00:27:07.880 --> 00:27:11.560]   from six to 800, maybe 900 to replace this.
[00:27:11.560 --> 00:27:12.560]   And that's fine.
[00:27:12.560 --> 00:27:13.560]   It's not like.
[00:27:13.560 --> 00:27:14.960]   Would you get the spin?
[00:27:14.960 --> 00:27:15.960]   Yeah.
[00:27:15.960 --> 00:27:17.800]   The spin is one.
[00:27:17.800 --> 00:27:20.080]   Dell makes one, but they only have an i3.
[00:27:20.080 --> 00:27:21.960]   Lenovo is a little too big for my iPhone.
[00:27:21.960 --> 00:27:22.960]   That's pretty high specs.
[00:27:22.960 --> 00:27:25.040]   8 gigs of RAM and an i5U process.
[00:27:25.040 --> 00:27:26.440]   They should have a really high specs.
[00:27:26.440 --> 00:27:30.040]   It's surprising how many don't go above four gigs.
[00:27:30.040 --> 00:27:31.040]   Yeah.
[00:27:31.040 --> 00:27:32.840]   And 8 gigs is nice to have.
[00:27:32.840 --> 00:27:35.280]   Even 8 gigs can get sluggish at times.
[00:27:35.280 --> 00:27:36.280]   Yeah.
[00:27:36.280 --> 00:27:40.400]   You can spec up quite a few of them at 8 gigs, but I mean, you know, it's not going to be
[00:27:40.400 --> 00:27:43.400]   a, it's not going to have a sell around in it, you know, or a penny.
[00:27:43.400 --> 00:27:46.160]   And those were capped out, you know, topped out at four.
[00:27:46.160 --> 00:27:53.120]   The m3s occasionally come with 8 gig, but mostly it's a core i5 or core i3, core i7.
[00:27:53.120 --> 00:27:56.560]   Those are the 8 gig or more capable.
[00:27:56.560 --> 00:27:58.480]   Yeah, I don't know.
[00:27:58.480 --> 00:27:59.480]   I actually had that.
[00:27:59.480 --> 00:28:00.880]   Is it just speeders?
[00:28:00.880 --> 00:28:06.160]   Is there an issue with maybe emulation or hypervisor kind of?
[00:28:06.160 --> 00:28:07.880]   I mean, what are they?
[00:28:07.880 --> 00:28:12.080]   Are they still doing just a chorout with with crustini?
[00:28:12.080 --> 00:28:13.760]   So it's it's running on them.
[00:28:13.760 --> 00:28:14.960]   It's running bare to the metal, right?
[00:28:14.960 --> 00:28:15.960]   It's not an emulator.
[00:28:15.960 --> 00:28:22.920]   No, no, it's it's actually a VM and so a container inside a VM.
[00:28:22.920 --> 00:28:26.240]   And for that reason, this is another downer for real Android developers.
[00:28:26.240 --> 00:28:28.240]   I am not one, of course.
[00:28:28.240 --> 00:28:31.720]   There's no Android virtual devices, no device emulation.
[00:28:31.720 --> 00:28:35.600]   Well, that's a, that is a light drive back because you'd like to be able to try it in
[00:28:35.600 --> 00:28:36.600]   an emulator before you.
[00:28:36.600 --> 00:28:37.600]   It is.
[00:28:37.600 --> 00:28:43.920]   I mean, you can certainly connect your Android device, USB to the Chromebook and push your
[00:28:43.920 --> 00:28:49.520]   app or run your app on that phone or device, but unless you've got every device, you know,
[00:28:49.520 --> 00:28:52.520]   that's out there, you certainly want a virtual emulated device.
[00:28:52.520 --> 00:28:56.160]   By the way, I, I, I, this is why I love geek so much.
[00:28:56.160 --> 00:28:59.320]   Kevin, I think I'm going to get personal here.
[00:28:59.320 --> 00:29:03.160]   I think, oh, you know, his dog has the flu, right?
[00:29:03.160 --> 00:29:07.120]   No, no, no, no, it's no, it's no, it's, I've got a wide processor.
[00:29:07.120 --> 00:29:10.240]   Oh, man, you got a wide process.
[00:29:10.240 --> 00:29:11.240]   The wide process.
[00:29:11.240 --> 00:29:12.240]   Sorry.
[00:29:12.240 --> 00:29:13.240]   Sorry.
[00:29:13.240 --> 00:29:14.240]   Contolences.
[00:29:14.240 --> 00:29:16.680]   Oh, are there, are there, are there hallmark cards for that?
[00:29:16.680 --> 00:29:18.640]   No, but you know what?
[00:29:18.640 --> 00:29:20.640]   That might be a good startup.
[00:29:20.640 --> 00:29:21.640]   Yeah.
[00:29:21.640 --> 00:29:26.440]   So I'm going to guess it has something to do with a virtual machine and support for the
[00:29:26.440 --> 00:29:27.440]   virtual machine.
[00:29:27.440 --> 00:29:30.880]   The wide process of this, the you process is missing in the why, but I don't know, but
[00:29:30.880 --> 00:29:33.080]   I wonder might, might be something that.
[00:29:33.080 --> 00:29:35.080]   I don't think so.
[00:29:35.080 --> 00:29:36.920]   Do you think it's just speed?
[00:29:36.920 --> 00:29:42.120]   I think it's just a power essentially because, you know, those, those wide processors can
[00:29:42.120 --> 00:29:45.320]   handle up to about five to seven and a half watts of heat.
[00:29:45.320 --> 00:29:47.200]   And then the other ones are 15.
[00:29:47.200 --> 00:29:49.360]   So they can run longer under load.
[00:29:49.360 --> 00:29:51.640]   And when you're developing, you need horsepower.
[00:29:51.640 --> 00:29:52.640]   So I get it.
[00:29:52.640 --> 00:29:53.640]   Yeah.
[00:29:53.640 --> 00:29:54.640]   Just go get a computer.
[00:29:54.640 --> 00:29:57.880]   Oh, I have a review of the spin.
[00:29:57.880 --> 00:29:58.880]   Okay.
[00:29:58.880 --> 00:30:03.480]   Now get a Chromebooks computer and put Linux on it.
[00:30:03.480 --> 00:30:07.360]   Don't, you know, I what's a computer?
[00:30:07.360 --> 00:30:13.760]   I see this all the time, especially with iPad users where they're just struggling and Chromebook
[00:30:13.760 --> 00:30:14.760]   users a little too.
[00:30:14.760 --> 00:30:15.760]   Hey, hey.
[00:30:15.760 --> 00:30:19.440]   I'm struggling and I just get a, get a freaking computer.
[00:30:19.440 --> 00:30:21.640]   Leo's using his PDPH.
[00:30:21.640 --> 00:30:23.200]   I have an 11 in the studio.
[00:30:23.200 --> 00:30:25.520]   Excuse me, please.
[00:30:25.520 --> 00:30:28.240]   And put Linux on it and, and all will be welling.
[00:30:28.240 --> 00:30:31.720]   It probably won't cost you as much and you'll get more horsepower and all of that stuff.
[00:30:31.720 --> 00:30:34.560]   I mean, I think you could be a little dogmatic in this.
[00:30:34.560 --> 00:30:36.120]   Yeah, we can.
[00:30:36.120 --> 00:30:39.120]   And if you're a developer, you probably have to have a Chromebook because you want to see
[00:30:39.120 --> 00:30:42.600]   if your Android app will run well on a Chromebook.
[00:30:42.600 --> 00:30:44.040]   It'll look good on that screen and all that stuff.
[00:30:44.040 --> 00:30:46.760]   I wish more companies would test against Chrome OS.
[00:30:46.760 --> 00:30:47.760]   Yeah.
[00:30:47.760 --> 00:30:50.760]   It's obviously, you actually, you don't need a Chromebook if you're one of those developers
[00:30:50.760 --> 00:30:54.000]   because there is a Chrome OS emulator in Android.
[00:30:54.000 --> 00:30:56.200]   See, so yeah.
[00:30:56.200 --> 00:30:59.480]   And that'll run on Linux and Windows and Mac OS, right?
[00:30:59.480 --> 00:31:00.480]   Correct.
[00:31:00.480 --> 00:31:03.120]   So as I said, I don't use it enough.
[00:31:03.120 --> 00:31:07.400]   Get a computer.
[00:31:07.400 --> 00:31:11.000]   They did rebrand all of the Google home stuff.
[00:31:11.000 --> 00:31:14.160]   It's all now Nest under Rick Osterlo.
[00:31:14.160 --> 00:31:15.160]   Right.
[00:31:15.160 --> 00:31:16.160]   Did you ask Rick about that?
[00:31:16.160 --> 00:31:17.160]   Is that good for him, right?
[00:31:17.160 --> 00:31:18.160]   Oh, yeah.
[00:31:18.160 --> 00:31:21.240]   No, he said people that they're not going to throw out boxes that are out there now.
[00:31:21.240 --> 00:31:22.880]   It's going to be a gradual rebrand.
[00:31:22.880 --> 00:31:25.000]   But as they do do things, they're not going to nest.
[00:31:25.000 --> 00:31:28.520]   I think it's this whole notion of helpful.
[00:31:28.520 --> 00:31:31.320]   Google is your helpful assistant now.
[00:31:31.320 --> 00:31:32.840]   It's your helpful assistant.
[00:31:32.840 --> 00:31:35.680]   And the helpful, your little plastic pal who's there to say.
[00:31:35.680 --> 00:31:36.680]   Yeah.
[00:31:36.680 --> 00:31:37.680]   Yeah.
[00:31:37.680 --> 00:31:38.680]   It's yours.
[00:31:38.680 --> 00:31:40.680]   You know, if you go back, how many years ago did we all talk about how you're all going
[00:31:40.680 --> 00:31:41.680]   to have your own agents?
[00:31:41.680 --> 00:31:42.680]   Yeah.
[00:31:42.680 --> 00:31:43.680]   Now you do.
[00:31:43.680 --> 00:31:44.680]   Yeah.
[00:31:44.680 --> 00:31:45.680]   Now you have these things that know.
[00:31:45.680 --> 00:31:49.040]   I have to say, you know, we have, I have a ring video doorbell.
[00:31:49.040 --> 00:31:52.080]   I have outdoor camps from Nest.
[00:31:52.080 --> 00:31:58.440]   I have a variety of devices in the house and I'm now, and maybe it's psychological because
[00:31:58.440 --> 00:32:02.280]   the Nest brand or because they all work together and I'm thinking of replacing it with the Nest
[00:32:02.280 --> 00:32:03.280]   doorbell.
[00:32:03.280 --> 00:32:04.280]   Oh, right.
[00:32:04.280 --> 00:32:06.880]   And because I know I want to get the home.
[00:32:06.880 --> 00:32:07.880]   What do you call it?
[00:32:07.880 --> 00:32:08.880]   Nest hub packs.
[00:32:08.880 --> 00:32:09.880]   Max hub.
[00:32:09.880 --> 00:32:11.880]   The next Max hub.
[00:32:11.880 --> 00:32:16.200]   Whatever they call that because that sounds great.
[00:32:16.200 --> 00:32:20.440]   But, and by the way, this is sneaky as heck because remember the original Google Home
[00:32:20.440 --> 00:32:22.200]   Hub didn't have a camera.
[00:32:22.200 --> 00:32:23.200]   Right.
[00:32:23.200 --> 00:32:24.520]   This one has a camera.
[00:32:24.520 --> 00:32:26.800]   So they said right at the beginning, for your living room.
[00:32:26.800 --> 00:32:27.800]   Yeah.
[00:32:27.800 --> 00:32:31.040]   So I was standing next to Rick Lee, just talking to me about this and, and, and, so he said,
[00:32:31.040 --> 00:32:34.000]   no, this is, this is probably for the kitchen or maybe the living room.
[00:32:34.000 --> 00:32:35.480]   And I said, yeah, not the bedroom for this one.
[00:32:35.480 --> 00:32:39.720]   He said, well, he said, I would, but that's me.
[00:32:39.720 --> 00:32:40.720]   Yeah.
[00:32:40.720 --> 00:32:43.960]   I, I'm, I'm eager for the, the, the, the, the, the, what it does though is it gives you
[00:32:43.960 --> 00:32:49.800]   all the benefits of the nest intern in cameras, which is it can, it'll notify you if it doesn't
[00:32:49.800 --> 00:32:51.520]   wreck it recognizes people.
[00:32:51.520 --> 00:32:54.720]   So in effect, you're getting a nest cam plus exactly.
[00:32:54.720 --> 00:32:55.720]   Exactly.
[00:32:55.720 --> 00:32:56.720]   Plus a good speaker.
[00:32:56.720 --> 00:33:00.080]   I think it looks for two 99 like a very, a 250 rather.
[00:33:00.080 --> 00:33:01.880]   Two, two, two, three, nine.
[00:33:01.880 --> 00:33:02.880]   I think yeah.
[00:33:02.880 --> 00:33:08.160]   And, and the two biggest, I think, ooh hands at the, uh, uh, he don't yesterday, at least
[00:33:08.160 --> 00:33:12.760]   when I was in the audience were being able to stop the music with your hand.
[00:33:12.760 --> 00:33:13.760]   That, yeah.
[00:33:13.760 --> 00:33:15.200]   But you have to talk to the hand.
[00:33:15.200 --> 00:33:17.000]   Now it was a little unclear in the keynote.
[00:33:17.000 --> 00:33:18.560]   Do you have to train it to do that?
[00:33:18.560 --> 00:33:19.760]   No, no, no, no, no, I did it.
[00:33:19.760 --> 00:33:20.760]   I did it.
[00:33:20.760 --> 00:33:21.760]   Right.
[00:33:21.760 --> 00:33:26.160]   He just, he, and, um, uh, it has to recognize you're looking at it.
[00:33:26.160 --> 00:33:29.960]   Oh, because it doesn't want to just take any old hand and suddenly stop.
[00:33:29.960 --> 00:33:32.600]   So recognizes that someone does it be you.
[00:33:32.600 --> 00:33:34.320]   Someone is looking at it and does that.
[00:33:34.320 --> 00:33:36.800]   The other advantage with the cameras you can have duo integration.
[00:33:36.800 --> 00:33:38.560]   So now you can make this like a Facebook portal.
[00:33:38.560 --> 00:33:39.560]   It's like a Facebook portal.
[00:33:39.560 --> 00:33:40.560]   Exactly.
[00:33:40.560 --> 00:33:46.200]   And the other, the other, from the audience was alarm going off stop.
[00:33:46.200 --> 00:33:49.680]   We should, that was everybody loved that.
[00:33:49.680 --> 00:33:50.680]   Right.
[00:33:50.680 --> 00:33:51.680]   That works now, which is wonderful.
[00:33:51.680 --> 00:33:52.680]   It does.
[00:33:52.680 --> 00:33:54.560]   Oh, you tried it this morning.
[00:33:54.560 --> 00:33:55.560]   Absolutely.
[00:33:55.560 --> 00:34:02.240]   That is funny how of all the things I announced that probably was the number one most tweeted
[00:34:02.240 --> 00:34:03.240]   and kind of, yeah.
[00:34:03.240 --> 00:34:04.240]   Yeah.
[00:34:04.240 --> 00:34:09.040]   Just, so when your alarm's going off instead of having to say, Hey, Google, cancel or stop,
[00:34:09.040 --> 00:34:10.040]   you can just stop.
[00:34:10.040 --> 00:34:13.560]   I'm waiting for the stories will come out that having to say, Hey, Google, made you wake
[00:34:13.560 --> 00:34:14.760]   up enough, you'd actually get up.
[00:34:14.760 --> 00:34:17.280]   Now you can just stop and fall back and sleep.
[00:34:17.280 --> 00:34:18.280]   Stop or worse.
[00:34:18.280 --> 00:34:20.200]   Wave your hand.
[00:34:20.200 --> 00:34:24.480]   Uh, so this video messages they, they touched on that.
[00:34:24.480 --> 00:34:25.640]   Is that a feature in duo?
[00:34:25.640 --> 00:34:27.080]   I don't remember the video.
[00:34:27.080 --> 00:34:29.240]   Can you leave a video message and duo?
[00:34:29.240 --> 00:34:30.240]   Thanks.
[00:34:30.240 --> 00:34:31.240]   You can't.
[00:34:31.240 --> 00:34:32.240]   Yeah.
[00:34:32.240 --> 00:34:36.320]   I think much more useful though, to have this on the living room screen where you come in
[00:34:36.320 --> 00:34:40.400]   and you see there's messages for you and you could buy voice commands.
[00:34:40.400 --> 00:34:42.040]   They play them back and it's on a screen.
[00:34:42.040 --> 00:34:43.040]   It's right there.
[00:34:43.040 --> 00:34:44.040]   You'd have to pull out your phone.
[00:34:44.040 --> 00:34:45.040]   I think that's kind of interesting.
[00:34:45.040 --> 00:34:49.800]   I don't know why this feels like a, a, a little bit of a leap, but it does.
[00:34:49.800 --> 00:34:53.040]   It doesn't like seem like something new video message from Marcer.
[00:34:53.040 --> 00:34:54.040]   Yeah.
[00:34:54.040 --> 00:34:57.040]   Um, yeah, we have the smaller version of this.
[00:34:57.040 --> 00:35:00.520]   Um, you have a Lenovo or the, well, no, no, no, no.
[00:35:00.520 --> 00:35:04.120]   I have two of the, what they used to call the Google home hub, the seven inch, which is
[00:35:04.120 --> 00:35:07.120]   now the, I love mine, but it doesn't have a camera.
[00:35:07.120 --> 00:35:08.120]   Correct.
[00:35:08.120 --> 00:35:11.520]   Which is why we have one in the bedroom and we have one right here.
[00:35:11.520 --> 00:35:12.520]   It's my light clock.
[00:35:12.520 --> 00:35:13.520]   It's right next to my bed.
[00:35:13.520 --> 00:35:14.520]   Right.
[00:35:14.520 --> 00:35:15.520]   And it's great.
[00:35:15.520 --> 00:35:16.520]   It gets dark when you turn out the lights.
[00:35:16.520 --> 00:35:17.520]   Yep.
[00:35:17.520 --> 00:35:19.600]   I love the slideshow because it's all my Google photos.
[00:35:19.600 --> 00:35:20.600]   So it's stuff.
[00:35:20.600 --> 00:35:21.600]   I, oh, I forgot.
[00:35:21.600 --> 00:35:23.360]   We, well, they're all, I haven't seen that in years.
[00:35:23.360 --> 00:35:25.600]   And it's the right size for, for a night table.
[00:35:25.600 --> 00:35:26.600]   Just the right size.
[00:35:26.600 --> 00:35:27.600]   The bigger one is kind of ridiculous.
[00:35:27.600 --> 00:35:28.600]   Yeah.
[00:35:28.600 --> 00:35:30.480]   The bigger one we have, which is all a no vote.
[00:35:30.480 --> 00:35:31.480]   Ten inch smart display.
[00:35:31.480 --> 00:35:34.320]   That is in our kitchen, right on the kitchen island.
[00:35:34.320 --> 00:35:39.560]   So every morning we have coffee, my wife and I together and watch the local news on YouTube
[00:35:39.560 --> 00:35:41.560]   TV right on the display.
[00:35:41.560 --> 00:35:42.560]   On your, on your Lenovo.
[00:35:42.560 --> 00:35:43.560]   Oh, yeah.
[00:35:43.560 --> 00:35:44.800]   That's been there since day one.
[00:35:44.800 --> 00:35:45.800]   Wow.
[00:35:45.800 --> 00:35:47.840]   And that's big enough for you to watch TV with it.
[00:35:47.840 --> 00:35:51.120]   Did you also have a TV set as we used to call them in your kitchen?
[00:35:51.120 --> 00:35:52.120]   TV set.
[00:35:52.120 --> 00:35:53.120]   No, no, no.
[00:35:53.120 --> 00:35:54.120]   I look forward to this.
[00:35:54.120 --> 00:35:55.120]   This is how we talk.
[00:35:55.120 --> 00:35:56.120]   Former TV set critic.
[00:35:56.120 --> 00:35:59.480]   Does it have a, it has come with little rabbit ears?
[00:35:59.480 --> 00:36:01.480]   Is that part of the set?
[00:36:01.480 --> 00:36:03.080]   And matching hairbrush?
[00:36:03.080 --> 00:36:10.320]   Yeah, but what they added, which I love, well, I haven't seen it live yet, is a YouTube
[00:36:10.320 --> 00:36:12.360]   TV guide on this.
[00:36:12.360 --> 00:36:16.120]   So it actually really is more like a little TV set now.
[00:36:16.120 --> 00:36:21.400]   We just say, Hey, I won't say the G word, but what we call it.
[00:36:21.400 --> 00:36:22.400]   Be a moh here.
[00:36:22.400 --> 00:36:23.400]   It's an interesting.
[00:36:23.400 --> 00:36:24.400]   Again, yeah.
[00:36:24.400 --> 00:36:25.400]   Yeah.
[00:36:25.400 --> 00:36:29.440]   We just tell it to watch NBC in the morning and watch our local news.
[00:36:29.440 --> 00:36:31.960]   And it just fires up YouTube TV to the right channel.
[00:36:31.960 --> 00:36:35.880]   But if you don't know what's on, you don't know what channel the tune to.
[00:36:35.880 --> 00:36:38.280]   So in the middle of the day, it's like, what do I want?
[00:36:38.280 --> 00:36:39.880]   I'll presume your agent's going to recommend things.
[00:36:39.880 --> 00:36:42.640]   The assistant's going to recommend things to you as well based on your.
[00:36:42.640 --> 00:36:46.560]   Oh, it already does in YouTube TV has recommendations based on that.
[00:36:46.560 --> 00:36:47.560]   I don't know about the assistant.
[00:36:47.560 --> 00:36:49.000]   There's more terrorist video for you.
[00:36:49.000 --> 00:36:50.000]   Yeah.
[00:36:50.000 --> 00:36:52.480]   You're not radicalized enough.
[00:36:52.480 --> 00:36:58.040]   Perhaps you perhaps you'd like to watch more.
[00:36:58.040 --> 00:37:02.000]   One of the things that happens at these keynotes, this reality distortion field is Google shows
[00:37:02.000 --> 00:37:04.400]   a lot of stuff that isn't yet a product.
[00:37:04.400 --> 00:37:08.240]   And they don't, I don't think they did a very good job yesterday of distinguishing.
[00:37:08.240 --> 00:37:09.240]   Last year was worse, I think.
[00:37:09.240 --> 00:37:10.240]   Yeah.
[00:37:10.240 --> 00:37:12.640]   I don't think they've showed that many new things yesterday in a way.
[00:37:12.640 --> 00:37:13.640]   Okay.
[00:37:13.640 --> 00:37:14.640]   What was, what was not.
[00:37:14.640 --> 00:37:16.840]   Do you feel like that everything there was something you'll be able to do?
[00:37:16.840 --> 00:37:22.200]   I mean, obviously you can't get the nest home max until summer, but.
[00:37:22.200 --> 00:37:23.200]   That's that's hard.
[00:37:23.200 --> 00:37:24.200]   That's easy.
[00:37:24.200 --> 00:37:25.200]   That's hard.
[00:37:25.200 --> 00:37:26.200]   I mean, did it was there?
[00:37:26.200 --> 00:37:27.200]   Here's the question.
[00:37:27.200 --> 00:37:31.160]   Is the, what I don't know is the 100 gig to half a gig.
[00:37:31.160 --> 00:37:32.160]   ML.
[00:37:32.160 --> 00:37:33.160]   Is that real?
[00:37:33.160 --> 00:37:34.160]   Now.
[00:37:34.160 --> 00:37:35.160]   No.
[00:37:35.160 --> 00:37:38.320]   Is that, is that multitasking ability?
[00:37:38.320 --> 00:37:43.160]   Well, they showed you, they showed you could multitask to complex tasks and.
[00:37:43.160 --> 00:37:47.400]   Oh, that thing, the contextual, they've been slowly building up towards that with assistant
[00:37:47.400 --> 00:37:48.400]   on.
[00:37:48.400 --> 00:37:49.400]   Right.
[00:37:49.400 --> 00:37:50.400]   Right.
[00:37:50.400 --> 00:37:51.400]   So that's not on your phone yet.
[00:37:51.400 --> 00:37:52.560]   I don't think.
[00:37:52.560 --> 00:38:00.280]   I don't think that is yet, but I'm just checking their blog post because they talk about shrinking
[00:38:00.280 --> 00:38:01.880]   the model down.
[00:38:01.880 --> 00:38:02.880]   It's coming.
[00:38:02.880 --> 00:38:04.560]   This is all part of the next generation assistant.
[00:38:04.560 --> 00:38:05.560]   So this is.
[00:38:05.560 --> 00:38:06.720]   This fall in the new phones, basically.
[00:38:06.720 --> 00:38:09.000]   In the new Pixel phones this year.
[00:38:09.000 --> 00:38:15.000]   So roughly October, this will presumably be a live product for Pixel phones.
[00:38:15.000 --> 00:38:16.720]   They've said that before and not delivered.
[00:38:16.720 --> 00:38:17.720]   Yes.
[00:38:17.720 --> 00:38:18.720]   I want to point out.
[00:38:18.720 --> 00:38:19.720]   So, okay.
[00:38:19.720 --> 00:38:22.960]   Well, and then was it two years ago we got Aloe.
[00:38:22.960 --> 00:38:23.960]   Right.
[00:38:23.960 --> 00:38:27.680]   And we got those things that nobody give.
[00:38:27.680 --> 00:38:32.520]   He started off with what looked like inbox because he started talking about smart replies.
[00:38:32.520 --> 00:38:35.480]   And I thought two sins and I too soon.
[00:38:35.480 --> 00:38:37.960]   So I've, yeah, exactly.
[00:38:37.960 --> 00:38:40.680]   I've recognized myself with smart replies.
[00:38:40.680 --> 00:38:42.760]   I guess I'm too much of an editor.
[00:38:42.760 --> 00:38:44.440]   Would it suggest something to me?
[00:38:44.440 --> 00:38:45.680]   I'll do a damn, I'm not using that.
[00:38:45.680 --> 00:38:46.680]   I'm going to pick up something else.
[00:38:46.680 --> 00:38:47.680]   You blew pencil.
[00:38:47.680 --> 00:38:48.680]   Well, okay.
[00:38:48.680 --> 00:38:49.680]   Let's do this here.
[00:38:49.680 --> 00:38:53.320]   And that would be a nice feature, by the way, if you could blue pencil.
[00:38:53.320 --> 00:39:00.360]   Yeah, there is a keyword blog, Aparna Chena-Purgato, who gave the speech on this about search
[00:39:00.360 --> 00:39:07.000]   in the demo, has a long piece that basically is your entire talk.
[00:39:07.000 --> 00:39:08.240]   So that's nice that they did that.
[00:39:08.240 --> 00:39:10.520]   So you can read all of these features.
[00:39:10.520 --> 00:39:13.320]   Here's the menu translation.
[00:39:13.320 --> 00:39:16.400]   So that's going to be this fall.
[00:39:16.400 --> 00:39:17.400]   I think so.
[00:39:17.400 --> 00:39:23.960]   And it's so, so many years ago when I wrote what would Google do, which led me to being
[00:39:23.960 --> 00:39:24.960]   here.
[00:39:24.960 --> 00:39:28.840]   You called, I argued that there was going to be a smart menu that was going to use the
[00:39:28.840 --> 00:39:32.600]   data of the restaurant to tell you what the most popular dishes were and what people had
[00:39:32.600 --> 00:39:34.000]   with what and so on and so forth.
[00:39:34.000 --> 00:39:35.680]   And finally, it's real.
[00:39:35.680 --> 00:39:36.840]   The Googly menu is here.
[00:39:36.840 --> 00:39:37.840]   They missed a bet though.
[00:39:37.840 --> 00:39:40.360]   It would be nice to have an augmented reality version of the meal.
[00:39:40.360 --> 00:39:41.520]   Listen, they said it would.
[00:39:41.520 --> 00:39:42.520]   They will.
[00:39:42.520 --> 00:39:46.000]   Yeah, they'll, they'll, they're going to pop up pictures, but I want to see it on the
[00:39:46.000 --> 00:39:47.000]   table.
[00:39:47.000 --> 00:39:52.080]   There is, I think it's on a cruise ship.
[00:39:52.080 --> 00:39:54.920]   There's a restaurant that projects the meal on your table.
[00:39:54.920 --> 00:39:55.920]   Oh, Jesus.
[00:39:55.920 --> 00:39:58.400]   And then they waiters come and put it there.
[00:39:58.400 --> 00:40:01.720]   Yeah, I think it's on a Royal Caribbean cruise ship.
[00:40:01.720 --> 00:40:03.360]   It's getting a little old now.
[00:40:03.360 --> 00:40:05.000]   No, it's a bad idea.
[00:40:05.000 --> 00:40:06.400]   I liked the translate.
[00:40:06.400 --> 00:40:08.200]   I thought what they showed, but we've seen this.
[00:40:08.200 --> 00:40:09.600]   This is not new, the ability to translate.
[00:40:09.600 --> 00:40:10.800]   Don't have used it.
[00:40:10.800 --> 00:40:11.800]   Yeah.
[00:40:11.800 --> 00:40:13.960]   It gets better and better and better.
[00:40:13.960 --> 00:40:17.520]   So they, so even though they announced some new features and lens, if it's coming in the
[00:40:17.520 --> 00:40:20.200]   next generation phone, that's fine.
[00:40:20.200 --> 00:40:25.440]   Maybe it does require the new hardware in the Pixel 4 or whatever they call it to do.
[00:40:25.440 --> 00:40:28.720]   Exactly what you were talking about, but those smaller models and all of that.
[00:40:28.720 --> 00:40:33.000]   The thing that I think was most BS yesterday, because I've lived through this, is the AR
[00:40:33.000 --> 00:40:34.480]   magazine page.
[00:40:34.480 --> 00:40:37.240]   Just point your phone at this magazine page and the magazine page won't come online.
[00:40:37.240 --> 00:40:38.240]   Come on.
[00:40:38.240 --> 00:40:40.440]   You'll be happy you have magazines again.
[00:40:40.440 --> 00:40:42.320]   It's the Q-cat all over again.
[00:40:42.320 --> 00:40:44.320]   All you need is a special piece of hardware.
[00:40:44.320 --> 00:40:49.580]   You can scan the magazine QR code and a giant tiger will appear.
[00:40:49.580 --> 00:40:50.580]   A shark.
[00:40:50.580 --> 00:40:55.080]   I actually mentioned the magazine and AR with lens in the blog post.
[00:40:55.080 --> 00:40:57.400]   If you see a dish, you'd like to cook.
[00:40:57.400 --> 00:41:01.080]   In an upcoming issue of Bon Appetit, you'll be able to point your camera at a recipe and
[00:41:01.080 --> 00:41:05.120]   have the page come to life and show you exactly how to make it.
[00:41:05.120 --> 00:41:06.120]   It's not AR though.
[00:41:06.120 --> 00:41:10.120]   This is, they just have a video basically of the preparation.
[00:41:10.120 --> 00:41:13.480]   Well, no, it's your, well, that's true.
[00:41:13.480 --> 00:41:14.480]   It's not.
[00:41:14.480 --> 00:41:16.480]   No, I think you're pointing the table.
[00:41:16.480 --> 00:41:19.160]   I think you're pointing the table.
[00:41:19.160 --> 00:41:20.160]   Not that it matters.
[00:41:20.160 --> 00:41:21.160]   No, it doesn't.
[00:41:21.160 --> 00:41:24.120]   I mean, we're talking gimmicks here.
[00:41:24.120 --> 00:41:29.680]   The thing I think is interesting is when they showed the shark and having the shark
[00:41:29.680 --> 00:41:34.520]   in, you know, on the stage and walking around, it made me think of this is what they wanted
[00:41:34.520 --> 00:41:36.040]   to do with class.
[00:41:36.040 --> 00:41:39.160]   This is what a heads-up display eventually would be.
[00:41:39.160 --> 00:41:46.000]   And give credit to Zuckerberg, he promoted the notion of the value of your camera on
[00:41:46.000 --> 00:41:48.680]   your phone for AR long before Google did.
[00:41:48.680 --> 00:41:49.680]   Right.
[00:41:49.680 --> 00:41:50.680]   So we have the hardware.
[00:41:50.680 --> 00:41:51.680]   I mean, Apple's doing this.
[00:41:51.680 --> 00:41:52.680]   Google's doing this.
[00:41:52.680 --> 00:41:58.520]   You can have, you know, Iron Man or what's his name?
[00:41:58.520 --> 00:42:00.000]   Danny Glover Jr.
[00:42:00.000 --> 00:42:05.240]   Childish Gamboop, you know, now in your, in the room with you or a shark.
[00:42:05.240 --> 00:42:06.240]   So it does.
[00:42:06.240 --> 00:42:08.840]   I was hoping for some VC jokes.
[00:42:08.840 --> 00:42:11.000]   There's a VC on the stage.
[00:42:11.000 --> 00:42:12.480]   Look at those teeth.
[00:42:12.480 --> 00:42:14.520]   Reminds me of our investors.
[00:42:14.520 --> 00:42:15.520]   D-d-d-d.
[00:42:15.520 --> 00:42:16.520]   All right.
[00:42:16.520 --> 00:42:19.560]   Let's take a little break.
[00:42:19.560 --> 00:42:24.680]   Kevin Tofel's here from about Chromebooks.com and Stacey's IOT podcast.
[00:42:24.680 --> 00:42:26.720]   Well, it's his and Stacey's.
[00:42:26.720 --> 00:42:29.880]   Always a pleasure to have you is Norm doesn't, I just want to be clear.
[00:42:29.880 --> 00:42:31.280]   Does not have the flu.
[00:42:31.280 --> 00:42:32.280]   No.
[00:42:32.280 --> 00:42:34.040]   You have a wide processor, but Norm.
[00:42:34.040 --> 00:42:36.160]   I was trying to think of something that wouldn't happen.
[00:42:36.160 --> 00:42:37.160]   Dogs don't get the flu.
[00:42:37.160 --> 00:42:39.800]   I couldn't, I didn't want to have the animal running away or something.
[00:42:39.800 --> 00:42:46.560]   So I actually spent four minutes thinking of what undoable catastrophe could befall
[00:42:46.560 --> 00:42:47.560]   him.
[00:42:47.560 --> 00:42:51.080]   And Norm has a normal person to having to buy processor.
[00:42:51.080 --> 00:42:52.080]   Wow.
[00:42:52.080 --> 00:42:53.080]   Norm has a U processor.
[00:42:53.080 --> 00:42:54.080]   I have a Y processor.
[00:42:54.080 --> 00:42:55.080]   There you go.
[00:42:55.080 --> 00:42:58.080]   Norm is running on full.
[00:42:58.080 --> 00:43:02.440]   And also of course from the, I like it, the New Art School of Journalism.
[00:43:02.440 --> 00:43:04.640]   I'm going to call it that from now on.
[00:43:04.640 --> 00:43:05.640]   I know I am.
[00:43:05.640 --> 00:43:06.640]   From the Newmark.
[00:43:06.640 --> 00:43:10.920]   From the Graduate School of Journalism.
[00:43:10.920 --> 00:43:14.800]   It is Mr. Jeff Jarvis in studio, which we really are thrilled to have you here.
[00:43:14.800 --> 00:43:17.360]   Thank you for making the trip up.
[00:43:17.360 --> 00:43:19.960]   Our show today brought to you by Casper.
[00:43:19.960 --> 00:43:24.880]   If you want to take a little nap on our Casper after the show, be my guest.
[00:43:24.880 --> 00:43:27.440]   We've got a, we've got the, I can't, I'm not supposed to talk about it yet, but a new
[00:43:27.440 --> 00:43:31.360]   Casper in the box ready to open.
[00:43:31.360 --> 00:43:33.640]   For years we've had the original Casper mattress.
[00:43:33.640 --> 00:43:35.520]   Not many, many years, a couple of years now.
[00:43:35.520 --> 00:43:36.520]   We love it.
[00:43:36.520 --> 00:43:41.680]   It combines multiple supportive memory foams for a sleep surface that gives you something
[00:43:41.680 --> 00:43:45.760]   kind of almost paradoxical, just the right sink and just the right bounce.
[00:43:45.760 --> 00:43:47.760]   It's both firm and soft.
[00:43:47.760 --> 00:43:51.440]   So your hips don't, you know, dig into a firm mattress, but your back is fully supported
[00:43:51.440 --> 00:43:53.440]   and it's breathable.
[00:43:53.440 --> 00:43:55.680]   So you never get hot.
[00:43:55.680 --> 00:43:58.880]   You regulate your temperature throughout the night and we know that that helps sleep.
[00:43:58.880 --> 00:44:03.600]   Casper is an online retailer of premium mattresses.
[00:44:03.600 --> 00:44:08.120]   The brilliant insight that the guys at Casper came up with when they founded the company
[00:44:08.120 --> 00:44:13.480]   was there is a huge markup when you buy a mattress at a mattress store as much as double
[00:44:13.480 --> 00:44:16.360]   the price of the actual price of that mattress.
[00:44:16.360 --> 00:44:18.200]   Sometimes even more.
[00:44:18.200 --> 00:44:21.040]   They said, well, what if we eliminated the mattress store?
[00:44:21.040 --> 00:44:27.160]   We sent it directly to the customer, cutting the costs of showrooms, resellers and pass
[00:44:27.160 --> 00:44:28.800]   that savings on to you.
[00:44:28.800 --> 00:44:29.800]   Brilliant, right?
[00:44:29.800 --> 00:44:31.840]   There was one problem.
[00:44:31.840 --> 00:44:35.080]   You said, well, I want to lie on it before I buy it.
[00:44:35.080 --> 00:44:39.920]   Well, as you probably know, I know this because I've bought mattresses that just did not work
[00:44:39.920 --> 00:44:40.920]   out.
[00:44:40.920 --> 00:44:44.960]   I lay them on, I lay on them in the mattress showroom, but 10 minutes in bright light during
[00:44:44.960 --> 00:44:47.800]   the day with a salesperson looking at you.
[00:44:47.800 --> 00:44:48.800]   That's no good.
[00:44:48.800 --> 00:44:50.520]   That's no, you can't even spoon with your wife.
[00:44:50.520 --> 00:44:51.760]   You can't do anything.
[00:44:51.760 --> 00:44:53.760]   You can't really test it.
[00:44:53.760 --> 00:44:57.960]   So Casper gives you the chance to test it, but in your house for a hundred nights, do
[00:44:57.960 --> 00:44:59.920]   whatever you want on it.
[00:44:59.920 --> 00:45:05.360]   Free shipping, but also if you don't like it, free returns and a 10 year limited warranty.
[00:45:05.360 --> 00:45:07.440]   These mattresses last a long time.
[00:45:07.440 --> 00:45:10.440]   And if for any reason in those first hundred nights you say, that's not the right mattress
[00:45:10.440 --> 00:45:11.440]   for me, they'll come.
[00:45:11.440 --> 00:45:13.840]   They'll take it, give you every penny back.
[00:45:13.840 --> 00:45:16.680]   So it's easy to buy online and it's completely risk free.
[00:45:16.680 --> 00:45:20.040]   And frankly, it's a better way to try a mattress.
[00:45:20.040 --> 00:45:25.400]   Get it in its environment and they come in a really compact box.
[00:45:25.400 --> 00:45:27.120]   It's actually kind of fun when you open it up.
[00:45:27.120 --> 00:45:33.080]   It includes everything you need to open up the box and goes, it expands.
[00:45:33.080 --> 00:45:34.240]   Smells great from day one.
[00:45:34.240 --> 00:45:39.080]   I've had mattresses that you've had to air out for days before you could sleep on them.
[00:45:39.080 --> 00:45:40.080]   Casper's awesome.
[00:45:40.080 --> 00:45:41.320]   Get a Casper mattress today.
[00:45:41.320 --> 00:45:42.520]   And we have a special deal.
[00:45:42.520 --> 00:45:43.760]   I want to emphasize this.
[00:45:43.760 --> 00:45:46.400]   This is new.
[00:45:46.400 --> 00:45:52.120]   Was $50 is now $100 off towards select mattresses at Casper.com/twig.
[00:45:52.120 --> 00:45:54.840]   But that is a limited time offer.
[00:45:54.840 --> 00:45:56.560]   They told me you can't keep talking about it.
[00:45:56.560 --> 00:45:57.560]   So this is it.
[00:45:57.560 --> 00:46:01.120]   $100 off, but you have to use the offer code twig at checkout.
[00:46:01.120 --> 00:46:02.440]   We really love Casper.
[00:46:02.440 --> 00:46:04.600]   They've been with us practically since the beginning.
[00:46:04.600 --> 00:46:05.600]   Casper.com/twig.
[00:46:05.600 --> 00:46:07.680]   Thank you, Casper, for your support.
[00:46:07.680 --> 00:46:10.880]   And thank you, our fair listeners.
[00:46:10.880 --> 00:46:14.480]   If you use the offer code twig, you'll save $100 and you'll let them know you heard it
[00:46:14.480 --> 00:46:17.640]   here, which we appreciate in terms and conditions apply.
[00:46:17.640 --> 00:46:22.360]   Casper, C-A-S-P-E-R.com/twig.
[00:46:22.360 --> 00:46:26.600]   Ryan and Mattress used to be the next worst thing from buying a car.
[00:46:26.600 --> 00:46:31.000]   Mattresses, these five mattresses were exactly the same, but there were always different
[00:46:31.000 --> 00:46:32.400]   brands and different names on them.
[00:46:32.400 --> 00:46:33.880]   You want to have a code in on that mattress?
[00:46:33.880 --> 00:46:35.040]   What about rest guide?
[00:46:35.040 --> 00:46:36.960]   You can't comparison shop mattresses.
[00:46:36.960 --> 00:46:38.440]   Now you can know what you're getting.
[00:46:38.440 --> 00:46:39.440]   That's really a good thing to do.
[00:46:39.440 --> 00:46:40.440]   See, the internet does good things.
[00:46:40.440 --> 00:46:41.920]   See what it makes possible.
[00:46:41.920 --> 00:46:42.920]   Yeah.
[00:46:42.920 --> 00:46:44.680]   How did I get to this story?
[00:46:44.680 --> 00:46:47.880]   I have no idea how we searched for bowwows in me house.
[00:46:47.880 --> 00:46:49.440]   How did you get here?
[00:46:49.440 --> 00:46:52.560]   What is this?
[00:46:52.560 --> 00:46:54.840]   This is a Google blog.
[00:46:54.840 --> 00:46:56.760]   Why do cats like boxes?
[00:46:56.760 --> 00:46:59.160]   Why do dogs lick, eat grass?
[00:46:59.160 --> 00:47:00.160]   Howl.
[00:47:00.160 --> 00:47:03.360]   Global pet preferences worldwide.
[00:47:03.360 --> 00:47:06.200]   Google, any information at all Google wants to know.
[00:47:06.200 --> 00:47:07.840]   I don't know how I got here.
[00:47:07.840 --> 00:47:08.840]   Why did you get to this?
[00:47:08.840 --> 00:47:09.840]   We'll use your back button.
[00:47:09.840 --> 00:47:12.240]   I typed keyword blog in this way.
[00:47:12.240 --> 00:47:15.200]   Let's go back.
[00:47:15.200 --> 00:47:16.600]   Is there anything else from I/O?
[00:47:16.600 --> 00:47:17.600]   No.
[00:47:17.600 --> 00:47:18.600]   I was not over.
[00:47:18.600 --> 00:47:20.160]   OK, they've updated Android Auto.
[00:47:20.160 --> 00:47:22.080]   Kevin, you want to talk about that?
[00:47:22.080 --> 00:47:23.080]   First--
[00:47:23.080 --> 00:47:24.080]   Which was that, Jeff?
[00:47:24.080 --> 00:47:25.080]   Android Auto changes.
[00:47:25.080 --> 00:47:26.080]   Ah.
[00:47:26.080 --> 00:47:31.200]   So Android Auto, in fact, this is something that's coming by the end of this month.
[00:47:31.200 --> 00:47:34.400]   The new-- I think it's called drive mode for Android.
[00:47:34.400 --> 00:47:38.920]   If you're using your phone in your car and you're connected to--
[00:47:38.920 --> 00:47:44.440]   Well, actually, if you're using your phone in your car, this is not specific to Android
[00:47:44.440 --> 00:47:45.440]   Auto.
[00:47:45.440 --> 00:47:47.120]   It's the auto app.
[00:47:47.120 --> 00:47:49.000]   It's the auto app on Android.
[00:47:49.000 --> 00:47:50.000]   Right.
[00:47:50.000 --> 00:47:51.000]   Yeah.
[00:47:51.000 --> 00:47:53.760]   Which they very confusingly call Android Auto.
[00:47:53.760 --> 00:47:54.760]   Right.
[00:47:54.760 --> 00:47:55.760]   Yes.
[00:47:55.760 --> 00:47:59.800]   So what they're doing is when you're using navigation and you get maybe a phone call
[00:47:59.800 --> 00:48:04.880]   that comes in or a song changes from one song to the next.
[00:48:04.880 --> 00:48:06.600]   So you want to see the title.
[00:48:06.600 --> 00:48:08.800]   That information is just down below at the bottom.
[00:48:08.800 --> 00:48:14.640]   It does not supersede the navigation maps that you can see all the time, which is wonderful
[00:48:14.640 --> 00:48:16.160]   because I like to see the maps.
[00:48:16.160 --> 00:48:20.360]   I don't want to switch between apps and not see the app or the map for a little while
[00:48:20.360 --> 00:48:21.400]   while I'm driving.
[00:48:21.400 --> 00:48:24.800]   Even though the directions still work, I like to see the map for some reason.
[00:48:24.800 --> 00:48:26.160]   I'm just weird.
[00:48:26.160 --> 00:48:29.800]   So there's that aspect and that's coming by the end of this month.
[00:48:29.800 --> 00:48:35.840]   And then Android Automotive, which is what they're basically building into some cars,
[00:48:35.840 --> 00:48:37.880]   I think, and I could be wrong.
[00:48:37.880 --> 00:48:38.880]   The whole star.
[00:48:38.880 --> 00:48:39.880]   Yeah.
[00:48:39.880 --> 00:48:40.880]   Whole star is a perfect example.
[00:48:40.880 --> 00:48:45.720]   And then there are other vehicle manufacturers that either are using it or plan to use it.
[00:48:45.720 --> 00:48:47.920]   That's kind of like the whole infotainment system.
[00:48:47.920 --> 00:48:50.280]   Everything just runs on Android Automotive.
[00:48:50.280 --> 00:48:55.960]   Now I did hear and I was curious about this, does that preclude you from using say CarPlay
[00:48:55.960 --> 00:49:00.400]   if you're an iOS user, maybe you got iOS and Android in the car.
[00:49:00.400 --> 00:49:01.560]   And apparently it does not.
[00:49:01.560 --> 00:49:03.040]   I'm not sure how that's going to work.
[00:49:03.040 --> 00:49:04.280]   Almost everybody gets both, right?
[00:49:04.280 --> 00:49:05.280]   There's very few cars that just--
[00:49:05.280 --> 00:49:11.400]   Yeah, but I don't know if Volvo, with Volvo, now that in the poll star for Volvo, Android
[00:49:11.400 --> 00:49:14.240]   is the OS of all of the UI and car.
[00:49:14.240 --> 00:49:15.240]   Oh, that's a different matter.
[00:49:15.240 --> 00:49:16.240]   That's not Android Auto.
[00:49:16.240 --> 00:49:17.240]   No.
[00:49:17.240 --> 00:49:18.240]   That's the OS.
[00:49:18.240 --> 00:49:19.240]   That's how you get your Android Auto.
[00:49:19.240 --> 00:49:20.240]   That's how you get your Android Auto.
[00:49:20.240 --> 00:49:21.240]   Okay.
[00:49:21.240 --> 00:49:22.240]   Right.
[00:49:22.240 --> 00:49:26.080]   So now we have Android Auto for your phone, Android Auto for your car, Android Auto motive
[00:49:26.080 --> 00:49:27.240]   for your car.
[00:49:27.240 --> 00:49:29.000]   If that weren't confusing enough.
[00:49:29.000 --> 00:49:30.600]   Okay.
[00:49:30.600 --> 00:49:34.360]   There is a new UI for the Android Auto on the screen in your car.
[00:49:34.360 --> 00:49:35.360]   Right, which is nice.
[00:49:35.360 --> 00:49:36.360]   And Kevin made the point that--
[00:49:36.360 --> 00:49:37.360]   Do you use this, right?
[00:49:37.360 --> 00:49:39.800]   I'm going to tell you a story in a second.
[00:49:39.800 --> 00:49:40.800]   Okay.
[00:49:40.800 --> 00:49:43.640]   But the other point is that if you do go to a full lap, then the directions come at the
[00:49:43.640 --> 00:49:44.840]   bottom of the screen.
[00:49:44.840 --> 00:49:46.840]   So you're always multitasking.
[00:49:46.840 --> 00:49:47.840]   Oh, that's nice.
[00:49:47.840 --> 00:49:50.440]   They redid the UI.
[00:49:50.440 --> 00:49:52.160]   It's very pretty.
[00:49:52.160 --> 00:49:53.160]   Accept that.
[00:49:53.160 --> 00:49:54.000]   Have I mentioned this before?
[00:49:54.000 --> 00:49:56.840]   I hate night mode.
[00:49:56.840 --> 00:49:57.840]   I hate it.
[00:49:57.840 --> 00:49:59.000]   I just don't like it.
[00:49:59.000 --> 00:50:00.880]   They're making it basically the default now.
[00:50:00.880 --> 00:50:02.720]   On Q.
[00:50:02.720 --> 00:50:04.080]   On next Android Auto.
[00:50:04.080 --> 00:50:06.560]   Not just Android Auto, but it's going to also be a feature in Q.
[00:50:06.560 --> 00:50:07.560]   I hate it.
[00:50:07.560 --> 00:50:08.560]   Well, feature okay because if I can turn it on.
[00:50:08.560 --> 00:50:09.920]   Yeah, because I hate it as a default.
[00:50:09.920 --> 00:50:10.920]   So here's the thing.
[00:50:10.920 --> 00:50:14.160]   So because I live love you to go, why do you hate the dark mode?
[00:50:14.160 --> 00:50:15.160]   I don't know.
[00:50:15.160 --> 00:50:16.160]   It really is your son.
[00:50:16.160 --> 00:50:17.160]   Why?
[00:50:17.160 --> 00:50:18.400]   Seriously.
[00:50:18.400 --> 00:50:22.000]   Everybody in the world wants dark mode.
[00:50:22.000 --> 00:50:23.000]   I don't.
[00:50:23.000 --> 00:50:24.000]   He's getting pink.
[00:50:24.000 --> 00:50:27.200]   You're dressed in dark.
[00:50:27.200 --> 00:50:30.720]   Yeah, it just wardrobe is dark mode.
[00:50:30.720 --> 00:50:32.440]   You are dark mode.
[00:50:32.440 --> 00:50:33.640]   I like contrast.
[00:50:33.640 --> 00:50:34.640]   Okay.
[00:50:34.640 --> 00:50:38.000]   No, I actually it's funny because everybody was all excited about dark mode on Mac OS with
[00:50:38.000 --> 00:50:39.240]   Mojave.
[00:50:39.240 --> 00:50:41.600]   I turn it on and I don't like it either.
[00:50:41.600 --> 00:50:42.600]   I turned it off.
[00:50:42.600 --> 00:50:43.840]   I went back.
[00:50:43.840 --> 00:50:47.480]   I don't feel like on the car, it's better as they pointed out yesterday.
[00:50:47.480 --> 00:50:49.800]   It's better on an OLED screen to be in dark mode.
[00:50:49.800 --> 00:50:50.800]   Use less battery.
[00:50:50.800 --> 00:50:51.800]   Fine, fine.
[00:50:51.800 --> 00:50:52.800]   But then the car is plugged in.
[00:50:52.800 --> 00:50:53.800]   Yeah, yeah, you don't care about them.
[00:50:53.800 --> 00:50:54.800]   I'm shrieking.
[00:50:54.800 --> 00:50:55.800]   So here's the thing.
[00:50:55.800 --> 00:51:00.560]   Because I am on this show and because I thus live love you to Google, I do everything Google
[00:51:00.560 --> 00:51:01.560]   is that I can do.
[00:51:01.560 --> 00:51:02.560]   Yes.
[00:51:02.560 --> 00:51:03.560]   So I bought my Mazda CX5.
[00:51:03.560 --> 00:51:04.560]   Yes.
[00:51:04.560 --> 00:51:05.560]   Did not come with Android Auto.
[00:51:05.560 --> 00:51:06.560]   Aww.
[00:51:06.560 --> 00:51:08.080]   Then the retrofit was possible.
[00:51:08.080 --> 00:51:09.080]   Oh good.
[00:51:09.080 --> 00:51:11.520]   Not it was the Mazda retrofit, right?
[00:51:11.520 --> 00:51:12.720]   So they put it in there.
[00:51:12.720 --> 00:51:15.560]   600 bucks later, I'm pissed.
[00:51:15.560 --> 00:51:16.560]   Why?
[00:51:16.560 --> 00:51:17.560]   It doesn't work well.
[00:51:17.560 --> 00:51:18.560]   It crashes all the time.
[00:51:18.560 --> 00:51:19.560]   It'll reboot.
[00:51:19.560 --> 00:51:23.080]   It'll just go off four times in a drive.
[00:51:23.080 --> 00:51:25.080]   And then the weird thing is the UI is...
[00:51:25.080 --> 00:51:26.480]   You think that's specific to your Mazda?
[00:51:26.480 --> 00:51:27.480]   Maybe this one.
[00:51:27.480 --> 00:51:28.480]   I don't know what it is.
[00:51:28.480 --> 00:51:31.520]   I'm going to have to go back in but it's going to just irritate me.
[00:51:31.520 --> 00:51:33.800]   But then the other weird thing is the UI is not well-throated at all.
[00:51:33.800 --> 00:51:35.680]   So I'm listening to Sirius.
[00:51:35.680 --> 00:51:36.680]   Yeah.
[00:51:36.680 --> 00:51:41.240]   And then I plug in the phone and so then it goes to Audible.
[00:51:41.240 --> 00:51:42.240]   Yeah.
[00:51:42.240 --> 00:51:43.240]   No, I don't want Audible now.
[00:51:43.240 --> 00:51:44.240]   Yeah.
[00:51:44.240 --> 00:51:49.960]   But before it doesn't, I have to go to the map to ways, turn that on, then I go back
[00:51:49.960 --> 00:51:52.840]   to Sirius, then it will go back to the audiobook.
[00:51:52.840 --> 00:51:57.120]   But now this time when I do it a second time, I can get Sirius and ways to do that.
[00:51:57.120 --> 00:51:58.120]   It's a work in progress.
[00:51:58.120 --> 00:51:59.120]   Whoo.
[00:51:59.120 --> 00:52:00.120]   Is it?
[00:52:00.120 --> 00:52:01.520]   And I don't know if that's Mazda specific.
[00:52:01.520 --> 00:52:03.920]   I think that is a little bit Android Auto, yeah.
[00:52:03.920 --> 00:52:05.960]   The crash you have a lot of experience with Android Auto.
[00:52:05.960 --> 00:52:06.960]   Yeah.
[00:52:06.960 --> 00:52:07.960]   I don't have a lot of experience with Android Auto.
[00:52:07.960 --> 00:52:10.440]   We had it when we were in Hawaii on vacation.
[00:52:10.440 --> 00:52:13.360]   And I did find it a little annoying.
[00:52:13.360 --> 00:52:15.000]   I thought CarPlay actually was easier.
[00:52:15.000 --> 00:52:17.960]   But Kevin, you said you like Android Auto.
[00:52:17.960 --> 00:52:18.960]   I do.
[00:52:18.960 --> 00:52:21.240]   I use both Android Auto and CarPlay.
[00:52:21.240 --> 00:52:23.800]   I don't really have too many problems with it.
[00:52:23.800 --> 00:52:25.840]   But then again, we're all doing different tasks.
[00:52:25.840 --> 00:52:27.400]   So I don't, you know, who knows.
[00:52:27.400 --> 00:52:29.200]   Kevin is basically always listening to the Beatles.
[00:52:29.200 --> 00:52:32.400]   So it's pretty obvious that when you get the car, just keep playing the Beatles.
[00:52:32.400 --> 00:52:33.400]   There's not a lot of time.
[00:52:33.400 --> 00:52:35.560]   How do you get 18 on Sirius XS?
[00:52:35.560 --> 00:52:37.360]   Papa channel 18 and you're done.
[00:52:37.360 --> 00:52:39.360]   Is that moving about the only guy on a rest of the day?
[00:52:39.360 --> 00:52:40.360]   Yes, it is.
[00:52:40.360 --> 00:52:41.360]   I did not.
[00:52:41.360 --> 00:52:43.760]   I was exhausted from yesterday with IO.
[00:52:43.760 --> 00:52:45.080]   So I did not see it yet.
[00:52:45.080 --> 00:52:46.720]   Danny Boyle's new movie.
[00:52:46.720 --> 00:52:47.720]   I can't wait.
[00:52:47.720 --> 00:52:50.400]   A universe in which the beat, actually it's not till June.
[00:52:50.400 --> 00:52:55.800]   So the universe in which the Beatles never existed except some guy for some reason remembers
[00:52:55.800 --> 00:52:56.800]   them.
[00:52:56.800 --> 00:53:02.040]   And since he's the only one that does, suddenly he is, they think, creating the Beatles songs
[00:53:02.040 --> 00:53:03.040]   in real time.
[00:53:03.040 --> 00:53:04.040]   Brilliant.
[00:53:04.040 --> 00:53:05.040]   Brilliant.
[00:53:05.040 --> 00:53:07.280]   It's a great idea for a movie.
[00:53:07.280 --> 00:53:08.280]   Yes.
[00:53:08.280 --> 00:53:11.600]   Kevin, that's Kevin's dream is to be that guy, I think.
[00:53:11.600 --> 00:53:13.920]   I know all the Beatles.
[00:53:13.920 --> 00:53:17.640]   Do you know all the songs, Kevin?
[00:53:17.640 --> 00:53:21.320]   I would say I'm pretty familiar with all the songs.
[00:53:21.320 --> 00:53:23.880]   So Star Wars, citing Eleanor Rigdon.
[00:53:23.880 --> 00:53:24.880]   Oh, that's easy.
[00:53:24.880 --> 00:53:25.880]   No, no, that's easy.
[00:53:25.880 --> 00:53:26.880]   That's what happened.
[00:53:26.880 --> 00:53:28.240]   There are obscure Beatles songs though.
[00:53:28.240 --> 00:53:29.240]   Oh, yes.
[00:53:29.240 --> 00:53:30.240]   Yeah.
[00:53:30.240 --> 00:53:31.240]   Yeah.
[00:53:31.240 --> 00:53:32.240]   Eleanor Rigdon.
[00:53:32.240 --> 00:53:33.240]   It's a obscure Beatles song.
[00:53:33.240 --> 00:53:34.240]   Well, so obscure.
[00:53:34.240 --> 00:53:35.240]   I don't remember it.
[00:53:35.240 --> 00:53:36.240]   Okay.
[00:53:36.240 --> 00:53:37.240]   What would you say, Kevin?
[00:53:37.240 --> 00:53:38.960]   Well, I'm going to say you do a Beatles maniac.
[00:53:38.960 --> 00:53:41.020]   He's actually, he calls himself a Beatleologist.
[00:53:41.020 --> 00:53:42.680]   He's even written for the Beatles.
[00:53:42.680 --> 00:53:43.680]   Wow.
[00:53:43.680 --> 00:53:50.840]   He has a Beatles collection of like lunchboxes and paper bow ties and hats and I also just
[00:53:50.840 --> 00:53:52.600]   stuff they sold over the years.
[00:53:52.600 --> 00:53:57.120]   But he, when challenged, well, he says, well, I bet you don't know everybody.
[00:53:57.120 --> 00:54:00.160]   And he somehow comes up with a song I never heard of.
[00:54:00.160 --> 00:54:01.400]   So there are a few.
[00:54:01.400 --> 00:54:06.280]   Anyway, yeah, there's a few, there's a few.
[00:54:06.280 --> 00:54:08.800]   Google Assistant was much faster on stage.
[00:54:08.800 --> 00:54:14.040]   Do you think that's an example of something that maybe won't be as fast in real life?
[00:54:14.040 --> 00:54:16.400]   They say it's going to be 10 times faster.
[00:54:16.400 --> 00:54:19.160]   But this is the point of having the model down.
[00:54:19.160 --> 00:54:20.160]   That's the model on the phone.
[00:54:20.160 --> 00:54:21.160]   This is the model on the phone.
[00:54:21.160 --> 00:54:22.160]   So we'll see.
[00:54:22.160 --> 00:54:24.560]   But then the processors are only going to faster and faster.
[00:54:24.560 --> 00:54:27.440]   So it's starting to be showing me what it could be like.
[00:54:27.440 --> 00:54:30.400]   I don't talk a lot to things devices yet.
[00:54:30.400 --> 00:54:31.800]   I think you both do more.
[00:54:31.800 --> 00:54:32.800]   Yeah.
[00:54:32.800 --> 00:54:36.200]   But it's funny because both Microsoft and Google will show these.
[00:54:36.200 --> 00:54:38.880]   And contextual conversations that go on and on.
[00:54:38.880 --> 00:54:39.880]   We'll move him to Friday.
[00:54:39.880 --> 00:54:41.880]   Now, what do I have on Thursday?
[00:54:41.880 --> 00:54:44.320]   And I don't ever want to operate in a world like that.
[00:54:44.320 --> 00:54:45.840]   That's not how I work.
[00:54:45.840 --> 00:54:48.600]   Maybe we're just not busy executives.
[00:54:48.600 --> 00:54:53.920]   Yeah, I use voice for interaction with my devices.
[00:54:53.920 --> 00:54:57.400]   I mean, it's even built in the assistants built into Chromebooks that you can have it
[00:54:57.400 --> 00:54:58.640]   always listening on your Chromebook.
[00:54:58.640 --> 00:55:04.400]   In fact, I use it every day, multiple times a day, but not, you know, we're at the, that,
[00:55:04.400 --> 00:55:06.200]   where I'm doing six different things.
[00:55:06.200 --> 00:55:08.280]   You know, I do it for one thing and I go.
[00:55:08.280 --> 00:55:11.200]   That was a little manic, to be honest with you.
[00:55:11.200 --> 00:55:12.200]   Yeah.
[00:55:12.200 --> 00:55:14.760]   It was just, it was to prove a point and show an example.
[00:55:14.760 --> 00:55:20.120]   When you get rid of the latency of a, you know, a connection and then the cloud and
[00:55:20.120 --> 00:55:24.720]   all the other things that has to do off device, I'm not surprised it's 10x faster.
[00:55:24.720 --> 00:55:26.440]   I mean, if it's all happening there.
[00:55:26.440 --> 00:55:30.400]   And I, and speaking to Jeff's point about hardware getting faster, you know, Google already
[00:55:30.400 --> 00:55:35.000]   has custom chips in these things for, you know, for the photography for security with the
[00:55:35.000 --> 00:55:36.000]   Titan M.
[00:55:36.000 --> 00:55:39.240]   Would not surprise me if they come up with a TensorFlow light chip.
[00:55:39.240 --> 00:55:42.640]   I was wondering if you had a TensorFlow in the phone would be fun.
[00:55:42.640 --> 00:55:44.440]   How would be interesting?
[00:55:44.440 --> 00:55:48.120]   We said this yesterday, you can already install TensorFlow on a Raspberry Pi.
[00:55:48.120 --> 00:55:50.960]   It's not like you need a power hungry chip.
[00:55:50.960 --> 00:55:57.360]   Well, and there are almost all the modern phones, including Apple's come with dedicated
[00:55:57.360 --> 00:55:59.200]   processing for machine learning.
[00:55:59.200 --> 00:56:02.320]   So I think that we are actually already starting to end of that here.
[00:56:02.320 --> 00:56:04.040]   But boy, there is nobody.
[00:56:04.040 --> 00:56:08.480]   Google seems to have an insurmountable lead and having done this for so long.
[00:56:08.480 --> 00:56:11.200]   They say Google Assistant is now on one billion devices.
[00:56:11.200 --> 00:56:14.800]   By the way, Android on two and a half billion active devices.
[00:56:14.800 --> 00:56:17.400]   That's double what Apple touted a couple of weeks ago.
[00:56:17.400 --> 00:56:18.400]   What was it?
[00:56:18.400 --> 00:56:20.920]   Android autos on a hundred billion.
[00:56:20.920 --> 00:56:22.360]   Yeah.
[00:56:22.360 --> 00:56:25.480]   But having a system on a billion devices means you're collecting data.
[00:56:25.480 --> 00:56:28.080]   It's a billion devices all the time.
[00:56:28.080 --> 00:56:29.080]   It's okay.
[00:56:29.080 --> 00:56:30.080]   No, it's good.
[00:56:30.080 --> 00:56:31.080]   It's good.
[00:56:31.080 --> 00:56:32.680]   It's how the models get better faster.
[00:56:32.680 --> 00:56:33.680]   Yeah.
[00:56:33.680 --> 00:56:36.120]   And I do think Google does a lot to anonymize that data.
[00:56:36.120 --> 00:56:37.120]   Not recording.
[00:56:37.120 --> 00:56:38.120]   Just as app.
[00:56:38.120 --> 00:56:39.360]   They're really taking a page of Apple's playbook.
[00:56:39.360 --> 00:56:44.440]   Apple doesn't record the beginning and ends of trips, just middle parts that are less identifiable.
[00:56:44.440 --> 00:56:46.440]   Things like that to anonymize the data.
[00:56:46.440 --> 00:56:51.480]   Well, when I call on Twitter, we now have the Google Maps Ashley Madison feature, which
[00:56:51.480 --> 00:56:52.480]   is in time.
[00:56:52.480 --> 00:56:53.800]   You know, and maps.
[00:56:53.800 --> 00:56:58.680]   Only you thought of that right away, didn't you?
[00:56:58.680 --> 00:56:59.680]   Okay.
[00:56:59.680 --> 00:57:04.240]   An interesting use of incognito and maps.
[00:57:04.240 --> 00:57:05.240]   You're right.
[00:57:05.240 --> 00:57:06.240]   Why else would you want to go?
[00:57:06.240 --> 00:57:07.240]   Why else would you want to go?
[00:57:07.240 --> 00:57:08.240]   Why else?
[00:57:08.240 --> 00:57:11.520]   Because this is the one they always give, which is such BS.
[00:57:11.520 --> 00:57:15.520]   I'm going to buy a gift for my wife and I don't want her to know that I'm headed to
[00:57:15.520 --> 00:57:16.520]   Tiffin.
[00:57:16.520 --> 00:57:17.520]   Yeah, right.
[00:57:17.520 --> 00:57:20.720]   Well, you're going to hear very soon the CIA.
[00:57:20.720 --> 00:57:25.040]   If you turn on any of the people on the maps, that's what they want you.
[00:57:25.040 --> 00:57:30.080]   Well, people always said that if you use PGP to encrypt your email, that's a sign you're
[00:57:30.080 --> 00:57:32.200]   guaranteed to be in the Prism database.
[00:57:32.200 --> 00:57:36.080]   When the Guardian went through the whole Snowden story, they thought, okay, we got
[00:57:36.080 --> 00:57:37.080]   get burner phones.
[00:57:37.080 --> 00:57:39.640]   They said, nope, if you're using the burner phones, that's the giveaway.
[00:57:39.640 --> 00:57:40.640]   Dead giveaway.
[00:57:40.640 --> 00:57:45.720]   And it's not like they can't spy on you with a burner phone.
[00:57:45.720 --> 00:57:47.040]   All right.
[00:57:47.040 --> 00:57:49.600]   So what else should we say?
[00:57:49.600 --> 00:57:52.640]   I just want to make sure that we don't miss any big things.
[00:57:52.640 --> 00:57:54.960]   Did they talk?
[00:57:54.960 --> 00:57:55.960]   Android Q.
[00:57:55.960 --> 00:57:56.960]   Yeah, we should talk about Android Q.
[00:57:56.960 --> 00:57:57.960]   Dark.
[00:57:57.960 --> 00:57:58.960]   Dark mode.
[00:57:58.960 --> 00:57:59.960]   Finally here.
[00:57:59.960 --> 00:58:02.400]   Jeff Jarvis has been begging for this.
[00:58:02.400 --> 00:58:06.040]   Did you, I haven't yet put Q on my Pixel 3.
[00:58:06.040 --> 00:58:09.520]   I was, I mean, I have the beta, but I just haven't downloaded it yet.
[00:58:09.520 --> 00:58:10.840]   Has anybody tried it played with it?
[00:58:10.840 --> 00:58:11.840]   Beta 3 is out.
[00:58:11.840 --> 00:58:12.840]   I do have it.
[00:58:12.840 --> 00:58:17.440]   And it was actually after I saw that video that you had on the screen a second ago from
[00:58:17.440 --> 00:58:18.440]   the Verge.
[00:58:18.440 --> 00:58:19.440]   Sure.
[00:58:19.440 --> 00:58:20.440]   What?
[00:58:20.440 --> 00:58:24.120]   Was that the review or was that the first look?
[00:58:24.120 --> 00:58:25.320]   Their first look, yeah.
[00:58:25.320 --> 00:58:26.320]   Yeah.
[00:58:26.320 --> 00:58:29.800]   And once I saw that, I got the Q3 beta going.
[00:58:29.800 --> 00:58:31.320]   It took a little while to download.
[00:58:31.320 --> 00:58:35.240]   I'm sure a lot of people were hitting the servers last night.
[00:58:35.240 --> 00:58:39.440]   You know, so far I like it.
[00:58:39.440 --> 00:58:45.640]   It's actually very iOS-like and I actually am an iOS iPhone user primarily, to be honest.
[00:58:45.640 --> 00:58:48.840]   And that's because the gestures, the swiping and all of that.
[00:58:48.840 --> 00:58:49.840]   Exactly.
[00:58:49.840 --> 00:58:52.000]   And I'm a dark mode.
[00:58:52.000 --> 00:58:55.320]   All the things kind of person, myself, unlike Jeff.
[00:58:55.320 --> 00:58:59.120]   And I turn that on and it doesn't work for every single app.
[00:58:59.120 --> 00:59:01.640]   That's where I really, and I agree with Jeff on that.
[00:59:01.640 --> 00:59:05.040]   When an app is not in dark mode, it blinds you.
[00:59:05.040 --> 00:59:13.000]   It's like, but there is a, and I saw this on nine to five Google today, you can go into
[00:59:13.000 --> 00:59:16.200]   the developer options of your phone, which I have enabled.
[00:59:16.200 --> 00:59:19.080]   And you can force dark mode even on apps that don't use it.
[00:59:19.080 --> 00:59:20.440]   And now I have total dark mode.
[00:59:20.440 --> 00:59:21.440]   Nice.
[00:59:21.440 --> 00:59:23.480]   I do like dark mode in the settings panel.
[00:59:23.480 --> 00:59:24.640]   I think that that's being dark.
[00:59:24.640 --> 00:59:26.640]   Just that just moved these days.
[00:59:26.640 --> 00:59:27.840]   Yeah, it does, doesn't it?
[00:59:27.840 --> 00:59:28.840]   About the internet.
[00:59:28.840 --> 00:59:29.840]   We're in a dark mode.
[00:59:29.840 --> 00:59:36.440]   The other thing, back to your phantomware point before, last year they announced this,
[00:59:36.440 --> 00:59:41.280]   this year they announced it again, and they still don't have it, which is the Google Maps
[00:59:41.280 --> 00:59:43.320]   walking follow the era.
[00:59:43.320 --> 00:59:44.320]   They showed it, didn't they?
[00:59:44.320 --> 00:59:45.320]   They showed it together.
[00:59:45.320 --> 00:59:47.320]   Now they said you could use it at I/O.
[00:59:47.320 --> 00:59:48.320]   Did you try it?
[00:59:48.320 --> 00:59:49.320]   That's what I thought.
[00:59:49.320 --> 00:59:50.320]   And it was no.
[00:59:50.320 --> 00:59:53.120]   All they did at I/O was a simple AR.
[00:59:53.120 --> 00:59:54.720]   You had to go and you had to scan a map.
[00:59:54.720 --> 00:59:56.280]   It would show the show you.
[00:59:56.280 --> 00:59:58.400]   That's where the snacks are, right?
[00:59:58.400 --> 00:59:59.400]   They didn't walk you around.
[00:59:59.400 --> 01:00:00.400]   They didn't walk you around.
[01:00:00.400 --> 01:00:03.880]   And I'm dying for that because I feel like I look like an absolute idiot.
[01:00:03.880 --> 01:00:05.440]   Well, you live in Manhattan.
[01:00:05.440 --> 01:00:06.560]   So that means you walk a lot.
[01:00:06.560 --> 01:00:07.560]   So walking directions are important.
[01:00:07.560 --> 01:00:08.560]   It's good for you.
[01:00:08.560 --> 01:00:09.800]   You ought to do that.
[01:00:09.800 --> 01:00:13.160]   But while a lot of us don't live in areas where you're going to walk to anything, and
[01:00:13.160 --> 01:00:14.320]   so that's less important.
[01:00:14.320 --> 01:00:17.840]   I know when I'm visiting a city, though, I love walking directions.
[01:00:17.840 --> 01:00:20.280]   But it's just driving me nuts because you're trying to get oriented.
[01:00:20.280 --> 01:00:21.880]   I'll go a block out of the way.
[01:00:21.880 --> 01:00:22.880]   Yeah.
[01:00:22.880 --> 01:00:23.880]   Oh, yeah, absolutely.
[01:00:23.880 --> 01:00:24.880]   I do that every time in New York.
[01:00:24.880 --> 01:00:27.960]   I go downtown, sit up, and then finally I realize, oh, I'm walking the wrong way.
[01:00:27.960 --> 01:00:29.160]   I have to turn around.
[01:00:29.160 --> 01:00:31.360]   Of course, I tried it out.
[01:00:31.360 --> 01:00:35.160]   I tried it out yesterday on my OG pixel and it worked.
[01:00:35.160 --> 01:00:36.160]   It works.
[01:00:36.160 --> 01:00:37.160]   So it's in Maps Now.
[01:00:37.160 --> 01:00:38.160]   It is now.
[01:00:38.160 --> 01:00:39.160]   Oh.
[01:00:39.160 --> 01:00:40.160]   It's in Maps Now.
[01:00:40.160 --> 01:00:41.160]   Really?
[01:00:41.160 --> 01:00:42.160]   Yeah.
[01:00:42.160 --> 01:00:43.160]   I walked in.
[01:00:43.160 --> 01:00:44.640]   I walked to Burger King with a little thing.
[01:00:44.640 --> 01:00:47.200]   Did you have to admit that?
[01:00:47.200 --> 01:00:48.200]   That's not a very far walk.
[01:00:48.200 --> 01:00:49.200]   You do this.
[01:00:49.200 --> 01:00:50.200]   Yeah.
[01:00:50.200 --> 01:00:52.000]   So it is now on Pixel smartphones.
[01:00:52.000 --> 01:00:57.440]   They say it's launched yesterday on Google Pixel smartphones.
[01:00:57.440 --> 01:00:58.520]   Should we try this?
[01:00:58.520 --> 01:01:00.320]   So how do you get into it?
[01:01:00.320 --> 01:01:02.760]   Is there an obvious button that you press?
[01:01:02.760 --> 01:01:04.920]   Just do walking directions.
[01:01:04.920 --> 01:01:05.920]   Okay.
[01:01:05.920 --> 01:01:06.920]   So it's like you're Burger King.
[01:01:06.920 --> 01:01:07.920]   How are you going to do that?
[01:01:07.920 --> 01:01:08.920]   Elroy's.
[01:01:08.920 --> 01:01:09.920]   Elroy's is better.
[01:01:09.920 --> 01:01:10.920]   That's why I was.
[01:01:10.920 --> 01:01:12.480]   But it's a longer walk.
[01:01:12.480 --> 01:01:13.480]   That's okay.
[01:01:13.480 --> 01:01:14.480]   So it's fractions.
[01:01:14.480 --> 01:01:16.080]   We're going to lose Jeff for about two hours now.
[01:01:16.080 --> 01:01:19.240]   Well, he walks to the burrito truck at that.
[01:01:19.240 --> 01:01:20.240]   Start AR.
[01:01:20.240 --> 01:01:21.240]   Oh, I missed it.
[01:01:21.240 --> 01:01:22.240]   I missed it.
[01:01:22.240 --> 01:01:23.240]   Oh shoot.
[01:01:23.240 --> 01:01:24.240]   So there is a button.
[01:01:24.240 --> 01:01:25.240]   This is a button that says start.
[01:01:25.240 --> 01:01:26.240]   Oh, I even chose.
[01:01:26.240 --> 01:01:27.240]   Oh, this is.
[01:01:27.240 --> 01:01:28.240]   Yeah.
[01:01:28.240 --> 01:01:29.240]   Don't sue us when you walk into a sewer.
[01:01:29.240 --> 01:01:30.240]   Yeah.
[01:01:30.240 --> 01:01:31.240]   Okay.
[01:01:31.240 --> 01:01:32.240]   Be safe.
[01:01:32.240 --> 01:01:33.240]   I can't leave out so I can.
[01:01:33.240 --> 01:01:34.240]   Here.
[01:01:34.240 --> 01:01:35.240]   Put it over here.
[01:01:35.240 --> 01:01:36.240]   Allow allow.
[01:01:36.240 --> 01:01:37.240]   This is good.
[01:01:37.240 --> 01:01:38.240]   So in order to get to Elroy's right now.
[01:01:38.240 --> 01:01:41.840]   Oh, first you have to scan the buildings across from you.
[01:01:41.840 --> 01:01:42.840]   Oh, Jesus.
[01:01:42.840 --> 01:01:43.840]   Ah, ah, ah.
[01:01:43.840 --> 01:01:46.840]   Oh, it doesn't work.
[01:01:46.840 --> 01:01:47.840]   It's got to worry.
[01:01:47.840 --> 01:01:48.840]   Excuse me.
[01:01:48.840 --> 01:01:49.840]   I'll go outside for a minute.
[01:01:49.840 --> 01:01:50.840]   It's got to orient.
[01:01:50.840 --> 01:01:51.840]   That's so weird.
[01:01:51.840 --> 01:01:54.480]   Well, go ahead and go outside.
[01:01:54.480 --> 01:01:55.480]   Jeff will be right back.
[01:01:55.480 --> 01:01:58.840]   Yeah, the weird thing is it has to orient on a building.
[01:01:58.840 --> 01:02:00.840]   Like, yeah, that's interesting.
[01:02:00.840 --> 01:02:02.600]   Well, that's what you're trying to do, right?
[01:02:02.600 --> 01:02:04.680]   When you're trying to figure out, do I go uptown or downtown?
[01:02:04.680 --> 01:02:06.080]   Do I turn left or right?
[01:02:06.080 --> 01:02:11.120]   So the problem is you don't have any information about what those buildings are supposed to
[01:02:11.120 --> 01:02:13.360]   be or which is left and which is right.
[01:02:13.360 --> 01:02:15.960]   So now, does it recognize your insides?
[01:02:15.960 --> 01:02:16.960]   Kevin, no.
[01:02:16.960 --> 01:02:20.520]   Yeah, you have to not say it's like looking for buildings.
[01:02:20.520 --> 01:02:24.800]   Yeah, so Jeff has gone out to the front of the studio and he's going to scan the buildings.
[01:02:24.800 --> 01:02:27.120]   I wonder how helpful that's going to be.
[01:02:27.120 --> 01:02:30.920]   Oh, I think it's I think it's helpful, but I don't know that a phone is the best thing.
[01:02:30.920 --> 01:02:35.400]   No, this is what you get when you've been doing street view for 10 years and you have
[01:02:35.400 --> 01:02:38.240]   the front of every building in America.
[01:02:38.240 --> 01:02:40.640]   It's kind of remarkable, really, that they can do.
[01:02:40.640 --> 01:02:41.640]   Did it work?
[01:02:41.640 --> 01:02:42.640]   Yes.
[01:02:42.640 --> 01:02:43.640]   Did you scan the buildings?
[01:02:43.640 --> 01:02:44.960]   It's so stunned.
[01:02:44.960 --> 01:02:45.960]   It does.
[01:02:45.960 --> 01:02:46.960]   It worked.
[01:02:46.960 --> 01:02:50.880]   It now knows that I should go out the door and turn right.
[01:02:50.880 --> 01:02:52.960]   It's pointing the direction I should go.
[01:02:52.960 --> 01:02:54.840]   Look at that.
[01:02:54.840 --> 01:02:56.760]   That's pretty cool.
[01:02:56.760 --> 01:02:59.280]   Very nice.
[01:02:59.280 --> 01:03:03.080]   So it goes into map mode when I'm not pointed at the real world.
[01:03:03.080 --> 01:03:06.120]   And then if I tip it up and point at the real world, it goes into augmented reality
[01:03:06.120 --> 01:03:07.120]   world.
[01:03:07.120 --> 01:03:09.240]   So turn left at the teachers, it looks like.
[01:03:09.240 --> 01:03:11.040]   Yeah, there they are.
[01:03:11.040 --> 01:03:13.760]   Oh, that's really neat.
[01:03:13.760 --> 01:03:14.760]   That is neat.
[01:03:14.760 --> 01:03:15.760]   Yay.
[01:03:15.760 --> 01:03:16.760]   Okay.
[01:03:16.760 --> 01:03:17.760]   Thank you, Google.
[01:03:17.760 --> 01:03:18.760]   Carsten, good work.
[01:03:18.760 --> 01:03:19.760]   Thank you, Clark.
[01:03:19.760 --> 01:03:20.760]   Thank you, Clark.
[01:03:20.760 --> 01:03:21.760]   Thank God he loves his Whoppers.
[01:03:21.760 --> 01:03:23.560]   That's all I can say.
[01:03:23.560 --> 01:03:26.600]   The title of his biography from Harvard to Burger King.
[01:03:26.600 --> 01:03:31.680]   I just want them to have that that impossible Whopper.
[01:03:31.680 --> 01:03:32.680]   Oh, yeah.
[01:03:32.680 --> 01:03:33.680]   They have it yet.
[01:03:33.680 --> 01:03:34.680]   They don't have it yet.
[01:03:34.680 --> 01:03:35.680]   Are you been asking?
[01:03:35.680 --> 01:03:36.680]   I checked.
[01:03:36.680 --> 01:03:38.840]   Doesn't Carl's junior have it already?
[01:03:38.840 --> 01:03:42.160]   It's like John Hine of Twit.
[01:03:42.160 --> 01:03:46.200]   John Hine did Jump the Shark.
[01:03:46.200 --> 01:03:48.680]   He's on the Howard Stern show now, producer there.
[01:03:48.680 --> 01:03:50.240]   And he wrote a junk food book.
[01:03:50.240 --> 01:03:51.240]   Oh, yeah.
[01:03:51.240 --> 01:03:54.200]   There's the, you know, that's a guaranteed best-selling.
[01:03:54.200 --> 01:03:55.720]   I actually want Taco Bell guy.
[01:03:55.720 --> 01:03:58.640]   Honestly, Elroy's is 10 times better than Taco Bell.
[01:03:58.640 --> 01:04:04.440]   And I love it that our local taco truck parks across the street from Taco Bell and
[01:04:04.440 --> 01:04:10.240]   Aluma hoping that people will go, I could go to Taco Bell or I could go to Elroy's.
[01:04:10.240 --> 01:04:11.840]   Oh, there's one more from yesterday.
[01:04:11.840 --> 01:04:17.880]   One more thing is I think they've kind of said this before, but bringing two factor
[01:04:17.880 --> 01:04:19.360]   to your phone as the device.
[01:04:19.360 --> 01:04:20.440]   I thought that was great.
[01:04:20.440 --> 01:04:23.600]   Your phone is actually the unlock device.
[01:04:23.600 --> 01:04:26.800]   I'm not sure how that's different from the approved and I thing that we have now.
[01:04:26.800 --> 01:04:29.840]   Oh, you have to just hit one button and yeah, I'm not exactly sure either.
[01:04:29.840 --> 01:04:31.600]   I think it's just a little easier.
[01:04:31.600 --> 01:04:34.120]   You don't have to tap the screen to approve it.
[01:04:34.120 --> 01:04:35.120]   Okay.
[01:04:35.120 --> 01:04:36.120]   Nice.
[01:04:36.120 --> 01:04:39.680]   Proximity like trying to push two factor, which is a can.
[01:04:39.680 --> 01:04:44.200]   You know the email server that we advertise, the helm email server does authentication
[01:04:44.200 --> 01:04:46.560]   through Bluetooth, Ellie proximity.
[01:04:46.560 --> 01:04:52.280]   So somebody, you can't create a password for an account unless you have the helm application
[01:04:52.280 --> 01:04:57.720]   on your phone and you are within Bluetooth, Ellie proximity of the helm server, which I
[01:04:57.720 --> 01:05:01.960]   think is a brilliant way to authenticate because if you're not standing next to the
[01:05:01.960 --> 01:05:03.680]   server, you can't authenticate.
[01:05:03.680 --> 01:05:04.680]   I think it's really smart.
[01:05:04.680 --> 01:05:09.640]   By the way, we earlier said that the home hub maxi, whatever was 249.
[01:05:09.640 --> 01:05:10.640]   It's 229.
[01:05:10.640 --> 01:05:12.840]   Yeah, it's actually a pretty good price.
[01:05:12.840 --> 01:05:13.840]   It is.
[01:05:13.840 --> 01:05:18.040]   They did mention folding phones, which I, we couldn't take it.
[01:05:18.040 --> 01:05:19.560]   Kevin and I couldn't help but the.
[01:05:19.560 --> 01:05:20.560]   Chortle?
[01:05:20.560 --> 01:05:21.560]   Chortle a little.
[01:05:21.560 --> 01:05:22.560]   Chortle.
[01:05:22.560 --> 01:05:26.760]   Since the only folding phones we know about are kind of, well, at least the galaxy is
[01:05:26.760 --> 01:05:27.760]   not doing so.
[01:05:27.760 --> 01:05:29.760]   Orch, I think is the title.
[01:05:29.760 --> 01:05:30.760]   Yeah.
[01:05:30.760 --> 01:05:31.760]   Orked.
[01:05:31.760 --> 01:05:32.760]   All right.
[01:05:32.760 --> 01:05:34.400]   I think we've done IO.
[01:05:34.400 --> 01:05:35.400]   We did IO.
[01:05:35.400 --> 01:05:38.440]   There were a couple of things though I was going to, I was wondering if Google would
[01:05:38.440 --> 01:05:43.000]   talk about and now I've, now they've slipped my, slipped my mind.
[01:05:43.000 --> 01:05:47.440]   Media was one I was hoping for but with the E3 gaming conference next month I can see
[01:05:47.440 --> 01:05:48.440]   why they haven't talked about it.
[01:05:48.440 --> 01:05:49.600]   Not one word about Stadia.
[01:05:49.600 --> 01:05:50.600]   Wasn't that interesting?
[01:05:50.600 --> 01:05:55.000]   They had a demonstration hub in the press room.
[01:05:55.000 --> 01:05:56.000]   So you could play it.
[01:05:56.000 --> 01:05:58.000]   You could play with it.
[01:05:58.000 --> 01:05:59.800]   Yeah.
[01:05:59.800 --> 01:06:03.520]   How about progressive web apps and probably have a track on that.
[01:06:03.520 --> 01:06:08.880]   They mentioned it a couple times in demos about how to bring up the PWA.
[01:06:08.880 --> 01:06:09.880]   Okay.
[01:06:09.880 --> 01:06:10.880]   I went to some.
[01:06:10.880 --> 01:06:14.840]   I'm excited by that because it means it's a kind of a cross platform development.
[01:06:14.840 --> 01:06:19.440]   It'll be very good for Chromebooks as well as mobile devices.
[01:06:19.440 --> 01:06:21.800]   So there was a protest plane over.
[01:06:21.800 --> 01:06:22.800]   Did you see it?
[01:06:22.800 --> 01:06:23.800]   Yeah.
[01:06:23.800 --> 01:06:27.640]   And its message made no sense.
[01:06:27.640 --> 01:06:28.640]   Not unusual.
[01:06:28.640 --> 01:06:30.360]   Here's a picture of the banner.
[01:06:30.360 --> 01:06:35.360]   Google control is not privacy.
[01:06:35.360 --> 01:06:37.560]   Privacy save local news.
[01:06:37.560 --> 01:06:38.560]   What?
[01:06:38.560 --> 01:06:39.560]   What?
[01:06:39.560 --> 01:06:42.400]   Google control is not privacy save local news.
[01:06:42.400 --> 01:06:44.600]   Makes no sense.
[01:06:44.600 --> 01:06:47.960]   I don't know what privacy has to do with local news at all.
[01:06:47.960 --> 01:06:48.960]   Exactly.
[01:06:48.960 --> 01:06:53.520]   I think it's some unemployed journalist with a flying license.
[01:06:53.520 --> 01:06:58.160]   So there's an article about in TechCrunch but they, nobody seems to have figured out.
[01:06:58.160 --> 01:07:00.160]   It wouldn't be so hard to figure out who it is and why.
[01:07:00.160 --> 01:07:01.160]   It was just assumed.
[01:07:01.160 --> 01:07:03.440]   They timed it well just assumed our began.
[01:07:03.440 --> 01:07:04.440]   Really?
[01:07:04.440 --> 01:07:06.440]   They're one plane.
[01:07:06.440 --> 01:07:07.440]   Huh?
[01:07:07.440 --> 01:07:08.440]   Yeah.
[01:07:08.440 --> 01:07:09.600]   It's a head scratcher folks.
[01:07:09.600 --> 01:07:12.280]   It's definitely not going to galvanize local actions.
[01:07:12.280 --> 01:07:13.720]   So the other thing about this.
[01:07:13.720 --> 01:07:19.280]   So this year they decided that this was going to be the carless IO.
[01:07:19.280 --> 01:07:21.360]   So you had to like literally drive the San Jose.
[01:07:21.360 --> 01:07:22.360]   Oh yeah.
[01:07:22.360 --> 01:07:23.360]   They didn't want you to.
[01:07:23.360 --> 01:07:27.560]   Even though there's plenty of parking outside shoreline is designed for concerts of 10
[01:07:27.560 --> 01:07:31.240]   I will confess what I got since I have a press pass.
[01:07:31.240 --> 01:07:32.240]   We were allowed to park.
[01:07:32.240 --> 01:07:38.640]   That's what I did as it should be because the parking lots there for the sandboxes and
[01:07:38.640 --> 01:07:39.640]   demos and things like that.
[01:07:39.640 --> 01:07:40.640]   Oh, so they did use them.
[01:07:40.640 --> 01:07:41.640]   Oh interesting.
[01:07:41.640 --> 01:07:44.280]   Well, when I was last there they did.
[01:07:44.280 --> 01:07:48.360]   Yeah, no, but they had plenty of parking before across the street.
[01:07:48.360 --> 01:07:49.360]   Gotcha.
[01:07:49.360 --> 01:07:52.600]   We should also mention Project Mainline.
[01:07:52.600 --> 01:07:54.080]   This was a quick mention.
[01:07:54.080 --> 01:07:59.600]   This is the idea that you can update older Android devices not by pushing it through the
[01:07:59.600 --> 01:08:03.160]   manufacturer or the carrier but actually making it part of Google Play.
[01:08:03.160 --> 01:08:08.880]   This has been an ongoing project of Google with Treble to make Android devices more secure.
[01:08:08.880 --> 01:08:13.840]   The problem of course in the past has been you counted on the manufacturers and the carriers
[01:08:13.840 --> 01:08:15.040]   to push that stuff.
[01:08:15.040 --> 01:08:18.760]   Now you could just push it down to the Google Store and people will in many cases get that
[01:08:18.760 --> 01:08:20.400]   update.
[01:08:20.400 --> 01:08:22.040]   That's really a big deal.
[01:08:22.040 --> 01:08:25.040]   It looked like they were getting more and more system components into...
[01:08:25.040 --> 01:08:26.040]   Much better for security.
[01:08:26.040 --> 01:08:27.040]   Mainline, yeah.
[01:08:27.040 --> 01:08:33.680]   We haven't talked about Android being forked and bifurcated in a long time.
[01:08:33.680 --> 01:08:36.800]   There used to be a big thing on the show because I think this is part of the problem
[01:08:36.800 --> 01:08:38.560]   is you couldn't get security updates.
[01:08:38.560 --> 01:08:39.560]   Right.
[01:08:39.560 --> 01:08:40.560]   Because you had an old version of Android.
[01:08:40.560 --> 01:08:42.880]   Does this kind of solve that?
[01:08:42.880 --> 01:08:44.640]   Well, there's still forks.
[01:08:44.640 --> 01:08:45.640]   There's still Amazon's for instance.
[01:08:45.640 --> 01:08:46.640]   No, I don't mean the forks.
[01:08:46.640 --> 01:08:50.000]   That was wrong word but I just mean that the fact that some phones don't get updated with
[01:08:50.000 --> 01:08:51.000]   the latest version of Android.
[01:08:51.000 --> 01:08:54.520]   The theory here is Google realized they'd lost because Android's open source, they'd lost
[01:08:54.520 --> 01:09:01.360]   control of the vast majority of Android phones because the developers, the manufacturers,
[01:09:01.360 --> 01:09:06.640]   the carriers had no tie to Google and Google couldn't force them to accept an update.
[01:09:06.640 --> 01:09:08.560]   And what is reason for it because it costs them money.
[01:09:08.560 --> 01:09:13.800]   It costs them bandwidth, not Google but the carriers bandwidth to push those out.
[01:09:13.800 --> 01:09:17.960]   The phone manufacturers themselves have to vet these updates to make sure they don't
[01:09:17.960 --> 01:09:20.960]   break any customizations of manufacturers.
[01:09:20.960 --> 01:09:29.760]   Make Google said that they did get a lot of contributions from device manufacturers.
[01:09:29.760 --> 01:09:35.040]   They said partners contributed "many changes and collaborated with us to ensure they ran
[01:09:35.040 --> 01:09:36.040]   well on their devices."
[01:09:36.040 --> 01:09:41.080]   So there is still that issue of whatever Google's going to push out over the play store, it
[01:09:41.080 --> 01:09:42.800]   better not break the phone.
[01:09:42.800 --> 01:09:44.080]   I saw it was pretty cool.
[01:09:44.080 --> 01:09:47.920]   I went to one of the developer sessions where of course I have no idea what's going on.
[01:09:47.920 --> 01:09:55.560]   But simple one line to say that you could force with a change force an update on the
[01:09:55.560 --> 01:09:56.800]   app.
[01:09:56.800 --> 01:10:00.640]   But then there's a step back which is you can offer the option of an update.
[01:10:00.640 --> 01:10:02.160]   Is this for the end user or the...
[01:10:02.160 --> 01:10:03.160]   This is for the end user.
[01:10:03.160 --> 01:10:04.160]   Yeah.
[01:10:04.160 --> 01:10:06.160]   So it was very slick.
[01:10:06.160 --> 01:10:07.160]   Yeah.
[01:10:07.160 --> 01:10:08.640]   It was impressive.
[01:10:08.640 --> 01:10:13.920]   I actually watched that one as well and it was a one-liner to just make a call to if there's
[01:10:13.920 --> 01:10:21.520]   a newer version of this app, just put a note at the bottom and hope the user says update
[01:10:21.520 --> 01:10:23.360]   or you can make it very non-obtrusive.
[01:10:23.360 --> 01:10:28.240]   So you had some controls as a developer to make it a better user experience but still
[01:10:28.240 --> 01:10:32.000]   get the update to the user before they have to go to the play store.
[01:10:32.000 --> 01:10:35.080]   In fact, they don't have to go to the play store at all for this.
[01:10:35.080 --> 01:10:36.080]   Right.
[01:10:36.080 --> 01:10:44.640]   So this is Treble, Mainline is a new feature that is empowered by Treble and there's a
[01:10:44.640 --> 01:10:45.640]   new file format.
[01:10:45.640 --> 01:10:48.880]   Some of these will be APKs, the traditional Android package format.
[01:10:48.880 --> 01:10:54.080]   But there is also going to be APEX, APEX, Android Pony Express files and the reason is
[01:10:54.080 --> 01:10:58.000]   sometimes these operating system patches have to be performed before a reboot.
[01:10:58.000 --> 01:11:00.920]   You can't just do it as if it's an app updating it.
[01:11:00.920 --> 01:11:05.000]   So APEX will actually load earlier in the booting process.
[01:11:05.000 --> 01:11:10.160]   So Android Upkates can be installed on a reboot and that's also a fairly critical
[01:11:10.160 --> 01:11:11.160]   change.
[01:11:11.160 --> 01:11:17.680]   It does, however, according to Venturebeat, introduce security risks, which is why Google
[01:11:17.680 --> 01:11:22.720]   is building in new failsafe mechanisms and enhanced test processes to make sure those
[01:11:22.720 --> 01:11:24.080]   updates are safe.
[01:11:24.080 --> 01:11:26.840]   We've seen Man in the Middle Attacks through updates.
[01:11:26.840 --> 01:11:32.520]   Now many times I suspect but at least most recently two times, one with C Cleaner where
[01:11:32.520 --> 01:11:37.880]   the update of C Cleaner was modified by a bad guy and so people who downloaded their
[01:11:37.880 --> 01:11:43.360]   update and it also happened, was it Asus?
[01:11:43.360 --> 01:11:44.360]   Was Asus Asus?
[01:11:44.360 --> 01:11:45.360]   Yes.
[01:11:45.360 --> 01:11:46.360]   Was Asus, wasn't it?
[01:11:46.360 --> 01:11:49.600]   So the Asus update files had been corrupted.
[01:11:49.600 --> 01:11:50.960]   They were still signed by Asus.
[01:11:50.960 --> 01:11:56.280]   They were still downloaded by the official Asus updater but it was an effectively supply
[01:11:56.280 --> 01:11:57.600]   chain attack.
[01:11:57.600 --> 01:12:00.960]   And so that's something you really have to start paying attention to because if you
[01:12:00.960 --> 01:12:07.720]   could make a malicious Apex file that gets stuck in the stream somehow, you could be updating
[01:12:07.720 --> 01:12:09.680]   these phones in exactly the wrong direction.
[01:12:09.680 --> 01:12:11.960]   You could be introducing problems.
[01:12:11.960 --> 01:12:16.080]   So Google is very good though about security.
[01:12:16.080 --> 01:12:17.680]   So there are a lot of questions.
[01:12:17.680 --> 01:12:21.920]   Venturebeat says how often will these modules be updated?
[01:12:21.920 --> 01:12:24.480]   What about devices that can't get Google Play updates?
[01:12:24.480 --> 01:12:27.760]   For instance, if you have a Chinese Android device that doesn't get Google Play updates,
[01:12:27.760 --> 01:12:29.520]   what happens there?
[01:12:29.520 --> 01:12:31.800]   It does require Android Q from the start.
[01:12:31.800 --> 01:12:35.160]   So if you've upgraded the phone to Q, in other words, all of our existing Android
[01:12:35.160 --> 01:12:37.800]   iPhones won't support this.
[01:12:37.800 --> 01:12:43.880]   Which means it also leaves out the vast majority of un-updatable Android 4 phones.
[01:12:43.880 --> 01:12:49.560]   So it's not a panacea but at least it's, I think it's one direction that makes a lot
[01:12:49.560 --> 01:12:50.560]   of sense.
[01:12:50.560 --> 01:12:53.160]   I think before I forget.
[01:12:53.160 --> 01:12:54.160]   You're bored actually.
[01:12:54.160 --> 01:12:55.160]   No, no, no.
[01:12:55.160 --> 01:12:57.720]   I just think that it was a good job.
[01:12:57.720 --> 01:13:02.040]   Before I forget, there actually was some big news that was very hush-hush.
[01:13:02.040 --> 01:13:08.320]   And my old gigo-owned colleague, Yanko Rucker, found out through an interview with Google
[01:13:08.320 --> 01:13:13.760]   that the works with Nest program is going away.
[01:13:13.760 --> 01:13:19.560]   And come June, that means a lot of the third-party integrations with Nest products will
[01:13:19.560 --> 01:13:21.360]   not work.
[01:13:21.360 --> 01:13:22.360]   And the big one.
[01:13:22.360 --> 01:13:23.360]   Including if this is that.
[01:13:23.360 --> 01:13:24.360]   Oh no.
[01:13:24.360 --> 01:13:26.520]   If this and that, that is correct.
[01:13:26.520 --> 01:13:27.520]   Yep.
[01:13:27.520 --> 01:13:28.520]   Oh.
[01:13:28.520 --> 01:13:29.520]   That's big.
[01:13:29.520 --> 01:13:30.520]   That's big, yeah.
[01:13:30.520 --> 01:13:36.320]   Now, in Yanko's headline, he says, "Also, Titan's smart home privacy rules.
[01:13:36.320 --> 01:13:37.680]   Is that why they're doing this?
[01:13:37.680 --> 01:13:39.680]   Is it a security thing?"
[01:13:39.680 --> 01:13:40.680]   You know what?
[01:13:40.680 --> 01:13:42.640]   I could see why Google is saying that it is.
[01:13:42.640 --> 01:13:49.480]   Because Stacey and I on the IoT show always say, "Once you've given your data over, you
[01:13:49.480 --> 01:13:51.800]   have no control over what happens to it."
[01:13:51.800 --> 01:13:58.120]   So Google is trying to tighten up who has access to what smart home data.
[01:13:58.120 --> 01:14:02.040]   Because again, once a third party has it, it's out there and they can do whatever they
[01:14:02.040 --> 01:14:03.040]   want.
[01:14:03.040 --> 01:14:04.760]   So I would say, yes, that's accurate.
[01:14:04.760 --> 01:14:08.000]   They are trying to tighten up the privacy a little bit with this.
[01:14:08.000 --> 01:14:12.240]   Yanko interviewed Google Vice President Rishi Chandra.
[01:14:12.240 --> 01:14:17.200]   And one of the things Chandra told him is that while it does break, if this and that,
[01:14:17.200 --> 01:14:21.760]   they're hoping to put much of that functionality into the Google Assistant.
[01:14:21.760 --> 01:14:27.120]   So you'd have routines, you couldn't go to a third party to do the routines, you'd have
[01:14:27.120 --> 01:14:31.520]   to go through the Google Assistant to do it.
[01:14:31.520 --> 01:14:32.760]   It's this double-sided.
[01:14:32.760 --> 01:14:33.840]   This is ecosystem lock-in.
[01:14:33.840 --> 01:14:34.840]   Right?
[01:14:34.840 --> 01:14:35.840]   It is.
[01:14:35.840 --> 01:14:38.120]   Interoperating with third-party services.
[01:14:38.120 --> 01:14:42.920]   Yeah, maybe that's a security flow, but it's also, I think, a real benefit.
[01:14:42.920 --> 01:14:46.320]   All right.
[01:14:46.320 --> 01:14:53.800]   The commitment to hardware hasn't changed at all, says Chandra.
[01:14:53.800 --> 01:14:56.040]   Stacey just, she must be listening.
[01:14:56.040 --> 01:14:57.040]   Hi, Stacey.
[01:14:57.040 --> 01:14:58.040]   Hi, Stacey.
[01:14:58.040 --> 01:14:59.040]   Hi, Stacey.
[01:14:59.040 --> 01:15:00.120]   She's out on the West Coast.
[01:15:00.120 --> 01:15:06.400]   She said, you know that if this Google thing means that you have to buy a Google home device
[01:15:06.400 --> 01:15:11.040]   that's in your house all the time, if you want the level of control that the, yeah.
[01:15:11.040 --> 01:15:12.600]   It's a system lock-in.
[01:15:12.600 --> 01:15:14.160]   Yeah, it's ecosystem lock-in.
[01:15:14.160 --> 01:15:19.360]   But that's what that new, by the way, that new Nest Home Hub Max will be, is it can be
[01:15:19.360 --> 01:15:21.440]   the hub.
[01:15:21.440 --> 01:15:23.680]   For Bluetooth and Wi-Fi devices, yes.
[01:15:23.680 --> 01:15:26.160]   I support thread, they said.
[01:15:26.160 --> 01:15:28.160]   Is that enough to do everything?
[01:15:28.160 --> 01:15:29.160]   Oh, I don't know.
[01:15:29.160 --> 01:15:33.640]   You still need Zigbee and Z-Wave and all of the protocols?
[01:15:33.640 --> 01:15:36.880]   Need might be too strong of a word, but I would like to see that happen.
[01:15:36.880 --> 01:15:41.760]   If you wanted to interoperate within your house in all the different devices as opposed
[01:15:41.760 --> 01:15:43.800]   just the ones you got from Nest.
[01:15:43.800 --> 01:15:48.840]   And locally as well as opposed to going to the cloud all the time, which again, on-device
[01:15:48.840 --> 01:15:52.080]   natural language processing, we don't have that in Google Homes today.
[01:15:52.080 --> 01:15:55.400]   So that's why they have to be cloud connected.
[01:15:55.400 --> 01:15:56.920]   Did they talk about YouTube at all?
[01:15:56.920 --> 01:15:58.920]   Do they talk about any other Google?
[01:15:58.920 --> 01:16:01.520]   Did they mention Google+, did they?
[01:16:01.520 --> 01:16:02.520]   Uh-oh.
[01:16:02.520 --> 01:16:04.520]   We have a user.
[01:16:04.520 --> 01:16:05.520]   Oh, cool.
[01:16:05.520 --> 01:16:07.960]   True side.
[01:16:07.960 --> 01:16:09.280]   I love it.
[01:16:09.280 --> 01:16:10.280]   I'm sad.
[01:16:10.280 --> 01:16:11.280]   It was sad to see it go.
[01:16:11.280 --> 01:16:13.680]   No, I don't think it was, I was very able to session on YouTube.
[01:16:13.680 --> 01:16:15.400]   I don't think they're not yesterday.
[01:16:15.400 --> 01:16:16.760]   YouTube's still part of Google, right?
[01:16:16.760 --> 01:16:17.760]   It's not an alphabet company.
[01:16:17.760 --> 01:16:18.920]   It's still part of Google.
[01:16:18.920 --> 01:16:25.240]   So, um, I'm just looking at other stories.
[01:16:25.240 --> 01:16:29.800]   I'm trying to find the schedule.
[01:16:29.800 --> 01:16:31.440]   And most of it is now other companies.
[01:16:31.440 --> 01:16:36.400]   I think we can, uh, we can wrap it up because I don't think we need to mention the F word
[01:16:36.400 --> 01:16:39.720]   or the T word or the S word.
[01:16:39.720 --> 01:16:44.120]   I think we can, uh, Google's adding time lapse mode.
[01:16:44.120 --> 01:16:45.120]   Let's do a change log.
[01:16:45.120 --> 01:16:46.360]   We do have a couple of change logs.
[01:16:46.360 --> 01:16:47.360]   Thanks.
[01:16:47.360 --> 01:16:48.360]   Yeah, let's do that.
[01:16:48.360 --> 01:16:54.200]   The Google change log.
[01:16:54.200 --> 01:16:57.240]   Time lapse mode comes to every pixel camera.
[01:16:57.240 --> 01:17:01.960]   It's debuting on your brand new phone, Jeff Jarvis, the Pixel 3A.
[01:17:01.960 --> 01:17:02.960]   Ooh.
[01:17:02.960 --> 01:17:06.800]   See, I use a Samsung phone and I've had time lapse and super slow-mo for a long time.
[01:17:06.800 --> 01:17:11.560]   Hyperlapse, they call it, uh, but the time lapse option will be coming to the camera on
[01:17:11.560 --> 01:17:13.560]   all pixel smartphones.
[01:17:13.560 --> 01:17:17.360]   It's, of course, now available if you have a new brand new one, a 3A, a 3XL, but it'll
[01:17:17.360 --> 01:17:22.960]   go on the, uh, as Jeff calls it, OG pixel, the pixel two and the pixel three following
[01:17:22.960 --> 01:17:23.960]   an update.
[01:17:23.960 --> 01:17:28.960]   You'll find it under the three dots where the more option is night sight and slum.
[01:17:28.960 --> 01:17:31.360]   Oh, I actually have it.
[01:17:31.360 --> 01:17:32.360]   Already.
[01:17:32.360 --> 01:17:33.360]   Yeah.
[01:17:33.360 --> 01:17:35.440]   And so do set as our studio eyes in frags.
[01:17:35.440 --> 01:17:36.440]   Right.
[01:17:36.440 --> 01:17:39.680]   Oh, I'm so real.
[01:17:39.680 --> 01:17:40.680]   Insular.
[01:17:40.680 --> 01:17:44.480]   Oh, most of it looks like they just started rolling it out.
[01:17:44.480 --> 01:17:45.480]   Nice.
[01:17:45.480 --> 01:17:47.160]   Yay.
[01:17:47.160 --> 01:17:48.680]   Time lapse.
[01:17:48.680 --> 01:17:51.400]   Um, yeah.
[01:17:51.400 --> 01:17:52.400]   Okay.
[01:17:52.400 --> 01:17:53.600]   It's in the change log.
[01:17:53.600 --> 01:17:54.600]   It's something new.
[01:17:54.600 --> 01:17:57.160]   We mentioned the ground up redesign for Android auto.
[01:17:57.160 --> 01:17:58.840]   That's coming this summer.
[01:17:58.840 --> 01:18:06.400]   There's a new video series from the webmaster team, the old Matt cuts webmaster team.
[01:18:06.400 --> 01:18:08.480]   SEO myth busting.
[01:18:08.480 --> 01:18:15.440]   I wish it were Matt, but it's somebody new Martin split from the webmaster trends team.
[01:18:15.440 --> 01:18:20.040]   It's common misconceptions on search engine optimization.
[01:18:20.040 --> 01:18:24.160]   That's actually something Matt always was very good at without labeling it such.
[01:18:24.160 --> 01:18:27.920]   Uh, but if you subscribe to the Google, Google webmaster channel on YouTube, you'll be able
[01:18:27.920 --> 01:18:32.920]   to, you'll get the notification when the new, the new episodes come out.
[01:18:32.920 --> 01:18:35.960]   SEO myth busters.
[01:18:35.960 --> 01:18:43.920]   There is a, uh, there is a new AI project from Google, Google AI that turns your face into
[01:18:43.920 --> 01:18:45.880]   a poem portrait.
[01:18:45.880 --> 01:18:50.280]   Oh, you want to try it?
[01:18:50.280 --> 01:18:51.280]   Let's do it here.
[01:18:51.280 --> 01:18:56.040]   It's, uh, it's arts experiments with dot with Google.com.
[01:18:56.040 --> 01:19:01.400]   Estevlin, it's at the boundaries of AI and human collaboration.
[01:19:01.400 --> 01:19:02.800]   Okay.
[01:19:02.800 --> 01:19:12.000]   So here's pirated with a morning wind slated with light of days, harmonious and strange
[01:19:12.000 --> 01:19:14.920]   with the first fair beam biased and frayed.
[01:19:14.920 --> 01:19:16.800]   And now they are the flash.
[01:19:16.800 --> 01:19:19.560]   I'm going to enter a donate a word, refolgent.
[01:19:19.560 --> 01:19:25.440]   Always a good word in poetry, refolgent.
[01:19:25.440 --> 01:19:33.560]   And now a poem featuring the word and a train, an algorithm trained in over 20 million words
[01:19:33.560 --> 01:19:38.840]   of 19th century poetry is generating your unique poem portrait.
[01:19:38.840 --> 01:19:40.400]   I'm doing burrito.
[01:19:40.400 --> 01:19:43.920]   Oh, first it has to access my camera.
[01:19:43.920 --> 01:19:44.920]   Okay.
[01:19:44.920 --> 01:19:45.920]   Wait a minute.
[01:19:45.920 --> 01:19:46.920]   Wait a minute.
[01:19:46.920 --> 01:19:48.560]   It's going to take a picture.
[01:19:48.560 --> 01:19:50.120]   Take a picture of me.
[01:19:50.120 --> 01:19:51.960]   Take a picture.
[01:19:51.960 --> 01:19:56.720]   Apparently it doesn't understand windows at all.
[01:19:56.720 --> 01:19:57.720]   Hello.
[01:19:57.720 --> 01:20:03.960]   Three, two, one.
[01:20:03.960 --> 01:20:06.840]   The camera wasn't authorized.
[01:20:06.840 --> 01:20:08.160]   That's the picture it got.
[01:20:08.160 --> 01:20:09.160]   Okay.
[01:20:09.160 --> 01:20:12.040]   Refolgent as the light of the wind.
[01:20:12.040 --> 01:20:15.400]   My freedom shall return with delight.
[01:20:15.400 --> 01:20:16.400]   That's pretty good.
[01:20:16.400 --> 01:20:24.600]   Burrito is green and some should set the eye that butte the shore of the sky.
[01:20:24.600 --> 01:20:25.600]   That sucks.
[01:20:25.600 --> 01:20:27.440]   You got it.
[01:20:27.440 --> 01:20:29.440]   You got Rob my friend.
[01:20:29.440 --> 01:20:30.440]   I got it.
[01:20:30.440 --> 01:20:31.440]   All right.
[01:20:31.440 --> 01:20:34.400]   That's the brand new Google AI arc.
[01:20:34.400 --> 01:20:37.120]   Can they give back to 20% time please?
[01:20:37.120 --> 01:20:41.600]   It is thank a teacher week.
[01:20:41.600 --> 01:20:42.960]   Thank a teacher.
[01:20:42.960 --> 01:20:46.360]   If you can read this, thank a teacher.
[01:20:46.360 --> 01:20:47.360]   Thank you.
[01:20:47.360 --> 01:20:48.360]   Thank you.
[01:20:48.360 --> 01:20:49.360]   Thank you.
[01:20:49.360 --> 01:20:50.360]   Thank you.
[01:20:50.360 --> 01:20:51.360]   Thank you.
[01:20:51.360 --> 01:20:52.360]   That's my favorite bumper sticker.
[01:20:52.360 --> 01:20:53.360]   That and the one.
[01:20:53.360 --> 01:20:57.200]   Wouldn't it be cool if the schools got all the funding they wanted in the military had
[01:20:57.200 --> 01:20:58.200]   a held a bake sale?
[01:20:58.200 --> 01:21:00.880]   That's my other favorite bumper sticker.
[01:21:00.880 --> 01:21:04.920]   So this is teacher appreciation week.
[01:21:04.920 --> 01:21:10.000]   And here's an article about it by the national teacher of the year, Rodney Robinson.
[01:21:10.000 --> 01:21:14.480]   That's the what's that wonderful company that you can put up your quest to the public
[01:21:14.480 --> 01:21:16.040]   to crowd fund.
[01:21:16.040 --> 01:21:18.200]   Get more pencils and stuff.
[01:21:18.200 --> 01:21:19.200]   Donors choose.
[01:21:19.200 --> 01:21:20.200]   Donors choose is great.
[01:21:20.200 --> 01:21:22.240]   If you want to thank a teacher, go to Donors choose.
[01:21:22.240 --> 01:21:23.240]   Yeah.
[01:21:23.240 --> 01:21:24.240]   You guys don't donors choose?
[01:21:24.240 --> 01:21:25.240]   Oh, nice.
[01:21:25.240 --> 01:21:28.240]   What kinds of things have you have you have you podcasting equipment?
[01:21:28.240 --> 01:21:29.240]   Podcasting equipment.
[01:21:29.240 --> 01:21:31.640]   That's the first thing I'll learn.
[01:21:31.640 --> 01:21:32.640]   Old stuff.
[01:21:32.640 --> 01:21:33.640]   Yeah.
[01:21:33.640 --> 01:21:34.640]   Yeah.
[01:21:34.640 --> 01:21:35.640]   Donors choose.org.
[01:21:35.640 --> 01:21:39.000]   There are lots of classroom projects available if you go there.
[01:21:39.000 --> 01:21:41.840]   There's a classroom build the future.
[01:21:41.840 --> 01:21:43.280]   Teachers and students all over the US.
[01:21:43.280 --> 01:21:45.360]   It's a shame really that we have to do this.
[01:21:45.360 --> 01:21:46.360]   It is.
[01:21:46.360 --> 01:21:47.360]   It is.
[01:21:47.360 --> 01:21:48.360]   It's wonderful.
[01:21:48.360 --> 01:21:51.240]   I would warrant nothing more important than education.
[01:21:51.240 --> 01:21:53.960]   I see Chromebook requests out there all the time.
[01:21:53.960 --> 01:21:54.960]   Yeah.
[01:21:54.960 --> 01:21:55.960]   You know.
[01:21:55.960 --> 01:21:56.960]   Oh, that's good.
[01:21:56.960 --> 01:21:57.960]   If you have an old crumb.
[01:21:57.960 --> 01:21:58.960]   Well, I think Kevin soon will.
[01:21:58.960 --> 01:21:59.960]   Yeah.
[01:21:59.960 --> 01:22:02.120]   The cast off is getting rid of your slate.
[01:22:02.120 --> 01:22:03.120]   Yeah.
[01:22:03.120 --> 01:22:05.840]   Finally, get ready.
[01:22:05.840 --> 01:22:15.000]   Here they come 53 brand new non binary emoji on pixel devices.
[01:22:15.000 --> 01:22:20.440]   If your gender is they them whatever.
[01:22:20.440 --> 01:22:27.660]   This is Taylor here gender fluid non binary or gender ambiguous.
[01:22:27.660 --> 01:22:33.440]   So this is not necessarily part of the Unicode emoji spec.
[01:22:33.440 --> 01:22:38.280]   But if you have a human in an emoji, it makes sense if you're going to do an emoji library
[01:22:38.280 --> 01:22:41.800]   to include a variety of gender choices.
[01:22:41.800 --> 01:22:42.800]   You watch billions.
[01:22:42.800 --> 01:22:43.800]   I do.
[01:22:43.800 --> 01:22:47.920]   It's amazing how the characters have all become absolutely fluid in saying they.
[01:22:47.920 --> 01:22:50.960]   They have no problem saying they have no problem saying they it's all entered in.
[01:22:50.960 --> 01:22:51.960]   Yeah.
[01:22:51.960 --> 01:22:52.960]   It's a great example.
[01:22:52.960 --> 01:22:55.800]   Even when they're saying how much they hate Taylor, they still call her they.
[01:22:55.800 --> 01:22:56.800]   Yeah.
[01:22:56.800 --> 01:22:57.800]   Or him they.
[01:22:57.800 --> 01:22:58.800]   Yeah.
[01:22:58.800 --> 01:22:59.800]   They they.
[01:22:59.800 --> 01:23:00.800]   No, they still call them.
[01:23:00.800 --> 01:23:01.800]   Yes.
[01:23:01.800 --> 01:23:02.800]   It's hard to get used to.
[01:23:02.800 --> 01:23:06.680]   Their father like that was a big even source of tension.
[01:23:06.680 --> 01:23:07.680]   Their father.
[01:23:07.680 --> 01:23:08.760]   They were their father.
[01:23:08.760 --> 01:23:12.080]   I think we got to come up with a better pronoun.
[01:23:12.080 --> 01:23:13.080]   It's hard.
[01:23:13.080 --> 01:23:17.520]   The Germans did something wacky where they they like insert parenthesis around because the
[01:23:17.520 --> 01:23:18.520]   words are gender.
[01:23:18.520 --> 01:23:19.520]   The whole language is gender.
[01:23:19.520 --> 01:23:20.520]   Don't know what you can do about it.
[01:23:20.520 --> 01:23:22.320]   Can you just use neuter in German?
[01:23:22.320 --> 01:23:25.960]   No, well, because no, because then it's not the actual word.
[01:23:25.960 --> 01:23:26.960]   Okay.
[01:23:26.960 --> 01:23:27.960]   Yeah.
[01:23:27.960 --> 01:23:29.400]   If German got rid of genders, it would be a very happy.
[01:23:29.400 --> 01:23:32.760]   It's the hardest part of old many languages is knowing what gender is.
[01:23:32.760 --> 01:23:33.960]   There is this box.
[01:23:33.960 --> 01:23:35.440]   I don't know.
[01:23:35.440 --> 01:23:43.120]   Google duo group calling now rolling out in the US, Canada and India.
[01:23:43.120 --> 01:23:44.120]   Are you excited?
[01:23:44.120 --> 01:23:52.840]   Oh, I was the other thing I was thinking the Google hub max thing.
[01:23:52.840 --> 01:23:53.840]   Yeah.
[01:23:53.840 --> 01:23:56.280]   Isn't that wouldn't that be a great office device?
[01:23:56.280 --> 01:23:57.280]   Yes.
[01:23:57.280 --> 01:23:59.080]   For for for skyping and stuff.
[01:23:59.080 --> 01:24:00.080]   Yeah.
[01:24:00.080 --> 01:24:02.480]   You use it that way, Kevin.
[01:24:02.480 --> 01:24:06.600]   I use it with my son that way, not in my office, but down in the kitchen.
[01:24:06.600 --> 01:24:07.760]   Absolutely.
[01:24:07.760 --> 01:24:11.640]   I think it'd be great to have an office and just say, you know, call me there.
[01:24:11.640 --> 01:24:12.800]   I can still have my computer.
[01:24:12.800 --> 01:24:13.800]   I can still do.
[01:24:13.800 --> 01:24:14.800]   They have to use duo though.
[01:24:14.800 --> 01:24:15.800]   That's the problem.
[01:24:15.800 --> 01:24:16.800]   Yeah.
[01:24:16.800 --> 01:24:20.000]   Dude, I mean, it's iOS and Android.
[01:24:20.000 --> 01:24:26.880]   It's not like you can't Google Express is rebranding to Google shopping.
[01:24:26.880 --> 01:24:29.360]   I hope the EU doesn't hear about this.
[01:24:29.360 --> 01:24:30.960]   Wasn't it shopping before?
[01:24:30.960 --> 01:24:31.960]   Yeah.
[01:24:31.960 --> 01:24:35.920]   And then you got mad at them for favoring Google shopping and Google said, well, we don't.
[01:24:35.920 --> 01:24:39.680]   It's not it's not existed product, but now now express.
[01:24:39.680 --> 01:24:43.600]   But Express isn't the same as Google shopping because I don't know.
[01:24:43.600 --> 01:24:48.040]   See the problem you have with the Nest devices, I use my Amazon Echo stuff all the time.
[01:24:48.040 --> 01:24:49.960]   They buy them from Amazon.
[01:24:49.960 --> 01:24:51.200]   Google has this shopping experience.
[01:24:51.200 --> 01:24:56.080]   I think a lot of this is all about making them giving them parity on the on the assistant
[01:24:56.080 --> 01:24:57.640]   with Amazon and you can buy some.
[01:24:57.640 --> 01:24:59.320]   Speaking of Amazon shopping.
[01:24:59.320 --> 01:25:03.960]   The laborless Amazon store has just come to New York.
[01:25:03.960 --> 01:25:05.720]   Have you ever bought anything in one of those?
[01:25:05.720 --> 01:25:06.920]   No, the Amazon go.
[01:25:06.920 --> 01:25:08.720]   Yeah, the Amazon go is in New York.
[01:25:08.720 --> 01:25:09.720]   Have you tried it?
[01:25:09.720 --> 01:25:11.840]   No, I just announced yesterday I can't wait to go downtown.
[01:25:11.840 --> 01:25:13.240]   I've got all the real stuff.
[01:25:13.240 --> 01:25:15.240]   I've tried to steal stuff.
[01:25:15.240 --> 01:25:16.240]   Do they take cash now?
[01:25:16.240 --> 01:25:18.920]   I thought I read that they were going to take cash there.
[01:25:18.920 --> 01:25:22.120]   Oh, they have to have a person, I guess not machine.
[01:25:22.120 --> 01:25:23.120]   Not necessarily.
[01:25:23.120 --> 01:25:24.120]   Yeah.
[01:25:24.120 --> 01:25:28.720]   The idea was you just use your phone to register as you and then you walk out.
[01:25:28.720 --> 01:25:29.720]   That's it from the sandwich.
[01:25:29.720 --> 01:25:30.720]   Yeah.
[01:25:30.720 --> 01:25:31.720]   Yeah.
[01:25:31.720 --> 01:25:35.440]   But a lot of they don't want to preclude people who may not have the technology and so on
[01:25:35.440 --> 01:25:36.440]   so forth.
[01:25:36.440 --> 01:25:39.960]   I know in Philly they just pass laws and Amazon said, okay, we'll take cash at these stores
[01:25:39.960 --> 01:25:40.960]   as well.
[01:25:40.960 --> 01:25:41.960]   Right.
[01:25:41.960 --> 01:25:43.960]   Yeah, because there's all kinds of places in New York that are cashless and they're going
[01:25:43.960 --> 01:25:44.960]   to get in trouble now.
[01:25:44.960 --> 01:25:46.440]   Oh, interesting.
[01:25:46.440 --> 01:25:47.440]   Mm.
[01:25:47.440 --> 01:25:48.440]   No.
[01:25:48.440 --> 01:25:50.840]   That, my friends, is the Google change log?
[01:25:50.840 --> 01:25:56.520]   A couple of quick hits and then we will get your picks of the week.
[01:25:56.520 --> 01:25:57.520]   This is quick.
[01:25:57.520 --> 01:26:01.560]   Well, it's because you're here and because we're going to dinner and because there's,
[01:26:01.560 --> 01:26:02.560]   I don't know.
[01:26:02.560 --> 01:26:03.560]   We spent all day yesterday.
[01:26:03.560 --> 01:26:06.880]   It's been all day yesterday talking about Google.
[01:26:06.880 --> 01:26:07.880]   I don't know.
[01:26:07.880 --> 01:26:08.880]   No, no, that's true.
[01:26:08.880 --> 01:26:09.880]   All right.
[01:26:09.880 --> 01:26:10.880]   I'll throw in a few non-Google stories.
[01:26:10.880 --> 01:26:11.880]   Oh, let me see if there's anything.
[01:26:11.880 --> 01:26:12.880]   Verizon.
[01:26:12.880 --> 01:26:13.880]   I'm having such a good time.
[01:26:13.880 --> 01:26:16.320]   Verizon buys Yahoo.
[01:26:16.320 --> 01:26:19.440]   Verizon kills porn on Tumblr.
[01:26:19.440 --> 01:26:24.120]   Tumbles for Tumblr's user engagement numbers.
[01:26:24.120 --> 01:26:27.240]   Verizon says, we're going to sell Tumblr.
[01:26:27.240 --> 01:26:36.320]   And HUB says, we're going to buy Tumblr and bring the porn back and the story.
[01:26:36.320 --> 01:26:38.480]   It hasn't happened yet, but wouldn't that be funny?
[01:26:38.480 --> 01:26:41.760]   You remember it was started by one of the, it was one of the cool.
[01:26:41.760 --> 01:26:45.440]   David Carp, who was 17 years old when he started it.
[01:26:45.440 --> 01:26:46.440]   And you know what?
[01:26:46.440 --> 01:26:50.040]   I think it's really sad because Tumblr was a great thing.
[01:26:50.040 --> 01:26:51.080]   Was the easiest way to blog.
[01:26:51.080 --> 01:26:55.120]   It was kind of like the young person's version of Google's blogger.
[01:26:55.120 --> 01:26:56.120]   Yep.
[01:26:56.120 --> 01:26:57.120]   Yep.
[01:26:57.120 --> 01:26:58.120]   And I think it was a year and nicer and simpler.
[01:26:58.120 --> 01:26:59.800]   I had a Tumblr one for a long time.
[01:26:59.800 --> 01:27:01.800]   Social was great.
[01:27:01.800 --> 01:27:03.360]   It did have, I mean, what do you do?
[01:27:03.360 --> 01:27:07.840]   It did have that problem where it wasn't completely consumed by adult content.
[01:27:07.840 --> 01:27:12.320]   No, it wasn't necessarily awful porn.
[01:27:12.320 --> 01:27:14.480]   There's plenty of places for that.
[01:27:14.480 --> 01:27:20.680]   It was just artsier efforts at expression with skin.
[01:27:20.680 --> 01:27:22.680]   No.
[01:27:22.680 --> 01:27:23.680]   Yeah.
[01:27:23.680 --> 01:27:25.680]   Anyway, I don't know.
[01:27:25.680 --> 01:27:29.800]   It was a place for people to express their gender fluidity.
[01:27:29.800 --> 01:27:30.800]   That's pretty ridiculous.
[01:27:30.800 --> 01:27:31.800]   They loved it.
[01:27:31.800 --> 01:27:32.800]   That worked.
[01:27:32.800 --> 01:27:34.600]   No, they loved them.
[01:27:34.600 --> 01:27:37.040]   Let's see that just said so.
[01:27:37.040 --> 01:27:40.040]   You were too old.
[01:27:40.040 --> 01:27:42.040]   You were too old.
[01:27:42.040 --> 01:27:48.560]   Microsoft's window of solitaire has been inducted into the video game Hall of Fame.
[01:27:48.560 --> 01:27:51.680]   And many answers on that.
[01:27:51.680 --> 01:27:52.680]   Yep.
[01:27:52.680 --> 01:27:53.680]   Yeah, should be.
[01:27:53.680 --> 01:27:54.680]   Absolutely should be.
[01:27:54.680 --> 01:28:02.400]   Did you know that Maisie Williams are a stark to her fans?
[01:28:02.400 --> 01:28:03.720]   Has an app?
[01:28:03.720 --> 01:28:06.280]   It's called Maisie?
[01:28:06.280 --> 01:28:09.560]   I can't wait till it ends.
[01:28:09.560 --> 01:28:10.560]   Daisy.
[01:28:10.560 --> 01:28:11.560]   Daisy.
[01:28:11.560 --> 01:28:16.600]   An app community designed to help artists grow in their careers is launching worldwide.
[01:28:16.600 --> 01:28:21.520]   It was created by Maisie Williams and film producer Dom Santry.
[01:28:21.520 --> 01:28:26.240]   This was her tease this morning on Twitter.
[01:28:26.240 --> 01:28:28.240]   She said soon.
[01:28:28.240 --> 01:28:32.880]   Coming today everywhere, everywhere for everyone.
[01:28:32.880 --> 01:28:33.880]   Daisy.
[01:28:33.880 --> 01:28:39.720]   You know, I like people use their fame for a lot of things.
[01:28:39.720 --> 01:28:42.120]   That's kind of a nice way to use your fame.
[01:28:42.120 --> 01:28:43.120]   Yeah.
[01:28:43.120 --> 01:28:44.920]   It is.
[01:28:44.920 --> 01:28:51.480]   I'm avoiding saying anything, spoiling at all here, which means I just have to close
[01:28:51.480 --> 01:28:52.480]   my mouth.
[01:28:52.480 --> 01:28:54.880]   Weird announcements at Microsoft build.
[01:28:54.880 --> 01:28:56.640]   Here's the weirdest one.
[01:28:56.640 --> 01:29:01.240]   Windows 10 will be getting a Linux kernel.
[01:29:01.240 --> 01:29:06.280]   It's not the main kernel, but it's another kernel, an actual Linux kernel.
[01:29:06.280 --> 01:29:10.120]   For a while, Microsoft's offered something called the Windows subsystem for Linux, which
[01:29:10.120 --> 01:29:17.600]   lets you run, or has let you run Linux apps in emulation on Windows 10.
[01:29:17.600 --> 01:29:22.360]   That people complain, I complain, because it's so slow because of the emulation.
[01:29:22.360 --> 01:29:24.400]   So Microsoft said, all right, here's what we're going to do.
[01:29:24.400 --> 01:29:29.920]   We're going to include a specially crafted Linux kernel, and it's going to run 20 times
[01:29:29.920 --> 01:29:37.000]   faster, making people wonder how long before they get rid of the Microsoft kernel.
[01:29:37.000 --> 01:29:41.320]   Honestly, I wouldn't mind.
[01:29:41.320 --> 01:29:42.800]   Why do you think they did that?
[01:29:42.800 --> 01:29:46.320]   Not just for the performance, obviously, but why are they pushing for Linux here?
[01:29:46.320 --> 01:29:51.760]   Well, that's a really fascinating story, which we talk about every week on Windows Weekly.
[01:29:51.760 --> 01:29:52.760]   This is the new Microsoft.
[01:29:52.760 --> 01:29:56.160]   And by the way, it's been hugely successful, right?
[01:29:56.160 --> 01:29:59.640]   So briefly, a trillion dollar company.
[01:29:59.640 --> 01:30:04.040]   Satya Nadella is on the cover of Bloomberg Business Week this week in the clouds with
[01:30:04.040 --> 01:30:06.120]   a halo.
[01:30:06.120 --> 01:30:12.840]   And what he's really done is take a company that made all its money on Windows and Office
[01:30:12.840 --> 01:30:18.040]   and moved it to the cloud, and honestly, the thing that's most sacrilegious is deprecate
[01:30:18.040 --> 01:30:19.200]   Windows.
[01:30:19.200 --> 01:30:23.440]   For a long time, Nadella would say at these build events, we want to be everywhere our
[01:30:23.440 --> 01:30:27.040]   customers are, i.e. IOS, Android.
[01:30:27.040 --> 01:30:29.720]   But the best experience will be on Windows.
[01:30:29.720 --> 01:30:31.400]   He has dropped that second clause.
[01:30:31.400 --> 01:30:33.800]   It's now we just want to be wherever our customers are.
[01:30:33.800 --> 01:30:39.440]   They bought GitHub, so they have a large community of open source developers who are looking
[01:30:39.440 --> 01:30:42.800]   at them, trying to figure out, is it the old Microsoft or is it something that's going
[01:30:42.800 --> 01:30:44.600]   to be some sort of new Microsoft?
[01:30:44.600 --> 01:30:48.720]   They've announced that they're going to abandon the engine in their Edge browser and make
[01:30:48.720 --> 01:30:50.600]   it based on Chromium.
[01:30:50.600 --> 01:30:54.520]   And they've already committed back to the Chromium open source project.
[01:30:54.520 --> 01:30:59.760]   Many commits, including accessibility and language commits, so they're a good partner
[01:30:59.760 --> 01:31:05.600]   in an open source project, despite the fact that it's kind of a googly project.
[01:31:05.600 --> 01:31:07.080]   This is not your grandfather's Microsoft.
[01:31:07.080 --> 01:31:09.800]   This is a very different Microsoft.
[01:31:09.800 --> 01:31:14.720]   And they had a problem because if you're a developer and you know this, Kevin, you probably
[01:31:14.720 --> 01:31:17.160]   don't want to use Windows unless you're developing for .NET.
[01:31:17.160 --> 01:31:23.040]   If you're doing Visual Basic or F# or C# or something like that, if you're developing
[01:31:23.040 --> 01:31:26.720]   in a business, doing line of business applications, okay.
[01:31:26.720 --> 01:31:33.080]   But most developers want to either use a Mac or Linux because they want the tool chain.
[01:31:33.080 --> 01:31:36.800]   Microsoft has a very good IDE, the Visual Studio.
[01:31:36.800 --> 01:31:40.920]   But I think they were losing mindshare.
[01:31:40.920 --> 01:31:46.360]   And so they put this WSL in as a way to, as kind of a stop saying, look, you can use
[01:31:46.360 --> 01:31:50.320]   Windows and still your Linux, usually your Linux tools, but I'll have to tell you as
[01:31:50.320 --> 01:31:52.560]   it, Krustini is much better.
[01:31:52.560 --> 01:31:56.800]   Krustini at least gives you the full speed Linux because Chrome OS is Linux.
[01:31:56.800 --> 01:31:59.680]   So you already have a Linux kernel in Chrome OS.
[01:31:59.680 --> 01:32:04.720]   Microsoft was, you know, through this translation layer, emulating Linux calls using the Windows
[01:32:04.720 --> 01:32:06.720]   kernel and it was really, really slow.
[01:32:06.720 --> 01:32:07.720]   I missed it.
[01:32:07.720 --> 01:32:10.840]   What went so wrong with the demo at Microsoft?
[01:32:10.840 --> 01:32:12.640]   Oh, that was a very sad thing.
[01:32:12.640 --> 01:32:13.880]   We could talk a little bit about that.
[01:32:13.880 --> 01:32:17.520]   You know that this year is the 50th anniversary of the moon landing.
[01:32:17.520 --> 01:32:23.440]   So at the beginning of the Microsoft build conference, they had planned to do, to demonstrate
[01:32:23.440 --> 01:32:27.160]   the HoloLens to virtual reality headset.
[01:32:27.160 --> 01:32:32.520]   John Noel, who is Creative Director at Industrial Light and Magic, and by the way, the guy who
[01:32:32.520 --> 01:32:37.880]   with his brother Thomas wrote Photoshop, was joined by Andrew Chakin, who wrote the book
[01:32:37.880 --> 01:32:38.880]   Man in the Moon.
[01:32:38.880 --> 01:32:41.080]   He's a Apollo historian.
[01:32:41.080 --> 01:32:42.920]   You know, do we have the rehearsal video?
[01:32:42.920 --> 01:32:44.160]   Can we show that?
[01:32:44.160 --> 01:32:51.840]   So they had in rehearsal, they had done this moon landing in augmented reality.
[01:32:51.840 --> 01:32:53.680]   It was really cool.
[01:32:53.680 --> 01:32:59.240]   Turns out, since 1999, John Noel has been collecting data about the Apollo landing.
[01:32:59.240 --> 01:33:00.240]   He's an Apollo fanatic.
[01:33:00.240 --> 01:33:03.560]   He was seven years old when it happened, he'll never forget it.
[01:33:03.560 --> 01:33:04.560]   And he'd been collecting data.
[01:33:04.560 --> 01:33:10.200]   He found it in 1999, a trove of telemetry from NASA that gave extremely detailed, "Here's
[01:33:10.200 --> 01:33:11.200]   the lunar module.
[01:33:11.200 --> 01:33:12.880]   They're going to show it landing on the moon."
[01:33:12.880 --> 01:33:18.120]   That telemetry showed exactly every second by second everything the lunar module did as
[01:33:18.120 --> 01:33:21.520]   Neil Armstrong by hand piloted onto the moon.
[01:33:21.520 --> 01:33:26.320]   So he'd been collecting that telemetry and over a period of time, you know, in between
[01:33:26.320 --> 01:33:30.760]   Star Wars and Star Trek movies where he was doing the special effects, would create renderings
[01:33:30.760 --> 01:33:31.840]   of this.
[01:33:31.840 --> 01:33:34.440]   So he was really excited to be able to put it into HoloLens.
[01:33:34.440 --> 01:33:38.680]   So they used the Unreal Engine to generate it from Epic.
[01:33:38.680 --> 01:33:40.080]   That's a story in itself, by the way.
[01:33:40.080 --> 01:33:41.560]   I'll get to that in a second.
[01:33:41.560 --> 01:33:47.800]   And they created this very detailed, very accurate recreation of the Apollo launch and
[01:33:47.800 --> 01:33:49.280]   landing.
[01:33:49.280 --> 01:33:54.160]   They, as you can see here, because we're watching the video, worked fine in rehearsal.
[01:33:54.160 --> 01:33:58.960]   But right about when the keynote was about to begin at 8.30 in the morning, it crashed.
[01:33:58.960 --> 01:34:00.240]   It failed.
[01:34:00.240 --> 01:34:03.560]   They delayed the keynote for 20 minutes trying to get it back.
[01:34:03.560 --> 01:34:05.840]   Finally, they could delay no longer.
[01:34:05.840 --> 01:34:10.760]   They opened the keynote, Noel and Chakin walked on a stage.
[01:34:10.760 --> 01:34:15.040]   They introduced it and then they kind of hung their heads and said, "I guess it's not going
[01:34:15.040 --> 01:34:16.240]   to work," and walked off.
[01:34:16.240 --> 01:34:17.240]   Oh, no.
[01:34:17.240 --> 01:34:18.240]   It was very.
[01:34:18.240 --> 01:34:23.160]   It was a tragic moment.
[01:34:23.160 --> 01:34:25.080]   What they were going to do was really cool.
[01:34:25.080 --> 01:34:30.360]   And really, the culmination of poor John Noel, this is 20 years of work.
[01:34:30.360 --> 01:34:33.560]   So if you get a chance, go see.
[01:34:33.560 --> 01:34:34.560]   It was Yanko, actually.
[01:34:34.560 --> 01:34:41.240]   Yanko Rekkers, who had interviewed Noel about this and had a great story for Variety about
[01:34:41.240 --> 01:34:44.800]   it, was all ready to put it out when they did the demo.
[01:34:44.800 --> 01:34:51.640]   And then this is Neil Armstrong stepping out of the moon in augmented reality, exactly
[01:34:51.640 --> 01:34:55.000]   as it happened, timing-wise and everything.
[01:34:55.000 --> 01:34:56.480]   It was really a cool--
[01:34:56.480 --> 01:34:59.840]   So henceforth, any bad demo is what Apollo did?
[01:34:59.840 --> 01:35:01.240]   Maybe.
[01:35:01.240 --> 01:35:06.000]   It reminded me of Michael Bay's horrific experience at the Samsung launch where the teleprompter
[01:35:06.000 --> 01:35:07.000]   stopped working.
[01:35:07.000 --> 01:35:08.000]   He didn't know what to say.
[01:35:08.000 --> 01:35:11.760]   So he just, again, hung his head and left.
[01:35:11.760 --> 01:35:15.680]   The side story that kind of ties back to this Linux thing is that Epic-- Tim Sweeney,
[01:35:15.680 --> 01:35:20.160]   an Epic Games, of course, the creators of the Unreal Engine in Fortnite, very, very wealthy.
[01:35:20.160 --> 01:35:22.280]   Tim was an outspoken critic of Microsoft.
[01:35:22.280 --> 01:35:24.800]   He was really pissed off at the Microsoft Store.
[01:35:24.800 --> 01:35:29.000]   And he had the feeling, as many did, that Microsoft was going to start to move all application
[01:35:29.000 --> 01:35:32.840]   development into its store, require you to ship through the store.
[01:35:32.840 --> 01:35:35.000]   And he saw that as a real detriment.
[01:35:35.000 --> 01:35:38.040]   Windows gaming is a big, big part of what Epic does.
[01:35:38.040 --> 01:35:40.440]   And it was very critical somehow.
[01:35:40.440 --> 01:35:43.880]   And I think Rekkers talks about this as well in his piece.
[01:35:43.880 --> 01:35:49.000]   Somehow Nadella convinced him that it's a new Microsoft that they are not going to do
[01:35:49.000 --> 01:35:54.080]   this, that they welcome, they impress not only open source but third party gaming.
[01:35:54.080 --> 01:35:59.040]   And suddenly Sweeney is at Microsoft events talking about-- Holly was at the HoloLens
[01:35:59.040 --> 01:36:00.640]   2 announcement.
[01:36:00.640 --> 01:36:07.000]   And I highly profiled in this demo had it come off because they used the Epic Unreal
[01:36:07.000 --> 01:36:08.640]   Engine to do it.
[01:36:08.640 --> 01:36:14.280]   So that is kind of an example of how Microsoft shift to a more-- a different Microsoft, more
[01:36:14.280 --> 01:36:17.560]   open Microsoft has really changed the world.
[01:36:17.560 --> 01:36:19.800]   All right, let's take a break.
[01:36:19.800 --> 01:36:23.160]   I think if there's anything else you want to get in here, I'm glad you mentioned that
[01:36:23.160 --> 01:36:27.240]   because I felt so bad and I really wanted to show that video.
[01:36:27.240 --> 01:36:30.320]   If you get a chance, go to variety.com.
[01:36:30.320 --> 01:36:33.040]   Yanko Rekkers, he's doing a great job there.
[01:36:33.040 --> 01:36:34.320]   Your former colleague at GigaOM.
[01:36:34.320 --> 01:36:35.320]   I really like it.
[01:36:35.320 --> 01:36:36.320]   Yeah, he's fantastic.
[01:36:36.320 --> 01:36:40.720]   Yeah, he is-- so variety is Hollywood's trade paper.
[01:36:40.720 --> 01:36:45.760]   And he has become the gateway to the digital world for all of these Hollywood types, which
[01:36:45.760 --> 01:36:47.560]   he's-- and it couldn't pick a better person to do it.
[01:36:47.560 --> 01:36:50.360]   I think he's just really fantastic.
[01:36:50.360 --> 01:36:54.600]   OK, yeah, I think we pretty much covered.
[01:36:54.600 --> 01:36:55.960]   I'm not going to say anything about Facebook.
[01:36:55.960 --> 01:36:56.960]   Did you go to FA?
[01:36:56.960 --> 01:36:58.280]   No, I didn't.
[01:36:58.280 --> 01:36:59.280]   You didn't.
[01:36:59.280 --> 01:37:00.280]   I didn't.
[01:37:00.280 --> 01:37:01.280]   Why not?
[01:37:01.280 --> 01:37:05.280]   In the last two, three years, they had a lot about media and this year they didn't
[01:37:05.280 --> 01:37:06.280]   have any.
[01:37:06.280 --> 01:37:09.360]   Well, that's kind of what happened to Facebook, right?
[01:37:09.360 --> 01:37:12.080]   They turned their back on media and it's all local and all.
[01:37:12.080 --> 01:37:13.880]   Or it's too scary a topic.
[01:37:13.880 --> 01:37:14.880]   Yeah, maybe.
[01:37:14.880 --> 01:37:20.360]   So I watched the keynote.
[01:37:20.360 --> 01:37:25.680]   Benedict Evans thinks that he still believes that this was a major change.
[01:37:25.680 --> 01:37:26.680]   Yep, that's real.
[01:37:26.680 --> 01:37:31.840]   That Zuckerberg is not blowing smoke when he says privacy is going to be our number one
[01:37:31.840 --> 01:37:32.840]   watchword.
[01:37:32.840 --> 01:37:35.000]   What was the line that was behind him on the stage?
[01:37:35.000 --> 01:37:36.200]   Privacy is the future.
[01:37:36.200 --> 01:37:40.320]   It's the future.
[01:37:40.320 --> 01:37:46.400]   It's kind of hard for companies like Google and Facebook that are built on tracking.
[01:37:46.400 --> 01:37:50.440]   So here's the thing that finally occurred to me is the problem is, well, two things
[01:37:50.440 --> 01:37:51.440]   I'll say.
[01:37:51.440 --> 01:37:55.760]   One is the problem is that Facebook is the first company that built its company around
[01:37:55.760 --> 01:37:57.680]   the behaviors of people.
[01:37:57.680 --> 01:37:58.680]   Right.
[01:37:58.680 --> 01:37:59.680]   Right.
[01:37:59.680 --> 01:38:01.680]   Google didn't.
[01:38:01.680 --> 01:38:03.080]   Apple didn't.
[01:38:03.080 --> 01:38:04.440]   Amazon didn't.
[01:38:04.440 --> 01:38:09.360]   Well every company has in its sense because you're trying to acquire customers.
[01:38:09.360 --> 01:38:11.240]   Google is purely human behavior.
[01:38:11.240 --> 01:38:14.280]   In fact, all of Facebook's content comes from its users.
[01:38:14.280 --> 01:38:16.000]   They generate no content of their own.
[01:38:16.000 --> 01:38:21.120]   And the second thing is that I think that we in the technology covering world and media
[01:38:21.120 --> 01:38:22.280]   are doing it wrong.
[01:38:22.280 --> 01:38:26.640]   At this point it's shifted and we should be covering not the technology but people's
[01:38:26.640 --> 01:38:29.120]   behavior on the technology.
[01:38:29.120 --> 01:38:30.120]   I think you're right.
[01:38:30.120 --> 01:38:33.080]   And so that's a radically different story.
[01:38:33.080 --> 01:38:37.000]   I've been observing that shift and I expected that shift all along.
[01:38:37.000 --> 01:38:40.600]   And I always liken it to this, remember all the stereo magazines and the 50s where all
[01:38:40.600 --> 01:38:45.040]   they talked about was, you know, how much power you'd get without distortion, your total
[01:38:45.040 --> 01:38:46.280]   RMS.
[01:38:46.280 --> 01:38:47.600]   It wasn't at all about the music.
[01:38:47.600 --> 01:38:52.400]   And then Rolling Stone comes along and suddenly nobody's talking about the hardware.
[01:38:52.400 --> 01:38:53.800]   It's all about the content.
[01:38:53.800 --> 01:38:55.200]   And I think that's another shift now.
[01:38:55.200 --> 01:38:59.160]   It's not about anything but about people and how they use it.
[01:38:59.160 --> 01:39:03.000]   It's not about speeds and feeds, processors, memory, none of that.
[01:39:03.000 --> 01:39:06.920]   And the bad and the good both is from human behavior on technology rather than the technology
[01:39:06.920 --> 01:39:07.920]   itself.
[01:39:07.920 --> 01:39:09.240]   But we still think it's a technology story.
[01:39:09.240 --> 01:39:10.240]   Yeah.
[01:39:10.240 --> 01:39:12.760]   And finally- Well it's killing me because I couldn't care less about people on much more
[01:39:12.760 --> 01:39:13.760]   than the hardware.
[01:39:13.760 --> 01:39:14.760]   It's just a smart one.
[01:39:14.760 --> 01:39:16.480]   So it's killing me.
[01:39:16.480 --> 01:39:18.600]   I mean honestly, this whole network was built.
[01:39:18.600 --> 01:39:19.920]   Well, your staff was great.
[01:39:19.920 --> 01:39:20.920]   Well, it's true though.
[01:39:20.920 --> 01:39:24.320]   I mean this whole network was built on which computers should you find.
[01:39:24.320 --> 01:39:27.840]   And what happens when it crashes?
[01:39:27.840 --> 01:39:31.080]   But we've been, you know, look at this show as a perfect example because this show, more
[01:39:31.080 --> 01:39:34.520]   than any show we've ever done on Twitch was about behavior, humans.
[01:39:34.520 --> 01:39:35.520]   Yeah.
[01:39:35.520 --> 01:39:38.960]   And we hardly ever talk about speeds and feeds.
[01:39:38.960 --> 01:39:44.000]   So the last piece of this is that I hear all the time that, you know, the Internet is
[01:39:44.000 --> 01:39:46.440]   a medium and just another medium.
[01:39:46.440 --> 01:39:49.240]   And I believe pretty firmly that it's not.
[01:39:49.240 --> 01:39:51.080]   I mean that clear on the show often times.
[01:39:51.080 --> 01:39:53.880]   But then it finally occurred to me that the rules turned around.
[01:39:53.880 --> 01:39:58.080]   That media is now a subset, media are a subset of the Internet.
[01:39:58.080 --> 01:39:59.080]   Yes.
[01:39:59.080 --> 01:40:02.120]   And along with other sectors, right?
[01:40:02.120 --> 01:40:05.040]   So media is now 80% on the Internet.
[01:40:05.040 --> 01:40:07.320]   Communication is 100% on the Internet.
[01:40:07.320 --> 01:40:09.480]   Finance is picking number 20% on the Internet.
[01:40:09.480 --> 01:40:11.560]   Crime is 10% on the Internet.
[01:40:11.560 --> 01:40:17.160]   Every sector of society, every industry is being brought into the Internet.
[01:40:17.160 --> 01:40:19.160]   So it becomes synonymous with the net.
[01:40:19.160 --> 01:40:24.280]   That just changes how we cover the net because it's no longer how the net is changing us,
[01:40:24.280 --> 01:40:25.280]   the net is us.
[01:40:25.280 --> 01:40:26.280]   The net is society.
[01:40:26.280 --> 01:40:27.280]   Right.
[01:40:27.280 --> 01:40:32.280]   So that's a very different way to think about and cover this as a result.
[01:40:32.280 --> 01:40:37.400]   But media and technology still treat it as a technology, a widget, a gadget.
[01:40:37.400 --> 01:40:38.400]   It's a sector.
[01:40:38.400 --> 01:40:39.400]   Right.
[01:40:39.400 --> 01:40:40.400]   It's not a sector.
[01:40:40.400 --> 01:40:41.400]   It's everything.
[01:40:41.400 --> 01:40:42.400]   It's everything.
[01:40:42.400 --> 01:40:43.400]   We're a sector of it.
[01:40:43.400 --> 01:40:44.400]   Yes.
[01:40:44.400 --> 01:40:45.400]   Not in a sector.
[01:40:45.400 --> 01:40:46.400]   And I for one, welcome the new board.
[01:40:46.400 --> 01:40:47.400]   Yeah.
[01:40:47.400 --> 01:40:48.400]   I knew this was going to happen.
[01:40:48.400 --> 01:40:50.600]   I've, because I've had a front row seat at that transformation.
[01:40:50.600 --> 01:40:56.080]   Really the way this podcast network has shifted, we still do a show on computer hardware.
[01:40:56.080 --> 01:40:59.440]   But increasingly, Twig and all the shows are not about computer hardware.
[01:40:59.440 --> 01:41:02.280]   I remember the early days of, how long is this show been going on now?
[01:41:02.280 --> 01:41:03.280]   12 years.
[01:41:03.280 --> 01:41:04.280]   No, this show.
[01:41:04.280 --> 01:41:06.120]   This show, we're episode 507, so 10 years.
[01:41:06.120 --> 01:41:07.120]   I can't do the math, okay.
[01:41:07.120 --> 01:41:08.320]   Almost a little less than 10 years.
[01:41:08.320 --> 01:41:09.320]   Really?
[01:41:09.320 --> 01:41:10.320]   Jesus, man.
[01:41:10.320 --> 01:41:11.320]   I know.
[01:41:11.320 --> 01:41:12.320]   I know.
[01:41:12.320 --> 01:41:15.680]   But I remember, you know, as I used to, we used to come on after Twig.
[01:41:15.680 --> 01:41:21.440]   And there'd be times in Twig where you'd be talking about the refresh rate of a new
[01:41:21.440 --> 01:41:22.440]   Samsung screen.
[01:41:22.440 --> 01:41:23.440]   Yeah.
[01:41:23.440 --> 01:41:24.680]   And you were in heaven.
[01:41:24.680 --> 01:41:27.640]   I kind of, I kind of am sad about it.
[01:41:27.640 --> 01:41:28.880]   But this is what happens with age.
[01:41:28.880 --> 01:41:32.040]   You'll understand this in about 20 years, Kevin.
[01:41:32.040 --> 01:41:36.720]   This is one of the things that happens with age is the world passes you by a little bit,
[01:41:36.720 --> 01:41:37.720]   right?
[01:41:37.720 --> 01:41:40.600]   And I'm still, I still embrace and I love, that's why I keep saying, why don't you just
[01:41:40.600 --> 01:41:42.120]   put Linux on it?
[01:41:42.120 --> 01:41:44.080]   Just put Linux on it.
[01:41:44.080 --> 01:41:46.520]   Everything can be better if you just put Linux on it.
[01:41:46.520 --> 01:41:49.480]   Because I still belong to that world and nobody cares about what operate.
[01:41:49.480 --> 01:41:50.480]   And that's kind of what Microsoft's saying.
[01:41:50.480 --> 01:41:52.280]   Nobody cares what your operating system is.
[01:41:52.280 --> 01:41:53.280]   It's irrelevant.
[01:41:53.280 --> 01:41:55.280]   That isn't even relevant.
[01:41:55.280 --> 01:41:57.720]   The network is the operating system.
[01:41:57.720 --> 01:42:00.000]   The network is the cloud.
[01:42:00.000 --> 01:42:02.280]   And you know, son used to say this for years.
[01:42:02.280 --> 01:42:05.520]   They'd say the network is a computer that was the sun microsystem slogan.
[01:42:05.520 --> 01:42:08.880]   And I always thought that doesn't make any sense at all.
[01:42:08.880 --> 01:42:10.080]   It's makes sense now, doesn't it?
[01:42:10.080 --> 01:42:11.080]   Now it does.
[01:42:11.080 --> 01:42:12.080]   Yeah.
[01:42:12.080 --> 01:42:13.080]   Yeah.
[01:42:13.080 --> 01:42:14.080]   They were right.
[01:42:14.080 --> 01:42:16.080]   This is why I really don't miss doing phone reviews.
[01:42:16.080 --> 01:42:17.280]   Like I used to at Yigong.
[01:42:17.280 --> 01:42:18.280]   Oh, I don't miss either.
[01:42:18.280 --> 01:42:19.280]   Except I have to.
[01:42:19.280 --> 01:42:20.280]   It's all okay.
[01:42:20.280 --> 01:42:21.280]   What are the specs?
[01:42:21.280 --> 01:42:22.280]   You know, compare.
[01:42:22.280 --> 01:42:23.920]   Give me a speed test.
[01:42:23.920 --> 01:42:29.400]   It feels to be honest, a little consumer promoting consumerism.
[01:42:29.400 --> 01:42:30.720]   I don't know what the word would be.
[01:42:30.720 --> 01:42:36.280]   It seems a little bit talking about the shiny new thing and ignoring the impact, what it
[01:42:36.280 --> 01:42:39.640]   means in society and how we operate opportunities as well.
[01:42:39.640 --> 01:42:40.640]   What can you do with it?
[01:42:40.640 --> 01:42:41.640]   Yeah.
[01:42:41.640 --> 01:42:47.200]   So you look at Google's keynote, come back around before we go into the last commercial.
[01:42:47.200 --> 01:42:50.000]   And what were the emphasizing humanity?
[01:42:50.000 --> 01:42:51.000]   Yep.
[01:42:51.000 --> 01:42:52.000]   Right?
[01:42:52.000 --> 01:42:54.440]   The woman and India, they're emphasizing the speech stuff.
[01:42:54.440 --> 01:42:55.640]   That's a really good point.
[01:42:55.640 --> 01:42:56.640]   Right?
[01:42:56.640 --> 01:42:58.480]   And they're trying to say we're helping.
[01:42:58.480 --> 01:42:59.480]   We're helping.
[01:42:59.480 --> 01:43:00.480]   We're Google.
[01:43:00.480 --> 01:43:01.480]   We're helping.
[01:43:01.480 --> 01:43:02.480]   Yes.
[01:43:02.480 --> 01:43:03.480]   But no, that's a very good point.
[01:43:03.480 --> 01:43:05.280]   It's not about the bits and the bytes.
[01:43:05.280 --> 01:43:06.280]   It's about what you do with it.
[01:43:06.280 --> 01:43:08.040]   This is a developers conference, right?
[01:43:08.040 --> 01:43:11.760]   If any place was going to get into the nitty gritty of the code, of course, they did more
[01:43:11.760 --> 01:43:13.920]   in the developers' keynote.
[01:43:13.920 --> 01:43:14.920]   But at the main keynote--
[01:43:14.920 --> 01:43:19.040]   To all the big tech companies realize this, when Google now clearly does, I guess Microsoft
[01:43:19.040 --> 01:43:20.520]   sort of does.
[01:43:20.520 --> 01:43:21.920]   It's really been their pivot.
[01:43:21.920 --> 01:43:22.920]   Does Apple--
[01:43:22.920 --> 01:43:23.920]   Mm.
[01:43:23.920 --> 01:43:24.920]   They're still--
[01:43:24.920 --> 01:43:25.920]   I think they're still tied--
[01:43:25.920 --> 01:43:26.920]   I think they're through.
[01:43:26.920 --> 01:43:27.920]   They say we're moving to services.
[01:43:27.920 --> 01:43:30.240]   But I think they're still tied in the notion of selling a product.
[01:43:30.240 --> 01:43:31.240]   They are.
[01:43:31.240 --> 01:43:32.240]   Facebook?
[01:43:32.240 --> 01:43:33.240]   Yeah.
[01:43:33.240 --> 01:43:34.240]   Yeah.
[01:43:34.240 --> 01:43:35.240]   Well, Facebook, Facebook, that was the problem for Facebook.
[01:43:35.240 --> 01:43:38.240]   They were the first in and didn't realize how complex this is.
[01:43:38.240 --> 01:43:39.240]   Amazon.
[01:43:39.240 --> 01:43:40.240]   I think they do.
[01:43:40.240 --> 01:43:41.400]   I think they do know.
[01:43:41.400 --> 01:43:46.720]   Because their number one profit center is Amazon Web Services, not what they sell.
[01:43:46.720 --> 01:43:51.120]   And I think the selling is really just an adjunct to what they really are.
[01:43:51.120 --> 01:43:52.120]   And I think they know it.
[01:43:52.120 --> 01:43:55.680]   I think Bezos is very smart.
[01:43:55.680 --> 01:43:57.760]   So it's sort of like green.
[01:43:57.760 --> 01:43:59.200]   It's made of people.
[01:43:59.200 --> 01:44:00.200]   Mm-hmm.
[01:44:00.200 --> 01:44:01.440]   Well, good insight.
[01:44:01.440 --> 01:44:02.440]   That's nice.
[01:44:02.440 --> 01:44:03.440]   Yeah.
[01:44:03.440 --> 01:44:04.440]   And I completely agree.
[01:44:04.440 --> 01:44:08.760]   And I think it's-- changing that context helps a lot in understanding what's going on
[01:44:08.760 --> 01:44:12.240]   and what we can do and should do about it.
[01:44:12.240 --> 01:44:13.240]   Right.
[01:44:13.240 --> 01:44:16.600]   And whom we should blame, which is not always technology companies, it's--
[01:44:16.600 --> 01:44:18.400]   we have met the enemy and it's us.
[01:44:18.400 --> 01:44:23.800]   And you know, I mean, for somebody like me to watch Intel just kind of fade into the
[01:44:23.800 --> 01:44:27.920]   sunset is such a shock to the system.
[01:44:27.920 --> 01:44:28.920]   Yeah.
[01:44:28.920 --> 01:44:31.760]   And frankly, I think that's what's happening in Apple, too, to be honest with you.
[01:44:31.760 --> 01:44:37.360]   I think Apple's fading into the sunset because what they've realized is we offer nothing
[01:44:37.360 --> 01:44:40.160]   except brand and fashion.
[01:44:40.160 --> 01:44:45.080]   We don't actually offer any tangible benefit.
[01:44:45.080 --> 01:44:49.200]   And Google to say, oh, what we really have to offer is a better life for you.
[01:44:49.200 --> 01:44:50.200]   That's huge.
[01:44:50.200 --> 01:44:51.200]   Yes.
[01:44:51.200 --> 01:44:53.680]   Apple's pretending that if you buy that new thing, your life will be better.
[01:44:53.680 --> 01:44:55.840]   Google's saying use this and your life will be better.
[01:44:55.840 --> 01:44:56.840]   Right.
[01:44:56.840 --> 01:44:59.640]   And Facebook was trying to say, if you all connect with each other, life will be better.
[01:44:59.640 --> 01:45:00.640]   Oops.
[01:45:00.640 --> 01:45:01.640]   Well, you're not so good at that, Pete.
[01:45:01.640 --> 01:45:02.640]   [LAUGHTER]
[01:45:02.640 --> 01:45:03.640]   Turns out not.
[01:45:03.640 --> 01:45:04.640]   Right.
[01:45:04.640 --> 01:45:06.840]   So Google's coming in and saying, well, we'll do a better job of that.
[01:45:06.840 --> 01:45:07.840]   We'll--
[01:45:07.840 --> 01:45:12.400]   It's a challenge though because what Google's offering is something essentially non-human
[01:45:12.400 --> 01:45:14.880]   as an intermediary, which is AI.
[01:45:14.880 --> 01:45:17.680]   Which in a time when humans are messing up the world, not about--
[01:45:17.680 --> 01:45:18.680]   Maybe.
[01:45:18.680 --> 01:45:20.680]   Yeah, that worries me.
[01:45:20.680 --> 01:45:24.960]   I do think that that's going to be one of the pitches of the future is let the machines
[01:45:24.960 --> 01:45:25.960]   do it.
[01:45:25.960 --> 01:45:27.920]   Why don't we talk about this a few months ago?
[01:45:27.920 --> 01:45:28.920]   Let go.
[01:45:28.920 --> 01:45:32.560]   This is the point of David Weinberger's book, which by the way, if you'd like to come,
[01:45:32.560 --> 01:45:40.680]   I'll be interviewing David Weinberger next Wednesday in New York at book culture on the
[01:45:40.680 --> 01:45:44.840]   Upper West Side, you go on and get tickets there, you should, just search for me and
[01:45:44.840 --> 01:45:46.640]   him and New York and you'll find it.
[01:45:46.640 --> 01:45:49.240]   He wrote the brilliant Clue Train manifest.
[01:45:49.240 --> 01:45:50.440]   He was co-author of Clue Train.
[01:45:50.440 --> 01:45:52.240]   He wrote to us wall pieces loosely joined.
[01:45:52.240 --> 01:45:53.240]   He wrote "Too Big to Know."
[01:45:53.240 --> 01:45:54.240]   He's absolutely brilliant.
[01:45:54.240 --> 01:45:56.200]   And his new book is called?
[01:45:56.200 --> 01:46:00.920]   Everyday Chaos, about AI, about just what you just said, is the machines are going to
[01:46:00.920 --> 01:46:02.760]   do it better than we can.
[01:46:02.760 --> 01:46:04.680]   Should we let them?
[01:46:04.680 --> 01:46:06.200]   A little choice.
[01:46:06.200 --> 01:46:08.680]   They're going to predict better than we can.
[01:46:08.680 --> 01:46:09.680]   Right?
[01:46:09.680 --> 01:46:13.560]   So, rethinking knowledge now that the facts aren't the facts.
[01:46:13.560 --> 01:46:14.800]   Oh, that's the, this is too big.
[01:46:14.800 --> 01:46:15.800]   That was called the everyday chaos.
[01:46:15.800 --> 01:46:16.800]   Oh, okay.
[01:46:16.800 --> 01:46:18.800]   The new one is everyday chaos.
[01:46:18.800 --> 01:46:19.800]   Wow.
[01:46:19.800 --> 01:46:21.920]   Carson, let's get him on.
[01:46:21.920 --> 01:46:23.160]   Please give me his info.
[01:46:23.160 --> 01:46:24.160]   I will do that.
[01:46:24.160 --> 01:46:25.320]   We should have a triangulation with him.
[01:46:25.320 --> 01:46:29.240]   I'm also, I think I promised to give you Craig Newmark.
[01:46:29.240 --> 01:46:30.240]   Yes.
[01:46:30.240 --> 01:46:34.520]   You've made so many promises.
[01:46:34.520 --> 01:46:37.600]   I don't know what's going on with you and Carson, but I think you guys should just go
[01:46:37.600 --> 01:46:40.760]   get a room and hash it out.
[01:46:40.760 --> 01:46:41.680]   That is so funny.
[01:46:41.680 --> 01:46:44.960]   Everyday chaos book dot com.
[01:46:44.960 --> 01:46:46.440]   Interesting.
[01:46:46.440 --> 01:46:47.440]   Yeah.
[01:46:47.440 --> 01:46:49.960]   I like it that you're in touch with big thinkers.
[01:46:49.960 --> 01:46:52.480]   You really, uh, notice he said that I'm not one.
[01:46:52.480 --> 01:46:53.640]   I've been touched with that.
[01:46:53.640 --> 01:46:54.640]   You kind of know them.
[01:46:54.640 --> 01:46:56.600]   Can introduce them to us.
[01:46:56.600 --> 01:46:59.240]   No, you're a big, you're a big thing.
[01:46:59.240 --> 01:47:00.240]   No, I'm not.
[01:47:00.240 --> 01:47:01.240]   He's your big thing.
[01:47:01.240 --> 01:47:02.240]   I hang out with him.
[01:47:02.240 --> 01:47:04.880]   Carson, however, you're a big thinker at least by us most.
[01:47:04.880 --> 01:47:05.880]   By association.
[01:47:05.880 --> 01:47:08.560]   He's a BTBA.
[01:47:08.560 --> 01:47:12.360]   Um, yeah, let's take a break.
[01:47:12.360 --> 01:47:17.120]   If you guys have a pick or anything you want to show the world, we can do that in a second.
[01:47:17.120 --> 01:47:20.800]   Our show today brought to you by Express VPN.
[01:47:20.800 --> 01:47:26.200]   I am glad sometimes we still talk about technology and how to use technology to protect your
[01:47:26.200 --> 01:47:32.760]   privacy, to protect your hardware, to protect yourself against cyber crime, crime stolen
[01:47:32.760 --> 01:47:33.760]   data.
[01:47:33.760 --> 01:47:35.480]   Uh, VPN is the best way to do it.
[01:47:35.480 --> 01:47:40.200]   If you're on an open wifi network, you're on the same network as anybody else in that
[01:47:40.200 --> 01:47:43.600]   area, including the guy in the hoodie in the corner there.
[01:47:43.600 --> 01:47:50.080]   If you are on even your home network, all your traffic is being watched by internet service
[01:47:50.080 --> 01:47:57.560]   provider and we know ISPs routinely sell that data on protect yourself, protect your privacy
[01:47:57.560 --> 01:48:01.520]   by encrypting your internet connection with a virtual private network and the number one
[01:48:01.520 --> 01:48:07.000]   VPN, according to tech radar is Express VPN.
[01:48:07.000 --> 01:48:08.000]   They do it all right.
[01:48:08.000 --> 01:48:09.000]   No logging.
[01:48:09.000 --> 01:48:10.840]   They keep track of nothing.
[01:48:10.840 --> 01:48:12.520]   You are private.
[01:48:12.520 --> 01:48:17.280]   You are encrypted from wherever you are all the way out to the Express VPN server.
[01:48:17.280 --> 01:48:18.760]   They have servers all over the world.
[01:48:18.760 --> 01:48:23.160]   So that means you can emerge on the public internet in the country of your choosing.
[01:48:23.160 --> 01:48:26.000]   All those servers means there's lots of bandwidth.
[01:48:26.000 --> 01:48:29.160]   So Express VPN is great that way too.
[01:48:29.160 --> 01:48:30.760]   It's fast.
[01:48:30.760 --> 01:48:33.360]   You won't know you're on a VPN and it's easy to use.
[01:48:33.360 --> 01:48:38.240]   They have Express VPN apps for Windows for Mac for every phone iOS and Android, every
[01:48:38.240 --> 01:48:42.560]   tablet and those apps one click and you're protected.
[01:48:42.560 --> 01:48:44.680]   Surf safely on public wifi.
[01:48:44.680 --> 01:48:47.480]   Stop being snooped on her having your data attacked.
[01:48:47.480 --> 01:48:49.920]   It's less than $7 a month.
[01:48:49.920 --> 01:48:55.080]   Express VPN protect your online activity today and get three extra months free with a one
[01:48:55.080 --> 01:49:02.080]   year package.
[01:49:02.080 --> 01:49:04.400]   Just go to express VPN dot com slash twig ex PR ESV p n dot com slash twig for three extra
[01:49:04.400 --> 01:49:05.400]   months free.
[01:49:05.400 --> 01:49:10.440]   When you buy that one year package, we thank Express VPN for keeping us safe, keeping us
[01:49:10.440 --> 01:49:13.920]   private and for keeping twit flowing.
[01:49:13.920 --> 01:49:17.640]   Thank you Express VPN and we thank you for using that URL and letting them know you heard
[01:49:17.640 --> 01:49:18.640]   it here.
[01:49:18.640 --> 01:49:21.880]   Express VPN dot com slash twig.
[01:49:21.880 --> 01:49:25.760]   Kevin TOEFL Python guru.
[01:49:25.760 --> 01:49:27.480]   Oh, that's putting it.
[01:49:27.480 --> 01:49:28.480]   That's over steady.
[01:49:28.480 --> 01:49:29.480]   Quite a bit.
[01:49:29.480 --> 01:49:36.200]   I've only taken two classes and mostly Java, but yeah, for our final project.
[01:49:36.200 --> 01:49:39.080]   And this is my pick, which kind of goes so cool.
[01:49:39.080 --> 01:49:45.440]   The theme of making technology more helpful to people for our final project, a group of
[01:49:45.440 --> 01:49:53.800]   myself and three other students took some open source software from open APS and we quote
[01:49:53.800 --> 01:49:57.720]   unquote hacked diabetes basically made a pancreas in the cloud using Python.
[01:49:57.720 --> 01:50:00.360]   We started with Java, but quickly switched over to Python.
[01:50:00.360 --> 01:50:02.360]   It's pancreas service.
[01:50:02.360 --> 01:50:03.360]   Yeah.
[01:50:03.360 --> 01:50:04.360]   Yeah, basically.
[01:50:04.360 --> 01:50:09.440]   Yeah, it's rather it's I learned an awful lot about diabetes for one thing and just also
[01:50:09.440 --> 01:50:12.000]   obviously about programming in general.
[01:50:12.000 --> 01:50:16.240]   So what you're seeing there is a real time chart from one of the actual students, a
[01:50:16.240 --> 01:50:22.080]   glucometer, which we read the data from put it up in the cloud on a MongoDB and then suck
[01:50:22.080 --> 01:50:26.840]   it back down into Google sheets using APIs with Python and also into data studio.
[01:50:26.840 --> 01:50:31.400]   So you can track blood sugars and insulin levels in a graph, which you can even share
[01:50:31.400 --> 01:50:34.080]   that with your practitioner or your health practitioner.
[01:50:34.080 --> 01:50:38.080]   But additionally, open APS software, it runs on a Raspberry Pi.
[01:50:38.080 --> 01:50:44.280]   That's part of this loop and it literally adjusts your insulin, insulin levels in quote
[01:50:44.280 --> 01:50:45.280]   unquote real time.
[01:50:45.280 --> 01:50:49.880]   It's every five minutes as opposed to what the student was doing before and that was
[01:50:49.880 --> 01:50:53.160]   manually hitting buttons to add insulin throughout the day.
[01:50:53.160 --> 01:50:58.560]   So it was a really fascinating project from, you know, from a human standpoint as well
[01:50:58.560 --> 01:51:00.920]   as a technical standpoint, I learned a ton.
[01:51:00.920 --> 01:51:05.760]   I also made a total goofball out of myself in the video overview of the project.
[01:51:05.760 --> 01:51:08.160]   I wanted to, I want to be these students.
[01:51:08.160 --> 01:51:11.480]   They're going for jobs and internships and I am not.
[01:51:11.480 --> 01:51:14.320]   So I wanted to let them discuss the technical part.
[01:51:14.320 --> 01:51:18.080]   So I basically played a goofy news announcer in our video.
[01:51:18.080 --> 01:51:20.000]   This is so cute, Kevin.
[01:51:20.000 --> 01:51:21.000]   That is awesome.
[01:51:21.000 --> 01:51:23.440]   I even put little CNN breaking news.
[01:51:23.440 --> 01:51:28.040]   But yeah, pretty much a goofball there.
[01:51:28.040 --> 01:51:30.760]   And I just interviewed these guys about the project.
[01:51:30.760 --> 01:51:31.760]   Wow.
[01:51:31.760 --> 01:51:33.400]   Makes me want to go back to school.
[01:51:33.400 --> 01:51:35.160]   Where did you do this course, Kevin?
[01:51:35.160 --> 01:51:37.160]   Oh, I'm at my local community college.
[01:51:37.160 --> 01:51:38.720]   Montgomery County Community College.
[01:51:38.720 --> 01:51:39.720]   Wow.
[01:51:39.720 --> 01:51:40.720]   Yeah.
[01:51:40.720 --> 01:51:41.720]   That's really ambitious.
[01:51:41.720 --> 01:51:42.720]   Fantastic.
[01:51:42.720 --> 01:51:43.720]   Very neat.
[01:51:43.720 --> 01:51:44.720]   Yeah.
[01:51:44.720 --> 01:51:46.080]   This is the second comp sci class that you take there.
[01:51:46.080 --> 01:51:50.440]   And the second half of this class, you literally do a project full time.
[01:51:50.440 --> 01:51:51.440]   There is no more class.
[01:51:51.440 --> 01:51:54.240]   It's literally doing a massive project.
[01:51:54.240 --> 01:51:59.600]   And then everybody presents their project in the atrium of the main building on the last
[01:51:59.600 --> 01:52:03.240]   day of school to the president, the university and other teachers.
[01:52:03.240 --> 01:52:04.920]   And companies come in as well.
[01:52:04.920 --> 01:52:07.120]   And it was such a great experience.
[01:52:07.120 --> 01:52:11.600]   And my hope is that I've been talking to them about being an adjunct instructor for
[01:52:11.600 --> 01:52:12.920]   the intro to programming class.
[01:52:12.920 --> 01:52:14.000]   You should be great at that.
[01:52:14.000 --> 01:52:15.000]   Yeah.
[01:52:15.000 --> 01:52:17.800]   I would love to do it because this is why I went back to school.
[01:52:17.800 --> 01:52:18.800]   Not just to learn.
[01:52:18.800 --> 01:52:22.640]   I could have done that online, but to mentor and share real world experience and what it
[01:52:22.640 --> 01:52:24.600]   was like to work at Google.
[01:52:24.600 --> 01:52:30.480]   I hope I gave back more than I got from this class to the students and the college as a
[01:52:30.480 --> 01:52:31.480]   whole.
[01:52:31.480 --> 01:52:32.480]   I loved it.
[01:52:32.480 --> 01:52:33.480]   You know, it reminds me.
[01:52:33.480 --> 01:52:40.440]   I watched before the Microsoft event on Monday, we watched the Imagine Cup, which is three
[01:52:40.440 --> 01:52:48.280]   teams in the finals, there are three college collegiate teams with an invention.
[01:52:48.280 --> 01:52:53.920]   And in this case, the winner of the Imagine Cup, Brian Chang, who is a freshman, he's 18
[01:52:53.920 --> 01:52:56.800]   years old, there he is standing next to Satya Nadella.
[01:52:56.800 --> 01:53:00.920]   He won $150,000 as a freshman at UCLA.
[01:53:00.920 --> 01:53:05.600]   This is something he's patented patent pending called Easy Glue Coase.
[01:53:05.600 --> 01:53:09.600]   It's the first I've ever seen real non-invasive glucose monitoring.
[01:53:09.600 --> 01:53:14.560]   It puts a lens on a smartphone, has a machine learning database, takes a picture of your
[01:53:14.560 --> 01:53:20.240]   eye and is able to, with some serious accuracy, measure blood sugar.
[01:53:20.240 --> 01:53:21.720]   Wasn't that what Google was going to do with the lens?
[01:53:21.720 --> 01:53:24.240]   They wanted to do with that lens.
[01:53:24.240 --> 01:53:29.360]   The thing here is kind of what you were doing is using machine learning to take a lot of
[01:53:29.360 --> 01:53:36.680]   readings and then the machine learns, well, this is what that glucose level looks like.
[01:53:36.680 --> 01:53:42.080]   I don't think the $150,000 he got from Microsoft is going to be much compared to how much he's
[01:53:42.080 --> 01:53:46.240]   going to get from some big company if he perfects.
[01:53:46.240 --> 01:53:47.920]   This is what's going on.
[01:53:47.920 --> 01:53:50.400]   These are young people doing amazing things.
[01:53:50.400 --> 01:53:53.240]   My mother was a brutal diabetic.
[01:53:53.240 --> 01:53:56.880]   And so when I was young, it's amazing the instinct you have.
[01:53:56.880 --> 01:53:57.880]   When I was a kid.
[01:53:57.880 --> 01:53:58.880]   You could tell.
[01:53:58.880 --> 01:54:02.200]   You could recognize before any way in the family when she was ready to go to a reaction.
[01:54:02.200 --> 01:54:04.000]   It's like those dogs that know you're your mother.
[01:54:04.000 --> 01:54:05.000]   It's almost offensive.
[01:54:05.000 --> 01:54:06.000]   You need to know.
[01:54:06.000 --> 01:54:07.000]   Yeah.
[01:54:07.000 --> 01:54:08.000]   Oh, yeah, absolutely.
[01:54:08.000 --> 01:54:09.000]   Absolutely the case.
[01:54:09.000 --> 01:54:10.200]   And so that's what machine learning does come in.
[01:54:10.200 --> 01:54:11.200]   Sure.
[01:54:11.200 --> 01:54:12.200]   It's the opportunity to.
[01:54:12.200 --> 01:54:13.200]   You had some instinct, some intuition.
[01:54:13.200 --> 01:54:14.960]   But what if you had- I could explain it.
[01:54:14.960 --> 01:54:18.080]   Many, many more data points and you could say, "Oh, yeah, this is what the eye looks like
[01:54:18.080 --> 01:54:19.920]   when the glucose level is 100 or not too long."
[01:54:19.920 --> 01:54:23.320]   So David Weinberg, this is a case, David Weinberg was the only one who did it in his book.
[01:54:23.320 --> 01:54:25.300]   He talks about the case of the chicken and sexers.
[01:54:25.300 --> 01:54:26.300]   Yeah.
[01:54:26.300 --> 01:54:33.240]   And Petaluma actually, I interviewed the last chicken sexer of Petaluma because when-
[01:54:33.240 --> 01:54:34.240]   This was many years ago.
[01:54:34.240 --> 01:54:35.240]   He's a nice old guy.
[01:54:35.240 --> 01:54:36.240]   He's in your heyday.
[01:54:36.240 --> 01:54:37.240]   He's retiring.
[01:54:37.240 --> 01:54:40.400]   This is when I was at my peak at K&BR.
[01:54:40.400 --> 01:54:45.340]   When a chick is born, they needed to determine if it's a male or a female and it's not obvious
[01:54:45.340 --> 01:54:46.340]   apparently.
[01:54:46.340 --> 01:54:47.340]   Right.
[01:54:47.340 --> 01:54:51.580]   And so you, even the chicken sexers don't really know how they know, but they know.
[01:54:51.580 --> 01:54:52.580]   So it's like-
[01:54:52.580 --> 01:54:53.580]   I did.
[01:54:53.580 --> 01:54:54.580]   I tried out.
[01:54:54.580 --> 01:54:55.580]   How do you know?
[01:54:55.580 --> 01:55:02.780]   So that machine learning is the new chicken sexer.
[01:55:02.780 --> 01:55:04.580]   And there's your show title, ladies.
[01:55:04.580 --> 01:55:06.580]   There's your title.
[01:55:06.580 --> 01:55:07.580]   I keep trying.
[01:55:07.580 --> 01:55:09.780]   What's your number, Mr. Jarvis?
[01:55:09.780 --> 01:55:10.780]   All right.
[01:55:10.780 --> 01:55:15.900]   Well, Matthew Ingram, our friend, tweeted that out of the New York Times financial statement,
[01:55:15.900 --> 01:55:21.860]   the paper's crossword is the fifth largest digital subscription product of any kind offered
[01:55:21.860 --> 01:55:23.060]   by a US news provider.
[01:55:23.060 --> 01:55:24.060]   I'm not surprised.
[01:55:24.060 --> 01:55:25.060]   I pay for it.
[01:55:25.060 --> 01:55:26.060]   Oh, yeah.
[01:55:26.060 --> 01:55:27.060]   Oh, love it.
[01:55:27.060 --> 01:55:28.060]   I love it.
[01:55:28.060 --> 01:55:31.820]   I start on Monday and I work my way up to Sunday.
[01:55:31.820 --> 01:55:37.540]   So when the Times talks about its digital subscriptions, it's including food and crosswords
[01:55:37.540 --> 01:55:38.540]   in there.
[01:55:38.540 --> 01:55:40.980]   So it's not just the fifth largest product of the Times.
[01:55:40.980 --> 01:55:41.980]   It's of any kind of-
[01:55:41.980 --> 01:55:42.980]   But it comes from a new star.
[01:55:42.980 --> 01:55:43.980]   Right.
[01:55:43.980 --> 01:55:44.980]   Right.
[01:55:44.980 --> 01:55:45.980]   So we-
[01:55:45.980 --> 01:55:46.980]   And I don't know if this is fair.
[01:55:46.980 --> 01:55:51.700]   Maybe Stacy would show it next week, but she just tweeted what her camera caught of her
[01:55:51.700 --> 01:55:55.940]   dog going to the door and it's so sad and sweet.
[01:55:55.940 --> 01:55:57.940]   Oh, because they're moving.
[01:55:57.940 --> 01:55:59.380]   Well, no, this is my husband's leaving.
[01:55:59.380 --> 01:56:01.300]   Oh, the husband's leaving.
[01:56:01.300 --> 01:56:02.540]   My wise system is picked up.
[01:56:02.540 --> 01:56:04.100]   The door just shut behind my husband.
[01:56:04.100 --> 01:56:05.460]   He's left the house.
[01:56:05.460 --> 01:56:06.460]   His dogs.
[01:56:06.460 --> 01:56:07.460]   Oh, look at the tail.
[01:56:07.460 --> 01:56:08.460]   It just goes.
[01:56:08.460 --> 01:56:14.540]   Oh, he keeps going away.
[01:56:14.540 --> 01:56:15.540]   That's hysterical.
[01:56:15.540 --> 01:56:22.540]   I was a beautiful fucking dog.
[01:56:22.540 --> 01:56:31.220]   So a good friend of our show and a great guy in general, Mike Elgin's son, Kevin, is a
[01:56:31.220 --> 01:56:37.980]   whiz with technology and he has launched a Kickstarter for something that is super, super
[01:56:37.980 --> 01:56:38.980]   cool.
[01:56:38.980 --> 01:56:41.220]   This is called Chatterbox.
[01:56:41.220 --> 01:56:42.880]   Just launched a couple of days ago.
[01:56:42.880 --> 01:56:45.460]   So there's still 58 days ago, but don't delay.
[01:56:45.460 --> 01:56:46.460]   You want these.
[01:56:46.460 --> 01:56:48.640]   They've already exceeded their goal.
[01:56:48.640 --> 01:56:52.380]   It's a smart speaker that kids build and program.
[01:56:52.380 --> 01:56:55.060]   So they're not relying on Amazon's Echo.
[01:56:55.060 --> 01:56:56.700]   We don't like this for our kids.
[01:56:56.700 --> 01:56:57.700]   Go ahead.
[01:56:57.700 --> 01:56:58.700]   You can play the audio.
[01:56:58.700 --> 01:56:59.700]   Let's- Let's give them a nice plug.
[01:56:59.700 --> 01:57:00.700]   Show my video.
[01:57:00.700 --> 01:57:05.600]   There's too many screens with too many distractions, not enough creativity and learning and definitely
[01:57:05.600 --> 01:57:07.940]   not enough privacy.
[01:57:07.940 --> 01:57:12.020]   Meet Chatterbox, the world's first building yourself, programming yourself smart speaker
[01:57:12.020 --> 01:57:13.400]   for kids.
[01:57:13.400 --> 01:57:17.360]   Think of it like an Amazon Alexa, but one focused on privacy that kids can build and
[01:57:17.360 --> 01:57:19.680]   teach to bring their creativity to life.
[01:57:19.680 --> 01:57:20.680]   They teach it.
[01:57:20.680 --> 01:57:25.000]   Our simple step-by-step instructions make problem-solving fun and hands-on.
[01:57:25.000 --> 01:57:26.000]   You just don't have to.
[01:57:26.000 --> 01:57:29.040]   Building Chatterbox is as easy as building a paper airplane.
[01:57:29.040 --> 01:57:33.040]   The visual skill builder helps kids learn about artificial intelligence while easily
[01:57:33.040 --> 01:57:35.240]   programming voice skills.
[01:57:35.240 --> 01:57:37.440]   Kids can set up magic words to control lights, and-
[01:57:37.440 --> 01:57:41.240]   Kevin worked at Tinker for a while, which did something similar on an iPad.
[01:57:41.240 --> 01:57:43.060]   Anything is possible.
[01:57:43.060 --> 01:57:48.420]   Chatterbox is a fun, personal and private way for kids to develop important communication,
[01:57:48.420 --> 01:57:49.420]   critical thinking-
[01:57:49.420 --> 01:57:51.860]   You did Piper, which was a build on yourself computer, right?
[01:57:51.860 --> 01:57:55.860]   Our team's collective experience has built educational software and harder products-
[01:57:55.860 --> 01:57:56.860]   There's Kevin right there.
[01:57:56.860 --> 01:58:01.580]   There's two-five millions of kids all over the world, where parents, educators, designers
[01:58:01.580 --> 01:58:02.580]   and developers-
[01:58:02.580 --> 01:58:03.580]   What's the-
[01:58:03.580 --> 01:58:04.580]   Let's make it-
[01:58:04.580 --> 01:58:09.060]   So, meet Chatterbox, and if you pledge $99, you'll get the super early bird.
[01:58:09.060 --> 01:58:10.980]   There's twelve left of those.
[01:58:10.980 --> 01:58:17.140]   Chatterbox smart speaker, build it yourself, and program it yourself smart speaker.
[01:58:17.140 --> 01:58:21.060]   If you pledge $119, that's going to be the second tier, and I think that's just, you
[01:58:21.060 --> 01:58:23.860]   know, as they've sold out the first tier.
[01:58:23.860 --> 01:58:27.380]   $238 for two of them, or, and then it goes on up.
[01:58:27.380 --> 01:58:30.180]   And so, if you want to sponsor a school and get eight of them-
[01:58:30.180 --> 01:58:32.180]   So, teachers, yeah, we can go to-
[01:58:32.180 --> 01:58:33.180]   Hand-Hand.
[01:58:33.180 --> 01:58:38.520]   You can sponsor a school, and that would be a really fun school project, I think.
[01:58:38.520 --> 01:58:42.500]   More than just building and programming, it really teaches kids how these things work,
[01:58:42.500 --> 01:58:46.760]   that they're not just a wizard in a box, you know, and that's really the problem, I think,
[01:58:46.760 --> 01:58:50.840]   as we move to this world where we go beyond what, you know, hardware understanding, somebody
[01:58:50.840 --> 01:58:54.680]   just becomes magical things, magical items, and I think it's important for-
[01:58:54.680 --> 01:58:57.080]   Especially for kids to contextualize it by understanding-
[01:58:57.080 --> 01:58:58.800]   Oh, that's just a machine.
[01:58:58.800 --> 01:58:59.800]   That's not-
[01:58:59.800 --> 01:59:00.800]   But it's not a person in there-
[01:59:00.800 --> 01:59:02.200]   That I'm in control of and-
[01:59:02.200 --> 01:59:03.200]   And I control it.
[01:59:03.200 --> 01:59:04.200]   Yeah.
[01:59:04.200 --> 01:59:06.080]   And this is how it's programmed by a human somewhere.
[01:59:06.080 --> 01:59:09.320]   I think really an important thing to understand and learn.
[01:59:09.320 --> 01:59:12.320]   So good for you, Kevin, it's on Kickstarter.
[01:59:12.320 --> 01:59:18.520]   Go to Kickstarter and search for "Chatterbox", smart speaker that kids build and program,
[01:59:18.520 --> 01:59:24.360]   and I'm glad he's already reached his goal, but there's still lots more to go.
[01:59:24.360 --> 01:59:25.360]   Well done, Kevin.
[01:59:25.360 --> 01:59:29.520]   Good for you, Mike Elkins, son Kevin Elkins.
[01:59:29.520 --> 01:59:30.720]   What age is it targeted at?
[01:59:30.720 --> 01:59:32.480]   I don't know, that's a good question.
[01:59:32.480 --> 01:59:33.480]   They show these kids are pretty young.
[01:59:33.480 --> 01:59:36.040]   We have a former kindergarten teacher now, IT guy.
[01:59:36.040 --> 01:59:37.040]   In the room.
[01:59:37.040 --> 01:59:38.040]   I've also-
[01:59:38.040 --> 01:59:39.040]   I've also-
[01:59:39.040 --> 01:59:42.760]   For my nephew who's going to be six.
[01:59:42.760 --> 01:59:44.680]   Kids as young as seven years old.
[01:59:44.680 --> 01:59:50.920]   So you know your kid and whether your kid could adjust to this.
[01:59:50.920 --> 01:59:53.560]   The programming is all dragging.
[01:59:53.560 --> 01:59:55.680]   It's all that kind of modular programming.
[01:59:55.680 --> 01:59:56.680]   Lock the heat or something.
[01:59:56.680 --> 01:59:57.680]   Lock the heat or something.
[01:59:57.680 --> 01:59:58.680]   Yeah, yeah, exactly.
[01:59:58.680 --> 02:00:02.400]   One of the cute things that I owe, they had a session which was pretty full for make
[02:00:02.400 --> 02:00:05.400]   your first assistant script.
[02:00:05.400 --> 02:00:08.240]   And all you have to do is do a spreadsheet.
[02:00:08.240 --> 02:00:10.880]   Yeah, it's amazing.
[02:00:10.880 --> 02:00:15.120]   This is going to be more than just a simple smart speaker though because they're incorporating
[02:00:15.120 --> 02:00:16.200]   Wolfram Alpha.
[02:00:16.200 --> 02:00:17.880]   It has a Raspberry Pi.
[02:00:17.880 --> 02:00:19.680]   It's powered by a microfte AI.
[02:00:19.680 --> 02:00:26.960]   So it is a sophisticated voice assistant that you'll probably end up using, which is
[02:00:26.960 --> 02:00:27.960]   also very cool.
[02:00:27.960 --> 02:00:34.760]   It comes with a Raspberry Pi 3B plus the special cheddar voice hat.
[02:00:34.760 --> 02:00:37.640]   We are done for this week in Google.
[02:00:37.640 --> 02:00:39.040]   Thank you everybody for being here.
[02:00:39.040 --> 02:00:40.040]   We had a great time.
[02:00:40.040 --> 02:00:41.880]   I want to thank our teachers.
[02:00:41.880 --> 02:00:45.720]   Remember it's Teacher Appreciation Week, Jared, Ryan, Brian and Roland.
[02:00:45.720 --> 02:00:49.640]   From Sacramento Unified School District, is that?
[02:00:49.640 --> 02:00:51.040]   All over.
[02:00:51.040 --> 02:00:52.040]   All over Sacramento.
[02:00:52.040 --> 02:00:54.760]   Notice they're here in shorts because it's hot up there.
[02:00:54.760 --> 02:00:57.640]   Yeah, they're probably freezing their buns off here.
[02:00:57.640 --> 02:00:58.640]   Yeah.
[02:00:58.640 --> 02:01:00.920]   It's always my favorite part of going on the Golden Gate Bridge is watching all the tourists
[02:01:00.920 --> 02:01:04.600]   and shorts saying, "But I thought it was summer."
[02:01:04.600 --> 02:01:07.240]   Anyway, thanks guys for what you do.
[02:01:07.240 --> 02:01:10.440]   Teaching is absolutely the most important thing anybody does.
[02:01:10.440 --> 02:01:12.440]   It beats podcasting.
[02:01:12.440 --> 02:01:16.880]   I can tell you that.
[02:01:16.880 --> 02:01:22.760]   We do this show every Wednesday, around 130, Pacific 430, Eastern 2030 UTC.
[02:01:22.760 --> 02:01:24.800]   If you want to come by and watch, you can't.
[02:01:24.800 --> 02:01:28.480]   We stream audio and video live at twit.tv/live.
[02:01:28.480 --> 02:01:31.400]   If you're watching the live stream, please join the chat room because they're doing the
[02:01:31.400 --> 02:01:36.360]   same thing at irc.twit.tv.
[02:01:36.360 --> 02:01:38.120]   You can get on-demand episodes of this.
[02:01:38.120 --> 02:01:41.080]   At every show we do at our website, twit.tv.
[02:01:41.080 --> 02:01:42.880]   For twig, it's twit.tv/twig.
[02:01:42.880 --> 02:01:44.760]   That's easy to remember.
[02:01:44.760 --> 02:01:50.240]   As I mentioned, we are now at 507 shows, so we're getting close in about 13 weeks.
[02:01:50.240 --> 02:01:52.040]   We will be at our 10th anniversary.
[02:01:52.040 --> 02:01:53.040]   Can you believe that?
[02:01:53.040 --> 02:01:56.120]   I can't believe you've let me do this for this long.
[02:01:56.120 --> 02:01:58.760]   I can't believe that we were young men when we started.
[02:01:58.760 --> 02:02:03.520]   We weren't, so that's why I can't believe it.
[02:02:03.520 --> 02:02:07.120]   You can download episodes at twig, twit.tv/twig, or subscribe.
[02:02:07.120 --> 02:02:08.280]   That's actually the best thing to do.
[02:02:08.280 --> 02:02:09.760]   Find your favorite podcast application.
[02:02:09.760 --> 02:02:12.240]   Subscribe, and that way it'll be on your phone.
[02:02:12.240 --> 02:02:16.760]   The minute it's available, of a Wednesday afternoon.
[02:02:16.760 --> 02:02:18.080]   Thanks everybody for being here.
[02:02:18.080 --> 02:02:20.080]   We'll see you next time on This Week in Google.
[02:02:20.080 --> 02:02:20.080]   Bye-bye.
[02:02:20.080 --> 02:02:30.080]   [MUSIC]

