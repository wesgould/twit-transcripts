;FFMETADATA1
title=Uber's Lyft-Off
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=430
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.000]   It's time for Twig this week at Google. We got a heavy duty big conversation going on about YouTube
[00:00:06.000 --> 00:00:10.300]   Whether they should be paying more attention to the weird stuff on YouTube kids
[00:00:10.300 --> 00:00:14.840]   Jeff Jarvis is gonna defend Facebook. We're gonna talk about
[00:00:14.840 --> 00:00:24.360]   Everything in the underneath is done including flying cars. Yes, they're coming to Los Angeles 2020. Maybe it's all coming up next on twig
[00:00:28.080 --> 00:00:31.240]   Netcasts you love from people you trust
[00:00:31.240 --> 00:00:42.880]   This is twig bandwidth for this week in Google is provided by cash fly C A C H E
[00:00:42.880 --> 00:00:45.360]   F L Y calm
[00:00:45.360 --> 00:00:54.280]   This is twig this week in Google episode 430 recorded Wednesday November 8th
[00:00:54.280 --> 00:00:56.760]   2017
[00:00:56.760 --> 00:00:58.760]   Ubers liftoff
[00:00:58.760 --> 00:01:04.520]   This week in Google is brought to you by Eero never think about Wi-Fi again with Eero's hyper-fast
[00:01:04.520 --> 00:01:11.880]   Super simple Wi-Fi system and now the second generation Eero is tri-band twice as fast for free overnight shipping visit
[00:01:11.880 --> 00:01:16.200]   Eero comm select overnight shipping at checkout and enter the code twig and
[00:01:16.200 --> 00:01:21.840]   By rocket mortgage from quick and loans home plays a big role in your life
[00:01:21.840 --> 00:01:24.280]   That's why quick and loans created rocket mortgage
[00:01:24.280 --> 00:01:29.360]   It lets you apply simply and understand the entire mortgage process fully so you can be confident
[00:01:29.360 --> 00:01:33.800]   You're getting the right mortgage for you get started at rocket mortgage calm slash twig
[00:01:33.800 --> 00:01:36.720]   and by
[00:01:36.720 --> 00:01:43.500]   Lighthouse lighthouse is the only security camera powered by the same technology that's in self-driving cars visit light
[00:01:43.500 --> 00:01:51.360]   Dot house slash twit to sign up for 15% off lighthouse when they ship and a chance to win a free lighthouse plus a year of service
[00:01:51.360 --> 00:01:53.360]   See site for contest rules
[00:01:54.120 --> 00:01:59.800]   It's time for twig this week in Google show where we get to oh, this is gonna be a good one
[00:01:59.800 --> 00:02:04.240]   I've been waiting for this one all week talk about some of the most interesting things happening in the world of technology
[00:02:04.240 --> 00:02:08.600]   Particularly with the with what Farhad Manju calls the frightful five
[00:02:08.600 --> 00:02:12.880]   Google Facebook Microsoft Apple Amazon
[00:02:12.880 --> 00:02:16.240]   Joining us right now from the
[00:02:16.240 --> 00:02:21.240]   Graduate School of Journalism at the City University of New York mr. Jeff Jarvis
[00:02:21.760 --> 00:02:28.000]   Buzz machine comm author of what would Google do public parts geek sparing gifts? Hey Jeffrey
[00:02:28.000 --> 00:02:32.280]   Hey nice to see you see you. We're gonna
[00:02:32.280 --> 00:02:37.440]   We're gonna see you off to Argentina next week. That'll be fun next week. Yes on hidden Tina
[00:02:37.440 --> 00:02:43.160]   I love winno saris. What a great. I've never been there. I've been dying over the three years. I'm only gonna be there for two days
[00:02:43.160 --> 00:02:45.840]   Yeah, but you'll have fun. Yeah
[00:02:45.840 --> 00:02:50.400]   Also here journalist par excellence Stacy Higginbotham
[00:02:50.880 --> 00:02:52.880]   She she was she's been around
[00:02:52.880 --> 00:02:57.640]   She was the gig home. I didn't realize you worked for time Warner time ink
[00:02:57.640 --> 00:03:01.520]   Time ink fortune. Oh fortune. That's right. Yeah. You were there for a cup of coffee
[00:03:01.520 --> 00:03:07.400]   A whole year actually really that's like that's like a cup and a half. It seemed like it went fast
[00:03:07.400 --> 00:03:13.880]   It did she was very smart. In fact, I saw you acknowledge that the other day. You did the right thing
[00:03:13.880 --> 00:03:19.840]   You started your own business Stacy on IOT. It's a newsletter. It's a podcast with Kevin Tofel and it's going great guns, right?
[00:03:20.320 --> 00:03:25.680]   It is it's actually doing really well. You got the I saw you on tech meme. You got the scoop on
[00:03:25.680 --> 00:03:28.600]   ADT and ring
[00:03:28.600 --> 00:03:36.760]   Well, actually law 360 was reporting from the courtroom. So I just brought that out. Yeah, but that's that's that's that's trick
[00:03:36.760 --> 00:03:41.000]   Right, I know but like I wasn't physically there as a journalist
[00:03:41.000 --> 00:03:45.360]   I'm like I got a good credit to the people who actually went and did the footwork absolutely
[00:03:45.360 --> 00:03:47.680]   What does you take on that? You know?
[00:03:48.760 --> 00:03:55.960]   I've talked to some people who say 80 this isn't the surprise ADT is very aggressive about 80t is super aggressive. They have been
[00:03:55.960 --> 00:04:02.880]   They have done some really shady things both they an alarm if you look at some of their lawsuits back and forth between each other
[00:04:02.880 --> 00:04:11.400]   You'll see some pretty crazy tactics. They claimed the ring they blocked and a judge agreed by the way judge gave him a temporary junction against rings new security
[00:04:11.400 --> 00:04:17.080]   System which they haven't they've announced but haven't shipped and I think may miss Christmas because of this
[00:04:17.960 --> 00:04:20.680]   Probably yes claiming it violates
[00:04:20.680 --> 00:04:23.840]   Their patents or property rights
[00:04:23.840 --> 00:04:28.600]   Yes, we should mention ring is a sponsor. I just want to say that real quickly rings a sponsor of the show
[00:04:28.600 --> 00:04:33.120]   But not that that changes anything doesn't make me more in favor of them or not. I
[00:04:33.120 --> 00:04:41.520]   Don't really know the facts. So ring so I don't know how much I don't know how many of the facts you actually want to know
[00:04:41.520 --> 00:04:45.280]   So that's probably cuz you know it all because I have a lot of
[00:04:46.040 --> 00:04:49.880]   It's very dull. So ADT alleges that
[00:04:49.880 --> 00:04:57.560]   They not only stole their intellectual property, but they did so in a pretty like James Bondi and kind of way so
[00:04:57.560 --> 00:05:06.000]   ADT had invested in zone off and was contracted was on off to do some have son off do some work for them
[00:05:06.000 --> 00:05:13.880]   I know the guys at Zonoff. I've known them for years the CEO my caris. Yeah, my caris super nice ethical dude
[00:05:15.640 --> 00:05:19.960]   Basically, they had a deal. They had an at imine deal that fell through
[00:05:19.960 --> 00:05:24.000]   While they were being shopped when that fell through
[00:05:24.000 --> 00:05:28.880]   They kind of ran out of money. They couldn't pay ADT back for a note and
[00:05:28.880 --> 00:05:33.400]   They were gonna shut down the entire company and then the next day
[00:05:33.400 --> 00:05:39.280]   Ring gave everybody is on off a job 75 people they hired 75 people
[00:05:39.280 --> 00:05:43.280]   I think all but one took the job and my caris went over now ADT
[00:05:43.640 --> 00:05:48.120]   alleges that my caris took all of ADT's IP because ADT once
[00:05:48.120 --> 00:05:53.960]   Once on off missed that payment ADT got all of their IP so ADT's like
[00:05:53.960 --> 00:05:59.880]   My caris handed Jamie Seminoff won you know our IP in a briefcase in a parking lot in Pennsylvania
[00:05:59.880 --> 00:06:07.360]   Which sounds really good, but I don't know if that's actually true. So a judge gave them a temporary injunction
[00:06:07.360 --> 00:06:10.880]   This sometimes happens and you go to court saying look while we're deciding this case
[00:06:10.880 --> 00:06:14.320]   Can you stop them from selling this infringing device the judge agreed?
[00:06:14.320 --> 00:06:19.600]   So that's bad news for ring they're gonna miss a big buying season meanwhile nest has a very similar device
[00:06:19.600 --> 00:06:21.920]   Which they're advertising heavily and I think it's kind of interesting
[00:06:21.920 --> 00:06:24.320]   I I have an article on the verge about this and
[00:06:24.320 --> 00:06:29.640]   Right next to it's it's gone now because I guess it was a carousel. I got an ADT ad
[00:06:29.640 --> 00:06:35.760]   so I don't know I should actually say ADT is an advertiser right now on my show yeah
[00:06:35.760 --> 00:06:40.600]   Well this I mean part of this is that this is a big and there's a nest ad
[00:06:40.600 --> 00:06:45.320]   So this is a big buying season right now for these kind of security devices
[00:06:45.320 --> 00:06:48.560]   Everybody has a new security company or a new security product
[00:06:48.560 --> 00:06:53.280]   So nest just launched theirs rain launched theirs and now is put put in an on hold
[00:06:53.280 --> 00:07:01.160]   Wink just launched a system. We've also got almond, which is a router company. It's smaller, but they just launched a system
[00:07:02.040 --> 00:07:08.000]   Notion just said that their stuff's gonna work with nest stuff. So oh my god. It's crazy. Yeah
[00:07:08.000 --> 00:07:10.440]   It just shows you it's a big market
[00:07:10.440 --> 00:07:13.480]   20% of
[00:07:13.480 --> 00:07:17.600]   American homes have monitored security
[00:07:17.600 --> 00:07:23.800]   But everybody's like interested in having security in my experience with monitored security is I don't like it
[00:07:23.800 --> 00:07:29.880]   We had monitored security we've had it we have it now every business has to have it
[00:07:29.880 --> 00:07:31.640]   But the false, you know
[00:07:31.640 --> 00:07:34.880]   I just got annoying the calls in the middle of my poor John took the brunt of those
[00:07:34.880 --> 00:07:39.840]   But the calls in the middle night because the one of the sensors went off and we never was there ever any?
[00:07:39.840 --> 00:07:44.880]   How many we we must have had a dozen in two years and there was never any real intruders
[00:07:44.880 --> 00:07:51.080]   And at finally at the point where the pedal in the police department says you're gonna start paying for these if
[00:07:51.080 --> 00:07:56.640]   If we have to keep those the problem. Yeah, and I think that monitored security often well
[00:07:56.640 --> 00:08:00.840]   There's just issues now they they they call you and that's you know
[00:08:00.840 --> 00:08:04.220]   Most of the time we were able to preempt it John would come down and see what was going on
[00:08:04.220 --> 00:08:08.000]   But I've gone down there to meet the police a couple of middle of the night
[00:08:08.000 --> 00:08:10.480]   rendezvous
[00:08:10.480 --> 00:08:12.480]   So I hate that I hate that
[00:08:12.480 --> 00:08:13.680]   so
[00:08:13.680 --> 00:08:16.680]   Good reason yeah, I don't know if it's a good thing for the home
[00:08:16.680 --> 00:08:19.560]   Maybe it is if you're gonna be a lot gone a long time, I guess
[00:08:19.560 --> 00:08:25.880]   Well, I just have the neighbors look in and feed the cat. What's it a lot? What are the plants pick up the mail?
[00:08:26.880 --> 00:08:28.880]   What are the cat feed the plan?
[00:08:28.880 --> 00:08:33.880]   Well as long as we're in in the courtroom
[00:08:33.880 --> 00:08:41.040]   Let's not let's not leave the courtroom just yet the Supreme Court has thrown out the Samsung Apple case
[00:08:41.040 --> 00:08:45.560]   Remember Apple sued Samsung for saying slide to unlock we own that a
[00:08:45.560 --> 00:08:49.320]   Samsung fought it lost fought it lost they finally
[00:08:50.040 --> 00:08:54.680]   Ended up appealing to the Supreme Court scotas declined to hear the appeal
[00:08:54.680 --> 00:08:59.720]   So Apple does owed slide to unlock and Samsung owes them several hundred million dollars
[00:08:59.720 --> 00:09:01.040]   for
[00:09:01.040 --> 00:09:07.120]   Fringing could they were moving to facial and fingerprint unlocking huh? Yeah, it's so funny
[00:09:07.120 --> 00:09:09.120]   I mean
[00:09:09.120 --> 00:09:14.960]   And you were talking about this earlier Jeff there were rumors and now it's been
[00:09:14.960 --> 00:09:18.040]   confirmed that
[00:09:19.640 --> 00:09:25.040]   According to the Department of Justice AT&T must jettison CNN
[00:09:25.040 --> 00:09:28.800]   For the time Warner merger to occur
[00:09:28.800 --> 00:09:36.100]   There's some politics in here of course because we know the president's not a fan of CNN and some think this is
[00:09:36.100 --> 00:09:38.320]   punitive
[00:09:38.320 --> 00:09:40.320]   This is coming from the White House
[00:09:40.320 --> 00:09:44.600]   I've been reading Twitter about it which of course now takes twice as long
[00:09:45.320 --> 00:09:50.560]   Oh them I buried the lead Twitter now 280 characters
[00:09:50.560 --> 00:09:55.520]   And everybody insists on writing their first tweet
[00:09:55.520 --> 00:09:58.360]   280 character tweet which usually means a bunch of crap
[00:09:58.360 --> 00:10:00.880]   Yeah, I get the job
[00:10:00.880 --> 00:10:06.880]   Anyway, this will this will pass over right I hope so
[00:10:06.880 --> 00:10:09.760]   hashtag 280 characters
[00:10:11.240 --> 00:10:15.840]   and but this you can say to you AT&T time working thing will go to court and
[00:10:15.840 --> 00:10:25.080]   Trump's public statements will probably have an impact sure and I mean I but you know one of his public statements during the campaign was
[00:10:25.080 --> 00:10:28.880]   He didn't want the merger to happen at all right?
[00:10:28.880 --> 00:10:35.600]   He said if I'm elected there will be no take TT time Warner merger, which I'm in favor of let's I don't think it's a good idea
[00:10:35.600 --> 00:10:40.840]   Problem is it's a it's not a competitive merger. It's it's a couple of memory mergers
[00:10:40.840 --> 00:10:44.600]   So does it's hard to stop it. Yeah. Yeah, yeah, there really is
[00:10:44.600 --> 00:10:52.320]   Last pass decided to use its 200 characters 280 characters to tweet 280 characters of bad passwords
[00:10:52.320 --> 00:11:00.400]   There's a lot of this. Oh, I have gotten so many I see so many people tweeting period return period return
[00:11:00.400 --> 00:11:06.800]   I'd like Pittsburgh Steelers took this occasion to tweet all of its former players in the Bitfoot Bowl Hall of Fame
[00:11:08.000 --> 00:11:10.800]   Here's a William Connells Williams poem
[00:11:10.800 --> 00:11:14.120]   Is it the plum?
[00:11:14.120 --> 00:11:20.680]   I have eaten the plums that were in the ice box and which you were probably saving for breakfast
[00:11:20.680 --> 00:11:25.440]   Forgive me. They were delicious so sweet and so cold
[00:11:25.440 --> 00:11:30.440]   Yeah, a good use of 280 characters one of the disorders
[00:11:30.440 --> 00:11:35.520]   There's another one put up the Trump bus quote. So yes a lot of them
[00:11:35.520 --> 00:11:42.200]   I saw that retweeted like crazy. Yeah, here's a good use of 280 characters Ohio State put up the Ohio State
[00:11:42.200 --> 00:11:48.400]   Well, you know the eyes of Texas are upon you. It might work
[00:11:48.400 --> 00:11:55.800]   Here's one
[00:11:55.800 --> 00:11:59.280]   Louise Dover tweeted Obama's sex Hillary sex Obama sex Hillary sex Obama sex
[00:11:59.280 --> 00:12:02.760]   That's creative Arby's
[00:12:04.320 --> 00:12:06.320]   MGM Studios the Lions roar
[00:12:06.320 --> 00:12:09.480]   Roar
[00:12:09.480 --> 00:12:16.400]   It goes on and on and on please kids let's grow up
[00:12:16.400 --> 00:12:21.480]   This is like when the Apple first came out with typefaces and every every letter looked like a ransom note
[00:12:21.480 --> 00:12:28.720]   or like the emoji's and emojis you're right if there's a blessing the 280 characters
[00:12:28.720 --> 00:12:31.840]   Goddamn and emoji karaoke's
[00:12:31.840 --> 00:12:40.000]   So, you know, so, you know
[00:12:40.000 --> 00:12:47.600]   Somebody said the 280 characters is Twitter's equivalent of fat Elvis fat Twitter
[00:12:47.600 --> 00:12:57.680]   We're gonna talk about this article and this really I think is gonna be the centerpiece of the show today because I really
[00:12:57.680 --> 00:12:59.680]   I found this fascinating
[00:13:00.000 --> 00:13:04.480]   I don't know James bridal. I don't know who he is. He says he's a writer and an artist
[00:13:04.480 --> 00:13:07.960]   it looks like his
[00:13:07.960 --> 00:13:16.000]   Avatar is Malcolm McDowell from that one where the kids the kids revolt against the
[00:13:16.000 --> 00:13:21.080]   British clockwork orange. No, no, it's an earlier one. What's it called?
[00:13:21.080 --> 00:13:25.680]   Where the the students revolt in the public school? I can't remember the name of it
[00:13:26.400 --> 00:13:30.760]   Anyway, he says he's a writer and artist, but he does he does observe something quite interesting
[00:13:30.760 --> 00:13:33.080]   about the
[00:13:33.080 --> 00:13:39.840]   Algorithically generated videos aimed at children young almost it makes his pre-verbal children that are
[00:13:39.840 --> 00:13:42.600]   decidedly disturbing and
[00:13:42.600 --> 00:13:46.320]   Whether this is intentional or accidental or it's the Russians
[00:13:46.320 --> 00:13:49.880]   We'll talk about that. That's gonna be the centerpiece a
[00:13:49.880 --> 00:13:52.400]   speaking of Russians Marissa Meyer says
[00:13:53.160 --> 00:14:00.480]   She's sorry that Yahoo was hacked and the Russians did it. She testified today in front of the Senate and she said
[00:14:00.480 --> 00:14:04.040]   You know I was on my watch and
[00:14:04.040 --> 00:14:09.160]   I apologize for the two massive data breaches
[00:14:09.160 --> 00:14:16.280]   She blames the Russians for it. Although there's apparently no evidence that the Russians were involved
[00:14:16.280 --> 00:14:20.480]   She said unfortunately while all our measures helped Yahoo
[00:14:21.240 --> 00:14:29.000]   successfully defend against the barrage of attacks by both private and state sponsored hackers Russian agents intruded on our systems and
[00:14:29.000 --> 00:14:31.280]   stole our users data
[00:14:31.280 --> 00:14:37.640]   In 2013 there was a breach of all three billion Yahoo accounts
[00:14:37.640 --> 00:14:40.960]   Yahoo had previously said one billion
[00:14:40.960 --> 00:14:50.200]   In March federal prosecutors charged to Russian intelligence agents and two hackers with masterminding a theft of half a billion Yahoo accounts
[00:14:50.600 --> 00:14:56.440]   That's actually the only charge it to at least the first charge against Russian spies for cyber crimes that I know of
[00:14:56.440 --> 00:15:02.120]   But the FBI says the 2013 breach was unrelated
[00:15:02.120 --> 00:15:08.360]   Meyer later said under questioning she didn't know if Russians were responsible for the 2013 breach
[00:15:08.360 --> 00:15:12.280]   But they sure are convenience scapegoat
[00:15:12.280 --> 00:15:19.120]   Well, no one knows still who did they at 2013 so that's the first time we've heard about Ms. Meyer since
[00:15:19.640 --> 00:15:21.640]   She left Yahoo
[00:15:21.640 --> 00:15:27.000]   Let's see
[00:15:27.000 --> 00:15:29.560]   Gosh there's actually today there is a a
[00:15:29.560 --> 00:15:32.200]   We're jam-packed
[00:15:32.200 --> 00:15:36.920]   Do you want to talk about the Facebook nudes or are we saving are we saving our no no?
[00:15:36.920 --> 00:15:40.040]   Why should we save that that's a heck of a story Facebook?
[00:15:40.040 --> 00:15:44.520]   I think we should just go into it Facebook says it can combat revenge porn
[00:15:44.520 --> 00:15:47.960]   Good, we don't like that Twitter Twitter says the same thing
[00:15:49.160 --> 00:15:53.280]   but in order to do it on Facebook you have to upload nudes of yourself and
[00:15:53.280 --> 00:15:58.160]   That nudes or is it is it is it the actual photos you're afraid are gonna get out?
[00:15:58.160 --> 00:16:03.560]   Oh, well, so are they hashing your fate or identity or are they hashing the actual photo?
[00:16:03.560 --> 00:16:09.280]   Yeah, it's actually the actual photo it says Facebook says if you send the photo you're worried about it to the company first
[00:16:09.280 --> 00:16:11.280]   it will make sure it doesn't show up because and
[00:16:11.280 --> 00:16:15.440]   They won't have they won't storm they'll create a digital, you know hash
[00:16:16.680 --> 00:16:21.320]   Which will help the interns the interns who make sure the hash work well. I'd be nervous
[00:16:21.320 --> 00:16:27.560]   Okay, well wait the daily beast just put up a story saying that it's actually going to be a person
[00:16:27.560 --> 00:16:35.520]   That looks at these so Facebook workers not an algorithm will look at volunteered nude photos first to stop revenge
[00:16:35.520 --> 00:16:38.600]   porn or let me drop this in
[00:16:38.600 --> 00:16:40.080]   comments
[00:16:40.080 --> 00:16:45.320]   First you upload an explicit image of yourself to Facebook messenger, which you can do by starting a conversation with yourself
[00:16:45.600 --> 00:16:49.000]   Then you flag it. There's a new flag called non-consensual
[00:16:49.000 --> 00:16:51.720]   intimate image
[00:16:51.720 --> 00:16:56.360]   At that point the social network builds what is referred to as a hash of the image
[00:16:56.360 --> 00:16:59.200]   Facebook says it's not storing the photos
[00:16:59.200 --> 00:17:01.400]   I don't know why anybody would need to look at it except
[00:17:01.400 --> 00:17:06.680]   I guess it could be misused to buy people to block other photos writer if another user tries to upload
[00:17:06.680 --> 00:17:14.360]   Yeah, yeah, exactly if another user tries to upload the same image on Facebook or Instagram Facebook will test it against its stored hashes and stop it
[00:17:14.360 --> 00:17:15.600]   so
[00:17:15.600 --> 00:17:19.160]   Yeah, I understand because you can't you gotta make sure it really is a
[00:17:19.160 --> 00:17:23.000]   explicit photo so some poor person has to go oh
[00:17:23.000 --> 00:17:27.400]   Oh
[00:17:27.400 --> 00:17:29.880]   Now that one's okay. Ah
[00:17:29.880 --> 00:17:33.080]   Like that for like eight hours a day
[00:17:33.080 --> 00:17:39.560]   Yeah, and with a real person involved, I'm sure nobody's news will ever get compromised
[00:17:41.720 --> 00:17:49.240]   Boss I don't I heard you're opening over there in the in the Facebook standards of practices, and I just like to apply
[00:17:49.240 --> 00:17:55.280]   You probably don't want to see the nudes for I don't the average American. I don't it's not a good job
[00:17:55.280 --> 00:17:58.160]   and I know I've heard I've heard from and
[00:17:58.160 --> 00:18:03.560]   Talk to actually I don't know if I've personally talked but I know people who have talked to the people who do these kinds of things for
[00:18:03.560 --> 00:18:05.320]   companies, you know
[00:18:05.320 --> 00:18:09.280]   Twitter and Facebook and and it's a terrible job that burnout is very high oh
[00:18:10.200 --> 00:18:12.800]   It's gotta be just a terrible job. You don't want that job
[00:18:12.800 --> 00:18:17.720]   But I'm willing to try I'm just I'll give them a shot. No, I wouldn't do it
[00:18:17.720 --> 00:18:20.680]   pictures oh
[00:18:20.680 --> 00:18:22.680]   That's a good idea
[00:18:22.680 --> 00:18:26.040]   Let me ask my iPhone if there's any brassier pictures of me
[00:18:26.040 --> 00:18:29.560]   So so hold on hold let me know the logic of this
[00:18:29.560 --> 00:18:34.960]   So imagine if there were a way that I could make a hash of my own photo and all I said is the hash
[00:18:34.960 --> 00:18:36.680]   Right there's a program. Let's be hash a photo
[00:18:36.680 --> 00:18:40.120]   That would be the way to do it, but then they still then wait wait wait
[00:18:40.120 --> 00:18:47.160]   Then the photo comes in and the alarm goes off says we poop poop. That's a bad photo. That's when you look at all you're right
[00:18:47.160 --> 00:18:50.760]   Okay, but if you have
[00:18:50.760 --> 00:18:54.200]   That makes sense
[00:18:54.200 --> 00:18:58.720]   No, no, no, if you send in the hash it has to be unhashed for the computer to actually
[00:18:58.720 --> 00:19:04.240]   No, that's what the Jeff saying just saying that you Facebook gives you everybody a program
[00:19:04.240 --> 00:19:09.520]   They can run locally on their photos. So the photos never do get uploaded just the hash of it. You're brilliant
[00:19:09.520 --> 00:19:11.800]   Jeff, would you call mark and tell them this?
[00:19:11.800 --> 00:19:18.440]   Then it uploads the hash to solve so many of their problem and the problem of course is the false positive or the or the prankster who wants to keep
[00:19:18.440 --> 00:19:21.320]   Photos of let's say Hillary Clinton photos off Facebook
[00:19:21.320 --> 00:19:28.120]   So when the alarm does go off that's when a human looks and says no, that's Hillary and lets it through or no
[00:19:28.120 --> 00:19:31.320]   Yeah, that's explicit and blocks it. Yeah, that would be much better
[00:19:32.280 --> 00:19:35.480]   So then the vulnerable person is only sending in a hash. Yes
[00:19:35.480 --> 00:19:43.280]   No, come on Facebook. Yeah, it's brilliant. I do say so myself. You have Mark's number column
[00:19:43.280 --> 00:19:52.600]   Mark about these explicit photos, but I want to point out it took Jeff about three seconds to think of the solution
[00:19:52.600 --> 00:19:55.160]   Why is Facebook like oh?
[00:19:55.160 --> 00:20:01.160]   Plus even even the instructions they have you got to share it with yourself and then report it and that's bizarre
[00:20:01.880 --> 00:20:03.880]   Yeah, yeah, I
[00:20:03.880 --> 00:20:10.640]   Think you should patent this plus. Yeah, well, there's another problem to it. So let's say that you and your honey
[00:20:10.640 --> 00:20:16.640]   Do something and the he presuming he's the bad guy takes the photos
[00:20:16.640 --> 00:20:22.720]   And it's on his phone and you don't have them so you're fearing what's we put up, but you don't have access to them
[00:20:22.720 --> 00:20:27.840]   That this mind my solution doesn't solve that that's all problem. Yeah, I can't
[00:20:28.600 --> 00:20:31.640]   I think that I would be honest. That's probably more often the case
[00:20:31.640 --> 00:20:40.080]   Well, the nudes that you send them aren't the ones that you're worried about it's supposed to be the ones you're worried about no
[00:20:40.080 --> 00:20:46.520]   Because then this is just stupid in so many ways. It's just it's just one of those publicity stunts
[00:20:46.520 --> 00:20:49.660]   It's a publicity stat and we gave him but what about the velocity is gonna be terrible
[00:20:49.660 --> 00:20:55.720]   The city is gonna be Facebook wants you to send them your sex pictures. Yeah, this is this this
[00:20:55.720 --> 00:20:58.480]   Hi
[00:20:58.480 --> 00:21:04.860]   Are we are we I'm we had a very good conversation with Ed bot about
[00:21:04.860 --> 00:21:07.680]   Facebook there
[00:21:07.680 --> 00:21:13.520]   I'm getting more and more disturbed by this company in particular, but in general by the big data
[00:21:13.520 --> 00:21:17.000]   Collection going on in all companies
[00:21:17.000 --> 00:21:22.840]   This is the the cashmere Hills got another great article now. Now. This is a lot of this is anecdotal
[00:21:22.840 --> 00:21:27.480]   So I'm curious what you think about this how it's the people you may know section in
[00:21:28.120 --> 00:21:33.300]   Facebook he's been on that for a while and here are the examples, you know, it started with her, right?
[00:21:33.300 --> 00:21:40.680]   She there her personal story, you know, I don't want to go. No, I can't remember all the details, but basically somebody she knows
[00:21:40.680 --> 00:21:45.400]   You know is connected with their family, but nobody else would know
[00:21:45.400 --> 00:21:51.360]   Was offered to her as people you may know on Facebook and it was like yeah, but that one it went well
[00:21:51.360 --> 00:21:54.860]   There's all kinds, but there's all kinds of connections to connection. She does know them
[00:21:55.080 --> 00:21:59.560]   We this this this is okay. All right, so let me oh my god. There was it might be it might not be
[00:21:59.560 --> 00:22:02.360]   I mean when you when I read these in there again, they're anecdotal
[00:22:02.360 --> 00:22:06.080]   Who could who knows it could be a coincidence? I'll give you a couple
[00:22:06.080 --> 00:22:13.800]   She says she's heard more than a hundred bewildering anecdotes. Here's one a man who years ago donated sperm to a couple secretly
[00:22:13.800 --> 00:22:15.800]   So they could have a child
[00:22:15.800 --> 00:22:19.320]   Had Facebook recommend the child as a person he should know
[00:22:19.320 --> 00:22:24.280]   He does know the couple still knows, but he's not friends with him on Facebook
[00:22:25.080 --> 00:22:30.960]   But he still knows the couple and there's friends or friends and if it ties him to that couple then
[00:22:30.960 --> 00:22:36.840]   The fact that that child is connected is not hard to see how that happens. Okay. Here's another one. Okay. I agree
[00:22:36.840 --> 00:22:43.000]   Look is pulling data from other places too. Mostly what I would guess in this case from other Facebook accounts
[00:22:43.000 --> 00:22:48.880]   No, I mean like your credit reports and not your credit reports. Do we know they're doing that I?
[00:22:50.280 --> 00:22:54.920]   Believe they talked about pulling data from
[00:22:54.920 --> 00:22:58.160]   transactions, okay
[00:22:58.160 --> 00:23:04.160]   Wouldn't require that to do this because I'm Jeff's right the scenario would be the couple
[00:23:04.160 --> 00:23:07.240]   He's not friends with him, but they have a friend in common
[00:23:07.240 --> 00:23:10.000]   Right and so
[00:23:10.000 --> 00:23:16.160]   He's friends with somebody who's friends with them and Facebook's algorithm would would parse that and say oh well since you know him
[00:23:16.160 --> 00:23:17.400]   You might know her
[00:23:17.400 --> 00:23:21.960]   And if you look at the people they recommend to me it goes way the hell out. I can't figure out
[00:23:21.960 --> 00:23:25.160]   I mean it's it's it's so far. It almost seems random
[00:23:25.160 --> 00:23:32.320]   All right, how about this one are very loose connections a social worker whose client called her by her nickname on their second visit
[00:23:32.320 --> 00:23:35.840]   Because she'd shown up in his people you may know
[00:23:35.840 --> 00:23:42.000]   Despite they're not having exchanged any contact information. It's again. I guess again. They may live in the front town
[00:23:42.000 --> 00:23:46.640]   They you know the maybe how did you get recommended the shrink in the first place?
[00:23:46.640 --> 00:23:49.640]   It may be through friends this one's tough, but again you're right
[00:23:49.640 --> 00:23:55.760]   You could you could it come up with a ways this could have happened and that's the problem with anecdotal evidence like this a woman
[00:23:55.760 --> 00:23:58.920]   whose father left her family when she was six
[00:23:58.920 --> 00:24:08.520]   Had his mistress of the time 40 years later suggested to her at Facebook not his current mistress your current
[00:24:08.520 --> 00:24:11.080]   Partner but somebody 40 years ago
[00:24:11.080 --> 00:24:15.520]   Once again, we don't know what the connections are we don't know okay
[00:24:16.480 --> 00:24:23.880]   Attorney who wrote I deleted Facebook after a recommended person people you may know a man who was defense counsel on one of my cases
[00:24:23.880 --> 00:24:27.840]   We'd only communicated through my work email, which is not connected to Facebook
[00:24:27.840 --> 00:24:33.520]   Which convinced me Facebook was scanning my work email. Yeah, I hear these stories all the time about Amazon Echo as well
[00:24:33.520 --> 00:24:39.320]   Remember that we we were having a conversation about traveling my husband and I in front of the Echo and
[00:24:39.320 --> 00:24:44.680]   All of a sudden Amazon started spamming me with luggage ads. That's somebody actually told me that
[00:24:45.080 --> 00:24:51.080]   Well, but I'm gonna go back to my story about the journalist who who told his family and friends on Facebook
[00:24:51.080 --> 00:24:56.480]   That all man. I fell down and I broke my phone and this happened and that happening story getting asked for multiple sclerosis
[00:24:56.480 --> 00:25:00.520]   It wasn't because somebody sat at Facebook and said this guy has MS
[00:25:00.520 --> 00:25:04.660]   It's because there were small signals there the advertiser had and
[00:25:04.660 --> 00:25:12.120]   Connections and AI systems to say people have multiple sclerosis happen to do these things and there signals that are that are not intuitive
[00:25:12.120 --> 00:25:14.900]   We don't know and yeah, welcome to the world of data folks
[00:25:14.900 --> 00:25:22.000]   But it's not some mysterious conspiratorial creepy thing. It's just there is an answer to how this happened
[00:25:22.000 --> 00:25:24.480]   We just don't know what the answer is and we don't like black boxes
[00:25:24.480 --> 00:25:28.960]   Okay, so Facebook has done deals with back in 2012
[00:25:28.960 --> 00:25:32.920]   It had a deal with data logics to get data from stores where you made purchases in
[00:25:32.920 --> 00:25:38.120]   2016 there was a story out saying Facebook looks at GPS data and
[00:25:38.760 --> 00:25:43.800]   contracted with Square to get their data for both whose in stores and what they're buying. Oh, wow
[00:25:43.800 --> 00:25:47.360]   So yes Facebook does get external data
[00:25:47.360 --> 00:25:51.120]   And it's just like magazines do deals with Axiom
[00:25:51.120 --> 00:25:53.840]   Yes
[00:25:53.840 --> 00:25:58.760]   I didn't know that you could opt out of a lot of those companies. I just found that out yesterday
[00:25:58.760 --> 00:26:00.320]   so
[00:26:00.320 --> 00:26:07.160]   researchers have determined that between four and eight pieces of data are all that's needed to de-anonymize someone and
[00:26:08.360 --> 00:26:12.120]   Location data actually requires less. So if you start thinking about that
[00:26:12.120 --> 00:26:17.840]   Facebook knows who you are and start making some surprising correlations and that's what's freaking people out
[00:26:17.840 --> 00:26:22.000]   Well, and I think cashmere acknowledges that and but what she says this is still cause for concern
[00:26:22.000 --> 00:26:25.600]   She says she writes behind the Facebook profile. You've built for yourself as another one a
[00:26:25.600 --> 00:26:30.320]   shadow profile built from the inboxes and smartphones of other Facebook users
[00:26:30.320 --> 00:26:37.880]   Contact information you've never given the network gets associated with your account making it easier for Facebook to more completely map your social
[00:26:38.640 --> 00:26:42.840]   connections really what's happening is we're reaching critical mass that the data
[00:26:42.840 --> 00:26:51.960]   Points that Facebook knows about you have gotten rich enough now that they can they can probably build a pretty good replica of your social graph
[00:26:51.960 --> 00:26:58.000]   By inference not direct connection, right? Not not even your social graph
[00:26:58.000 --> 00:27:04.080]   This is people who you have relationships with that you don't even you may not even be aware of right so it's
[00:27:05.000 --> 00:27:10.720]   Well, here's the example with the attorney Facebook. She says is not scanning the work email of the attorney of course
[00:27:10.720 --> 00:27:12.160]   I mean it might look like that
[00:27:12.160 --> 00:27:15.360]   But it likely has her work address on file
[00:27:15.360 --> 00:27:20.680]   Even if she never gave it to Facebook herself because if anyone who has lawyers addressing their contacts has shown
[00:27:20.680 --> 00:27:23.120]   Shared their contacts with Facebook which most everybody does
[00:27:23.120 --> 00:27:27.800]   Then the company can link her to anyone else who has it like the Defense Council in one of her cases
[00:27:27.800 --> 00:27:33.280]   So cashmere is not asserting any creepy no she's pointing out
[00:27:33.280 --> 00:27:38.680]   Same this person writing about this. Yeah, she's just pointing out that this is the nature. This is how Facebook works. I
[00:27:38.680 --> 00:27:43.080]   Where is people you may know because I'm in I don't I
[00:27:43.080 --> 00:27:47.360]   Don't see any of the history all the time. Oh, yeah, I don't see any of my page
[00:27:47.360 --> 00:27:54.720]   It comes up in your feed that's what I thought but I'm but I guess I don't know anybody Facebook has gotten less
[00:27:54.720 --> 00:27:58.800]   Do you have you know why only almost always only see as you Jeff?
[00:28:00.080 --> 00:28:05.920]   You sick of me. I know you're sick of me. You I see your tweets. Look at this. This is my Facebook page right now
[00:28:05.920 --> 00:28:08.120]   It's all Jeff Jarvis all the time
[00:28:08.120 --> 00:28:19.160]   And then there's an ad Richard painter you're what Richard painter on TV now
[00:28:19.160 --> 00:28:26.000]   He's he's the ethics. He was the bush ethics guy who's on he's on MSNBC constantly. He's like Dick Tracy
[00:28:26.000 --> 00:28:29.760]   He talks like Dick Tracy. It's just he's the greatest. I love this guy
[00:28:30.080 --> 00:28:32.080]   He's amazing
[00:28:32.080 --> 00:28:38.160]   I'm trying to look for this of course right now Facebook's not recommending to me. I get it all the time
[00:28:38.160 --> 00:28:43.760]   Yeah, I think maybe they turn this off after cashmere's article
[00:28:43.760 --> 00:28:49.680]   Hmm all go to friends and then go to find friends
[00:28:49.680 --> 00:28:54.480]   Oh, maybe I can do it my hand and then no, that's that's
[00:28:55.040 --> 00:29:01.800]   Sent requests. I know I've seen this before and I've accepted a bunch of friends that way and it's mostly people
[00:29:01.800 --> 00:29:05.720]   Oh, yeah, go to friends little little two people on the top
[00:29:05.720 --> 00:29:15.240]   And then scroll find your seats. Oh here is people being pymk. I know Daniel. I don't know florice
[00:29:15.240 --> 00:29:19.840]   I don't know Fernando. I had a little bit that'll group used to go there. They're not my dentist anymore
[00:29:19.840 --> 00:29:23.280]   Scottborn mutual friend
[00:29:24.000 --> 00:29:26.000]   And I know aunt. What's that him?
[00:29:26.000 --> 00:29:29.200]   huh, huh
[00:29:29.200 --> 00:29:32.200]   James guards guardian. Yeah, this guy's dead
[00:29:32.200 --> 00:29:38.200]   Shouldn't I mean literally he'd passed away a few years ago should should he be in there?
[00:29:38.200 --> 00:29:42.280]   I'll follow him anyway
[00:29:42.280 --> 00:29:44.720]   It hasn't been a lot of his timeline lately
[00:29:44.720 --> 00:29:48.600]   Huh
[00:29:48.600 --> 00:29:53.440]   Mine's pretty terrible is it like bad bad suggestions, but I am
[00:29:53.760 --> 00:29:56.880]   Never on Facebook like I I don't know
[00:29:56.880 --> 00:30:03.320]   So they get very little for me that I like the top person is someone I have no idea
[00:30:03.320 --> 00:30:10.440]   There's let's see there's also 71 pending friend requests. So oh, I've got 960
[00:30:10.440 --> 00:30:13.360]   Like Facebook's probably like I give up on you
[00:30:13.360 --> 00:30:19.800]   But did for a while it did for a while no one could could make a friend request for me for like two years and I kind of enjoyed it
[00:30:19.800 --> 00:30:23.960]   Oh, I just ignore everything at least but it's very hard
[00:30:23.960 --> 00:30:30.480]   I you always talk about moral panic and Jeff and I understand that and of course I agree with you
[00:30:30.480 --> 00:30:36.040]   But at the same time, it's very hard not to sometimes look at this stuff even if you know better and say it's kind of creepy
[00:30:36.040 --> 00:30:41.560]   That's why it needs explanation the creepiness is the mystery and that's why transparency matters
[00:30:41.560 --> 00:30:46.040]   That's why just saying this is what's happening folks and and and they may not want to because
[00:30:46.040 --> 00:30:48.760]   There's a lot of okay
[00:30:48.760 --> 00:30:55.240]   I don't think that it is just creepy. I think people are taking Facebook signals or
[00:30:55.240 --> 00:31:02.320]   signals from other data and they're applying it towards things like credit report or credit card
[00:31:02.320 --> 00:31:12.160]   Yeah, mortgage lending insurance payments and they're making actual real-world decisions based on this and people don't understand
[00:31:12.160 --> 00:31:15.840]   It's gone out of their control
[00:31:16.480 --> 00:31:22.680]   So social redlining and I would argue that some of the same worries we have about
[00:31:22.680 --> 00:31:31.480]   For example that the YouTube story we're gonna talk about later are very similar issues here. We're odd. They are they are we don't understand
[00:31:31.480 --> 00:31:38.320]   They're closely related. I agree. Yeah, but but that doesn't mean that we outlaw them now. It means we try to understand them better
[00:31:38.320 --> 00:31:44.400]   You can't understand how a computer so this is MIT actually put out
[00:31:44.400 --> 00:31:50.880]   I want to say it was two weeks ago some good research MIT tech review had some an article about how
[00:31:50.880 --> 00:31:58.920]   Researchers are trying to open up the black box of AI because what you do when you're building a neural network when you're building these kind of algorithms
[00:31:58.920 --> 00:32:04.320]   It depending on how you're teaching the computer, but you're giving it lots of examples
[00:32:04.320 --> 00:32:09.600]   You're tweaking it when it's right to help push it towards the right answers the answers you want
[00:32:09.600 --> 00:32:13.320]   But the computer figures all this out on its own
[00:32:13.840 --> 00:32:19.880]   So we really don't actually understand how it's coming to some of the conclusions that comes to all we can do is say
[00:32:19.880 --> 00:32:22.080]   Yeah, that's right. Good job
[00:32:22.080 --> 00:32:29.320]   I you know my list of people is actually pretty good. There's probably about about 15 people in here. I would like to know
[00:32:29.320 --> 00:32:36.480]   There's a lot of ones. I don't but here's a posting from Stanley labor and it's he's I don't even know who he is
[00:32:36.480 --> 00:32:42.840]   It's a it's actually I said the images and this are great. Do you know who he is? No, this was was this is an example of technopatic
[00:32:42.960 --> 00:32:47.040]   Okay, okay. Let me let me just mention it and yeah
[00:32:47.040 --> 00:32:51.320]   You all can be the judge by the way his title is you won't understand this
[00:32:51.320 --> 00:32:57.800]   He said this is my exit interview from social media and he and he kind of talks about each
[00:32:57.800 --> 00:33:00.480]   service and
[00:33:00.480 --> 00:33:04.840]   Then talks about the the insights he's learned. I remember the internet before Google
[00:33:04.840 --> 00:33:09.300]   F you forever Google for breaking email for everyone in the world
[00:33:09.640 --> 00:33:16.040]   F you forever for enticing users who don't know any better into relying upon your services and then shuttering them with little warning
[00:33:16.040 --> 00:33:21.120]   Your extensions to establish standards and every changing policies make it increasingly impossible for
[00:33:21.120 --> 00:33:26.960]   Individuals to host and control their own information. Yes. I am aware. This is no accident
[00:33:26.960 --> 00:33:32.720]   Search quality is no longer a core competency of Google's the internet's premier search engine
[00:33:32.720 --> 00:33:36.720]   Two people type the same search each receives a different result. Yes, I'm aware
[00:33:36.720 --> 00:33:44.440]   This is likewise no accident the quality of ads displayed alongside various Google services has steadily devolved from semi-relevant to absolutely
[00:33:44.440 --> 00:33:46.400]   irrelevant at all times
[00:33:46.400 --> 00:33:48.400]   Yes, I'm aware. This is no accident
[00:33:48.400 --> 00:33:55.080]   Then he's talking about he has negatives in each column and malware served is one of them. That's true insight
[00:33:55.080 --> 00:34:02.120]   This is kind of a joke Google does not want you to know or remember anything if at all possible
[00:34:02.120 --> 00:34:05.080]   Then he talks about Facebook
[00:34:05.960 --> 00:34:12.360]   mandatory nonlinear curation of user contributed content malware served gamification of personal interaction
[00:34:12.360 --> 00:34:17.760]   Degrades human health. I can't I can't disagree with him on that gamification
[00:34:17.760 --> 00:34:21.600]   Right, that's what they're doing of personal interaction
[00:34:21.600 --> 00:34:25.280]   Degrades human health. It's bad for you
[00:34:25.280 --> 00:34:28.240]   No
[00:34:28.240 --> 00:34:30.320]   He says Facebook provides worldwide
[00:34:30.320 --> 00:34:35.880]   24/7 telepathic content with every contact with every person I or members of my extended network have
[00:34:35.880 --> 00:34:37.240]   Ever met
[00:34:37.240 --> 00:34:40.720]   How many degrees of separation between me and the worst person alive?
[00:34:40.720 --> 00:34:45.560]   Now that person knows that my mom threw up after breakfast and wants to offer advice
[00:34:45.560 --> 00:34:50.880]   Twi I kind of enjoyed this Twitter worldwide
[00:34:50.880 --> 00:34:58.360]   24/7 telepathic content with every person I or people I used to think of his friends ever met or favorited or we tweeted
[00:34:58.360 --> 00:35:05.240]   How many degrees of separation between me and the worst person alive now that person knows the GPS coordinates of my bedroom?
[00:35:06.120 --> 00:35:13.240]   Game of vacation of personal interaction degrades human health. You see but hold on right there. That's the line that got quoted
[00:35:13.240 --> 00:35:19.640]   Right, I know that by the way, that's me. Yeah, well others did as well. Okay, and and
[00:35:19.640 --> 00:35:26.360]   It's just it there's a damage being done here, and that's what concerns me. I'm about to give up
[00:35:26.360 --> 00:35:31.240]   I'm tired of this fight. I think there's a damage being done to us the game of I think it's exactly right
[00:35:31.720 --> 00:35:36.280]   What what used to be a benevolent and in fact valuable?
[00:35:36.280 --> 00:35:44.440]   Thing in life, which was interaction between two humans on a real genuine intimate level has been degraded
[00:35:44.440 --> 00:35:48.440]   Into these into these tweet storms. Hold on. Yeah, hold on
[00:35:48.440 --> 00:35:54.280]   Rally check so I you know here this oh my god, panic, panic, panic
[00:35:54.280 --> 00:35:59.240]   So go through your facebook feed and point out the first piece of degrading horrible crap you find
[00:35:59.640 --> 00:36:02.840]   You're gonna be you're gonna be scrolling for a few hours really it's yeah
[00:36:02.840 --> 00:36:08.920]   I don't see degrading horrible crap in my and I use facebook a lot so it's gonna it's gonna ope up stuff to me. No
[00:36:08.920 --> 00:36:16.040]   And even even twitter. I mean, you know, unless you let's you want to go follow the jury, right? It's all friends and nice people. Yes
[00:36:16.040 --> 00:36:19.880]   Yes, we're losing sight of that. You know, it's
[00:36:19.880 --> 00:36:23.960]   It's more subtle than that. It's more subtle than that. Oh
[00:36:23.960 --> 00:36:27.560]   It's it's uh
[00:36:27.720 --> 00:36:30.920]   I don't know. I think it's pseudo relationship
[00:36:30.920 --> 00:36:36.360]   So there's been okay, no stop. Okay, so there's plenty of things
[00:36:36.360 --> 00:36:43.640]   This guy, you know, jeff you can scream technoc plan panic, but there are plenty of people who have the mown to the fact that these
[00:36:43.640 --> 00:36:49.160]   Online services in even software and apps are
[00:36:49.160 --> 00:36:55.880]   Gamifying things to get you to spend more time to boost their metrics and a lot of people
[00:36:56.600 --> 00:36:58.600]   Are rightly concerned about this kind of digital
[00:36:58.600 --> 00:37:03.960]   Hey stacy we make a living doing that rain headlines and journalism
[00:37:03.960 --> 00:37:08.360]   Yes, we do, but we also
[00:37:08.360 --> 00:37:12.600]   Right. Okay. We're not that good at it
[00:37:12.600 --> 00:37:14.760]   Also offer
[00:37:14.760 --> 00:37:17.240]   I'm not i'm not trying to be holy. I'm just saying
[00:37:17.240 --> 00:37:20.840]   That this is this is a thing that's happened. It's also I was gonna
[00:37:20.840 --> 00:37:25.480]   Make the point that we have been worried about this with tv forever
[00:37:26.280 --> 00:37:28.280]   and and and
[00:37:28.280 --> 00:37:34.200]   See this this is exactly what moral panic is the tv is gonna ruin society that that gives no respect
[00:37:34.200 --> 00:37:40.760]   To the members of society that we are such people that we could be pulled along. I don't believe that without this debate
[00:37:40.760 --> 00:37:49.720]   Then we do get unfettered manipulation and I would say so I actually tweeted this because I thought this was a really good example of
[00:37:49.720 --> 00:37:52.520]   digital manipulation that is
[00:37:53.160 --> 00:38:00.680]   Horribly uncool and takes advantage of vulnerable people and it was uh the interneting with the mandahas column from the new york times this week
[00:38:00.680 --> 00:38:06.520]   Which was a four minute video talking about beauty apps and filters for various
[00:38:06.520 --> 00:38:08.440]   thus
[00:38:08.440 --> 00:38:10.600]   Instagram, snapchat, etc
[00:38:10.600 --> 00:38:11.400]   And
[00:38:11.400 --> 00:38:20.040]   Basically the idea was that they're pushing just insane standards of beauty like facial bleaching knows mushing and all of these other things to give you
[00:38:20.040 --> 00:38:22.520]   give young girls
[00:38:23.320 --> 00:38:24.280]   the
[00:38:24.280 --> 00:38:26.280]   plastic looking faces
[00:38:26.280 --> 00:38:27.720]   um
[00:38:27.720 --> 00:38:29.080]   and the
[00:38:29.080 --> 00:38:36.360]   They talked about how gamification is really driving these girls back and forth and it does have a measurable effect on their self-esteem
[00:38:36.360 --> 00:38:39.160]   And we can't ignore that and we shouldn't ignore that
[00:38:39.160 --> 00:38:44.760]   And you can call it technopanic, but I think we should be having these conversations because otherwise
[00:38:44.760 --> 00:38:48.600]   We're just giving them a pass to take advantage of everyone
[00:38:48.600 --> 00:38:51.000]   And that's not cool
[00:38:51.000 --> 00:38:53.000]   And that's that's my thought
[00:38:53.000 --> 00:38:55.000]   Yeah, but but but the bottom line
[00:38:55.000 --> 00:39:00.280]   It's people doing the bad stuff. Yeah, well, it's and and the norm is that we need to create the norm
[00:39:00.280 --> 00:39:06.760]   You're right. We have the conversation, but blaming just the technology and the platform. No, no, there are people who are doing bad things
[00:39:06.760 --> 00:39:09.640]   It's not blame and the corporations using
[00:39:09.640 --> 00:39:11.640]   technology
[00:39:11.640 --> 00:39:13.800]   In ways that we're never you can turn that around with you
[00:39:13.800 --> 00:39:18.520]   You could also say that the technology exists and people come in and exploit was meant for good. Yes
[00:39:18.680 --> 00:39:21.160]   Well, how did Matt cut to make his make his salary?
[00:39:21.160 --> 00:39:24.440]   Right because something good was created that people then exploited for bad
[00:39:24.440 --> 00:39:28.600]   And and they have to put enough resources into it to try to stop that from happening
[00:39:28.600 --> 00:39:34.920]   But the chicken and egg here is not that the big bad technology company comes along and says how can we ruin society and make a fortune doing so?
[00:39:34.920 --> 00:39:37.000]   I don't believe that
[00:39:37.000 --> 00:39:43.320]   That's what you hear. It's scale and that is what's scaring people. There are good things that are very scary
[00:39:43.960 --> 00:39:48.360]   One you can very cheaply learn how to gamify people
[00:39:48.360 --> 00:39:52.680]   Incredibly effectively so a b testing costs nothing on these platforms, right?
[00:39:52.680 --> 00:40:01.480]   So you've basically scaled the ability to manipulate people incredibly effectively and that is that is something we should be talking about in
[00:40:01.480 --> 00:40:03.800]   What do you want to do about it?
[00:40:03.800 --> 00:40:05.800]   That's the question. I
[00:40:05.800 --> 00:40:10.360]   I'll tell you what I think we need to hold corporations feet to the fire and this is going to get
[00:40:10.840 --> 00:40:14.120]   uh continue with this next segment which we're going to talk about the
[00:40:14.120 --> 00:40:18.280]   Gamification of one year olds and their attention
[00:40:18.280 --> 00:40:24.200]   Uh, but I think we need I think these companies and this is what scott galloway saying as well
[00:40:24.200 --> 00:40:32.280]   A time of you also have to hold individuals who do this stuff. See it's you're blaming strictly the platform. You're not blaming the actors
[00:40:32.280 --> 00:40:35.160]   Well who you mean you can't believe
[00:40:35.160 --> 00:40:37.720]   Who do you mean?
[00:40:38.520 --> 00:40:41.720]   Is who makes that that bad thing that goes to kids?
[00:40:41.720 --> 00:40:44.440]   You gotta go to that state
[00:40:44.440 --> 00:40:51.960]   As a state you have to enact laws to prevent bad things as a platform you have to
[00:40:51.960 --> 00:40:57.080]   Laws are probably not the right but enact rules and punish people and right now
[00:40:57.080 --> 00:41:03.000]   I think the issue especially with the youtube thing is that they're not actually paying attention
[00:41:03.000 --> 00:41:06.600]   They're not devoting the resources to police the platforms for these bad
[00:41:06.840 --> 00:41:12.600]   Worsen that they're actively abrogating their responsibility because of the holy grail of growth
[00:41:12.600 --> 00:41:19.640]   In the stock market and you can say that for twitter you can say that for facebook growth is everything and that's why they're
[00:41:19.640 --> 00:41:22.280]   Abrogating their responsibility because it's financially
[00:41:22.280 --> 00:41:26.840]   Important to them that they have this kind of user growth. They need to be a game of fun
[00:41:26.840 --> 00:41:28.920]   I mean come in and advocate for the devil here to both of you
[00:41:28.920 --> 00:41:31.800]   Number number one
[00:41:31.800 --> 00:41:34.920]   Uh, I am what i'm trying to suggest here is you don't only go over the platform
[00:41:34.920 --> 00:41:38.520]   There are actors who are manipulating the platform to do bad things you've got to go after them
[00:41:38.520 --> 00:41:43.560]   You can't just say the technologist should fix this that's that the the person who's doing the bad act
[00:41:43.560 --> 00:41:47.880]   But don't you agree with stacey that the platforms have a responsibility number two number two
[00:41:47.880 --> 00:41:51.080]   The platforms do have a responsibility right but you are way
[00:41:51.080 --> 00:41:54.520]   Under estimating the difficulty that scale brings
[00:41:54.520 --> 00:41:57.080]   I am not
[00:41:57.080 --> 00:42:01.640]   Right there's so let me finish this one. Let me finish this one. Okay. Let me finish this
[00:42:01.960 --> 00:42:05.880]   I got two against one here. Normally, I'm the interruptor. I know but I get two against one here
[00:42:05.880 --> 00:42:11.080]   Al Franken how could you not have seen that rubles about those ads? Well, come on
[00:42:11.080 --> 00:42:15.320]   We all know that sophistication they weren't looking for the signal of rubles and political ads
[00:42:15.320 --> 00:42:18.760]   It was only after the fact they said oh crap rubles and political ads
[00:42:18.760 --> 00:42:23.800]   Right and there was no system in that there's built-in scale is hard. Yes scale is profitable
[00:42:23.800 --> 00:42:27.640]   But scale is also hard scale is what enables you're at married to sell or jam
[00:42:27.880 --> 00:42:34.120]   Scale is what enables us all to make videos and podcasts and make a living at it scale is what enables all this good stuff
[00:42:34.120 --> 00:42:39.800]   But scale is also difficult now should we hold them responsible? Yes, but in a reasonable way
[00:42:39.800 --> 00:42:45.160]   And just saying everything is their fault the darn platforms to this while we're taking advantage of all the stuff
[00:42:45.160 --> 00:42:48.200]   I don't think that's just being I don't think it's being honest
[00:42:48.200 --> 00:42:52.040]   And I also think it's just being productive because if you go after the platforms
[00:42:52.040 --> 00:42:56.600]   Somehow you've also got what after the bad actors who are manipulating the platforms
[00:42:56.920 --> 00:43:02.520]   Actually, we are in league with the platforms against those guys, but we're not treating it that way. Okay now I'm done
[00:43:02.520 --> 00:43:07.640]   Okay, so couple things one when it's a money-making issue
[00:43:07.640 --> 00:43:14.040]   These companies find it's worth their while to actually deal with their problems that are caused by scale
[00:43:14.040 --> 00:43:17.160]   So we see this with things like crappy search results
[00:43:17.160 --> 00:43:20.680]   Google for example comes in and is like oh, hey, you know what?
[00:43:20.680 --> 00:43:25.880]   We're seeing a lot of people do these stupid how-to articles that are useless content
[00:43:25.880 --> 00:43:29.000]   We're gonna start downgrading those Facebook does it too with
[00:43:29.000 --> 00:43:33.880]   Upward now upward. Thank you. I was like the headline site, which by the way
[00:43:33.880 --> 00:43:37.880]   I was started with the best of motivations up where they was and did the worst to media
[00:43:37.880 --> 00:43:40.280]   I know they're they're lovely
[00:43:40.280 --> 00:43:45.160]   But they're not evil, but they were gaming at the system. So you've got both
[00:43:45.160 --> 00:43:45.640]   Amy
[00:43:45.640 --> 00:43:47.000]   Find Amy
[00:43:47.000 --> 00:43:48.040]   Google
[00:43:48.040 --> 00:43:50.760]   No, no, no, no, you're no, she's right. Jeff. I mean
[00:43:51.560 --> 00:43:58.440]   Link farms demand media time and time again. You're right. There are bad actors who say we're gonna take advantage of the algorithm
[00:43:58.440 --> 00:44:01.560]   Make a lot of money and when there's things happen
[00:44:01.560 --> 00:44:06.120]   Google is able to stop it. Well, that's why mac cuts has a job
[00:44:06.120 --> 00:44:08.840]   Exactly
[00:44:08.840 --> 00:44:12.440]   You said you let me you said you let me finish and you're pulling the up worthy straw man
[00:44:12.440 --> 00:44:14.120]   people
[00:44:14.120 --> 00:44:16.120]   People who raised up worthy
[00:44:18.120 --> 00:44:20.120]   The rest of this is
[00:44:20.120 --> 00:44:23.880]   Public up with his motor was to use these techniques to inform people get them
[00:44:23.880 --> 00:44:26.200]   Jeff. I think they went off
[00:44:26.200 --> 00:44:30.760]   All right, don't be don't be mean. Okay. Let me finish
[00:44:30.760 --> 00:44:38.200]   After okay, so they have made these efforts, but when it comes to things that are
[00:44:38.200 --> 00:44:41.240]   profitable in a way
[00:44:41.240 --> 00:44:44.360]   That isn't significant to them and doesn't appear to have a lot of harm
[00:44:44.360 --> 00:44:50.680]   I guarantee you they're going to start having people moderate the youtube kids content after all this kerfuffle because it's not going to be worth it to them
[00:44:50.680 --> 00:44:54.520]   But up until this point up until someone pointed it out
[00:44:54.520 --> 00:45:01.480]   They were content to let this go and the reason they'll figure it out is because Disney and Nickelodeon are going to be like
[00:45:01.480 --> 00:45:07.240]   Hello, your brand is being totally tarnished and they're going to pull their content. So
[00:45:07.240 --> 00:45:10.840]   The point here is these guys
[00:45:12.840 --> 00:45:14.360]   They are
[00:45:14.360 --> 00:45:18.920]   They are not benign. They understand scale is hard, but when they want to
[00:45:18.920 --> 00:45:24.920]   They can address it. And so I don't think it's wrong to sometimes say oh guys
[00:45:24.920 --> 00:45:32.120]   Uh, you're letting a lot of crap through here and you need to fix it. It's not techno panic. It's a legitimate
[00:45:32.120 --> 00:45:38.360]   Democratic way to like engage with the platform. All right pause. Yeah, we agree we agree
[00:45:38.360 --> 00:45:40.600]   We agree, but there's two there's two one more second layer
[00:45:41.000 --> 00:45:43.480]   All I'm saying is that if that's all you do
[00:45:43.480 --> 00:45:46.520]   Then that means that you blame the technology for everything
[00:45:46.520 --> 00:45:52.200]   I say that in addition to that you need two more things one is you need to go after the actors who actually do the bad acts
[00:45:52.200 --> 00:45:54.760]   And I don't hear a lot of that going on very hard
[00:45:54.760 --> 00:45:59.240]   Well, it's in international the problem is it's international and law enforcement
[00:45:59.240 --> 00:46:05.960]   Knowledge that number two is less acknowledged that this isn't that easy for the platform to say oh rubles and ads
[00:46:05.960 --> 00:46:09.960]   Why didn't you know that easier for the platform than for inter paul
[00:46:10.920 --> 00:46:15.240]   Uh, i'm saying it all comes together and i'm saying that we also as individuals
[00:46:15.240 --> 00:46:20.680]   We have a responsibility to change the norm so we don't go supporting fights online and troll
[00:46:20.680 --> 00:46:27.800]   We all agree all of these things have to happen, but I think stacy's point is very well taken that if you want to be a platform
[00:46:27.800 --> 00:46:36.760]   I agree you no longer get to say oh, we're just you know a common carrier. We're not responsible and I feel that many companies, you know
[00:46:38.600 --> 00:46:42.680]   Twitter is a very good example. I think facebook's a very good example google. Maybe pretty good example
[00:46:42.680 --> 00:46:46.840]   Many platforms just say you know this we're just a carrier. It's not our fault
[00:46:46.840 --> 00:46:48.920]   Let's take a break
[00:46:48.920 --> 00:46:54.760]   Uh, because this conversation will continue and we get to this uh, something is wrong on the internet
[00:46:54.760 --> 00:46:57.320]   And I didn't be much done already to go
[00:46:57.320 --> 00:47:01.720]   Well, I have to show you these videos and uh, and I want to warn people
[00:47:01.720 --> 00:47:07.160]   That not to let your two-year-olds watch what we're gonna do in a minute because it's disturbing
[00:47:07.800 --> 00:47:12.280]   And if you're uh, if I don't think it's so disturbing that an adult will be harmed by it
[00:47:12.280 --> 00:47:16.440]   But I hate to think of a little kid watching some of these things. We're gonna take a break come back with more
[00:47:16.440 --> 00:47:20.120]   Jeff Jarvis professor journalism university over student york
[00:47:20.120 --> 00:47:23.800]   the the master of the socratic dialogue
[00:47:23.800 --> 00:47:26.920]   Stacey higginbotham
[00:47:26.920 --> 00:47:33.640]   Uh journalist par excellence stacey on iot.com and i'm just the guy sitting in between them
[00:47:34.680 --> 00:47:37.480]   Our show today brought brought to you by euro
[00:47:37.480 --> 00:47:46.280]   euro started in what it was about 2015 26 early 2016. I remember reading about him getting very excited and pre-ordering
[00:47:46.280 --> 00:47:52.200]   Uh, it was the very first uh, consumer mesh system
[00:47:52.200 --> 00:47:55.720]   There have been business systems designed to do this in fact we'd used some
[00:47:55.720 --> 00:48:00.040]   But nobody ever thought of doing this in the house euro did and it was brilliant
[00:48:00.680 --> 00:48:04.680]   The idea of not a not a Wi-Fi extender but solving ever, you know
[00:48:04.680 --> 00:48:07.560]   increasingly worse Wi-Fi issues
[00:48:07.560 --> 00:48:14.040]   With a mesh Wi-Fi network that includes a base station and multiple access points was brilliant
[00:48:14.040 --> 00:48:20.360]   Since then they've learned from hundreds of thousands of systems and one of the advantages of being the market leader is
[00:48:20.360 --> 00:48:26.280]   That means they know a lot about what makes Wi-Fi break and what makes it work
[00:48:26.680 --> 00:48:33.720]   The second generation euro is now out. Oh, I love it. I put it here the an hero and two beacons the little
[00:48:33.720 --> 00:48:36.680]   satellite stations in my house
[00:48:36.680 --> 00:48:42.200]   And it is it is the best euro we've ever had more speed more range same high quality design
[00:48:42.200 --> 00:48:46.440]   Actually, I think it's even more elegant the euro beacons just plug into the wall. They have a little night light
[00:48:46.440 --> 00:48:49.400]   So we I put them in the halls and it's very convenient
[00:48:49.400 --> 00:48:55.240]   With the addition of the third five gigahertz radio the second generation euro is tri-band
[00:48:55.960 --> 00:49:01.560]   And by the way twice as fast as it's predecessor so you could do a lot more watch more movies listen to more music
[00:49:01.560 --> 00:49:03.480]   surf and
[00:49:03.480 --> 00:49:06.680]   and email to your heart's content and
[00:49:06.680 --> 00:49:14.440]   They've added a thread radio. So now they can connect to low power devices like locks doorbells other sensors and more
[00:49:14.440 --> 00:49:20.520]   If you need more easy to add another beacon every 1500 square feet should have its own
[00:49:20.520 --> 00:49:25.000]   You plug it into a wall. It's easy. If there's an outlet there's Wi-Fi
[00:49:25.800 --> 00:49:30.520]   We have a I like euro so much that I gave my mom my original system
[00:49:30.520 --> 00:49:33.080]   Went out to Providence Rhode Island set it up for
[00:49:33.080 --> 00:49:38.680]   And it really solved the problem she had as a art studio out in the backyard that she couldn't get Wi-Fi
[00:49:38.680 --> 00:49:41.160]   She really loves listening to internet radio streaming internet radio
[00:49:41.160 --> 00:49:46.920]   Including this show and so now she can do that we put it I put an euro out there in an euro in her kitchen
[00:49:46.920 --> 00:49:50.680]   I have myself we have three euros in our home
[00:49:51.560 --> 00:49:56.680]   And I just it's a set it and forget it and euro has so many nice features now including
[00:49:56.680 --> 00:49:59.320]   a great parental control features
[00:49:59.320 --> 00:50:05.240]   Uh, we can you can identify every device that attaches to your network and associate it with a member of the household
[00:50:05.240 --> 00:50:11.080]   So I have all of the things Michael uses his phone his computer his tablet in his account
[00:50:11.080 --> 00:50:14.920]   And I can say I can actually do it with the echo. I could say echo pause michaels Wi-Fi
[00:50:14.920 --> 00:50:17.880]   And suddenly he's offline
[00:50:18.040 --> 00:50:22.120]   It's not I do it. I do it because I love him we do it because we care
[00:50:22.120 --> 00:50:26.120]   We actually have a set time that the internet goes off 10 p.m
[00:50:26.120 --> 00:50:28.360]   Doesn't come back on till mine next morning
[00:50:28.360 --> 00:50:34.280]   It also filters so he can't see stuffy. We don't want him to see on the internet really works very very nicely
[00:50:34.280 --> 00:50:36.840]   Uh, of course the adult computers aren't filtered
[00:50:36.840 --> 00:50:40.920]   But they're well they are because they're filtered against malware and other attacks
[00:50:40.920 --> 00:50:45.720]   And the euro automatically updates the firmware gets updated all the time. I'll give you an example
[00:50:45.720 --> 00:50:47.720]   You know about the crack of vulnerability?
[00:50:48.120 --> 00:50:49.320]   that that
[00:50:49.320 --> 00:50:52.120]   Minute was possible for a bad guy to snoop on your Wi-Fi
[00:50:52.120 --> 00:50:58.120]   Within 24 hours of that being revealed it was revealed on a monday by tuesday
[00:50:58.120 --> 00:51:01.960]   euro had automatically put it pushed out of fixed to all of their customers
[00:51:01.960 --> 00:51:04.280]   Nobody's that good
[00:51:04.280 --> 00:51:06.760]   They're the best incredibly fast response time
[00:51:06.760 --> 00:51:11.720]   Automatic updates. It's what you need what you have to have an iot device
[00:51:11.720 --> 00:51:14.440]   It and it is the best router out there
[00:51:15.800 --> 00:51:20.200]   Say simply enough iOS and uh android app makes it easy to set up but also easy to control
[00:51:20.200 --> 00:51:25.800]   No more dead zones. No more buffering Wi-Fi that works. We've got free overnight shipping because I know you want it now
[00:51:25.800 --> 00:51:30.040]   euro eer o dot com use the offer code twig and
[00:51:30.040 --> 00:51:35.720]   Uh, you might want to consider my example if you've got family members who are complaining about their internet
[00:51:35.720 --> 00:51:41.960]   Be a very nice thing this Thanksgiving or this holiday season to go visit them and
[00:51:41.960 --> 00:51:44.920]   Have a nice package of euros wrapped up for them
[00:51:45.560 --> 00:51:49.640]   eer o dot com get free overnight shipping with the offer code twig
[00:51:49.640 --> 00:51:53.160]   Something is wrong on the internet
[00:51:53.160 --> 00:52:02.040]   So what's basically happening here is happening not just on youtube but everywhere the idea that
[00:52:02.040 --> 00:52:09.960]   Videos are starting to be algorithmically generated with algorithmically generated titles and hashtags
[00:52:09.960 --> 00:52:13.160]   Because people have observed that little kids
[00:52:13.800 --> 00:52:16.840]   Nonverbal little kids are using tablets watching
[00:52:16.840 --> 00:52:22.120]   YouTube and some of them fairly benign like this one. This is the finger family song
[00:52:22.120 --> 00:52:25.480]   watching youtube videos enjoying them
[00:52:25.480 --> 00:52:28.680]   and then um
[00:52:28.680 --> 00:52:35.960]   Mashing the next button or auto playing to the next video now. This is the i've shown you this because it kind of sets it up
[00:52:35.960 --> 00:52:40.360]   This is the finger family and some people might think this is a little creepy to begin with
[00:52:41.320 --> 00:52:46.040]   Would you now when you get kids commercials out there kids shows out there because of her creepy too, but kids like them
[00:52:46.040 --> 00:52:52.680]   Yeah, it's bright colors. It's nonverbal and by the way, it's something a child of three or four
[00:52:52.680 --> 00:52:56.840]   Identifies with it's things. They know their family their hands their bedroom
[00:52:56.840 --> 00:53:00.440]   Uh, so it's like any nursery song
[00:53:00.440 --> 00:53:06.280]   It's you know, it's very approachable and by the way, it's very catchy. It couldn't stop singing it after i watched this video
[00:53:06.280 --> 00:53:09.240]   Here's the song
[00:53:10.200 --> 00:53:15.800]   We've gone to finger family park where there's a giant hand
[00:53:15.800 --> 00:53:17.800]   and
[00:53:17.800 --> 00:53:23.560]   And a boy and a girl on the hand dancing it has a little bit of a bollywood flavor to it
[00:53:23.560 --> 00:53:28.760]   Daddy finger daddy finger
[00:53:28.760 --> 00:53:35.640]   I'm guessing this must have originated in india or somewhere like that
[00:53:37.240 --> 00:53:43.160]   By the way, notice still worse notice the subscribe to our channel suggested little miss muffin nursery rhyme
[00:53:43.160 --> 00:53:48.440]   Of course, something will auto play right after it. So that's the i don't know if this is original, but that's
[00:53:48.440 --> 00:53:55.880]   It's kind of a benign example. It's a little creepy. They got parents and kids on fingers like i like to say Barney's creepy
[00:53:55.880 --> 00:53:57.480]   It's like barney
[00:53:57.480 --> 00:54:03.320]   Uh, what's what's that one with the the big the big roly poly beasts walked in the colorful
[00:54:03.320 --> 00:54:05.240]   land
[00:54:05.240 --> 00:54:08.680]   British one. Oh, teletubbies. Yeah, teletubbies. Yeah, that's creepy
[00:54:08.680 --> 00:54:14.600]   And then there's is there's another meme some of these by the way are created intentionally created creepy light from forchan and others
[00:54:14.600 --> 00:54:20.280]   This is the meme of the wrong heads. So this is where disney might get involved. There's a laden
[00:54:20.280 --> 00:54:23.080]   same song
[00:54:23.080 --> 00:54:26.120]   And now different heads from the aladdin movie
[00:54:26.120 --> 00:54:31.240]   Are on there and a little kid from a different disney
[00:54:32.920 --> 00:54:36.760]   I don't actually think she comes from disney at all is crying. She's another character
[00:54:36.760 --> 00:54:39.080]   from despicable me. That's right
[00:54:39.080 --> 00:54:45.000]   So you could say well, that's not so bad that's teaching kids
[00:54:45.000 --> 00:54:47.000]   I don't know
[00:54:47.000 --> 00:54:53.000]   She's coming out to climb them. She's telling them that that's the wrong head right so I can see that this is this is not that bad
[00:54:53.000 --> 00:54:54.280]   That's benign
[00:54:54.280 --> 00:54:56.600]   Okay, now we get creepy you ready to get creepy
[00:54:56.600 --> 00:55:02.440]   And we can't watch the whole thing you can read the article James bridles
[00:55:02.440 --> 00:55:04.920]   Which is very long which is long on
[00:55:04.920 --> 00:55:07.320]   a medium
[00:55:07.320 --> 00:55:11.240]   But this so what's happened this one has been generated humans have to create these
[00:55:11.240 --> 00:55:17.160]   But he says small animation teams because it's very primitive animation can crank these out
[00:55:17.160 --> 00:55:23.960]   They're auto generated in terms of names and text so the name of this is buried alive outdoor playground finger family
[00:55:23.960 --> 00:55:27.480]   song nursery rhymes animation education
[00:55:27.480 --> 00:55:30.120]   learning video
[00:55:30.920 --> 00:55:32.920]   Uh, and this combines a lot of the
[00:55:32.920 --> 00:55:37.480]   Stuff you might have seen we're gonna have a hamburger emoji fight now
[00:55:37.480 --> 00:55:39.560]   Sorry
[00:55:39.560 --> 00:55:43.960]   No, they're cheeses in the right spot. Well, they also they're purple now here comes the hulk and a motorcycle
[00:55:43.960 --> 00:55:47.560]   McDonald's by the way featuring prominently here
[00:55:47.560 --> 00:55:51.480]   Um, oh actually I jumped ahead didn't I
[00:55:51.480 --> 00:55:56.440]   No, I should yeah, I should go to the beginning which is the creepiest part. Here's the joker
[00:55:56.440 --> 00:56:00.520]   Walking along he throws a can and a guy on a motorcycle with spider-man
[00:56:01.080 --> 00:56:03.640]   Knox him off motorcycle crashes he picks up
[00:56:03.640 --> 00:56:08.040]   That creepy creepy creepy cry picks up a spider-man
[00:56:08.040 --> 00:56:14.200]   Now here comes some creepy clown does the same thing to the hulk knocks him out picks him up
[00:56:14.200 --> 00:56:17.080]   carries him off
[00:56:17.080 --> 00:56:25.880]   Uh, then this is really creepy is a young woman sitting in her kitchen. Who is it Elsa? It's Elsa from frozen
[00:56:25.880 --> 00:56:28.520]   Yeah, and then just not dressed in her stuff
[00:56:28.920 --> 00:56:34.120]   And then he sprays something on her and she passes out and carries her off
[00:56:34.120 --> 00:56:36.760]   Is that punisher? Who is that it's some
[00:56:36.760 --> 00:56:44.440]   Some cartoon character. Here's purple spider girl death comes in with a screen mask and a size knocks her out
[00:56:44.440 --> 00:56:47.960]   Again the screen now I
[00:56:47.960 --> 00:56:54.200]   The problem is because of the title on this apparent might and bright colors in the listen the music playful music apparent might
[00:56:54.920 --> 00:56:58.920]   Very well accidentally let a kid watch this maybe they started with daddy finger
[00:56:58.920 --> 00:57:01.880]   now they've been buried alive these characters
[00:57:01.880 --> 00:57:04.360]   and they're
[00:57:04.360 --> 00:57:06.360]   Dancing around them, but fortunately
[00:57:06.360 --> 00:57:09.160]   This gets really creepy here comes
[00:57:09.160 --> 00:57:12.280]   baby heads on
[00:57:12.280 --> 00:57:19.400]   And you know what's going on here another spider character in the hulk with baby heads
[00:57:19.400 --> 00:57:22.120]   spotted they call the police
[00:57:24.040 --> 00:57:26.040]   the police woman is going to come
[00:57:26.040 --> 00:57:29.640]   rescue the people who is also Elsa also Elsa
[00:57:29.640 --> 00:57:32.520]   Well, but see little kids they go. Oh, it's Elsa
[00:57:32.520 --> 00:57:35.880]   And now the police have come
[00:57:35.880 --> 00:57:41.400]   in her Chevy pickup and she beats up all the bad guys
[00:57:41.400 --> 00:57:45.400]   the baby head people dig up the characters
[00:57:45.400 --> 00:57:50.360]   Everybody's free death gets a kick in the head
[00:57:50.520 --> 00:57:52.520]   And
[00:57:52.520 --> 00:57:55.960]   I don't know it's
[00:57:55.960 --> 00:58:03.560]   Is this creepy it seems creepy to me and I wouldn't want a one-year-old to watch it seems no
[00:58:03.560 --> 00:58:06.040]   nightmare material
[00:58:06.040 --> 00:58:08.040]   And and there's a
[00:58:08.040 --> 00:58:10.840]   It's not the worst. I won't show you the worst stuff, but
[00:58:10.840 --> 00:58:14.600]   Um, you wouldn't I think so you said stacey you have a
[00:58:14.600 --> 00:58:17.320]   personal experience to relate to this
[00:58:17.320 --> 00:58:18.920]   Yeah, so
[00:58:18.920 --> 00:58:20.040]   Anna
[00:58:20.040 --> 00:58:26.040]   My daughter she watches youtube a lot and she's actually really cautious about what she watches because she is
[00:58:26.040 --> 00:58:30.680]   You know, we've had the debate about what you can't see you can't unsee on the internet
[00:58:30.680 --> 00:58:35.880]   Um, so every now and then she comes across and there are characters
[00:58:35.880 --> 00:58:42.040]   It is a creepy show that's like on adult swim or something
[00:58:42.040 --> 00:58:48.920]   And they put these in the types of videos that she likes to watch. I think it was a minecraft video
[00:58:48.920 --> 00:58:54.680]   So it's not necessarily for kids and this wasn't on youtube kids, but it just came through
[00:58:54.680 --> 00:59:00.120]   Oh, that's the other point. This is not on just on regular youtube. This is on the especially protected
[00:59:00.120 --> 00:59:04.920]   For kids that's youtube kids. That's the key of the problem here
[00:59:04.920 --> 00:59:09.960]   And that's where stacey's point before is right that you've got to if you're taking responsibility. You got to do it right
[00:59:09.960 --> 00:59:14.520]   You can't f about right. Well in google's messed up. I mean so in in this case
[00:59:15.240 --> 00:59:19.960]   That's a problem, but they also with their kids content, you know, once a kid is above 13
[00:59:19.960 --> 00:59:24.520]   Because and they're not under copa protection and amazon does the same thing
[00:59:24.520 --> 00:59:27.080]   They suddenly go into this world of like
[00:59:27.080 --> 00:59:34.920]   Oh, you're an adult now. Whereas many of us who have 13 year olds or have experienced 13 years olds would be like, uh, it's yeah, not really
[00:59:34.920 --> 00:59:44.360]   But yeah, so my daughter like saw these things and like freaked out and she comes running to me and she's like
[00:59:45.080 --> 00:59:51.880]   Oh these creepy people. Apparently they're super popular, but they are in fact our 14 year old watches
[00:59:51.880 --> 00:59:55.400]   Some really creepy videos called hug me. I'm scared
[00:59:55.400 --> 01:00:00.440]   Um, and there there's an it's an ironic. I think for that age. I don't know
[01:00:00.440 --> 01:00:02.840]   Yeah
[01:00:02.840 --> 01:00:06.840]   So what bridal writes is the system is complicit in the abuse
[01:00:06.840 --> 01:00:11.800]   He says youtube and google are complicit in the system the architecture they have built
[01:00:12.520 --> 01:00:19.640]   To extract the maximum revenue from online video is being hacked by hacked in the sense of used
[01:00:19.640 --> 01:00:29.000]   By persons unknown to abuse children perhaps not even deliberately but at a massive scale his point is there are millions of these
[01:00:29.000 --> 01:00:35.880]   And he thinks that there is some responsibility at the google and youtube have
[01:00:37.640 --> 01:00:44.520]   But it would be nice. Oh go on. I think that your point jeff is well taken that given the volume of videos on youtube
[01:00:44.520 --> 01:00:45.880]   There's an issue
[01:00:45.880 --> 01:00:49.960]   It's going to be very difficult to police this but if you're if you're putting out a child safe product
[01:00:49.960 --> 01:00:53.080]   That's where stacey and I could couldn't read more that is where
[01:00:53.080 --> 01:00:59.880]   You say above all that you've got to go to the expense and you got to have the scarcity to say that there's some structure that you can be
[01:00:59.880 --> 01:01:05.240]   Near perfectly guaranteeing that this child's safe or else you're hurting your brand you're hurting children you're hurting
[01:01:05.720 --> 01:01:09.320]   Um, you're hurting your everything half you're hurting your partners. You're doing everything you do
[01:01:09.320 --> 01:01:11.960]   And then there's a separate
[01:01:11.960 --> 01:01:16.840]   So so all i'm saying is yeah, we we absolutely agree there they're they're so that's where the scale thing
[01:01:16.840 --> 01:01:23.640]   Doesn't really you're gonna limit the scale of it. You're gonna be limited size because you're gonna have quality and we get back to quality and flight to quality is not a bad thing
[01:01:23.640 --> 01:01:26.040]   Now can you do that with youtube as a whole?
[01:01:26.040 --> 01:01:32.040]   That's where it becomes more difficult, but it's but but in terms of this limited project. Yeah
[01:01:32.680 --> 01:01:37.080]   It's the ongoing conversation we've had for the last few years about
[01:01:37.080 --> 01:01:44.840]   And by the way, it's starting what's interesting is starting to migrate into the public sphere as well the non technical public sphere about how
[01:01:44.840 --> 01:01:47.480]   this
[01:01:47.480 --> 01:01:49.480]   free speech
[01:01:49.480 --> 01:01:53.960]   democratization that the internet promised has kind of turned a little sour in some ways
[01:01:53.960 --> 01:02:01.800]   Yes, but you see that's the problem leo that's that's where michael gurgs. Yeah, but not all the you know, when jerks
[01:02:02.040 --> 01:02:04.520]   Not a small percentage of thin layer
[01:02:04.520 --> 01:02:07.640]   Now you're now you're throwing out the entire internet
[01:02:07.640 --> 01:02:12.920]   I'm not but I I feel like there are people
[01:02:12.920 --> 01:02:18.840]   In jeff you might you probably you and I probably agree, but this idea that they're
[01:02:18.840 --> 01:02:25.160]   They're bad people or people who behave like jerks on the internet that is always going to be the case
[01:02:25.160 --> 01:02:30.600]   The question is I think we have to figure out how to deal with them
[01:02:31.160 --> 01:02:31.880]   and
[01:02:31.880 --> 01:02:36.360]   Ideally the platforms will take a take a role here not in a
[01:02:36.360 --> 01:02:40.360]   You know, maybe not banning people or maybe establishing clear rules
[01:02:40.360 --> 01:02:46.280]   But also give people on the platforms tools to avoid things well, they have those tools and this well, I know
[01:02:46.280 --> 01:02:52.920]   I mean you two kids has blocking and everything and I think that that's the rationale. That's the justification facebook and google and others use
[01:02:52.920 --> 01:02:54.520]   Well
[01:02:54.520 --> 01:02:56.760]   Our users will curate it our users will
[01:02:57.400 --> 01:03:02.040]   We'll do it. Uh our users will kill fake news, but I don't see any evidence as that's happening
[01:03:02.040 --> 01:03:08.840]   Well, right and so that's and that's where we get into the questions of scale and abuse of the scale
[01:03:08.840 --> 01:03:16.360]   And that's I mean, it's basically people hacking the algorithms and we got it's your stacy go ahead. Sorry
[01:03:16.360 --> 01:03:19.720]   Uh, no
[01:03:19.720 --> 01:03:23.960]   I'm all lags. I didn't hear you go keep keep going. Sorry
[01:03:25.240 --> 01:03:27.240]   We have to figure out how to
[01:03:27.240 --> 01:03:34.920]   Recognize that when it happens and put a stop to it as quickly as possible and that I mean that was matt cuts his job
[01:03:34.920 --> 01:03:40.360]   You know for search. So I think we just have to figure that out across
[01:03:40.360 --> 01:03:45.240]   Especially across tell parents to stop using youtube's babies
[01:03:45.240 --> 01:03:53.960]   So that's yes, but youtube isn't necessary. I mean perfectly good parents put a phone in front of their kids
[01:03:54.440 --> 01:03:56.440]   For
[01:03:56.440 --> 01:04:01.480]   The difference was you could trust if it was on pbs or was bbc you could kind of trust it
[01:04:01.480 --> 01:04:04.360]   Or you bought a cd rom and you knew what was on it. Yeah
[01:04:04.360 --> 01:04:06.520]   Um
[01:04:06.520 --> 01:04:09.080]   And I even edited out the bad stuff in bambi
[01:04:09.080 --> 01:04:14.280]   I made a special videotape of bambi without the bad stuff. I think disney's gonna arrest you
[01:04:14.280 --> 01:04:16.680]   Yeah, disney doesn't like when you do that
[01:04:16.680 --> 01:04:19.160]   Well, it was for personal consumption only
[01:04:19.800 --> 01:04:25.960]   So I I think I think that the stacey the the larger issue, which I don't think we've kind of dealt with is the full
[01:04:25.960 --> 01:04:28.440]   implications of scale
[01:04:28.440 --> 01:04:32.200]   Right, and I'm gonna pardon me here get your drinking glasses ready folks
[01:04:32.200 --> 01:04:34.680]   Um, if you go back to gutenberg
[01:04:34.680 --> 01:04:41.000]   The problem is sorry the problem was that you had a scale that was once grabbed one book one year
[01:04:41.000 --> 01:04:46.280]   And and society could deal with that and you went to people who read the books out loud to you and you knew what they were and they were
[01:04:46.520 --> 01:04:50.200]   Guaranteed that's fine. Then suddenly. Oh my god all these books and erasmus says there's too many of them
[01:04:50.200 --> 01:04:53.640]   It's gonna ruin society. We can't handle this right then we come up with norms
[01:04:53.640 --> 01:04:57.480]   We come up with systems to grapple with that now. We're an exponentially larger scale
[01:04:57.480 --> 01:05:03.720]   And the scale is wonderful on the one hand it opens up the possibility that we can all publish and all make podcasts and all these wonderful things
[01:05:03.720 --> 01:05:05.880]   But then inevitably come
[01:05:05.880 --> 01:05:12.920]   Spammers which is first the economic piece and that you're right is where the company is motivated to hire med cuts to fix the spam
[01:05:12.920 --> 01:05:15.000]   It's an economic problem both ways around
[01:05:15.000 --> 01:05:19.880]   The next thing that happens the next thing that happens are the really nefarious actors at first though
[01:05:19.880 --> 01:05:24.360]   This is the problem and this is where people are complaining the first reaction the company has is oh good
[01:05:24.360 --> 01:05:26.360]   We're gonna make even more money
[01:05:26.360 --> 01:05:30.360]   They only are only when their feeder puts to the fire
[01:05:30.360 --> 01:05:32.280]   Do they stop this stuff?
[01:05:32.280 --> 01:05:35.560]   Okay, so it's a part of our job and the ecosystem is to put their feet to the fire
[01:05:35.560 --> 01:05:41.880]   Which which I agree with but but how we do that i'm part of the i just help start the open brand safety network where we're identifying the worst
[01:05:41.880 --> 01:05:43.880]   Of the worst of the worst of the fake news sites
[01:05:43.880 --> 01:05:49.240]   There is money going from major advertisers still to those places because nobody has put their feet to the fire to say hello
[01:05:49.240 --> 01:05:53.640]   It's what sleeping giants did with advertisers and Breitbart. We need to do it large scale now
[01:05:53.640 --> 01:05:57.400]   So i'm agreeing with all this my problem with this entire discussion though
[01:05:57.400 --> 01:06:01.560]   Is that if if you stop at blaming the platform and expecting they can fix it
[01:06:01.560 --> 01:06:06.840]   You'll both be disappointed and be harming the good that can come from it just like you know
[01:06:07.720 --> 01:06:12.360]   Okay, they don't have to fix it, but they don't have to be complicit in it and I feel right now they're complicit in it
[01:06:12.360 --> 01:06:17.480]   I see that's where that's where I disagree. I don't think I don't think there's somebody sitting there saying
[01:06:17.480 --> 01:06:21.800]   Oh, if we get rid of this horrible creepy stuff, we could lose a hundred thousand dollars
[01:06:21.800 --> 01:06:22.680]   No, I think it's
[01:06:22.680 --> 01:06:23.880]   We're talking about last week
[01:06:23.880 --> 01:06:28.600]   It's not they're saying it's not worth our time to deal with or they're doing what twitter did
[01:06:28.600 --> 01:06:34.040]   We talked about this last week where the guy the safety guy go from twitter goes to twitter and says
[01:06:34.520 --> 01:06:40.360]   There's all these spots and spam and russian actors and twitter says yeah, but we need the growth numbers right now
[01:06:40.360 --> 01:06:43.080]   Don't you think that happens all the time?
[01:06:43.080 --> 01:06:49.560]   Yeah, yeah, unfortunately they are complicit. We know they're complicit, but they all of them. I don't think so
[01:06:49.560 --> 01:06:51.800]   I think that facebook and google are a higher plane
[01:06:51.800 --> 01:06:55.400]   I don't think they sit there and say we can make money off of crap
[01:06:55.400 --> 01:07:00.360]   I think they realize that some level of the crap is damaging to them and damaging to what they do and their missions
[01:07:00.600 --> 01:07:03.560]   They are not dealing with it. No, they're not and that's the problem
[01:07:03.560 --> 01:07:05.880]   Yeah, and do we need to hold their feet to the fire to make them do it?
[01:07:05.880 --> 01:07:11.400]   Yes, but the problem I have is the cynicism that says oh the only reason they're running these platforms to make money
[01:07:11.400 --> 01:07:14.840]   And they'll do anything to make money and they'll put all this crap up there
[01:07:14.840 --> 01:07:17.480]   That's that's that's naive
[01:07:17.480 --> 01:07:20.840]   The only reason they're doing this is actually to make money
[01:07:20.840 --> 01:07:23.640]   Is it the only reason they're doing it to make money?
[01:07:23.640 --> 01:07:30.520]   Yeah, do you really believe that bark sucker bird cares about connecting the world? Yes, I absolutely do
[01:07:30.520 --> 01:07:37.240]   I believe I believe that Larry I believe that Larry and Sergey absolutely boo absolutely believe that they are
[01:07:37.240 --> 01:07:39.320]   They are making the world's knowledge accessible
[01:07:39.320 --> 01:07:42.040]   absolutely
[01:07:42.040 --> 01:07:43.400]   But
[01:07:43.400 --> 01:07:49.000]   They are there to make money as publicly traded companies. They're there to be only we all are we're they here to eat too
[01:07:49.000 --> 01:07:52.600]   Yeah, part of you know
[01:07:52.600 --> 01:07:56.360]   And even and and I say we're not rich people but even rich people
[01:07:56.840 --> 01:08:01.640]   Then think they have more power than they have and try to look at Rickett's closing DNA info and
[01:08:01.640 --> 01:08:04.040]   LAS know that this week
[01:08:04.040 --> 01:08:06.440]   You know, yeah, we are human
[01:08:06.440 --> 01:08:11.880]   And and we have to check our motives, but yes, I absolutely believe that I have no problem believing the sucker
[01:08:11.880 --> 01:08:15.960]   If I didn't believe that I sure as hell wouldn't have taken their money to start the news integrity initiative
[01:08:15.960 --> 01:08:20.840]   I wouldn't be if I didn't believe that google were essentially good. I wouldn't believe it's worth being on a show over week about them
[01:08:22.600 --> 01:08:26.840]   Oh, see I'm on the show because I I feel like someone has to act as a check against their power
[01:08:26.840 --> 01:08:30.120]   I think it's both
[01:08:30.120 --> 01:08:32.120]   But if I didn't believe there was something good
[01:08:32.120 --> 01:08:35.800]   Essentially here it wouldn't be worth checking them then screw up
[01:08:35.800 --> 01:08:41.720]   I believe there's something good here that's worth preserving and that's their stewardship of the internet and that's what worries me
[01:08:41.720 --> 01:08:43.720]   Is they've got to make sure they're responsible stewards?
[01:08:43.720 --> 01:08:51.000]   Okay, I can agree with keeping them responsible stewards. That makes sense. There is also though the issue of scale. I mean
[01:08:51.640 --> 01:08:56.600]   Sure, uh, erasmus didn't like all the books goop mr. Pump putting out but
[01:08:56.600 --> 01:09:02.760]   We're at a much different scale now. It's global. It's uh, it's
[01:09:02.760 --> 01:09:08.600]   Pervasive in fact scales the problem. It's hard to correct because of scale scales the benefit and the problem
[01:09:08.600 --> 01:09:14.760]   But it was the same then we'll figure it out if the problem is if you don't do you do you give up hope that we can figure it out?
[01:09:14.760 --> 01:09:18.360]   No, I but I do
[01:09:21.000 --> 01:09:27.880]   I do think that we can't that there their motives are not always pure and we have to realize that
[01:09:27.880 --> 01:09:30.600]   And we have to question
[01:09:30.600 --> 01:09:35.880]   That because otherwise I agree with you Stacy. You can assume their motive is profit
[01:09:35.880 --> 01:09:40.600]   If it's not then they're mismanaging there. That's all if that's all you're presuming it is
[01:09:40.600 --> 01:09:43.400]   All but there
[01:09:43.400 --> 01:09:45.560]   It matters
[01:09:45.560 --> 01:09:50.600]   Well, okay. Let me tell you something mark zuckaberg is and his company are not out
[01:09:51.080 --> 01:09:58.840]   Optimizing the news algorithm for benefit benefit to society that would that's not their their op that's right
[01:09:58.840 --> 01:10:02.120]   They're optimizing the news algorithm for engagement and money
[01:10:02.120 --> 01:10:04.520]   Right
[01:10:04.520 --> 01:10:09.880]   There no they would say that mark and say would say I want to connect you to what the people want they're giving them that yes
[01:10:09.880 --> 01:10:13.880]   Engage you are responsible so they can make I think it's a bit of a comp out, but that's what they would say
[01:10:13.880 --> 01:10:17.960]   Yeah, but that's the point is that mark can say all those things he wants to say about his
[01:10:18.520 --> 01:10:22.120]   lofty goals and ambitions, but in fact the news algorithm is not
[01:10:22.120 --> 01:10:28.440]   Optimized for lofty goals and ambitions is optimized to make money as as with any company
[01:10:28.440 --> 01:10:36.360]   It's also a conflict as a conflict of late stage capitalism with immense power
[01:10:36.360 --> 01:10:41.400]   Computational power with immense computational data and a critical mass
[01:10:41.400 --> 01:10:46.680]   of of knowledge and data and you combine all those you've got a toxic stew
[01:10:47.720 --> 01:10:54.680]   That is optimized in a way that is bad for society. Okay. Well, that's where I think you're going way overboard
[01:10:54.680 --> 01:10:57.400]   This is where i'll bring in like god when my god would have a new law here
[01:10:57.400 --> 01:11:01.720]   And and he's been he's been screaming on twitter for the last week or so more eloquently
[01:11:01.720 --> 01:11:04.680]   Following him. I know I about why we're that
[01:11:04.680 --> 01:11:12.360]   He's arguing the evidence that we're in a moral panic his definition of a moral panic was that when you're when you have to convince people that it's all awful
[01:11:12.360 --> 01:11:15.080]   And they don't really feel that way that you're trying to instill a moral panic
[01:11:15.080 --> 01:11:17.880]   And his argument is that most people keep using facebook and they're fine with facebook
[01:11:17.880 --> 01:11:23.480]   And I repeat my challenge before go through your facebook feed and find what's so ugly and awful and society ruining in it
[01:11:23.480 --> 01:11:27.880]   It's not there. It's not there. I think it's more bad actors manipulate this
[01:11:27.880 --> 01:11:33.800]   Yes, are there bad people are there trolls that I hate on twitter? Do I wish twitter would stop enabling women to be
[01:11:33.800 --> 01:11:37.320]   Harris? Yes, do they need to have higher standards?
[01:11:37.320 --> 01:11:42.920]   Absofrigging lutely is do not do not be evil good enough? No be good. Jeff is it has to be the new standard
[01:11:42.920 --> 01:11:45.880]   If your standard for damage is whether people like it or not
[01:11:45.880 --> 01:11:49.560]   Then we should just have McDonald's food served at every meal
[01:11:49.560 --> 01:11:53.080]   For any product on earth leo is do people like it or not any product
[01:11:53.080 --> 01:11:58.440]   That's your starting standard. Is there a market for it? That's a every single product we make including books
[01:11:58.440 --> 01:12:05.160]   So but it doesn't mean it's helpful
[01:12:05.160 --> 01:12:10.360]   How newspapers are so leo people like crap. Yeah, but well fortunately magazines
[01:12:10.360 --> 01:12:13.560]   I think your mindset it does come from that
[01:12:13.560 --> 01:12:21.240]   Generation where newspapers were published by people who had a higher felt a higher mission
[01:12:22.840 --> 01:12:29.560]   I can introduce you to lots of publishers. Not all not all the New York Post you can't say the post has a higher mission
[01:12:29.560 --> 01:12:34.440]   I think it's worse than much of silicon, but I think the New York Times and the Washington Post
[01:12:34.440 --> 01:12:37.080]   I think Dean best gay and
[01:12:37.080 --> 01:12:40.360]   I think they have they I think have a sense of
[01:12:40.360 --> 01:12:44.040]   Societal obligation that's high maybe not the publisher
[01:12:44.040 --> 01:12:49.000]   But the but the editorial staff that of good papers have a higher sense make it the money
[01:12:50.440 --> 01:12:54.520]   Well, and there may not be making that was a problem. We had we had editorial just said all that that money stuff
[01:12:54.520 --> 01:12:59.320]   Somebody else will handle that we didn't take responsibility for it. We didn't know we can't stand above that we were awful
[01:12:59.320 --> 01:13:04.440]   But that's the way that's the way it was supposed to be was a division of labor
[01:13:04.440 --> 01:13:10.680]   And that the and and we try to do that here the editorial side is not impinged upon by the money-making side
[01:13:10.680 --> 01:13:13.240]   And it led to poor stewardship from us on the editorial side
[01:13:13.240 --> 01:13:17.880]   And we didn't take responsibility for the decisions being made. That's how we ended up in clickbait land. Don't you think it's worse now?
[01:13:18.920 --> 01:13:21.080]   I mean, don't you think if you I mean
[01:13:21.080 --> 01:13:27.000]   I meant this wasn't a paragon of virtue, but I think it was a lot it was a lot better model than what we've got today
[01:13:27.000 --> 01:13:30.200]   There's nobody at Facebook. There's nobody at Facebook arguing
[01:13:30.200 --> 01:13:35.480]   I well, maybe there is I don't know arguing well. We got us. We got to do something about this
[01:13:35.480 --> 01:13:38.040]   Oh, I
[01:13:38.040 --> 01:13:43.720]   Think there are I think the problem is don't scare you than that in a way. No, here's the thing I've seen and I've seen this again and again
[01:13:43.720 --> 01:13:46.120]   in their public pronouncements
[01:13:46.120 --> 01:13:51.240]   We think oh they know how to do this. They're just choosing not to make money. No, the problem is it's bigger than them
[01:13:51.240 --> 01:13:53.080]   They don't know how to do it
[01:13:53.080 --> 01:13:57.160]   They don't know how to fix this otherwise they fix it. They don't know how that's what's scarier
[01:13:57.160 --> 01:14:00.440]   Right is that that's what scale brings
[01:14:00.440 --> 01:14:05.720]   Um, I think there's no pressure on them not to fix it. I think there's economic pressure on not to fix it
[01:14:05.720 --> 01:14:09.960]   I think they honestly don't know. I think we I think we impute a lot more brilliance
[01:14:10.600 --> 01:14:15.480]   And ability in them than they have well, then it's a runaway train with no conductor
[01:14:15.480 --> 01:14:20.680]   No, this is this is called progress in the future and some things are gonna go right and some things are going wrong
[01:14:20.680 --> 01:14:23.000]   And the problem is don't lose side to the things that are going right
[01:14:23.000 --> 01:14:29.320]   There's a tremendous amount that's right about the internet and this discussion is entirely throwing that out with the bathwater
[01:14:29.320 --> 01:14:33.480]   That's that's what I fear. That's what I hear the discussion all around right now is oh my god
[01:14:33.480 --> 01:14:37.160]   The internet is broken the internet is horrible. No, you use it every day
[01:14:37.640 --> 01:14:44.680]   You're not bombarded with crap every minute every day. I don't I don't think everyone thinks the internet is broken or horrible
[01:14:44.680 --> 01:14:47.880]   I think people are asking legitimate questions
[01:14:47.880 --> 01:14:53.880]   About what the implications are when their decisions are manipulated by social platforms
[01:14:53.880 --> 01:14:56.600]   when these platforms
[01:14:56.600 --> 01:14:59.960]   Abdicate their responsibility in certain areas
[01:14:59.960 --> 01:15:04.840]   And how we're going to handle that and how we should hold them responsible
[01:15:05.400 --> 01:15:11.320]   The role that these platforms understanding the role these platforms play in our lives and the implications
[01:15:11.320 --> 01:15:13.800]   That's not a bad thing to say
[01:15:13.800 --> 01:15:17.720]   It's I'm agreeing with you Stacey. I agree, but that's no that's not the tone that I'm seeing
[01:15:17.720 --> 01:15:22.760]   You go to my my friends at the guardian. They have gone full under techno panic
[01:15:22.760 --> 01:15:28.760]   And and there was a columnist they had columnist they had two days ago whose whole lead was I hate the internet
[01:15:28.760 --> 01:15:33.160]   I hate everything about the internet. I hate the internet. I'm hearing that tone constantly
[01:15:33.480 --> 01:15:36.520]   That is the way these things get debated
[01:15:36.520 --> 01:15:40.600]   I mean look at look at what happens with things about you know
[01:15:40.600 --> 01:15:43.240]   Look at the soda attacks in New York city
[01:15:43.240 --> 01:15:47.320]   You have people in either extreme and then you have people debating throughout the spectrum
[01:15:47.320 --> 01:15:53.560]   And so yes do extreme viewpoints get people to pay attention to an issue. Yes
[01:15:53.560 --> 01:15:56.840]   Then they will come in and have these discussions and
[01:15:56.840 --> 01:16:01.480]   I understand your worries about a techno panic
[01:16:01.480 --> 01:16:03.640]   But I feel like you're constantly bringing
[01:16:03.640 --> 01:16:08.680]   You run all the way down that slippery slope and you're like tag no panic, but
[01:16:08.680 --> 01:16:13.000]   Then you come back up and you're like, okay, I agree with you on this but but you're
[01:16:13.000 --> 01:16:15.640]   I'm just saying how we have this discussion
[01:16:15.640 --> 01:16:18.200]   The problem is that I see the discussion look at look at the congressional hearings
[01:16:18.200 --> 01:16:20.440]   We're having the discussion right now
[01:16:20.440 --> 01:16:26.120]   People's working like al Franken right who knows better the whole rubles thing
[01:16:26.120 --> 01:16:27.640]   It was for it was for show
[01:16:27.640 --> 01:16:32.680]   Right and it was it was scoring points against not only facebook but the net
[01:16:32.680 --> 01:16:37.000]   And that's a guy who knows better than a guy who's smart and that's where it's happening right now
[01:16:37.000 --> 01:16:39.240]   And the result can be
[01:16:39.240 --> 01:16:43.640]   What we see in the EU is you can get horrendous decisions like the right to be forgotten
[01:16:43.640 --> 01:16:47.720]   You can get decisions like like charging the platforms 500 million
[01:16:47.720 --> 01:16:49.880]   Dollars every time something wrong is there
[01:16:49.880 --> 01:16:52.360]   You can get to the point where where
[01:16:52.360 --> 01:16:55.800]   We expect them to decide what's right and wrong
[01:16:56.600 --> 01:16:59.800]   We were seeing the fruits of this and that's that's this is not a
[01:16:59.800 --> 01:17:02.680]   Idol worry
[01:17:02.680 --> 01:17:07.560]   It's we are empowering the politicians to do bad things to our beloved internet would we
[01:17:07.560 --> 01:17:12.040]   We talk crap on the internet all the time and we don't also remind them of the good that we have to preserve the good
[01:17:12.040 --> 01:17:19.320]   Let me instead of using the soundbite from al Franken give you a quote from the article he wrote in your favorite publication the guardian
[01:17:19.320 --> 01:17:22.120]   Um, whoops. I've gone. I've gone away from it
[01:17:22.680 --> 01:17:28.840]   And the title is we must not let big tech threaten our security freedoms and democracy. There's an opinion piece
[01:17:28.840 --> 01:17:32.840]   And he says last week's hearings demonstrated these companies
[01:17:32.840 --> 01:17:36.280]   May not be up to the challenge they've created for themselves
[01:17:36.280 --> 01:17:40.680]   In some instances it seems they fail to take common sense precautions
[01:17:40.680 --> 01:17:43.800]   To prevent the spread of propaganda misinformation
[01:17:43.800 --> 01:17:46.840]   And hate speech
[01:17:46.840 --> 01:17:52.520]   The platforms at big tech and I actually think this is pretty astute the platforms at big tech have designed
[01:17:53.240 --> 01:17:55.240]   May now be so large and unruly
[01:17:55.240 --> 01:18:01.320]   That we can't trust the companies to get it right when they do start paying attention if you have five million advertisers a month
[01:18:01.320 --> 01:18:08.040]   Using your highly sophisticated nearly instantaneous ad platform. Can you ever really know who all of them are?
[01:18:08.040 --> 01:18:13.160]   Can you ever so he's there? He's answering his question by the way
[01:18:13.160 --> 01:18:19.080]   So what do you want to happen then so what's what's the answer that we prevent people from advertising so aunt mary can't sell her jam
[01:18:20.040 --> 01:18:25.320]   That we have actually they can develop okay jeff they can actually develop
[01:18:25.320 --> 01:18:32.840]   Algorithms that pay attention to signals that are important to question to say hey, this looks suspicious
[01:18:32.840 --> 01:18:36.120]   Maybe we hunt this up to someplace else
[01:18:36.120 --> 01:18:42.840]   Frank is being positive here. He's saying but he's saying with great power goes great responsibility and these companies have to be held
[01:18:42.840 --> 01:18:46.120]   To account I think that's fair
[01:18:46.120 --> 01:18:50.040]   Easably so it cannot be it will not ever be the same as running the new york times
[01:18:50.040 --> 01:18:53.480]   And we don't want it to be because then because you and I couldn't get into the new york times
[01:18:53.480 --> 01:18:56.520]   Right that's sure would not be on mainstream
[01:18:56.520 --> 01:19:01.960]   Broadcast right and so and so if we want to control safe completely cleaned up atmosphere
[01:19:01.960 --> 01:19:07.320]   What's the what's the level of responsibility? He's not saying it needs to be controlled in safe
[01:19:07.320 --> 01:19:10.680]   He's saying that they need to put things in place to
[01:19:11.400 --> 01:19:16.440]   Make this to to make them not complicit so they see when things are going wrong
[01:19:16.440 --> 01:19:19.960]   It's basically like saying hey, you built this awesome city
[01:19:19.960 --> 01:19:25.640]   But you know what maybe you should put some streetlights in because sometimes it gets dark and your people are going to want to see what's happening
[01:19:25.640 --> 01:19:31.800]   That's what we're agreeing about that. We were agreeing about that but but it's very important to have a discussion of where is that line
[01:19:31.800 --> 01:19:35.720]   And and and what the implications are and who gets kicked off
[01:19:35.720 --> 01:19:38.600]   Well, I saw a list of you know
[01:19:39.560 --> 01:19:43.000]   Of of of unreliable sites that shouldn't be advertised on
[01:19:43.000 --> 01:19:47.720]   Well, some odd things got onto that list and what it what happens when brightborn ends of that list and what you know
[01:19:47.720 --> 01:19:51.960]   Where are you in this world? This is not easy. This is really hard and again
[01:19:51.960 --> 01:19:56.360]   I don't think we should presume that they know how to do this. They're just refusing to which is kind
[01:19:56.360 --> 01:19:58.280]   I think what prekensane
[01:19:58.280 --> 01:20:00.200]   Which I agree with is maybe they don't know how to do it
[01:20:00.200 --> 01:20:02.840]   But let's acknowledge how hard this task is
[01:20:02.840 --> 01:20:05.480]   Really hard everyone he says
[01:20:05.480 --> 01:20:10.920]   They need to figure it out. That's that's also part of this. He's saying you can't just
[01:20:10.920 --> 01:20:13.560]   I mean it'd be like if someone
[01:20:13.560 --> 01:20:18.200]   Created Frankenstein you created Frankenstein now we got to figure out how to
[01:20:18.200 --> 01:20:25.000]   How to reign him in how to make sure that he doesn't go to help anyone to say oh, it's just it's very hard to keep control of franken
[01:20:25.000 --> 01:20:26.920]   Danger danger folks danger danger
[01:20:26.920 --> 01:20:32.200]   So that's exactly what Iran says in China says in your study in a precedent and then study the mechanism that we we have to
[01:20:32.440 --> 01:20:36.920]   We want to are you suggesting jeff that should be no regulation? No, I'm not just in that role
[01:20:36.920 --> 01:20:40.760]   I'm suggesting have the discussion from saying the internet is a good. Let's protect the good of it
[01:20:40.760 --> 01:20:46.200]   Here's the here's the question. That's has the discussion here the question away here. The question is franken says
[01:20:46.200 --> 01:20:49.640]   Yeah, we agree. We all agree. There's good stuff there
[01:20:49.640 --> 01:20:55.960]   How do you hear the questions franken says we have to think about how did big big tech come to control so many aspects of our lives
[01:20:55.960 --> 01:20:59.320]   How is it using our soul? Well, that's right there. That's loaded as hell
[01:20:59.800 --> 01:21:05.800]   Doesn't control my life. That's that's fundamentally patronizing and consul and insulting to the citizens who vote
[01:21:05.800 --> 01:21:13.080]   That's that's exactly that. Oh, the technology makes us do bad things. No, a few bad people do bad things using the technology big
[01:21:13.080 --> 01:21:20.360]   No, there is social redlining there is the use of this data to
[01:21:20.360 --> 01:21:24.840]   Decide on whether or not to hire people there are real world implications
[01:21:25.400 --> 01:21:30.840]   And so is it controlling your life in like a scary mind control way? No, but it is
[01:21:30.840 --> 01:21:33.880]   Narrowing your choices in ways that people don't
[01:21:33.880 --> 01:21:40.760]   Tremendous ways you never had before as well. You can go to link in a good job. You never could have done cool stuff, but
[01:21:40.760 --> 01:21:46.840]   And and I'm aware of that but I'm unaware of the bad stuff that's happening the punishments
[01:21:46.840 --> 01:21:52.760]   And I think that lack of awareness is really important. Look about and that's what I think
[01:21:54.360 --> 01:22:01.160]   We should be having a debate about that's where we should be shining the light of transparency that we all are so excited about
[01:22:01.160 --> 01:22:04.760]   As journalists onto these platforms saying hey
[01:22:04.760 --> 01:22:10.200]   Who's using your stuff? What data do you collect? What do you what's exactly?
[01:22:10.200 --> 01:22:15.400]   Profile, this is exactly the okay. So we all agree that first question you're right a little loaded second question
[01:22:15.400 --> 01:22:19.880]   How is big tech using our personal information to strengthen its reach in its bottom line?
[01:22:20.360 --> 01:22:25.720]   Are these companies engaging in anti-competitive behavior that restricts the free flow of information in commerce?
[01:22:25.720 --> 01:22:32.920]   He says this is a net neutrality issue too. Are they failing to take simple precautions to respect our privacy and protect our democracy?
[01:22:32.920 --> 01:22:35.320]   Simple precautions not we're not asking
[01:22:35.320 --> 01:22:40.120]   Impossible things and finally what role we want them to do our democracy, but okay well
[01:22:40.120 --> 01:22:47.080]   What role should these companies play in our lives and how do we ensure transparency and accountability from them going forward?
[01:22:47.240 --> 01:22:53.560]   So there's two different burdens here. There's a I agree with you jeff if you feel like technology controlling your lives the burden is on you
[01:22:53.560 --> 01:22:57.960]   Not the technology. So there is definitely a burden on the individual
[01:22:57.960 --> 01:23:05.160]   But I think that it's reasonable for us as a society through our government to ask these companies
[01:23:05.160 --> 01:23:10.520]   Um, you know to be to preserve competition to protect privacy
[01:23:10.520 --> 01:23:16.680]   I don't think those are unreasonable things to protect our security. Those are reasonable things to ask a company to do the same government
[01:23:16.760 --> 01:23:24.200]   That just today tried to for is trying to force time Warner to sell CNN out of spite the same government
[01:23:24.200 --> 01:23:29.960]   Well, that's that's who you want to regulate the speech. That's the figure out. And I know I sound like a libertarian. I know
[01:23:29.960 --> 01:23:37.080]   No, and I I have covered politics forever. I have dealt with the net neutrality debate for example
[01:23:37.080 --> 01:23:42.680]   over several successive FCC's including I was going to say colon pal Michael pal
[01:23:44.280 --> 01:23:49.400]   But this is this is the fundamental compromise we make when we have the government enact laws
[01:23:49.400 --> 01:23:58.120]   we vote in a group of people who feel the way we feel or the opposite of the way we feel the way politics goes now
[01:23:58.120 --> 01:24:04.200]   And we have them craft laws that other people may decide to enact differently
[01:24:04.200 --> 01:24:06.520]   And that is the risk we take
[01:24:06.520 --> 01:24:08.760]   With a democracy and they believe
[01:24:09.320 --> 01:24:12.360]   People back on is to decide what should be law and shouldn't be law
[01:24:12.360 --> 01:24:18.680]   And if I also the you the you I think has already gone way overboard possible and I fear that here
[01:24:18.680 --> 01:24:22.840]   There but there's also the risk you have to acknowledge of a backlash
[01:24:22.840 --> 01:24:26.600]   If these companies don't start to take some responsibility amen
[01:24:26.600 --> 01:24:31.800]   It's why I press too. It could go very far in the opposite direction and all this great internet stuff
[01:24:31.800 --> 01:24:37.080]   Would not be amen. Yeah amen. That's exactly what's happening. I think that's what Michael god one's point is now
[01:24:37.480 --> 01:24:41.080]   We've gone too far. We're gonna. Yeah, we're gonna lose they didn't do enough
[01:24:41.080 --> 01:24:45.240]   Listen, I've been arguing that too and I can give you chapter and verse about how I pressure them and
[01:24:45.240 --> 01:24:47.800]   Would be a good thing for them to do something before
[01:24:47.800 --> 01:24:52.760]   People start to say well, yeah, they should have said an entirely new standard for transparency
[01:24:52.760 --> 01:24:56.280]   Political advertising for example. They should go way beyond a required of tv
[01:24:56.280 --> 01:25:03.560]   They should be releasing every single political ad in a way that's searchable with an api with all of the the targeting data
[01:25:03.880 --> 01:25:07.960]   They should be doing all of that they should have done that before anybody even breathe the idea of legislation
[01:25:07.960 --> 01:25:11.560]   They should have seen this coming they weren't smart enough to same with this porn thing
[01:25:11.560 --> 01:25:15.320]   Same with another story. We have on the rundown that they're they're not smart enough
[01:25:15.320 --> 01:25:19.400]   This is why I've argued they should hire journalists so they can just get a sense of public responsibility
[01:25:19.400 --> 01:25:23.800]   I agree with all of that they have they're messing it up. They're messing up our internet
[01:25:23.800 --> 01:25:30.120]   And that's why I fear but I also think there's why I need to keep pressure back on the idea of us turning into EU regulation
[01:25:30.120 --> 01:25:32.200]   That scares me worse
[01:25:33.080 --> 01:25:37.400]   I actually think gdpr is is decent regulation. I don't
[01:25:37.400 --> 01:25:40.840]   I mean, I don't want to get I don't want to get a new parent. All right. Let's take a break
[01:25:40.840 --> 01:25:42.920]   We'll have more rest relax
[01:25:42.920 --> 01:25:49.240]   Get a you know just taking that stuff back to your back some coffee coming down and fights
[01:25:49.240 --> 01:25:53.560]   It's really you know, it's it's really an interesting time
[01:25:53.560 --> 01:25:59.240]   I feel like there is
[01:25:59.640 --> 01:26:02.360]   There's something bro. It's not sky net. I'm not saying it's sky net
[01:26:02.360 --> 01:26:05.800]   But there's something brewing right now and the capabilities to protect big tech companies
[01:26:05.800 --> 01:26:12.520]   That is new unique and and can be a little worrisome don't and I just also be magnificent
[01:26:12.520 --> 01:26:14.920]   It could be well and there's a lot of magnificent stuff
[01:26:14.920 --> 01:26:19.320]   But in order to keep that as we said to keep the magnificent stuff flowing we've got to make sure that we
[01:26:19.320 --> 01:26:23.720]   And they responsibly handle the stuff that's damn damaging
[01:26:23.720 --> 01:26:28.440]   Yeah, but I'll always say Leo is that is that for those of us who are around technology and are savvy or about it
[01:26:28.600 --> 01:26:32.360]   It's up to us to defend the good stuff because that's that's not happening now
[01:26:32.360 --> 01:26:37.800]   That's that's the that's that's we have we have our special responsibility to do both
[01:26:37.800 --> 01:26:40.760]   Absolutely to both it'd be critical of them and I you always better now
[01:26:40.760 --> 01:26:43.000]   You're being you're being I get it now
[01:26:43.000 --> 01:26:46.680]   so you you're pushing a little harder on the other side because
[01:26:46.680 --> 01:26:53.240]   And there are plenty of people of luddites of non technically savvy people who are pushing hard in the other direction
[01:26:53.240 --> 01:26:57.560]   So that's why I wrote public parts because I said there are plenty of protectors of privacy
[01:26:57.560 --> 01:27:01.000]   I am well confident that privacy will be well represented as an issue
[01:27:01.000 --> 01:27:01.480]   That's fair
[01:27:01.480 --> 01:27:05.160]   But sharing is not well-receptible because of it right now. Yeah, yeah
[01:27:05.160 --> 01:27:10.200]   Yeah, I think we as usual agree at all some matters of substance here
[01:27:10.200 --> 01:27:12.360]   It's a matter of its own
[01:27:12.360 --> 01:27:16.360]   And a matter of priority and a matter of what we each fear is different. Yeah
[01:27:16.360 --> 01:27:18.120]   Fair enough
[01:27:18.120 --> 01:27:20.760]   But now you got to make some money because that's all you really care about Leo
[01:27:20.760 --> 01:27:22.920]   That's all I care about is making money
[01:27:22.920 --> 01:27:24.520]   So he says, "Leo, you read this"
[01:27:24.520 --> 01:27:26.680]   But I read that so you guys don't have to
[01:27:27.240 --> 01:27:29.240]   Here's the here's the Chinese wall
[01:27:29.240 --> 01:27:32.760]   Right here between you and this following ad
[01:27:32.760 --> 01:27:37.080]   Our show is our show is brought to you today
[01:27:37.080 --> 01:27:42.760]   By the way, you know, this is a big challenge for us about what should we accept ads for companies like Microsoft and apple
[01:27:42.760 --> 01:27:47.240]   Uh companies that we directly cover or not. That's an interesting question too
[01:27:47.240 --> 01:27:48.440]   I mean
[01:27:48.440 --> 01:27:53.400]   When we fortunately have not had to answer because Microsoft and apple have never come calling our show today
[01:27:55.000 --> 01:27:59.480]   Hit hit google never no our show today brought to you by
[01:27:59.480 --> 01:28:03.640]   It's called this week in google. You could have you could own it. Would you take it out for google?
[01:28:03.640 --> 01:28:08.840]   We actually have the only ads we've ever taken for google and I feel good about it this summer of code
[01:28:08.840 --> 01:28:13.080]   Yeah, because that's a that's a good thing and uh, I'm not sure
[01:28:13.080 --> 01:28:15.800]   I think it would be very difficult for us to take pixel 2 ads for instance
[01:28:15.800 --> 01:28:22.040]   I think that would be a real challenge. Well, meanwhile talking about something like a mortgage company that does it better for
[01:28:22.520 --> 01:28:25.800]   In a way the technologist would love. I think that's completely appropriate
[01:28:25.800 --> 01:28:31.560]   Uh, it'll get your ad. I'll stop it. Yeah. No, no, no, I know it's all part and parcel of the same thing
[01:28:31.560 --> 01:28:34.440]   Our show is brought to you by quick and loans. They created something
[01:28:34.440 --> 01:28:40.120]   Called rocket mortgage that answers a real need and a need that I felt last time we bought a house
[01:28:40.120 --> 01:28:43.000]   We went to a bank
[01:28:43.000 --> 01:28:50.760]   First of all went to a bank. That's nothing anybody ever wants to do second going through the papers in my attic in my file cabinet at work
[01:28:51.000 --> 01:28:55.400]   Trying to find my banks payments bank statements pay stubs all that paperwork
[01:28:55.400 --> 01:29:01.080]   Never had everything I needed so we had a fact stuff and facts more stuff and then they wanted more stuff
[01:29:01.080 --> 01:29:06.360]   So we have it was just like this endless painful process death by a thousand paper cuts
[01:29:06.360 --> 01:29:11.240]   Then along comes rocket mortgage. I'll tell you the next time I'm doing it with rocket mortgage quick and loans
[01:29:11.240 --> 01:29:14.840]   They know a little bit about this the number two lender in the country 92 billion dollars
[01:29:14.840 --> 01:29:19.480]   in home loans out there number one in customer satisfaction thanks to
[01:29:20.200 --> 01:29:29.080]   JD power and their surveys for seven consecutive years in primary mortgage origination for four consecutive years in mortgage servicing
[01:29:29.080 --> 01:29:31.480]   They're the best and I tell you what?
[01:29:31.480 --> 01:29:35.160]   Going forward they're even better because a rocket mortgage rocket mortgage
[01:29:35.160 --> 01:29:40.920]   Gives you the confidence you need when when it comes to buying a home or refinancing your home one because it's a transparent process
[01:29:40.920 --> 01:29:45.560]   You will understand every step of the way you'll see it happen and it happens in minutes in minutes
[01:29:47.480 --> 01:29:51.080]   It's kind of amazing plus no paperwork none at all
[01:29:51.080 --> 01:29:55.880]   Because they have trusted financial trusted partners with all the big financial institutions
[01:29:55.880 --> 01:29:59.160]   All you have to do is give them some basic information. They say is this you yes
[01:29:59.160 --> 01:30:03.160]   Give us permission to ask the bank for your uh, paste statements or whatever. Yes
[01:30:03.160 --> 01:30:07.160]   Then based on your income assets and credits they'll crunch those numbers
[01:30:07.160 --> 01:30:13.320]   And they'll give you a home loan all the options for which you qualify you choose the down payment the rate the term
[01:30:13.880 --> 01:30:19.400]   And you're done you're approved in minutes at the open house. You show the realtor
[01:30:19.400 --> 01:30:24.440]   We're approved. Yeah. I love the big button on this is download the approval letter and print it out
[01:30:24.440 --> 01:30:30.120]   You're approved rocket mortgage from quick and lines apply simply understand fully
[01:30:30.120 --> 01:30:37.000]   And mortgage confidently equal housing lender license in all 50 states and mls consumer access dot our number 30 30
[01:30:37.000 --> 01:30:38.920]   Go to rocket mortgage dot com slash twig
[01:30:38.920 --> 01:30:43.400]   Rocket mortgage dot com slash twig. Maybe you're not buying the day, but do bookmark that
[01:30:44.040 --> 01:30:48.040]   So that you will be ready that house comes along rocket mortgage
[01:30:48.040 --> 01:30:50.040]   dot com
[01:30:50.040 --> 01:30:51.480]   slash
[01:30:51.480 --> 01:30:53.480]   twig
[01:30:53.480 --> 01:30:58.360]   I like this google's adding wait times for restaurants
[01:30:58.360 --> 01:31:02.440]   That's a nice feature. I wonder how they actually know that
[01:31:02.440 --> 01:31:08.280]   Have they already done that? Uh, Austin does it they have the popular times. I've seen the popular
[01:31:08.920 --> 01:31:13.960]   Popular times which is different from wait time. Yeah, okay. The tsa in austin airport actually
[01:31:13.960 --> 01:31:19.320]   Oh, yeah, Wi-Fi signals from someone's phone. So maybe they're doing that. So this is well, that's really that's how they do
[01:31:19.320 --> 01:31:22.200]   That's it. That's yeah, there's a data collection, huh?
[01:31:22.200 --> 01:31:29.560]   So they're gonna do it for google's published a blog post yesterday that says skip the line restaurant wait times on search and on maps
[01:31:29.560 --> 01:31:32.200]   So this is in addition to popular times
[01:31:32.200 --> 01:31:35.320]   Um plan your visit people
[01:31:35.320 --> 01:31:40.600]   But typically spend 45 minutes to two hours here peak wait time up to one hour 45 minutes from eight to nine p.m
[01:31:40.600 --> 01:31:43.000]   That's the red bar there
[01:31:43.000 --> 01:31:47.880]   wait times for a million sit down restaurants around the world that allow walk-ins
[01:31:47.880 --> 01:31:50.920]   So it's not for reservations as if you just walk in the door
[01:31:50.920 --> 01:31:56.600]   And uh, and you tap you could tap on the hour bar and you'll see the wait time for that
[01:31:56.600 --> 01:32:03.160]   Period typical wait time. Wait, so is there assumption. Oh, sorry based on his anonymized historical data
[01:32:04.120 --> 01:32:10.600]   Huh, so do they know how like wait time varies tremendously depending on how many people are in your party for example?
[01:32:10.600 --> 01:32:14.520]   So they don't seem to be that granular. Yeah, just
[01:32:14.520 --> 01:32:20.360]   You don't want to you don't want to think of it. What was like this is the sign filled episodes that could never have yes
[01:32:20.360 --> 01:32:23.240]   The Chinese restaurant
[01:32:23.240 --> 01:32:25.240]   Such a great episode
[01:32:25.240 --> 01:32:30.200]   Talk about nothing happening there the whole show is them waiting in line. I just put it in the rundown
[01:32:30.200 --> 01:32:32.680]   um
[01:32:32.840 --> 01:32:37.240]   Right the fact that that that that you would now know that you shouldn't be waiting there George
[01:32:37.240 --> 01:32:42.120]   They would it would kind of just ruin the whole thing
[01:32:42.120 --> 01:32:44.200]   It was the whole thing
[01:32:44.200 --> 01:32:47.480]   But you know, I think there's probably probably other material
[01:32:47.480 --> 01:32:53.720]   I wish sign felt did a show. I wish there was show like that today. I don't know if there is but uh
[01:32:53.720 --> 01:32:57.560]   The Chinese are you can watch this on youtube
[01:33:01.320 --> 01:33:03.320]   Oh, it's only four and a half minutes
[01:33:03.320 --> 01:33:05.960]   But you get the idea the whole the whole show is them
[01:33:05.960 --> 01:33:10.760]   Is it hulu that has sign felt or was it somebody this is youtube, but you can't
[01:33:10.760 --> 01:33:13.720]   On garbage trucks garbage cans and garbage men
[01:33:13.720 --> 01:33:19.000]   You're never going to stop crime. We should at least be clean. You know, it's interesting. I'll look as podcast
[01:33:19.000 --> 01:33:21.800]   Yeah, it's about to say George
[01:33:21.800 --> 01:33:24.840]   God you fit fit right in
[01:33:26.760 --> 01:33:32.360]   We didn't talk about this last week the rogue twitter employee who's shut down Donald trumps account. I feel like that maybe didn't
[01:33:32.360 --> 01:33:38.360]   No, I'm surprised the name hasn't come out. I want to between that and the woman who gave the finger to the these are two
[01:33:38.360 --> 01:33:40.920]   social media heroes these days. Yeah, so the woman
[01:33:40.920 --> 01:33:47.240]   Uh who was bicycling when uh trumps motorcade went by on the way to the golf course and gave the motorcade the finger
[01:33:47.240 --> 01:33:49.240]   Got fired
[01:33:49.240 --> 01:33:51.240]   Because she put the picture
[01:33:51.240 --> 01:33:52.840]   on her
[01:33:52.840 --> 01:33:57.960]   social media page I guess on her facebook page became her profile picture and the company which is
[01:33:57.960 --> 01:34:01.480]   Coincidentally or not a con a military contractor
[01:34:01.480 --> 01:34:07.800]   Fired her and then the latest is she's received several hundred thousand job offers
[01:34:07.800 --> 01:34:15.160]   What a weird world we live in for the for the person who killed the account for 11 minutes
[01:34:15.160 --> 01:34:17.560]   Of my joke was that I want to hold a twitter tape parade
[01:34:17.560 --> 01:34:22.440]   Uh, you know, I just want to you want to know who it is a surprise that has a command
[01:34:22.440 --> 01:34:28.040]   And yet and yet isn't it a little disturbing it is it is ultimately
[01:34:28.040 --> 01:34:31.320]   but as a as a kind of moment of
[01:34:31.320 --> 01:34:32.840]   of
[01:34:32.840 --> 01:34:39.000]   Um civil disobedience was once the person lost their job. Well, no the person was on the way out. That's what happened
[01:34:39.000 --> 01:34:42.040]   I think that's what they did. This was their last the story went out
[01:34:42.040 --> 01:34:46.200]   This was this person's last day and as a gesture on their last day
[01:34:46.200 --> 01:34:49.400]   But yeah, I think this would be a similar situation where
[01:34:50.440 --> 01:34:56.440]   They would probably get a lot of job offers. Although I wouldn't want to offer a job to somebody who would do something like that to be on
[01:34:56.440 --> 01:34:59.400]   So this person was a contractor. Yeah, what's
[01:34:59.400 --> 01:35:02.040]   You know what's stressing is
[01:35:02.040 --> 01:35:09.160]   That this person that yeah, they had access to an account that you know at one point was
[01:35:09.160 --> 01:35:13.880]   Close to starting more with the north korea. So I'm like, oh
[01:35:13.880 --> 01:35:20.200]   That feels like perhaps lack security and and that to me was the big I agree. Yes. It was funny
[01:35:20.200 --> 01:35:23.000]   But that was kind of like the other question that went through my mind
[01:35:23.000 --> 01:35:27.400]   Well, could the same person have tweeted on the president's behalf, but twitter was quick to say no
[01:35:27.400 --> 01:35:35.160]   No, they don't have access to tweet and they don't have access to dms. Okay, so read dms. Oh, god
[01:35:35.160 --> 01:35:37.480]   Worse
[01:35:37.480 --> 01:35:39.480]   Yeah, dm kim johnon
[01:35:39.480 --> 01:35:41.480]   you little
[01:35:41.480 --> 01:35:47.640]   Rocket boy rocket boy. I'm I was go ahead fire rocket us fire one. Come on. Go ahead
[01:35:48.600 --> 01:35:50.600]   Oh boy. Yeah
[01:35:50.600 --> 01:35:56.200]   Oh trudo and macron's twitter tweets dms about trump that would be hilarious. Oh lord
[01:35:56.200 --> 01:35:57.560]   Well, that would be
[01:35:57.560 --> 01:35:59.880]   Yeah, really they wield a lot of power these twitter guys
[01:35:59.880 --> 01:36:01.800]   His twits
[01:36:01.800 --> 01:36:03.800]   This was just some temporary
[01:36:03.800 --> 01:36:07.320]   That's a common problem people think twitter and twitter the same thing. They're not
[01:36:07.320 --> 01:36:10.680]   God
[01:36:10.680 --> 01:36:12.920]   By the way twit preceded twitter
[01:36:13.800 --> 01:36:19.240]   Oh quite a while you're right by yes by a lot. How long have you been a lot but a couple of years
[01:36:19.240 --> 01:36:24.200]   And I interviewed have williams at the time and I said have why did you name your company twitter?
[01:36:24.200 --> 01:36:30.760]   Because he knew about us because remember prior to twitter was audio which is a podcast network and we were one of his top podcasts
[01:36:30.760 --> 01:36:35.800]   So he completed so he knew about us and still call this company twitter
[01:36:35.800 --> 01:36:38.200]   He told me at the time. Well, I didn't think either one of us was going anywhere
[01:36:38.200 --> 01:36:43.160]   If he'd called it he was half reader would you let him problem
[01:36:44.040 --> 01:36:45.000]   I
[01:36:45.000 --> 01:36:48.840]   Don't know it's just so close to twit and people confuse it all the time
[01:36:48.840 --> 01:36:54.920]   Well, that's not a terrible thing getting confused for twitter. Well, it is if an advertiser confuses you
[01:36:54.920 --> 01:37:01.160]   And says i'm gonna buy some ads on twitter on twit. I like that leo. Let me get some twitter ads
[01:37:01.160 --> 01:37:05.000]   People used to think that I was the uh owner of uh buzzfeed
[01:37:05.000 --> 01:37:09.880]   Oh, yeah, because buzz machine is your machine. Yeah. Yeah people's all this jump drivers
[01:37:09.880 --> 01:37:12.680]   Jeff Jeff Jarvis greater buzzfeed. That's a no
[01:37:12.680 --> 01:37:15.720]   Oh dear
[01:37:15.720 --> 01:37:17.720]   I do I do not have these issues
[01:37:17.720 --> 01:37:23.480]   There aren't not many higgin bathrooms out there not true. I think there's a higgin bathroom steel in the uk
[01:37:23.480 --> 01:37:30.600]   There is higgin bothoms a bookstore in india. Good. I get lots of pictures of it. That's good. There is a famous astronaut
[01:37:30.600 --> 01:37:32.440]   um
[01:37:32.440 --> 01:37:37.720]   Joan higgin bothom. Um, she's got a wikipedia page. I wish she's in I wish her name would harvey higgin bathroom
[01:37:37.720 --> 01:37:42.760]   That would be a good name. There's a judge. There's a famous traumas. Do you have a boy? Will you name him harvey?
[01:37:42.760 --> 01:37:46.440]   I'm not having any more children and no enough is enough
[01:37:46.440 --> 01:37:49.480]   No, no harvey's
[01:37:49.480 --> 01:37:54.920]   No markets harvey you get off and I travel harvey higgin bothom. That would be such a good name
[01:37:54.920 --> 01:37:58.760]   I forgot is this andrew's name or your name?
[01:37:58.760 --> 01:38:05.560]   Uh, oh my name. It's not your maiden name. It's my name. It is your maiden name. Oh, okay. So you own it
[01:38:06.200 --> 01:38:10.040]   Yeah, you've had a whole life. It's a totally different name. Yes, whole life. You've had that name
[01:38:10.040 --> 01:38:13.320]   And my dog has that name
[01:38:13.320 --> 01:38:15.320]   Hello dog. What's your dog's full name?
[01:38:15.320 --> 01:38:18.200]   Sophie the terrible higgin bothom
[01:38:18.200 --> 01:38:21.720]   You gotta give a dog a middle name
[01:38:21.720 --> 01:38:24.600]   The terrible the terrible. Okay, that's her middle name
[01:38:24.600 --> 01:38:27.800]   Are you gonna go next time you're in vegas?
[01:38:27.800 --> 01:38:30.920]   Are you gonna take a ride?
[01:38:30.920 --> 01:38:32.920]   On navia
[01:38:32.920 --> 01:38:39.320]   It's a french flash french startup self-driving startup. It's got a little shuttle eight passengers
[01:38:39.320 --> 01:38:43.080]   There is a safety driver. I'm sad to say. Oh
[01:38:43.080 --> 01:38:48.360]   Yeah, I don't know the world's most boring job by the way. It's a self-driving
[01:38:48.360 --> 01:38:52.680]   It's the largest self-driving project in the u.s. Even though it only goes in a 0.6 mile loop
[01:38:52.680 --> 01:38:57.800]   How fast does it go downtown? It's downtown. So it's not fast
[01:38:58.680 --> 01:39:04.520]   Lidar dps v2i vehicle. Oh, that's that thing we were talking about vehicle to vehicle infrastructure
[01:39:04.520 --> 01:39:09.320]   That will let it communicate with sensors embedded in the vegas traffic signals
[01:39:09.320 --> 01:39:15.880]   Um, oh, I'm sad. There's a safety driver google starting to give rides to people without a safety driver, you know
[01:39:15.880 --> 01:39:18.120]   Well, wait, hold on because la
[01:39:18.120 --> 01:39:21.720]   I have I have relevant information so la la las vegas
[01:39:21.720 --> 01:39:27.400]   I went there and I actually saw and spent time in their innovation district in wandered around with their head of
[01:39:27.720 --> 01:39:32.040]   Technology and innovation who is actually going to be on the podcast next week as I guess
[01:39:32.040 --> 01:39:33.480]   um
[01:39:33.480 --> 01:39:34.440]   And
[01:39:34.440 --> 01:39:38.600]   I learned so much about smart cities and how they're actually
[01:39:38.600 --> 01:39:45.240]   Implemented and the guy who is in charge of this his name is michael sure would you should have him on your show because he is the geekiest geek
[01:39:45.240 --> 01:39:48.600]   Who is so fun and he's done everything from give
[01:39:48.600 --> 01:39:50.200]   on
[01:39:50.200 --> 01:39:52.120]   Amazon echo skill
[01:39:52.120 --> 01:39:58.920]   So you can actually on the echo show watch city council meetings, but you can all it also ask it to ask you or tell you
[01:39:58.920 --> 01:40:01.480]   Sorry, here we go madam a
[01:40:01.480 --> 01:40:08.440]   What is you know? Where should I play baseball today? And it'll give you the like city parks that have oh, that's awesome
[01:40:08.440 --> 01:40:12.200]   So he's built all that and now they're talking about creating like
[01:40:12.200 --> 01:40:14.760]   uh
[01:40:14.760 --> 01:40:20.680]   Roads like lighting variable lane roads where you like light up the roads based on traffic patterns
[01:40:20.760 --> 01:40:22.760]   Oh, so go this way go that way
[01:40:22.760 --> 01:40:28.440]   Sign film once again. This is what cramer did when he took it when he when he sponsored part of the lie
[01:40:28.440 --> 01:40:32.600]   One in wider lanes. That's right. See everything goes back to sign film. Sorry
[01:40:32.600 --> 01:40:36.040]   So no, this guy is a total nerd. He's so great
[01:40:36.040 --> 01:40:41.320]   And he did promise me a ride on this bus when I go to l.x. So I am totally taking him up. I
[01:40:41.320 --> 01:40:44.520]   Have you ever been in the world self-driving cars?
[01:40:44.520 --> 01:40:46.920]   I don't think so
[01:40:46.920 --> 01:40:48.760]   I've what you know
[01:40:48.760 --> 01:40:52.920]   Unless you count the times that I fall out of sleep at the wheel. No, I'm just kidding. I'm just kidding
[01:40:52.920 --> 01:40:57.480]   Uh, here's uh, here's four hours ago. Danica Patrick
[01:40:57.480 --> 01:41:00.200]   who is a race car driver
[01:41:00.200 --> 01:41:02.600]   and uh pen and teller riding
[01:41:02.600 --> 01:41:05.960]   the first self-driving vehicle in las vegas
[01:41:05.960 --> 01:41:10.040]   Uh, this is obviously before the ride. They're smiling
[01:41:10.040 --> 01:41:18.120]   Uh, I wish they were they'd not be smiling at you. I'm just kidding. I would like I would like to see a picture of them because teller just wouldn't shut up
[01:41:18.360 --> 01:41:22.280]   Yeah, right. He is such a gab gab fest gabbermouth
[01:41:22.280 --> 01:41:25.160]   Um
[01:41:25.160 --> 01:41:29.480]   All right, what about this disney story? I like this story. I thought jeff jarvis might have something to say about this
[01:41:29.480 --> 01:41:38.600]   Uh, los angeles times has been doing a series of articles on how disney manipulates anahime to reduce its tax bill and get special treatment and favors
[01:41:38.600 --> 01:41:40.520]   disney didn't like that too much
[01:41:40.520 --> 01:41:47.400]   Disney which by the way owns esp n and apc is actually a fairly large journalistic entity in its own right
[01:41:48.200 --> 01:41:51.960]   I said la times reporters can no longer come to review our movies
[01:41:51.960 --> 01:41:58.840]   And uh at which point the new york times said well arm then our report our reviewers are not even movies
[01:41:58.840 --> 01:42:05.480]   That was really fascinating. That was awesome right on I was so excited at which point did they said oh screw it never mind
[01:42:05.480 --> 01:42:12.840]   I think that's great when the la the new york times sticks up for the la times
[01:42:13.800 --> 01:42:21.640]   Well, uh other your journalist did it too. Like I think usa today and yes, it's right. I mean I'm just saying like all like I was so excited that this was
[01:42:21.640 --> 01:42:24.680]   Journalist standing up for journalism
[01:42:24.680 --> 01:42:31.320]   This is this is the essence of the church and stay when I was when I was a dot dot to make this really trivial. That's what I do
[01:42:31.320 --> 01:42:33.720]   Um, I was tv critic of people
[01:42:33.720 --> 01:42:38.120]   I gave terrible reviews to hallmark hall of fame specials because they were so treakly and awful
[01:42:38.120 --> 01:42:40.840]   You jerk
[01:42:41.560 --> 01:42:44.680]   But you were eight years old states. Of course they were good for you
[01:42:44.680 --> 01:42:48.280]   I really like the channel cheesy movies
[01:42:48.280 --> 01:42:52.760]   They say that's no I don't that's this is a this is a hidden vice of many people I know
[01:42:52.760 --> 01:42:57.960]   Um, but they pulled all of their it's a hallmark advertising from people. Yeah, that's what happens
[01:42:57.960 --> 01:43:00.040]   That's what they used to do. Yeah
[01:43:00.040 --> 01:43:07.080]   And I don't so I don't blame it for that. I guess I will say that I once wrote a story at gig at home that really upset a large
[01:43:07.640 --> 01:43:12.600]   Company and they said to my ceo we're gonna pull our ads
[01:43:12.600 --> 01:43:18.520]   and our subscription to your we had a research service and my ceo was like
[01:43:18.520 --> 01:43:25.880]   All right, then see you later for it. Bye. And they they did it for like a hot second and then
[01:43:25.880 --> 01:43:31.880]   Eventually my ceo he didn't even tell me that's how like which is the right thing to do the right thing is to not to not
[01:43:31.880 --> 01:43:36.280]   influence you. Yeah, I found out like a year later because we were drinking and he was like
[01:43:36.280 --> 01:43:42.360]   By the way, I totally stood up for you. Yay. Oh, thanks. Yay
[01:43:42.360 --> 01:43:45.400]   And then the company went out of business said yeah
[01:43:45.400 --> 01:43:50.360]   Well, I'll never mind all that see if only if only see stacey instead of holding up for principle
[01:43:50.360 --> 01:43:54.760]   If only it had gone out for money and only money you just don't be employed there and everything would be okay
[01:43:54.760 --> 01:44:00.520]   Well, what I'm hoping is I can get podcasters to all agree to boycott apple until they let me into their events
[01:44:01.160 --> 01:44:06.360]   Yeah, yeah, not gonna happen not gonna happen. Yeah, but really is disney's behavior any different?
[01:44:06.360 --> 01:44:09.320]   No
[01:44:09.320 --> 01:44:14.360]   No, it's just weird that disney's in a is a has a bc. I mean like yeah, it's a slight problem
[01:44:14.360 --> 01:44:17.960]   It doesn't make money for them. It's a big company
[01:44:17.960 --> 01:44:21.880]   There's lots of arms by the way. Also the the new york film critics
[01:44:21.880 --> 01:44:28.360]   Circle said well then we're at los angeles film critics association said well then the boss is safe from christ
[01:44:28.360 --> 01:44:35.080]   They're not eligible for any of our awards in that case. I don't know. I don't think the oscar said the same thing. No, I noticed
[01:44:35.080 --> 01:44:38.360]   No oscar thing
[01:44:38.360 --> 01:44:42.600]   uber this is this is an example of just press release
[01:44:42.600 --> 01:44:47.160]   We might have flying cars in la by 2020
[01:44:47.160 --> 01:44:52.600]   Then again, we might not then we might not it could not have it might not have
[01:44:52.600 --> 01:44:57.160]   Los angeles will be the third test city nasa will provide logistical support
[01:44:57.560 --> 01:45:01.720]   The uber has a flying car project called very nicely elevate
[01:45:01.720 --> 01:45:09.640]   Uh and uber's headed project product jeff hold and announced at the web summit in lisbend today the company is adding
[01:45:09.640 --> 01:45:12.520]   los angeles to dallas fort worth and do by
[01:45:12.520 --> 01:45:17.000]   As cities where they hope to pilot their aerial taxi service
[01:45:17.000 --> 01:45:21.000]   By 2020 and here is an artist's depiction
[01:45:21.000 --> 01:45:23.640]   What it might be like
[01:45:23.640 --> 01:45:25.640]   Do you get in the flying?
[01:45:25.960 --> 01:45:32.280]   Taxi can I say that both the private jet thing we were talking about earlier and this have a woman business traveler
[01:45:32.280 --> 01:45:37.480]   Who is right on? You know, I was like, yeah, because even when i'm traveling for business
[01:45:37.480 --> 01:45:42.200]   I'm i'm definitely in the lounges and stuff. I am definitely a minority
[01:45:42.200 --> 01:45:47.160]   Really? So i'm like, yeah, oh, that's interesting like just as a woman. She's ascending
[01:45:47.160 --> 01:45:51.960]   Uh the elevator. She is used her uber app to call a flying car
[01:45:53.000 --> 01:45:55.320]   And is now going out of the roof of the building
[01:45:55.320 --> 01:45:59.800]   And is being ushered along with other women and one guy
[01:45:59.800 --> 01:46:05.800]   Oh, no, there's another guy into her flying car, which really looks surprisingly like a helicopter
[01:46:05.800 --> 01:46:10.920]   Not sure how this is different from flying a helicopter
[01:46:10.920 --> 01:46:18.920]   Yeah, they've had this in new york city for years. Yeah, you know, so you're staying in the pan ambling. It's a helicopter folks
[01:46:20.120 --> 01:46:22.120]   You know, I imagine a one person drone
[01:46:22.120 --> 01:46:26.280]   Is there no driver? No, there's even a driver
[01:46:26.280 --> 01:46:28.920]   Oh, but look at his fancy screen
[01:46:28.920 --> 01:46:36.360]   Or far, far, far, straight out of the backyard, we'd walk down there relax and enjoy the ride
[01:46:36.360 --> 01:46:40.200]   Yeah, these got to go to a special transit center. It's just like drive taking the helicopter
[01:46:40.200 --> 01:46:45.720]   Yeah, so the question is if you buy the uber helicopter, do you get the uber cars that take you back to your house?
[01:46:45.720 --> 01:46:48.440]   I think that's what just happened. Yeah, that's the other car that's in the new car
[01:46:49.080 --> 01:46:51.720]   From so the last mile problem is solved
[01:46:51.720 --> 01:46:59.720]   Oh, that is a tremendous pun being made in the the chat. Oh, yeah. What do they say? It's a liftoff
[01:46:59.720 --> 01:47:04.600]   L Y fd
[01:47:04.600 --> 01:47:05.720]   Totally
[01:47:05.720 --> 01:47:10.680]   Listening to a really good book right now. What's it called? You might like it Leo? You might like it Stacy
[01:47:10.680 --> 01:47:14.600]   Um, oh, I say I've changed my launcher for what's it about?
[01:47:14.600 --> 01:47:17.000]   It's about uh
[01:47:17.000 --> 01:47:19.000]   Capitalism in american. Oh god
[01:47:19.000 --> 01:47:26.680]   No, but it's it's I learned all kind americana a 400 year history by a a startup guy named booshing of awesome
[01:47:26.680 --> 01:47:28.760]   That that does sound good actually
[01:47:28.760 --> 01:47:35.080]   It's really very good. Um, it starts off with mayflower and the economics of it. They weren't really it was an economic thing
[01:47:35.080 --> 01:47:37.080]   See, it's all capitalism. It's all
[01:47:37.080 --> 01:47:39.080]   I get
[01:47:39.080 --> 01:47:42.200]   And i'm up to the telegraph now. It's I think you might like this one
[01:47:42.200 --> 01:47:45.960]   What's it?
[01:47:45.960 --> 01:47:51.160]   americana a 400 year history of capitalism by boobh you shrimmy vassen
[01:47:51.160 --> 01:47:54.600]   Good
[01:47:54.600 --> 01:47:56.600]   Audible wishlist it's
[01:47:56.600 --> 01:47:59.480]   Recommendation for y'all you do what's that?
[01:47:59.480 --> 01:48:07.240]   Is it's kind of nerdy? I don't know jeff look like it, but Leo you might it's called a mind-of-play how clawed shannon invented the information
[01:48:07.240 --> 01:48:11.560]   Yeah, okay. Yeah, it came out over the summer, but we
[01:48:12.440 --> 01:48:16.840]   Wought that like a board back. Well, yes, jeff in my right or what?
[01:48:16.840 --> 01:48:22.360]   Yeah, you're right. You're right. Yeah, I think I bought it and I started listening to it because I'm interested in Shannon
[01:48:22.360 --> 01:48:26.280]   I'm shannon's pastor. Maybe I did no I didn't no I didn't
[01:48:26.280 --> 01:48:32.440]   No, but wasn't there an issue with one of the authors that we were talking. Oh, was there an issue?
[01:48:32.440 --> 01:48:36.840]   Tarnation is he a jerk? I can't remember
[01:48:37.560 --> 01:48:43.880]   I should write a book because there are no issues with me. You should you never harassed me. I'll vouch for you
[01:48:43.880 --> 01:48:51.560]   I I'm kind of snippy sometimes and I have threatened to punch you so I've I consider that well playful
[01:48:51.560 --> 01:48:53.640]   banter
[01:48:53.640 --> 01:48:57.960]   I think that he deserved it space rep parte and I deserved it as well
[01:48:57.960 --> 01:49:02.760]   No, I think I hope that it's the book clawed shannon deserves because of course he's without him
[01:49:02.760 --> 01:49:06.200]   We don't really have what we you know technology. We don't have computers
[01:49:07.080 --> 01:49:08.760]   So
[01:49:08.760 --> 01:49:10.760]   Or mobile broadband
[01:49:10.760 --> 01:49:15.000]   Oh, yes, because of the what is that the TDMA did he'd have inventor?
[01:49:15.000 --> 01:49:22.120]   Uh, no, he did part of shannon's law is how you cram the number of bits per Hertz in your spectrum
[01:49:22.120 --> 01:49:28.120]   So how many literal bits can you cram in a Hertz of spectrum? Oh, yeah, jimmy sony
[01:49:28.120 --> 01:49:31.640]   he's another one of he was an early uh early victim of
[01:49:32.840 --> 01:49:37.480]   The trend that's going on right now. I'm gonna close that article because I don't want to you know
[01:49:37.480 --> 01:49:39.800]   It's the gocker. I don't want to give money credit
[01:49:39.800 --> 01:49:46.680]   Is it cocker? We I I'm okay with gocker. Okay. Well, yeah people are missing gocker now. Yeah
[01:49:46.680 --> 01:49:51.160]   You know you look at all this Weinstein stuff. Wow, they did that
[01:49:51.160 --> 01:49:54.600]   Yep. Yeah, you got to figure nix just going
[01:49:54.600 --> 01:49:59.880]   Oh, Nick did you read next piece? No, Nick wrote an excellent piece about just that. Oh, yeah
[01:50:00.600 --> 01:50:03.560]   Okay, so if he's so good, why didn't you see expose all this stuff earlier?
[01:50:03.560 --> 01:50:08.520]   Well, they did they were they were working on one's okay. Okay. Okay. People were listening to it because it was a gossip brag right
[01:50:08.520 --> 01:50:15.320]   That's what they should have worked on not outing people who didn't you know, I mean that's what they should have worked on
[01:50:15.320 --> 01:50:19.160]   Today's gossip is tomorrow's news. If you go to medium. It's the top post
[01:50:19.160 --> 01:50:22.520]   That's all right. I'm not a fan
[01:50:22.520 --> 01:50:28.040]   All right. I remember that he wrote nasty articles about actually Nick Nick and Nick and I made up
[01:50:29.800 --> 01:50:32.840]   Oh, that's nice. Yeah, it was actually was those nice
[01:50:32.840 --> 01:50:36.200]   Okay
[01:50:36.200 --> 01:50:38.280]   All right, I'm gonna I'm gonna let you guys know
[01:50:38.280 --> 01:50:41.720]   Yes that you you are married to Nick Denton
[01:50:41.720 --> 01:50:43.480]   No
[01:50:43.480 --> 01:50:45.480]   I'm getting hungry. I oh
[01:50:45.480 --> 01:50:48.520]   That that we can do something about is it. Oh, it's so
[01:50:48.520 --> 01:50:53.560]   We still can talk but I just want to take a break and i'm gonna give you
[01:50:53.560 --> 01:50:58.840]   Do we have anything to say about how will he been tall all being arrested by the Saudi prince?
[01:50:59.480 --> 01:51:03.000]   I don't know anything about Saudi politics except what I've read in the New York Times
[01:51:03.000 --> 01:51:04.440]   Yeah, I have nothing to add here
[01:51:04.440 --> 01:51:08.760]   He of course I have to talk about it one of the big investors at twitter at lift at apple
[01:51:08.760 --> 01:51:12.040]   Saudi billionaire
[01:51:12.040 --> 01:51:14.680]   You know, it's they claim it's a corruption investigation
[01:51:14.680 --> 01:51:19.320]   But I think the people most people I trust say it's just a power consolidation. There's oh, yeah
[01:51:19.320 --> 01:51:24.680]   There's a big but they're the best part is they're arrested and put on the ritz. Yeah. Well, at least he's got a nice digs
[01:51:24.680 --> 01:51:28.040]   Ritz is not to have you about the brand association. I don't think
[01:51:28.200 --> 01:51:30.200]   I
[01:51:30.200 --> 01:51:33.160]   I never met the guy. I don't know
[01:51:33.160 --> 01:51:38.120]   Snapchat wants to redesign so that it's easier for the oldsters to use
[01:51:38.120 --> 01:51:43.480]   See this is this is old people. Well, I made that part of the ruination of facebook
[01:51:43.480 --> 01:51:48.040]   But I never met somebody under 25 who found snapchat in the least bit difficult to use
[01:51:48.040 --> 01:51:52.280]   Right remember they're struggling their stocks been going down as they're
[01:51:52.840 --> 01:51:58.680]   In fact the latest quarterly results were bad because they've been losing a lot of users. So they want to
[01:51:58.680 --> 01:52:07.640]   They realized that what was a benefit in the in the early days of being hard to understand so that you know teenagers would show other teenage
[01:52:07.640 --> 01:52:09.720]   Oh, you do this
[01:52:09.720 --> 01:52:15.800]   Is no longer benefit. I don't know. So they want they want to that's what happened is instagram got everybody over 25
[01:52:15.800 --> 01:52:20.040]   Oh, there was a story I was going to ask us about yes
[01:52:20.680 --> 01:52:22.680]   We're going through quickly because we can get to dinner
[01:52:22.680 --> 01:52:28.360]   No, and I I give you that as the warning like you've got half an hour before I get hanging
[01:52:28.360 --> 01:52:31.480]   Oh, no. No, we're we're almost done. Yeah, but I got a
[01:52:31.480 --> 01:52:35.560]   The texas the texas church shooting
[01:52:35.560 --> 01:52:40.280]   And the fbi having challenges decrypting the gunman's phone
[01:52:40.280 --> 01:52:46.120]   Um, I thought it was interesting that they asked about fingerprint reading what I did wonder though
[01:52:46.120 --> 01:52:48.440]   What happens with face ID?
[01:52:50.040 --> 01:52:52.840]   They would just they would just say look at this
[01:52:52.840 --> 01:52:57.240]   It's just like taking a fingerprint. What now he's but he's dead
[01:52:57.240 --> 01:53:02.440]   So they wouldn't be a dead body to unlock your eyes need to be open well
[01:53:02.440 --> 01:53:04.040]   Okay, if you have attention
[01:53:04.040 --> 01:53:07.560]   There's a setting on the face ID that says do I have to pay attention or not?
[01:53:07.560 --> 01:53:12.760]   And you can turn that off. It's on by default. I turn it off because it means it it works more often
[01:53:12.760 --> 01:53:15.320]   You don't have to look at it specifically
[01:53:15.320 --> 01:53:18.520]   And then it might work, but they I think your eyes have to be open. I do
[01:53:19.000 --> 01:53:21.000]   But I guess I don't know that's I was like
[01:53:21.000 --> 01:53:25.240]   What does here's my I might only point on this, but they were gonna see the whole thing now
[01:53:25.240 --> 01:53:27.160]   You see why we need to be able to get cell phones
[01:53:27.160 --> 01:53:29.480]   But what did they do before there were cell phones?
[01:53:29.480 --> 01:53:32.280]   Did they ever solve crimes before there were cell phones?
[01:53:32.280 --> 01:53:37.800]   The fact that this is this is wiretapping. Yeah, well, but the fact that they did either
[01:53:37.800 --> 01:53:39.800]   They had no recording of your I
[01:53:39.800 --> 01:53:42.760]   But it's more than your phone calls in here. There's everything in here
[01:53:42.760 --> 01:53:46.760]   Right the fact that we have now something we carry in our pocket that has our entire lives on here
[01:53:46.840 --> 01:53:50.200]   Of course is a goal mine for law enforcement, but it's a brand new goal mine
[01:53:50.200 --> 01:53:54.200]   This is something that had never existed before and I would submit it should be protected
[01:53:54.200 --> 01:53:56.360]   Just as the contents of your mind are protected. Yes
[01:53:56.360 --> 01:53:59.800]   It's not a wiretap on a phone call. It's not a phone
[01:53:59.800 --> 01:54:02.440]   This is the problem with
[01:54:02.440 --> 01:54:07.000]   Well, wait, you can't I'm being written to technologies or laws being written to technology now to principle
[01:54:07.000 --> 01:54:11.240]   This is the problem I have with worrying about the legislators going after the internet
[01:54:11.240 --> 01:54:15.880]   Uh, wait, they know who did it first last mail. They know who did it
[01:54:16.840 --> 01:54:19.720]   I guess he wants to see if anybody else did it. Anybody else did it. Yeah
[01:54:19.720 --> 01:54:26.360]   Well, so the other thing is you you do have and you have had in the past access to people's computers
[01:54:26.360 --> 01:54:30.200]   So like if I am once I am arrested for committing a crime
[01:54:30.200 --> 01:54:32.520]   They can search my entire home
[01:54:32.520 --> 01:54:37.240]   They can grab my computer. They can try to get all they want off of it. So
[01:54:37.240 --> 01:54:41.800]   And I'm not arguing. They can't do anything about it
[01:54:41.800 --> 01:54:45.160]   Well, and there's a guy who's in jail with an indeterminate sentence
[01:54:45.960 --> 01:54:48.120]   Because he won't unlock his uh hard drive
[01:54:48.120 --> 01:54:54.680]   Yes, that's right. But it's a weird case because the judge said this is different because we know there's child pornography on there
[01:54:54.680 --> 01:55:00.120]   So it's you know, but that's the point and the same thing with fingerprints and face ID
[01:55:00.120 --> 01:55:04.520]   Uh traditionally courts have held that they can't get a password
[01:55:04.520 --> 01:55:10.040]   But they can get fingerprint just as they do when you go in and you get fingerprinted or take it piece of your hair for dna
[01:55:10.040 --> 01:55:12.760]   Uh, but not all courts some courts say
[01:55:12.760 --> 01:55:13.960]   Or you want to go after your husband?
[01:55:13.960 --> 01:55:17.480]   Yeah, or we didn't know what that story. Yeah. Yeah, what there was a
[01:55:17.480 --> 01:55:20.200]   So there's a what I don't even know the flight
[01:55:20.200 --> 01:55:22.840]   That was it was a
[01:55:22.840 --> 01:55:26.520]   It was a radian nationals on a flight to somewhere in the middle east
[01:55:26.520 --> 01:55:31.960]   Why the husband falls asleep? They were they were going to bali going to bali
[01:55:31.960 --> 01:55:35.560]   As one does one of the nice vacation as one does
[01:55:35.560 --> 01:55:40.840]   It's on catar airlines or cotar airlines and the wife reaches over takes the guy's phone puts his finger on it
[01:55:41.320 --> 01:55:45.800]   Because he's asleep and discovers he's having an affair starts screaming
[01:55:45.800 --> 01:55:48.840]   Plane has to has to turn back
[01:55:48.840 --> 01:55:53.560]   It doesn't turn back it lands it diverts diverts diverts it diverts
[01:55:53.560 --> 01:55:57.240]   No, no bali for them. I'm like we we got to tell the story, right
[01:55:57.240 --> 01:55:58.760]   Yeah
[01:55:58.760 --> 01:56:02.360]   And they were actually allowed on a later flight to bali so they
[01:56:02.360 --> 01:56:05.400]   Calm down and you can get on the next flight
[01:56:05.400 --> 01:56:07.000]   Just calm down
[01:56:07.000 --> 01:56:10.840]   We don't care if you have an affair or you put your finger on the fingerprint reader. Just calm down
[01:56:11.480 --> 01:56:13.480]   And she was she was drinking
[01:56:13.480 --> 01:56:14.920]   so
[01:56:14.920 --> 01:56:17.960]   I as as a woman who is very
[01:56:17.960 --> 01:56:23.080]   Very expressive. I would say had I discovered that my husband was saying there
[01:56:23.080 --> 01:56:29.400]   Of course you would they might they might have to divert the plane. I would have just I would have just sat there and cried myself. Oh, no
[01:56:29.400 --> 01:56:32.200]   No
[01:56:32.200 --> 01:56:36.680]   Although her child was with her silently sobbed into my airline coffee
[01:56:38.920 --> 01:56:44.120]   You're not supposed to drink the coffee on your lines. I know they don't they don't it not just the pots the whole water system
[01:56:44.120 --> 01:56:46.440]   All right. This is more up your alley
[01:56:46.440 --> 01:56:49.000]   link
[01:56:49.000 --> 01:56:55.160]   Oh, we talked about that on the podcast. I figured you would harmony sending out emails to owners of the harmony link
[01:56:55.160 --> 01:57:00.440]   Which by the way is a cloud based service including you know hardware has a cloud base back end
[01:57:00.440 --> 01:57:06.600]   That after march 2018 it's going to stop working in other words if you bought a link it will be bricked
[01:57:06.920 --> 01:57:08.920]   Yeah
[01:57:08.920 --> 01:57:11.880]   Yeah, this is a terrible thing
[01:57:11.880 --> 01:57:18.440]   And so I'm what is what is logitech to do if it's an obsolete product. They don't they have to support it forever
[01:57:18.440 --> 01:57:23.480]   Here's my thing. I've been calling for this my my celebratory like
[01:57:23.480 --> 01:57:26.360]   I've been calling for this for a long time
[01:57:26.360 --> 01:57:29.080]   companies need to
[01:57:29.080 --> 01:57:32.120]   Create expiration dates for consumer products
[01:57:32.520 --> 01:57:36.920]   If it's a software based product you need to say and this is happening in the enterprise
[01:57:36.920 --> 01:57:39.640]   Microsoft's always done this when they when they put out a version of windows
[01:57:39.640 --> 01:57:42.440]   There's a like yeah, so does google so does apple
[01:57:42.440 --> 01:57:47.800]   So now when you're you know this solves a lot of the problems because consumers can say oh
[01:57:47.800 --> 01:57:51.240]   I don't want to buy this because it's only got a five year lifespan
[01:57:51.240 --> 01:57:59.880]   So that that's my take kevin has another take but I don't remember what I take is don't buy anything that requires a cloud surface
[01:58:01.160 --> 01:58:03.160]   Or if you do make sure you can
[01:58:03.160 --> 01:58:07.320]   Subplant it and keep it running if you want to keep using it
[01:58:07.320 --> 01:58:11.800]   Also a good option
[01:58:11.800 --> 01:58:17.240]   Here's one we could just do the headline and move on Greta van Susteren is launching an app called
[01:58:17.240 --> 01:58:19.800]   sorry
[01:58:19.800 --> 01:58:24.120]   And I think that's fair for what huh everything
[01:58:24.120 --> 01:58:29.800]   Uh, it's actually uh, you you will get to accept or reject apologies from a friend
[01:58:31.080 --> 01:58:37.320]   Or from a public figure if it's a public figure figure we all get to see and vote on accept or reject
[01:58:37.320 --> 01:58:43.560]   So my god, it's like hot or not for public shame. Yeah, so Kathy Griffin. She gives as an example
[01:58:43.560 --> 01:58:51.960]   On her facebook page credit as as an example. Kathy griffer. Remember she uh, she picture of her with a chopped off head of donald trump
[01:58:51.960 --> 01:58:54.200]   She could apologize. I'm sorry
[01:58:54.200 --> 01:58:56.120]   Was it enough or not?
[01:58:56.120 --> 01:59:01.400]   People would vote how would she know if her apology was accepted without the vote counter asks Greta or how about when a cable
[01:59:01.400 --> 01:59:06.760]   News network apologizes for blunder apol politician for cheating on a spouse dove soap for that commercial
[01:59:06.760 --> 01:59:10.520]   It's just endless says Greta van Susteren
[01:59:10.520 --> 01:59:14.200]   Everybody's got an app
[01:59:14.200 --> 01:59:19.800]   I think this is this is an app for the times. Sorry the sorry app
[01:59:19.800 --> 01:59:25.160]   Let's take a break picks of the week coming up and i'm gonna start with talking about the lighthouse
[01:59:25.160 --> 01:59:30.120]   Which I know stacey you've you you told me I was a dork for not buying
[01:59:30.120 --> 01:59:35.480]   So I went out and bought one and I love it lighthouses the only security camera
[01:59:35.480 --> 01:59:38.440]   That has a lydar in it
[01:59:38.440 --> 01:59:43.080]   You might say well, it's it's not lighter. Well, it's like our like right. What is it?
[01:59:43.080 --> 01:59:48.520]   It is a time of light sensor. Sorry time of flight sensor time of flight
[01:59:48.520 --> 01:59:50.040]   Sensor
[01:59:50.040 --> 01:59:53.000]   Uh, it doesn't spin around like the lydar on top of a google car
[01:59:53.000 --> 01:59:57.720]   It's right here, but what it does do is it gets makes a 3d map of everything out there
[01:59:57.720 --> 02:00:02.920]   And that allows it. This is a security camera. It's got infrared light so it can see in the dark
[02:00:02.920 --> 02:00:08.680]   It allows it to know what's out there people pet it can distinguish between the two
[02:00:08.680 --> 02:00:13.160]   It knows when you're there when family members are there strangers are there
[02:00:13.160 --> 02:00:14.920]   and
[02:00:14.920 --> 02:00:21.480]   If you could set it's got artificial intelligence built into it so you could say let me know if my daughter doesn't come home by three
[02:00:22.200 --> 02:00:24.840]   Let me know if you see any pictures of my pets
[02:00:24.840 --> 02:00:30.840]   Uh, it continually analyzes the 3d map of the room to detect movement
[02:00:30.840 --> 02:00:33.720]   To know what's in the map
[02:00:33.720 --> 02:00:36.600]   I remember I had another brand of security camera
[02:00:36.600 --> 02:00:40.600]   Which constantly told me that that the mylar balloons
[02:00:40.600 --> 02:00:43.480]   In the corner where people invading my house
[02:00:43.480 --> 02:00:50.120]   The lighthouse does not get fooled by mylar balloons and what's really cool is it will notify you on your phone
[02:00:50.120 --> 02:00:53.160]   They either somebody I don't recognize in your house show you a picture
[02:00:53.160 --> 02:00:59.880]   And then right below it it says you have a button to call 911 turn off security or just ignore it
[02:00:59.880 --> 02:01:04.920]   I love this thing light dot house slash twit
[02:01:04.920 --> 02:01:09.480]   Is so smart and I love the uh the ai
[02:01:09.480 --> 02:01:14.280]   See so okay here. I'll give you an example look right now. Somebody just walked through
[02:01:14.280 --> 02:01:17.400]   our house, okay
[02:01:17.400 --> 02:01:20.920]   And it it pinged me alert triggered in the living room
[02:01:20.920 --> 02:01:25.560]   All okay call 911. I could decide how I want to handle that
[02:01:25.560 --> 02:01:28.280]   It recognizes people
[02:01:28.280 --> 02:01:30.680]   So here's all the pictures of me that it's seen
[02:01:30.680 --> 02:01:37.800]   And if it sees some things that look like me is it is this you I can say yeah, that's me. That's me. That's me. That's me
[02:01:37.800 --> 02:01:42.440]   That's not me. That's not me. So that those are all me
[02:01:42.440 --> 02:01:45.080]   So I can say those those are all
[02:01:45.480 --> 02:01:48.440]   Leel report confirm six images. So I'm training it
[02:01:48.440 --> 02:01:52.600]   It really is cool. 1080p live stream two-way talk
[02:01:52.600 --> 02:01:55.080]   Oh, I didn't mention that you could talk to people in fact
[02:01:55.080 --> 02:01:58.680]   There's this great thing it will ping you if somebody stands for the camera waves
[02:01:58.680 --> 02:02:05.160]   Somebody's waving so when your kids get home they can wave and you go. Yeah. Oh, hi. You're home. Yeah, I'm home from school
[02:02:05.160 --> 02:02:07.640]   Okay, thanks for letting me know
[02:02:07.640 --> 02:02:12.840]   It's got three different permission settings. So you've got owner member and um
[02:02:13.560 --> 02:02:16.120]   Camera with the third one is I should look at it
[02:02:16.120 --> 02:02:19.000]   So you can say look I I'm the primary owner
[02:02:19.000 --> 02:02:23.560]   But Lisa is a member and don't bother and what oh and this is very important for Lisa
[02:02:23.560 --> 02:02:27.240]   She doesn't like the idea of cameras being on when we're walking around in the house
[02:02:27.240 --> 02:02:30.600]   She's want to see it people to see her in her jammies or whatever or just want to put it on the internet
[02:02:30.600 --> 02:02:35.240]   So whenever we're home cameras off we're home. It's secure. We don't have to worry about it
[02:02:35.240 --> 02:02:39.400]   I can go on and on. This is so cool. You can say what do the kids do while I was out yesterday?
[02:02:39.400 --> 02:02:42.200]   You can get a timeline. It will play back video
[02:02:43.000 --> 02:02:47.720]   A video timeline. Look at this leo leport left leo leport arrived. I know exactly what time I got home
[02:02:47.720 --> 02:02:51.400]   What time I this is incredible. I can say when it pings me
[02:02:51.400 --> 02:02:56.360]   Uh, I could say, you know, uh, are there children in the house
[02:02:56.360 --> 02:03:03.640]   I can't say how old are they old or what so i'm gonna leave uh, i'm gonna leave security alerts on
[02:03:03.640 --> 02:03:06.360]   But i'm gonna say it's all okay. I can get a recap
[02:03:06.360 --> 02:03:11.240]   Of all the pet activity in the house or all the people activity in the house
[02:03:12.120 --> 02:03:18.680]   This is so cool. Um, I I can go on and on. This is the smartest home security camera
[02:03:18.680 --> 02:03:24.840]   I have ever seen. I love the ai built into it. I love the 3d sensor the time of flight sensor built into it
[02:03:24.840 --> 02:03:27.960]   Um and it's not available
[02:03:27.960 --> 02:03:30.600]   So thank you everybody. Good night. No
[02:03:30.600 --> 02:03:37.400]   Pre-orders were sold out. It was it went out like crazy, but they're gonna they're making them as fast as they can
[02:03:37.400 --> 02:03:41.560]   And they're gonna ship a new batch very soon. I want you to get in there and get it
[02:03:42.200 --> 02:03:44.840]   Light dot house slash twit
[02:03:44.840 --> 02:03:51.000]   You'll get 15 off the lighthouse when it ships and by the way a chance to win your own lighthouse plus a year of service free
[02:03:51.000 --> 02:03:54.200]   That's a 399 value lighthouse
[02:03:54.200 --> 02:03:57.240]   Is at light dot house
[02:03:57.240 --> 02:04:04.280]   Slash twit put your email address in there and they'll notify you the minute they're ready to go and I think it's going to be soon
[02:04:04.280 --> 02:04:07.320]   And you'll get 15 off as well
[02:04:07.960 --> 02:04:12.280]   This thing is so cool. We have we this is our studio one. I have them all over everywhere
[02:04:12.280 --> 02:04:15.880]   Because it's just it's much smarter and it it doesn't ping me
[02:04:15.880 --> 02:04:18.680]   when um
[02:04:18.680 --> 02:04:24.200]   When it's not a problem and it and best of all unlike any of these are cameras when we're home
[02:04:24.200 --> 02:04:28.200]   Camera's off. I like that light that house slash twit
[02:04:28.200 --> 02:04:33.400]   Seaside for contest rules and of course 15 off lighthouse when they ship now
[02:04:34.680 --> 02:04:37.240]   What's stasis thing the world wants to know?
[02:04:37.240 --> 02:04:42.360]   Okay, you know how we were talking about security earlier. Yeah, we were
[02:04:42.360 --> 02:04:47.720]   It was it was a thing. Oh, I never I haven't I haven't unwrapped my uh
[02:04:47.720 --> 02:04:52.920]   See I prefer something like lighthouse to a monitoring service. I want to monitor it and then I can decide what's going on
[02:04:52.920 --> 02:04:54.920]   right I like that better
[02:04:54.920 --> 02:04:57.240]   so what I have here and
[02:04:57.240 --> 02:04:59.160]   Is
[02:04:59.160 --> 02:05:02.360]   This is this is very diy very nerdy so
[02:05:03.080 --> 02:05:07.640]   Be advised and don't I am touching the boards just ignore me
[02:05:07.640 --> 02:05:10.840]   So this is from a company called connect with a k
[02:05:10.840 --> 02:05:16.600]   Like a raspberry pie or something. What is it? It's it's it's a little board connected is the name of it
[02:05:16.600 --> 02:05:20.760]   And what it does is it connects in with your wired security
[02:05:20.760 --> 02:05:22.840]   uh system
[02:05:22.840 --> 02:05:26.200]   so what this does is that you connect the wires in through here
[02:05:26.200 --> 02:05:28.200]   and
[02:05:28.200 --> 02:05:35.240]   You put them in and then it will take all the sensors that are wired into your house and your existing old school security system
[02:05:35.240 --> 02:05:36.760]   and
[02:05:36.760 --> 02:05:38.920]   You can now integrate them with smart things
[02:05:38.920 --> 02:05:41.800]   And
[02:05:41.800 --> 02:05:46.680]   It's pretty cool for a certain class of people who is not like this is not something your grandma
[02:05:46.680 --> 02:05:52.200]   But this doesn't use and this is that thing where if you're buying a cloud service and they discontinue it you're out of luck
[02:05:52.200 --> 02:05:54.280]   This you you you own this
[02:05:54.520 --> 02:06:01.240]   You own this um and you have an app that it will connect to on your phone. Are you gonna use open source software with it or
[02:06:01.240 --> 02:06:03.240]   Oh
[02:06:03.240 --> 02:06:06.680]   There is an app. Uh, it is open source. Oh, that's
[02:06:06.680 --> 02:06:13.480]   Um now I have not installed this because I thought I had a wired security system and it turns out I don't I have wireless
[02:06:13.480 --> 02:06:15.640]   I was like oh
[02:06:15.640 --> 02:06:20.040]   So I have not seen this working. Oh, look. Here's the brains. Um
[02:06:20.040 --> 02:06:22.600]   La la
[02:06:22.600 --> 02:06:24.360]   La la
[02:06:24.360 --> 02:06:30.760]   So basically all this works together. It is a solution for a problem that some people will have
[02:06:30.760 --> 02:06:33.160]   and it is
[02:06:33.160 --> 02:06:39.160]   Probably a nice one. I just need to find a home to install it in and see so my hope was to actually tell you
[02:06:39.160 --> 02:06:40.840]   Hey
[02:06:40.840 --> 02:06:45.320]   This is amazing or this sucks, but well you will we'll hear about it
[02:06:45.320 --> 02:06:53.400]   Where what is the website for connect? Oh, it is can oh it is pre-launch.connected with a k.io
[02:06:53.720 --> 02:07:01.080]   Connected with a k I know it's it's complicated. Well, somebody or no, it's connected. It's not complicated. It's an alarm panel
[02:07:01.080 --> 02:07:03.800]   Coming soon to kickstarter
[02:07:03.800 --> 02:07:06.200]   So it is early early days. Yes
[02:07:06.200 --> 02:07:09.240]   So what I have is a di y kit and
[02:07:09.240 --> 02:07:12.760]   This is something like you would put in the back of your eggs
[02:07:12.760 --> 02:07:17.240]   If you wanted it to be on your wall, you would have to put it in the back of your existing alarm panel
[02:07:17.240 --> 02:07:20.200]   They say that that's a problem because the metal
[02:07:20.840 --> 02:07:23.720]   Enclosure is going to mess with the Wi-Fi signal. They are wrong
[02:07:23.720 --> 02:07:32.280]   They sell this ugly plastic blue thing that you can stick it on but no sane person is actually going to want, you know, boards on a wall
[02:07:32.280 --> 02:07:33.560]   Yeah
[02:07:33.560 --> 02:07:38.040]   So this is very diy, but it does solve a real problem indeed indeed
[02:07:38.040 --> 02:07:41.240]   Jeff Jarvis, well, do you have a number?
[02:07:41.240 --> 02:07:46.360]   Well, we could have a number complaining about the worry about Facebook and how it changes the world
[02:07:46.360 --> 02:07:48.440]   Which is the trump's campaign
[02:07:48.520 --> 02:07:51.240]   Managers or one of us can be a manager said that they raised
[02:07:51.240 --> 02:07:53.960]   $280 million via Facebook
[02:07:53.960 --> 02:07:57.960]   $280 million as I think it was Josh Marshall said in a wag ish twit
[02:07:57.960 --> 02:08:01.000]   Well, we got to look into that one 260 million dollar donation
[02:08:01.000 --> 02:08:05.400]   But rather than talking about that I prefer this number
[02:08:05.400 --> 02:08:11.960]   Yesterday across america seven trans people were elected to public office. Isn't that amazing?
[02:08:11.960 --> 02:08:15.240]   I blew me away
[02:08:16.760 --> 02:08:23.720]   Including and I love this in Virginia the eight what was he six or seven term congressman eight term congressman who
[02:08:23.720 --> 02:08:33.720]   Who created the law against a bathroom or a state state state legislature? Yeah, yeah, I'm sorry state legislator create the the tran anti transgender bathroom law
[02:08:33.720 --> 02:08:36.360]   so
[02:08:36.360 --> 02:08:38.280]   What's her name? I can't remember her name shanaka
[02:08:38.280 --> 02:08:41.560]   I want to say donna kah tanaka, but anyway
[02:08:41.560 --> 02:08:44.440]   She said well screw that beat him
[02:08:45.800 --> 02:08:47.960]   Eat him and do you hear what she said afterwards?
[02:08:47.960 --> 02:08:49.400]   Danica rome
[02:08:49.400 --> 02:08:55.240]   Danica rome that's right and afterwards asked about him about her the opponent. She said bob is now a constituent of mine
[02:08:55.240 --> 02:09:00.680]   I don't complain about constituents. Oh, that's so great. Is that classy good for you?
[02:09:00.680 --> 02:09:07.080]   She's taking the michelle obama high road high road, baby. Yep. Yep. Yep. So it's pretty amazing
[02:09:07.080 --> 02:09:09.240]   This is and this is this by the way, I'm gonna argue
[02:09:09.240 --> 02:09:12.600]   The internet has a role in this
[02:09:12.600 --> 02:09:20.520]   The fact that we have gay marriage in this country, which I consider a good is because people suddenly realized that people could come out and you can recognize that you have someone in your family
[02:09:20.520 --> 02:09:23.640]   And they're they're not awful and evil and other they're people
[02:09:23.640 --> 02:09:28.840]   They're people same now for trans we have this whole thing going on about trans and bathrooms and all this huha
[02:09:28.840 --> 02:09:33.800]   But the internet enables people to stand up and say I i'm one you got a problem with that
[02:09:33.800 --> 02:09:39.400]   You go back to gay marriage and I think you could say that that same thing happened that kind of cultural shift
[02:09:40.040 --> 02:09:45.240]   Happened and it happened so fast and I think the internet was a key factor that and and you know what?
[02:09:45.240 --> 02:09:48.840]   Social networks were a key factor. So no you're right. See
[02:09:48.840 --> 02:09:55.000]   Why don't you do that from now on you could be our chief internet cheerleader show us all the good things. No, I gotta complain to okay
[02:09:55.000 --> 02:09:58.760]   That's all fine
[02:09:58.760 --> 02:10:02.840]   The uh, robert g marshall was a 13 term
[02:10:02.840 --> 02:10:05.880]   Incumbent delegate
[02:10:06.280 --> 02:10:11.240]   In the virginia what do they call it the they have a funny name for it? Uh, the house of
[02:10:11.240 --> 02:10:14.600]   Barris de birds
[02:10:14.600 --> 02:10:19.880]   Deepens he'll call himself. Yeah. He called himself virginia's chief homophobe
[02:10:19.880 --> 02:10:23.480]   And introduced a bathroom bill which died in committee
[02:10:23.480 --> 02:10:26.600]   Uh danica rum beat him
[02:10:26.600 --> 02:10:29.400]   That's gotta hurt
[02:10:29.400 --> 02:10:32.840]   Did you see um his sister?
[02:10:34.920 --> 02:10:39.560]   Is it she went on to twitter? I gotta find her now she went on twitter and basically that's my brother and
[02:10:39.560 --> 02:10:43.640]   I'm sorry. I lost his job, but you know, basically he just said karma, man. Oh wow
[02:10:43.640 --> 02:10:48.840]   Yeah, marshall refused to debate rome and refused to call her
[02:10:48.840 --> 02:10:50.920]   her
[02:10:50.920 --> 02:10:52.920]   Insisted on calling her him
[02:10:52.920 --> 02:10:54.440]   what a
[02:10:54.440 --> 02:10:56.440]   Yeah
[02:10:56.440 --> 02:10:58.440]   bag
[02:10:59.320 --> 02:11:01.320]   leather Richard
[02:11:01.320 --> 02:11:06.520]   All right, we gotta go because stacey's dying and i'm i don't have anything right off the top
[02:11:06.520 --> 02:11:08.280]   What it was jacie stacey
[02:11:08.280 --> 02:11:10.040]   Yes, sir. What's for dinner? What's for dinner?
[02:11:10.040 --> 02:11:13.640]   We're going out tonight. Nice. What he was old. So what's for dinner? Where you going?
[02:11:13.640 --> 02:11:15.800]   I don't know yet that
[02:11:15.800 --> 02:11:16.440]   Oh
[02:11:16.440 --> 02:11:20.360]   We live in a democratic household. Oh, do you say you use it?
[02:11:20.360 --> 02:11:22.840]   You spin the spoon
[02:11:22.840 --> 02:11:24.600]   When your daughter wins, where do you end up going?
[02:11:24.600 --> 02:11:26.760]   Uh, it depends
[02:11:26.760 --> 02:11:28.520]   Kids tend to be tyrants don't they?
[02:11:29.320 --> 02:11:33.640]   No, she's not a tyrant. She just doesn't eat a lot of stuff. Good. Uh, you just have to talk about her
[02:11:33.640 --> 02:11:40.760]   So that that's true. D's just Mexican food. That's actually where I sent the queso picture. Sounds nice. Oh, yes
[02:11:40.760 --> 02:11:43.240]   You're going to have some queso, baby
[02:11:43.240 --> 02:11:46.680]   But I don't know if if it's queso night or not because we did have that Friday
[02:11:46.680 --> 02:11:50.520]   You can't think of sushi dipped in queso queso
[02:11:50.520 --> 02:11:52.520]   queso
[02:11:52.520 --> 02:11:55.240]   queso every day
[02:11:55.240 --> 02:11:57.240]   I will tell you queso
[02:11:57.560 --> 02:11:59.320]   Where did I go? I went to a place
[02:11:59.320 --> 02:12:02.520]   They take a soap a pia without the honey and powdered sugar
[02:12:02.520 --> 02:12:06.120]   They stuff it with brisket and they pour queso on it
[02:12:06.120 --> 02:12:09.160]   It does sound
[02:12:09.160 --> 02:12:15.080]   Bad but good, you know, it's so so good. It's bad or so bad. It's good. I'm not sure which
[02:12:15.080 --> 02:12:21.240]   Oh, here's the other rooms on it doesn't be see right now. Nice. I'm gonna run because uh, I don't want anybody to die of starvation
[02:12:21.240 --> 02:12:26.920]   I thank you all for being here. We do this week in google. What a fun conversation you guys are so good
[02:12:27.720 --> 02:12:32.120]   Jeff Jarvis city university in New York. What would google do buzz machine calm
[02:12:32.120 --> 02:12:35.080]   all over my facebook feed
[02:12:35.080 --> 02:12:37.480]   Stacy he can boss. That's what's the problem facebook
[02:12:37.480 --> 02:12:39.800]   It is Stacy at iot.com
[02:12:39.800 --> 02:12:45.320]   All over my facebook feed. I wish i wish i had more i want more stacey on my facebook feed
[02:12:45.320 --> 02:12:50.600]   But you can get more stacey in your inbox if you subscribe to a great newsletter and listen to our podcast with kevin
[02:12:50.600 --> 02:12:53.640]   Thank you stacey. Thank you jeff. We do this show
[02:12:54.200 --> 02:12:56.360]   every wednesday 130 pacific 430 eastern
[02:12:56.360 --> 02:13:00.360]   That's now 21 30 utc because we're on standard time once again
[02:13:00.360 --> 02:13:02.760]   21 30 utc, please stop by
[02:13:02.760 --> 02:13:09.240]   Watch out youtube.com/twit or twit.tv/live join us in the chatroom at irc.twit.tv and
[02:13:09.240 --> 02:13:16.680]   If you can't be here live subscribe so you'll get a copy in your in your podcast client your your pocket cast your overcast
[02:13:16.680 --> 02:13:19.640]   Your iTunes every single week. You don't want to miss this show
[02:13:19.640 --> 02:13:22.520]   We'll see you next time on this week in google
[02:13:22.520 --> 02:13:25.360]   [MUSIC PLAYING]
[02:13:25.360 --> 02:13:28.700]   [MUSIC PLAYING]
[02:13:28.700 --> 02:13:31.280]   (upbeat music)
[02:13:31.280 --> 02:13:34.360]   (gentle piano music)

