;FFMETADATA1
title=All the Good Stuff at Google I/O
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=456
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.200]   It's time for Twig this week in Google. It's Google I/O Day and we've got all the good stuff from the keynote.
[00:00:06.200 --> 00:00:12.220]   Stacey Higginbotham is joining us oddly enough. She's in Seattle for Microsoft's build developer conference.
[00:00:12.220 --> 00:00:18.480]   Jeff Jarvis is at Google I/O. He'll join us from the press tent and Kevin Tofel of about Chromebooks.com.
[00:00:18.480 --> 00:00:20.480]   It's all coming up next on Twig.
[00:00:20.480 --> 00:00:27.040]   Netcast you love from people you trust.
[00:00:27.040 --> 00:00:33.040]   This is Twig.
[00:00:33.040 --> 00:00:41.040]   Bandwidth for this week in Google is provided by cash fly. C A C H E F L Y dot com.
[00:00:41.040 --> 00:00:53.040]   This is Twig this week in Google. Episode 456 recorded Tuesday May 8th, 2018.
[00:00:53.040 --> 00:00:56.040]   All the good stuff at Google I/O.
[00:00:57.040 --> 00:01:00.040]   This week in Google is brought to you by Rocket Mortgage.
[00:01:00.040 --> 00:01:05.040]   From Quick and Lones. Home plays a big role in your life. That's why Quick and Lones created Rocket Mortgage.
[00:01:05.040 --> 00:01:10.040]   It lets you apply simply and understand the entire mortgage process fully so you can be confident.
[00:01:10.040 --> 00:01:16.040]   You're getting the right mortgage for you. Get started at rocketmortgage.com/twig.
[00:01:16.040 --> 00:01:20.040]   It's time for Twig this week in Google to show we cover the latest news from Google.
[00:01:20.040 --> 00:01:23.040]   And there is a lot of news from Google today.
[00:01:23.040 --> 00:01:28.040]   It's not just from Google from everybody because developer conferences are going on simultaneously.
[00:01:28.040 --> 00:01:37.040]   That's why Stacey Higginbotham sent her to Seattle to cover Microsoft's build for Stacey on IOT.com.
[00:01:37.040 --> 00:01:38.040]   Hi Stacey.
[00:01:38.040 --> 00:01:42.040]   Hello. I actually sat next to your Windows weekly people the other day.
[00:01:42.040 --> 00:01:47.040]   They told me. They said it was Matt Stacey. We crossed the streams.
[00:01:47.040 --> 00:01:48.040]   Indeed.
[00:01:48.040 --> 00:01:49.040]   How nice.
[00:01:49.040 --> 00:01:54.040]   I take it you're there mostly because of the IOT. Any IOT announcements by yourself.
[00:01:54.040 --> 00:01:58.040]   Yeah, it was a big year for IOT for them. I'm sure you guys covered it yesterday.
[00:01:58.040 --> 00:02:00.040]   A little bit, but I would love to hear more.
[00:02:00.040 --> 00:02:05.040]   AI on IOT, which is combining that one. But two buzzwords.
[00:02:05.040 --> 00:02:07.040]   I was going to say two acronyms.
[00:02:07.040 --> 00:02:08.040]   Two.
[00:02:08.040 --> 00:02:12.040]   Two kind of meaningless buzzwords in one. It's amazing.
[00:02:12.040 --> 00:02:17.040]   Now we sent Jeff to Mountain View because he's at Google I/O.
[00:02:17.040 --> 00:02:20.040]   Jeff Jarvis from Buzz machine. Hi Jeff.
[00:02:20.040 --> 00:02:25.040]   I am at the press tent, which is noisy and I'll mute and not know what I'm talking.
[00:02:25.040 --> 00:02:30.040]   But here we are. I'm a journalist. Hello. Hello. Say, Google, Pandora people.
[00:02:30.040 --> 00:02:31.040]   Hello.
[00:02:31.040 --> 00:02:34.040]   Look at that. Good looking bunch. That's nice.
[00:02:34.040 --> 00:02:37.040]   I love it. The Google does their events. And I bet Stacey you're a little jealous.
[00:02:37.040 --> 00:02:41.040]   They do their events. Sure line. It's outside. The weather's beautiful.
[00:02:41.040 --> 00:02:46.040]   In fact, I don't know if you knew Jeff, but all through the keynote, we heard Bird Song.
[00:02:46.040 --> 00:02:50.040]   There were their birds flying around, you know, Sundar and all over.
[00:02:50.040 --> 00:02:53.040]   I wonder if it was going to land on the stage or poop on the stage.
[00:02:53.040 --> 00:02:54.040]   But no, they don't.
[00:02:54.040 --> 00:02:59.040]   Also joining us, we've assigned him to his own home base, Kevin Tofol.
[00:02:59.040 --> 00:03:04.040]   By the way, Kevin, of course, is Stacey's partner on the IOT podcast.
[00:03:04.040 --> 00:03:09.040]   But he has a new enterprise he started last month about Chromebooks.com.
[00:03:09.040 --> 00:03:10.040]   Hi, Kevin.
[00:03:10.040 --> 00:03:14.040]   Howdy. Howdy. Thanks for sending me to my home office. I appreciate that.
[00:03:14.040 --> 00:03:16.040]   We're saving on the expenses today.
[00:03:16.040 --> 00:03:23.040]   I haven't said anybody anywhere, but I'm pretending that we assigned people to these events.
[00:03:23.040 --> 00:03:28.040]   So, Kevin, it's great to have you. It's great to have all three of you here to talk about.
[00:03:28.040 --> 00:03:34.040]   And I think this show, if you wish, talk about other things, and I'm sure there's lots to talk about.
[00:03:34.040 --> 00:03:39.040]   In fact, I've been following your writing and posts about Christina,
[00:03:39.040 --> 00:03:42.040]   which unfortunately Google did not announce at all.
[00:03:42.040 --> 00:03:47.040]   They didn't say anything about Chrome OS, and it's my guess that they may be doing that at another keynote.
[00:03:47.040 --> 00:03:50.040]   There's one starting and a little bit for developers, right?
[00:03:50.040 --> 00:03:57.040]   There is. The developer keynote starts in 16 minutes, so I'm going to kind of cheat and watch it muted next to me
[00:03:57.040 --> 00:04:00.040]   because I want to see all the Chromebook stuff, maybe some Android things,
[00:04:00.040 --> 00:04:05.040]   for me and Stacey on the IOT podcast and whatever else they want to talk about.
[00:04:05.040 --> 00:04:10.040]   If you have something breaks, you can just jump in and say,
[00:04:10.040 --> 00:04:14.040]   "Hey, they just announced something." But briefly, because I know Jeff will be interested in this.
[00:04:14.040 --> 00:04:20.040]   He's a Chromebook user. I am too. I have the new Pixelbook. What is Christina?
[00:04:20.040 --> 00:04:27.040]   Christina is a project that Google has been working on, and we should see it on at least the Pixelbook
[00:04:27.040 --> 00:04:32.040]   with other devices to follow. In the next, I'll say three to six weeks.
[00:04:32.040 --> 00:04:36.040]   If you're on the developer channel, you can enable Project Christina.
[00:04:36.040 --> 00:04:41.040]   What it is is it actually runs a container, a separate sandbox container on your Chromebook,
[00:04:41.040 --> 00:04:47.040]   and it runs a full version of Linux. What that does is it helps bridge the remaining app gap,
[00:04:47.040 --> 00:04:52.040]   because we have Android app support now. But if you want to develop apps or do something else
[00:04:52.040 --> 00:04:58.040]   where an Android app isn't going to cut it, you can run full Linux apps on your Chromebook.
[00:04:58.040 --> 00:05:03.040]   There's no security issue involved there because it's not. You're not in developer mode.
[00:05:03.040 --> 00:05:09.040]   You're actually running regular Chrome OS. Google's worked really hard to make it seamless, safe.
[00:05:09.040 --> 00:05:15.040]   I've already installed Android Studios to developers if you use an IDE to create anything,
[00:05:15.040 --> 00:05:21.040]   any kind of apps. You can do that, or we'll be able to do that soon on the Pixelbook with other Chromebooks
[00:05:21.040 --> 00:05:23.040]   to follow as they continue to evolve that.
[00:05:23.040 --> 00:05:29.040]   I love this idea because... So can you choose your Linux as you can?
[00:05:29.040 --> 00:05:33.040]   By the way, apparently Google developers have a taste for toasted bread products because
[00:05:33.040 --> 00:05:42.040]   crouton, which precedes crustini, allowed you to using chorout and defeating the secure boot,
[00:05:42.040 --> 00:05:49.040]   install Linux, a real Linux, any version of Linux on your Chromebook. Is this any version of Linux,
[00:05:49.040 --> 00:05:52.040]   or is it kind of a special version? How does it work?
[00:05:52.040 --> 00:05:55.040]   It comes with Debian Linux for now.
[00:05:55.040 --> 00:05:59.040]   Very nice. They may change that, but for now it's Debian.
[00:05:59.040 --> 00:06:02.040]   I'm happy with that. That's a great choice.
[00:06:02.040 --> 00:06:06.040]   So can you do anything? You can do a Linux in that sandbox.
[00:06:06.040 --> 00:06:12.040]   Correct. You can. You get a Linux terminal that works just the same as regular Linux.
[00:06:12.040 --> 00:06:17.040]   You can install whatever packages you'd like from wherever you want that are supported.
[00:06:17.040 --> 00:06:24.040]   They don't have graphics or GPU acceleration yet, so don't get excited about if you play games on Steam
[00:06:24.040 --> 00:06:29.040]   that does work. You can install it, but the framerates are terrible. They're working on that.
[00:06:29.040 --> 00:06:33.040]   Again, this is the kind of stuff I've been tracking over at about Chromebooks now for the past month.
[00:06:33.040 --> 00:06:34.040]   I'm really excited about it.
[00:06:34.040 --> 00:06:38.040]   Can you use a GUI, or is it so slow that it's not even...
[00:06:38.040 --> 00:06:41.040]   No, I don't see any way to enable a GUI yet.
[00:06:41.040 --> 00:06:46.040]   To be honest, I don't think that would be a good idea, because you really want two user interfaces.
[00:06:46.040 --> 00:06:52.040]   You already have a GUI, so just make it... So the apps, the Linux apps are really a window on top of Chrome OS.
[00:06:52.040 --> 00:06:55.040]   That's correct. It's a sizable window.
[00:06:55.040 --> 00:06:57.040]   Am I muted now or not?
[00:06:57.040 --> 00:06:58.040]   You're not.
[00:06:58.040 --> 00:07:00.040]   Sorry, I can't see.
[00:07:00.040 --> 00:07:06.040]   So, Kevin, does it affect the overall operation of the Chromebook pixel?
[00:07:06.040 --> 00:07:14.040]   Is it taken up too much memory? Too many tabs open? Do you notice any other impact of running that?
[00:07:14.040 --> 00:07:21.040]   So that's a great question, Jeff. I haven't really seen any impact, and I've been monitoring some of the hardware resources doing this.
[00:07:21.040 --> 00:07:27.040]   But I don't want to say too much, because I suspect Google has not yet optimized for performance yet,
[00:07:27.040 --> 00:07:31.040]   especially because it's only in the dev channel.
[00:07:31.040 --> 00:07:37.040]   I would say, though, you're not going to want to spend $250 on a Chromebook and expect to do this and have it done well.
[00:07:37.040 --> 00:07:42.040]   You're going to want gobs of memory and storage and so on, and a good CPU to be honest.
[00:07:42.040 --> 00:07:45.040]   Now, for the first time, I'm sorry I didn't get the high-end pixel book.
[00:07:45.040 --> 00:07:50.040]   I got the entry-level pixel book, and I wish I had the faster processor in more RAM.
[00:07:50.040 --> 00:07:54.040]   I have the idea to give out something to us today, and give out something to us today.
[00:07:54.040 --> 00:07:57.040]   We don't know what maybe it's a pixel book. I doubt it.
[00:07:57.040 --> 00:07:59.040]   That would not be nice, huh?
[00:07:59.040 --> 00:08:06.040]   We'll talk more about that. Kevin, if they say anything during the developer keynote that you want to jump in with, please don't hesitate.
[00:08:06.040 --> 00:08:15.040]   But obviously, the big story this week on this week in Google is Google I/O, and the keynote, which just wrapped up minutes ago,
[00:08:15.040 --> 00:08:19.040]   and started off with Sundar Pachai making a joke about it.
[00:08:19.040 --> 00:08:21.040]   It was pretty funny, actually.
[00:08:21.040 --> 00:08:31.040]   The big emoji snafu at Google, he did say, "We got the cheeseburger wrong, but I got to say I'm a vegetarian, so I didn't know."
[00:08:31.040 --> 00:08:40.040]   They fixed it, put the cheese on top, but then he said, "I don't understand why the beer, the foam was floating on the beer in our emoji, but we have fixed that as well."
[00:08:40.040 --> 00:08:44.040]   But he did it with good humor, and I thought that was a great way to kick the keynote off.
[00:08:44.040 --> 00:08:55.040]   I would have rather that he kicked it off such a kicked off build yesterday with a nice little thing about ethics and privacy, and actually pro-GDPR kind of stuff.
[00:08:55.040 --> 00:08:57.040]   But that's just me.
[00:08:57.040 --> 00:09:00.040]   It's sad when Sundar was going to.
[00:09:00.040 --> 00:09:10.040]   Yeah, you're right. You think of Facebook had to do the confession of sins and penance, and Microsoft did that,
[00:09:10.040 --> 00:09:14.040]   and they never really, one of the Europeans here said they never mentioned privacy.
[00:09:14.040 --> 00:09:21.040]   And with all these new features and all these new ways, things are going to listen to you all the time, it is an issue, and it would have been nice if they mentioned it more.
[00:09:21.040 --> 00:09:34.040]   It actually almost felt like with some of the features they talked about that Google was doing the opposite, was saying, "Look, we know a lot about you, and we're going to make sure that we take advantage of that to surface useful tools for you."
[00:09:34.040 --> 00:09:36.040]   And that is the trade-off.
[00:09:36.040 --> 00:09:37.040]   Yeah.
[00:09:37.040 --> 00:09:38.040]   Yeah, that is.
[00:09:38.040 --> 00:09:44.040]   In fairness, they didn't mention privacy specifically, but if you heard the right buzzwords, they sort of did.
[00:09:44.040 --> 00:09:54.040]   Because when they talked about a lot of the features we're going to discuss with machine learning, they said, "This is either on-device or this is on-device and cloud with TPU."
[00:09:54.040 --> 00:09:59.040]   So when they say on-device, that lends itself to some aspect of privacy, I think.
[00:09:59.040 --> 00:10:06.040]   Yeah, I'm sure that's why they're doing it, although they didn't really even acknowledge that.
[00:10:06.040 --> 00:10:13.040]   It was more about performance, and they were actually very interested, I think, to talk about the hybrid.
[00:10:13.040 --> 00:10:28.040]   They're even doing it on the self-driving vehicles, the hybrid of having some intelligence on the device or on the car and still using the cloud and TPU to add more smarts, which I think is not a bad way to go.
[00:10:28.040 --> 00:10:31.040]   It's a little scary in a car if you lose your connection.
[00:10:31.040 --> 00:10:34.040]   I hope the car doesn't drive into a wall.
[00:10:34.040 --> 00:10:42.040]   Well, yeah, you have to plan. That's why they have to add that intelligence at the edge, because connections and cars are variable.
[00:10:42.040 --> 00:10:50.040]   Yeah. All right. Well, let's see. I guess we could just go in order. You want to go in chronological order? Might as well.
[00:10:50.040 --> 00:11:03.040]   It's funny because Microsoft yesterday talked about diabetic retinopathy, and you would think this is like the illness of the moment because SOTage is so dark but short.
[00:11:03.040 --> 00:11:11.040]   I just want to say, I'm just so dark, but shy. Predicting cardiovascular risk using your retina, and really what it's about is machine learning.
[00:11:11.040 --> 00:11:17.040]   In fact, it can go beyond that. It can actually, using your retina, do a lot more.
[00:11:17.040 --> 00:11:23.040]   I wonder if this ties into the lively stuff they're doing with those smart contact lenses. I don't know.
[00:11:23.040 --> 00:11:27.040]   I assume it's because they have an amazing database of training data.
[00:11:27.040 --> 00:11:28.040]   Right.
[00:11:28.040 --> 00:11:33.040]   I mean, if you've been doing contact lens studies, for example, I bet you have a lot of pictures of retinas.
[00:11:33.040 --> 00:11:41.040]   He showed the demo we've showed actually on the show of multiple people talking at once, and the AI's ability to distinguish the different voices.
[00:11:41.040 --> 00:11:46.040]   Funny enough, Sotage and Arpachai yesterday.
[00:11:46.040 --> 00:11:52.040]   This is a problem because the overlapping keynotes don't give them a chance to change their content.
[00:11:52.040 --> 00:11:54.040]   Wait, you keep saying Sondar Pachai, but you're not.
[00:11:54.040 --> 00:12:00.040]   Sotage and Adela, yes, pardon me. SN, not SP. Sorry. Yes, I apologize.
[00:12:00.040 --> 00:12:05.040]   But there wasn't, I mean, did you notice that, Stacy, having been at the build of keynote yesterday?
[00:12:05.040 --> 00:12:11.040]   There were obviously big overlaps, machine learning at the edge, bringing AI everywhere.
[00:12:11.040 --> 00:12:21.040]   And the reason is because all of these companies have to find a way to adapt their business model to a world that doesn't have screens, where you're going to have to basically,
[00:12:21.040 --> 00:12:32.040]   you're going to need an operating system that runs in the cloud, on device, and mediate somewhere in a middle point, right, a third place.
[00:12:32.040 --> 00:12:35.040]   So what is that going to look like?
[00:12:35.040 --> 00:12:38.040]   I bet it's Google Kubernetes is kind of the sense here.
[00:12:38.040 --> 00:12:44.040]   But how do you, how do you adapt your business model to handle this?
[00:12:44.040 --> 00:12:48.040]   Microsoft is betting on services that it pulls down from these.
[00:12:48.040 --> 00:12:55.040]   Google is betting on Kubernetes and it's machine learning to just make better end consumer products.
[00:12:55.040 --> 00:12:58.040]   So, you know, we'll see.
[00:12:58.040 --> 00:13:08.040]   Yeah. They showed, I thought, a very heartwarming segment about a young woman who is a developer, but also, it looked like she had cerebral palsy.
[00:13:08.040 --> 00:13:15.040]   She had some sort of movement issues using Gboard on an Android phone and Morse code.
[00:13:15.040 --> 00:13:24.040]   They've added Morse code to Google's keyboard for Android Gboard, so she could use her head stick to communicate.
[00:13:24.040 --> 00:13:25.040]   That was really amazing.
[00:13:25.040 --> 00:13:31.040]   It was really touching, but it was the point, Leo, that it could predict the words better.
[00:13:31.040 --> 00:13:43.040]   I was the machine learning part of the bus because the Morse code, she could, you know, do every word if you wanted to, but it seemed to be that it was predictive of the words, which we'll get to in a minute, but I don't know if they're getting better at that.
[00:13:43.040 --> 00:13:47.040]   Is that how it worked? I couldn't tell what the ML part was.
[00:13:47.040 --> 00:13:51.040]   Well, that's an interesting point. Yeah, must have been that there was some prediction.
[00:13:51.040 --> 00:13:54.040]   Yeah, I think it was the auto complete.
[00:13:54.040 --> 00:14:07.040]   The fact that they added Morse code understanding with one piece of it to Gboard, but then the auto complete feature, which is ML based, and then that led into the Gmail, the new Gmail composition where it completes the sentences for you.
[00:14:07.040 --> 00:14:18.040]   Yeah, so he's been sending a lot of emails to Google employees, much to their dismay about Taco Tuesday and stuff.
[00:14:18.040 --> 00:14:20.040]   That was a cool demo. You hit tab.
[00:14:20.040 --> 00:14:25.040]   Here's the animated GIF from an gadgets website.
[00:14:25.040 --> 00:14:31.040]   You hit tab and it will kind of guess what the, not just the next word, but the rest of the sentence would say.
[00:14:31.040 --> 00:14:38.040]   So you type let's and it would say, do you mean get together soon? And you hit tab and it would type that in.
[00:14:38.040 --> 00:14:46.040]   I don't have it yet in my Gmail, so I can't vouch for how accurate it is, but it was as with all these demos. Very impressive.
[00:14:46.040 --> 00:14:50.040]   Is it learning? Would it learn custom to you as well?
[00:14:50.040 --> 00:15:01.040]   It seemed to. I'll tell you why I think so, because he typed Taco Tuesdays in the subject and it did suggest after let's get together, it suggested for tacos.
[00:15:01.040 --> 00:15:06.040]   So it's clearly looking at what's going on and adding.
[00:15:06.040 --> 00:15:12.040]   And Stacy, to answer the question of personalization, you can start to type your home address.
[00:15:12.040 --> 00:15:13.040]   That's right.
[00:15:13.040 --> 00:15:15.040]   You know, it has that information.
[00:15:15.040 --> 00:15:19.040]   It literally starts to auto fill that for you because it knows it's you.
[00:15:19.040 --> 00:15:23.040]   Yeah, but I'm like, Casey's also asking about the kind of grammar you use or other things.
[00:15:23.040 --> 00:15:26.040]   I always get free to the auto auto answer that it uses.
[00:15:26.040 --> 00:15:29.040]   Somebody has to mention points and I think I don't use any estimation points.
[00:15:29.040 --> 00:15:31.040]   I don't see it's perfect for me, Jeff.
[00:15:31.040 --> 00:15:32.040]   Yeah.
[00:15:32.040 --> 00:15:34.040]   Stacy and I are big on ex-boy.
[00:15:34.040 --> 00:15:44.040]   I find myself looking back at my email saying I have, I have an ex-boy and every sentence I really should pick and choose here.
[00:15:44.040 --> 00:15:52.040]   I talked to somebody the other day who said she dated a Yale guy and he said, you should use only three exclamation points in life and I've used one.
[00:15:52.040 --> 00:15:54.040]   I agree.
[00:15:54.040 --> 00:16:00.040]   However, nowadays in modern communication, you don't have as many ways to show nuance.
[00:16:00.040 --> 00:16:06.040]   An exclamation point is one way to say go up at the end of the sentence, I guess.
[00:16:06.040 --> 00:16:13.040]   So I happen to be sitting next to a brilliant journalist from a Swiss newspaper during the entire keynote.
[00:16:13.040 --> 00:16:20.040]   And it was really interesting to see the keynote from a European's eyes because you see things like that on a complete.
[00:16:20.040 --> 00:16:22.040]   And you and I say, oh, that's cool.
[00:16:22.040 --> 00:16:23.040]   It's in my brain.
[00:16:23.040 --> 00:16:24.040]   It's what I'm going to say.
[00:16:24.040 --> 00:16:26.040]   And the Europeans are saying, oh.
[00:16:26.040 --> 00:16:27.040]   Yeah.
[00:16:27.040 --> 00:16:28.040]   It's in my brain.
[00:16:28.040 --> 00:16:30.040]   It knows what I'm going to say.
[00:16:30.040 --> 00:16:32.040]   Yeah.
[00:16:32.040 --> 00:16:34.040]   So does anybody here have it?
[00:16:34.040 --> 00:16:35.040]   I have the new Gmail.
[00:16:35.040 --> 00:16:39.040]   I think everybody does, or can if they go to the settings.
[00:16:39.040 --> 00:16:43.040]   But I don't want Casey and I got a lot of Twitter instructions.
[00:16:43.040 --> 00:16:45.040]   Many people about how to set it up.
[00:16:45.040 --> 00:16:46.040]   Is he sweet?
[00:16:46.040 --> 00:16:47.040]   Yes, we do finally.
[00:16:47.040 --> 00:16:48.040]   Yeah.
[00:16:48.040 --> 00:16:50.040]   But you don't have the auto complete yet, right?
[00:16:50.040 --> 00:16:51.040]   No, no, no, no.
[00:16:51.040 --> 00:16:59.040]   Almost everything they talked about is rolling out in the next few weeks.
[00:16:59.040 --> 00:17:03.040]   Except for one thing that I know Jeff's going to want to talk a lot about.
[00:17:03.040 --> 00:17:09.200]   We're going to hold off on Google News because I want to do this in chronological order as
[00:17:09.200 --> 00:17:11.040]   if you're watching the keynote live.
[00:17:11.040 --> 00:17:16.040]   And now you're going to hear in the background, they have sound up for the other keynote.
[00:17:16.040 --> 00:17:17.040]   So good.
[00:17:17.040 --> 00:17:18.040]   Great.
[00:17:18.040 --> 00:17:21.040]   Well, we should mention that Jeff is in the press tent.
[00:17:21.040 --> 00:17:25.040]   That's where he's able to get the best bandwidth.
[00:17:25.040 --> 00:17:26.040]   And it's fine.
[00:17:26.040 --> 00:17:27.040]   I don't mind.
[00:17:27.040 --> 00:17:29.040]   He's just muting himself when he doesn't have anything to say.
[00:17:29.040 --> 00:17:30.040]   So if it gets noisy.
[00:17:30.040 --> 00:17:34.040]   Many, many people wish I would do this more often.
[00:17:34.040 --> 00:17:38.040]   We do get a nice, it's a nice indicator that you're about ready to talk.
[00:17:38.040 --> 00:17:40.040]   So it helps with latency issues.
[00:17:40.040 --> 00:17:41.040]   It does.
[00:17:41.040 --> 00:17:42.040]   I like that.
[00:17:42.040 --> 00:17:46.040]   Stacey on the other hand is in some sort of weird Microsoft soundproof boost.
[00:17:46.040 --> 00:17:47.040]   It does not soundproof.
[00:17:47.040 --> 00:17:51.040]   I'm hoping that you guys like, they didn't have any real setups for this.
[00:17:51.040 --> 00:17:55.040]   So there's like every now and then you hear the hand dryer from the bathroom go off because
[00:17:55.040 --> 00:17:57.040]   it's one of those big dice and it's like.
[00:17:57.040 --> 00:18:03.040]   You know what I like and I want to make this a trend completely parenthetically.
[00:18:03.040 --> 00:18:06.040]   I was in as you know in Japan on vacation last couple of weeks.
[00:18:06.040 --> 00:18:11.040]   By the way, thank you Jason Hall for filling in for me while I was gone.
[00:18:11.040 --> 00:18:16.040]   They don't have paper towels in the men's room or I presume the women's room all night in check.
[00:18:16.040 --> 00:18:19.040]   They do have sometimes blowers, but most of the time nothing.
[00:18:19.040 --> 00:18:20.040]   Yes.
[00:18:20.040 --> 00:18:26.040]   And the expectation is well, you have your little handkerchief, your pocket towel.
[00:18:26.040 --> 00:18:28.040]   It's like the hitchhiker's guy to the galaxy.
[00:18:28.040 --> 00:18:30.040]   You've got to bring your own tell.
[00:18:30.040 --> 00:18:31.040]   Yeah.
[00:18:31.040 --> 00:18:34.040]   So I just ordered 14 handkerchiefs online because that's brilliant.
[00:18:34.040 --> 00:18:36.040]   I'm not going to use paper towels anymore at all.
[00:18:36.040 --> 00:18:39.040]   I think this is what this could change the world.
[00:18:39.040 --> 00:18:40.040]   Save the world.
[00:18:40.040 --> 00:18:45.040]   Do you believe that the hand dryers in fact blow fecal matter on your hands?
[00:18:45.040 --> 00:18:47.040]   I don't think it's a belief.
[00:18:47.040 --> 00:18:48.040]   I think it's a fact.
[00:18:48.040 --> 00:18:52.040]   You only dry your hands though after you've washed them.
[00:18:52.040 --> 00:18:55.040]   So why is there still fecal matter on the fecal matter is in the bathroom.
[00:18:55.040 --> 00:19:01.040]   And the dryer is sucking in air and blowing it on your hands.
[00:19:01.040 --> 00:19:02.040]   Oh my God.
[00:19:02.040 --> 00:19:04.040]   You guys just go live in your bubble.
[00:19:04.040 --> 00:19:05.040]   It's going to be fine.
[00:19:05.040 --> 00:19:09.040]   This is why this is why I don't use the bathroom outside of the house.
[00:19:09.040 --> 00:19:14.040]   It's erotic weekly.
[00:19:14.040 --> 00:19:15.040]   I don't.
[00:19:15.040 --> 00:19:16.040]   There was a study.
[00:19:16.040 --> 00:19:19.040]   There've been several studies that say it's actually doing that.
[00:19:19.040 --> 00:19:23.040]   I don't know if those Dyson blade dryers do that.
[00:19:23.040 --> 00:19:28.040]   But certainly the old fashioned hand dryers which don't work anyway.
[00:19:28.040 --> 00:19:30.040]   That's what those tests were on.
[00:19:30.040 --> 00:19:32.040]   But again, your pocket hanky.
[00:19:32.040 --> 00:19:33.040]   Bring it with you.
[00:19:33.040 --> 00:19:35.040]   I think this is brilliant.
[00:19:35.040 --> 00:19:36.040]   I was really.
[00:19:36.040 --> 00:19:41.040]   I watched a man yesterday who sat next to me blow his nose on his pocket hanky a couple times.
[00:19:41.040 --> 00:19:43.040]   And then pull off his glasses.
[00:19:43.040 --> 00:19:45.040]   Stare speculatively at the minimum.
[00:19:45.040 --> 00:19:46.040]   Me too.
[00:19:46.040 --> 00:19:48.040]   And then I'm like he's not going to do it.
[00:19:48.040 --> 00:19:49.040]   He's not doing it.
[00:19:49.040 --> 00:19:50.040]   And then he did it.
[00:19:50.040 --> 00:19:52.040]   He wiped his glasses.
[00:19:52.040 --> 00:19:55.040]   Well, there is the clean corner versus the dirty corners.
[00:19:55.040 --> 00:19:56.040]   I was done.
[00:19:56.040 --> 00:19:57.040]   No difference.
[00:19:57.040 --> 00:19:59.040]   One for each pocket.
[00:19:59.040 --> 00:20:00.040]   Clean and dirty.
[00:20:00.040 --> 00:20:03.040]   You knew he was going to do that and you didn't get video.
[00:20:03.040 --> 00:20:04.040]   I did not.
[00:20:04.040 --> 00:20:06.040]   I didn't want to keep that.
[00:20:06.040 --> 00:20:07.040]   But yeah.
[00:20:07.040 --> 00:20:11.040]   Google Photos has some new will have in the next few weeks.
[00:20:11.040 --> 00:20:12.040]   In the coming weeks.
[00:20:12.040 --> 00:20:17.040]   Some new features including what I was really impressed with a colorized feature.
[00:20:17.040 --> 00:20:25.040]   Where you could take old family photos and turn them into color photos as well as a feature
[00:20:25.040 --> 00:20:28.040]   that will turn photo black and white except for a certain region.
[00:20:28.040 --> 00:20:30.040]   There's third party tools that will do all these things.
[00:20:30.040 --> 00:20:33.040]   But this is built into photos which is nice.
[00:20:33.040 --> 00:20:40.040]   There's suggested actions which will kind of say hey we could make this better this way.
[00:20:40.040 --> 00:20:45.040]   They pitched the sharing feature which they pitched last year again.
[00:20:45.040 --> 00:20:46.040]   I don't know.
[00:20:46.040 --> 00:20:47.040]   Maybe they, I don't know.
[00:20:47.040 --> 00:20:49.040]   Maybe it works better now.
[00:20:49.040 --> 00:20:50.040]   I don't know.
[00:20:50.040 --> 00:20:53.440]   If the picture is too dark, AI will recommend brightening it.
[00:20:53.440 --> 00:20:56.040]   If it spots a document, I love this.
[00:20:56.040 --> 00:21:00.040]   It will convert it to PDF.
[00:21:00.040 --> 00:21:02.040]   And that is really cool.
[00:21:02.040 --> 00:21:05.040]   And lens, I'm jumping a little bit ahead because lens was a little later.
[00:21:05.040 --> 00:21:12.440]   But lens will also allow you, if you take a picture of text, to copy the text and paste
[00:21:12.440 --> 00:21:13.840]   it.
[00:21:13.840 --> 00:21:16.720]   Even though it's a picture of text.
[00:21:16.720 --> 00:21:21.400]   So I had a question about this because didn't Google last year say they were going to like
[00:21:21.400 --> 00:21:27.040]   remove weird stuff from your photos like fences and trees growing out of people's heads or
[00:21:27.040 --> 00:21:28.760]   something like that.
[00:21:28.760 --> 00:21:30.400]   They had it.
[00:21:30.400 --> 00:21:31.400]   Maybe.
[00:21:31.400 --> 00:21:36.880]   You know, one of the things they did say and they have done is if it sees receipts and
[00:21:36.880 --> 00:21:40.400]   things that you aren't really photos or you're just keeping track of it a little offer to
[00:21:40.400 --> 00:21:41.400]   archive those.
[00:21:41.400 --> 00:21:42.400]   And it does do that.
[00:21:42.400 --> 00:21:43.400]   Yes, that does happen.
[00:21:43.400 --> 00:21:46.480]   But it never offered because I was really excited about taking.
[00:21:46.480 --> 00:21:48.160]   Oh, I know what you're talking about.
[00:21:48.160 --> 00:21:49.160]   Power lines and stuff.
[00:21:49.160 --> 00:21:50.160]   Yes, they had a picture.
[00:21:50.160 --> 00:21:54.200]   They showed a kid playing baseball in Little League and there was a fence over it.
[00:21:54.200 --> 00:21:55.200]   Wait a minute.
[00:21:55.200 --> 00:21:56.200]   What's happening here?
[00:21:56.200 --> 00:21:57.200]   Something strange has happened.
[00:21:57.200 --> 00:21:59.940]   Jeff's hair seems to have fallen out.
[00:21:59.940 --> 00:22:01.640]   Oh no, it's.
[00:22:01.640 --> 00:22:03.160]   Hey, Maryam.
[00:22:03.160 --> 00:22:05.280]   Hey, Maryam.
[00:22:05.280 --> 00:22:07.520]   Hey, wait a minute.
[00:22:07.520 --> 00:22:08.860]   She's muted.
[00:22:08.860 --> 00:22:13.860]   Jeff has to push the button.
[00:22:13.860 --> 00:22:14.860]   There we go.
[00:22:14.860 --> 00:22:15.860]   There you go.
[00:22:15.860 --> 00:22:16.860]   Oh, hello, everybody.
[00:22:16.860 --> 00:22:17.860]   Welcome to Google I/O.
[00:22:17.860 --> 00:22:18.860]   Nice to see you.
[00:22:18.860 --> 00:22:21.980]   I'm just saying, sit down, put this on.
[00:22:21.980 --> 00:22:22.980]   I'm like, OK.
[00:22:22.980 --> 00:22:24.620]   Are you having fun?
[00:22:24.620 --> 00:22:25.620]   Of course I am.
[00:22:25.620 --> 00:22:26.620]   It's pretty awesome.
[00:22:26.620 --> 00:22:29.500]   You know, it's kind of like the big playground for developers.
[00:22:29.500 --> 00:22:34.220]   This is definitely the most playground feeling conference if you're a developer ever.
[00:22:34.220 --> 00:22:37.380]   So Google knows how to do this, right?
[00:22:37.380 --> 00:22:40.820]   Since we've got you, what was your favorite thing they talked about?
[00:22:40.820 --> 00:22:45.220]   I think honestly so far, you know, I find the news are a little mad, but I do feel
[00:22:45.220 --> 00:22:49.100]   like the Android P features they went through are pretty exciting.
[00:22:49.100 --> 00:22:51.900]   And some of the lens stuff like being able to select text.
[00:22:51.900 --> 00:22:52.900]   Yeah.
[00:22:52.900 --> 00:22:53.900]   Right.
[00:22:53.900 --> 00:22:54.900]   Wasn't that amazing?
[00:22:54.900 --> 00:22:55.900]   Yeah.
[00:22:55.900 --> 00:22:58.460]   And then you know, Google Maps, like AR and Google Maps, this visual positioning system
[00:22:58.460 --> 00:23:01.340]   they talked about, I think is pretty awesome.
[00:23:01.340 --> 00:23:03.340]   There's definitely some good stuff here.
[00:23:03.340 --> 00:23:07.100]   I kind of felt the way most stuff is a little bit overconfident, you know.
[00:23:07.100 --> 00:23:08.860]   But the most autonomous car stuff is.
[00:23:08.860 --> 00:23:12.580]   I mean, you know, I'm a big supporter of it, but at the same time, you know, as a journalist
[00:23:12.580 --> 00:23:15.140]   after kind of due diligence, right?
[00:23:15.140 --> 00:23:17.420]   But yeah, I think it's been so far.
[00:23:17.420 --> 00:23:18.420]   Good IO.
[00:23:18.420 --> 00:23:19.900]   There is a developer keynote going on.
[00:23:19.900 --> 00:23:21.940]   I'm checking that out right now.
[00:23:21.940 --> 00:23:24.260]   So yeah, Jeff put me on the spot, but here I am.
[00:23:24.260 --> 00:23:27.220]   So I'm going to get back to him and you guys can hear me.
[00:23:27.220 --> 00:23:28.220]   Nice to see you.
[00:23:28.220 --> 00:23:31.460]   Tank girl, Miriam Jwar, host of the Mobile Tech podcast.
[00:23:31.460 --> 00:23:33.500]   Great to have you.
[00:23:33.500 --> 00:23:34.500]   Have fun at IO.
[00:23:34.500 --> 00:23:36.060]   Bye bye.
[00:23:36.060 --> 00:23:41.140]   So that takes us a little bit out of chronology, but I'm going to be anal and put us back in
[00:23:41.140 --> 00:23:45.860]   the chronology.
[00:23:45.860 --> 00:23:49.220]   So those are Google photos features.
[00:23:49.220 --> 00:23:57.900]   Then let's see, Taco Tuesday, Google Assistant, actually there was some really, a lot of sizzle
[00:23:57.900 --> 00:23:59.180]   in this keynote.
[00:23:59.180 --> 00:24:04.100]   And as always, some of this sizzle will not emerge or will emerge much later.
[00:24:04.100 --> 00:24:08.500]   But there was some interesting stuff with Assistant that I look forward to seeing six
[00:24:08.500 --> 00:24:14.900]   new voices, and they said that'll come soon, including a demo of John Legend recording some
[00:24:14.900 --> 00:24:16.380]   stuff.
[00:24:16.380 --> 00:24:17.620]   So here's my question.
[00:24:17.620 --> 00:24:20.540]   What voices do you want them to do?
[00:24:20.540 --> 00:24:21.540]   Sean Connery.
[00:24:21.540 --> 00:24:23.340]   Oh, oh, sure.
[00:24:23.340 --> 00:24:24.860]   I don't know.
[00:24:24.860 --> 00:24:29.140]   I want to say the best of the better Gilbert Godfrey.
[00:24:29.140 --> 00:24:30.940]   No, no, no.
[00:24:30.940 --> 00:24:32.420]   I want Scarlett Johansson.
[00:24:32.420 --> 00:24:34.500]   I want Scarlett Johansson.
[00:24:34.500 --> 00:24:37.900]   Scarlett Johansson was another good suggestion.
[00:24:37.900 --> 00:24:42.220]   So let me see if I can find the Google Assistant commercial.
[00:24:42.220 --> 00:24:49.500]   Apparently John Legend and Kevin Durant are getting big money from Google because they
[00:24:49.500 --> 00:24:51.740]   were all over.
[00:24:51.740 --> 00:24:52.740]   Let me see.
[00:24:52.740 --> 00:24:55.500]   This is make Google do it.
[00:24:55.500 --> 00:24:57.420]   Is this going to show off the voice?
[00:24:57.420 --> 00:24:58.420]   I don't think it is.
[00:24:58.420 --> 00:25:02.460]   It's just John and Chrissy just playing with stuff.
[00:25:02.460 --> 00:25:05.020]   Yeah, that's your hand.
[00:25:05.020 --> 00:25:06.020]   Yeah.
[00:25:06.020 --> 00:25:08.740]   So they had John Legend in the studio.
[00:25:08.740 --> 00:25:12.140]   He sang Happy Birthday, but he also did some voice stuff.
[00:25:12.140 --> 00:25:18.620]   And then Sundar Pachai actually showed that he could have John Legend's voice telling him
[00:25:18.620 --> 00:25:20.460]   he had an appointment.
[00:25:20.460 --> 00:25:25.660]   So somehow they're taking the voice and modifying the Assistant's voice.
[00:25:25.660 --> 00:25:27.900]   So it sounds like John Legend.
[00:25:27.900 --> 00:25:28.900]   They showed that off.
[00:25:28.900 --> 00:25:29.900]   Remember how they could generate auto-generate?
[00:25:29.900 --> 00:25:30.900]   Right.
[00:25:30.900 --> 00:25:33.980]   We talked about this a couple of shows ago at the auto generation of different voices.
[00:25:33.980 --> 00:25:36.300]   So that's really crazy.
[00:25:36.300 --> 00:25:42.220]   And I think we even brought this up, how this could be crazy for fake news and creating,
[00:25:42.220 --> 00:25:46.140]   you know, you know how people edit right now things like all the speeches of Obama singing
[00:25:46.140 --> 00:25:49.060]   songs and they just edit all his voices together.
[00:25:49.060 --> 00:25:53.780]   Something like this, you just grab somebody who has a lot of audio footage and you can
[00:25:53.780 --> 00:25:57.820]   just, boom, make a Stacy sounding robot if you wanted to.
[00:25:57.820 --> 00:25:59.900]   Yeah, those days are coming apparently.
[00:25:59.900 --> 00:26:00.900]   Yeah.
[00:26:00.900 --> 00:26:06.700]   One thing that struck me about the six voices is they all sounded American and even white.
[00:26:06.700 --> 00:26:09.660]   There was no sense of a variety of voices.
[00:26:09.660 --> 00:26:14.540]   Well I think that's why they showed John Legend because while he doesn't sound, you know,
[00:26:14.540 --> 00:26:19.180]   really urban, he does, I mean, he doesn't sound like a professional voice.
[00:26:19.180 --> 00:26:21.580]   Actually, I think I have the video if you want to.
[00:26:21.580 --> 00:26:24.340]   Let's see if we can show it here.
[00:26:24.340 --> 00:26:26.380]   Am I getting audio from Sundar?
[00:26:26.380 --> 00:26:27.380]   The Google Assistant.
[00:26:27.380 --> 00:26:28.380]   Yeah.
[00:26:28.380 --> 00:26:33.020]   Our vision for the perfect assistant is that it's naturally conversational.
[00:26:33.020 --> 00:26:38.420]   It's there when you need it so that you can get things done in the real world.
[00:26:38.420 --> 00:26:41.220]   And we are working to make it even better.
[00:26:41.220 --> 00:26:46.060]   We want the assistant to be something that's natural and comfortable to talk to.
[00:26:46.060 --> 00:26:51.780]   And to do that, we need to start with the foundation of the Google Assistant, the voice.
[00:26:51.780 --> 00:26:55.740]   Today that's how most users interact with the assistant.
[00:26:55.740 --> 00:26:58.660]   Our current voice is codenamed Holly.
[00:26:58.660 --> 00:27:00.380]   She was a real person.
[00:27:00.380 --> 00:27:05.180]   She spent months in our studio and then we stitched those recordings together to create
[00:27:05.180 --> 00:27:06.180]   voice.
[00:27:06.180 --> 00:27:12.300]   But 18 months ago, we announced a breakthrough from our DeepMind team called WaveNet.
[00:27:12.300 --> 00:27:15.100]   I think this is what we were talking about a couple of weeks ago.
[00:27:15.100 --> 00:27:20.660]   WaveNet actually models the underlying raw audio to create a more natural voice.
[00:27:20.660 --> 00:27:27.620]   It's closer to how human speed, the pitch, the pace, even all the pauses that convey
[00:27:27.620 --> 00:27:28.620]   meaning.
[00:27:28.620 --> 00:27:30.900]   We want to get all of that right.
[00:27:30.900 --> 00:27:37.260]   So we've worked hard with WaveNet and we are adding as of today six new voices to the
[00:27:37.260 --> 00:27:38.260]   Google Assistant.
[00:27:38.260 --> 00:27:41.180]   Let's have them say hello.
[00:27:41.180 --> 00:27:42.980]   Good morning everyone.
[00:27:42.980 --> 00:27:45.580]   I'm your Google Assistant.
[00:27:45.580 --> 00:27:48.180]   Welcome to Shoreline Empathy Theater.
[00:27:48.180 --> 00:27:50.780]   We hope you'll enjoy Google I/O.
[00:27:50.780 --> 00:27:52.620]   Back to you, Sundar.
[00:27:52.620 --> 00:27:59.940]   Yeah, they're very kind of white, European, non-accented, and answery voices.
[00:27:59.940 --> 00:28:03.740]   But that's where this gets more interesting when you could take real people.
[00:28:03.740 --> 00:28:06.140]   So these are the generic.
[00:28:06.140 --> 00:28:11.820]   It's a great way to get the right accents, languages, and dialects right globally.
[00:28:11.820 --> 00:28:14.620]   You know, WaveNet can make this much easier.
[00:28:14.620 --> 00:28:20.740]   With this technology, we started wondering who we could get into this studio with an amazing
[00:28:20.740 --> 00:28:21.740]   voice.
[00:28:21.740 --> 00:28:22.740]   Take a look.
[00:28:22.740 --> 00:28:23.740]   This is the Jaden Mongeviz.
[00:28:23.740 --> 00:28:29.140]   Who's a type of North African semolina in granules made from crushed durum wheat?
[00:28:29.140 --> 00:28:32.340]   So he does not have in any way a kind of a polished voice.
[00:28:32.340 --> 00:28:33.340]   I mean, he's obviously a polished singer.
[00:28:33.340 --> 00:28:36.380]   I want a fluff of sweet eyes and a fluffy tail who likes my high heels.
[00:28:36.380 --> 00:28:38.380]   But he sounds like a normal person.
[00:28:38.380 --> 00:28:44.460]   Happy birthday to the person whose birthday it is.
[00:28:44.460 --> 00:28:51.580]   Happy birthday to you.
[00:28:51.580 --> 00:28:53.420]   John Legend.
[00:28:53.420 --> 00:28:57.700]   He would probably tell you he don't want to brag, but he'll be the best assistant you
[00:28:57.700 --> 00:28:58.700]   ever had.
[00:28:58.700 --> 00:29:00.660]   Can you tell me where you live?
[00:29:00.660 --> 00:29:07.900]   You can find me on all kinds of devices, phones, Google Homes, and if I'm lucky, in
[00:29:07.900 --> 00:29:09.900]   your heart.
[00:29:09.900 --> 00:29:16.780]   So that's less impressive than what you're about to hear, which is now that modified
[00:29:16.780 --> 00:29:19.420]   John Legend voice is coming out of the assistant.
[00:29:19.420 --> 00:29:23.260]   He didn't spend all the time in the studio answering every possible question that you
[00:29:23.260 --> 00:29:24.580]   could ask.
[00:29:24.580 --> 00:29:29.580]   But WaveNet allowed us to shorten the studio time, and the model can actually capture the
[00:29:29.580 --> 00:29:31.500]   richness of his voice.
[00:29:31.500 --> 00:29:36.680]   His voice will be coming later this year in certain contexts so that you can get responses
[00:29:36.680 --> 00:29:37.980]   like this.
[00:29:37.980 --> 00:29:39.540]   Good morning, Sundar.
[00:29:39.540 --> 00:29:42.820]   Now in Mountain View, it's 65 with clear skies.
[00:29:42.820 --> 00:29:46.420]   Today, it's predicted to be 75 degrees and sunny.
[00:29:46.420 --> 00:29:50.100]   At 10 AM, you have an event called Google I/O Kino.
[00:29:50.100 --> 00:29:52.780]   Then at 1 PM, you have Margaritas.
[00:29:52.780 --> 00:29:54.700]   Have a wonderful day.
[00:29:54.700 --> 00:29:57.940]   That is really natural sounding.
[00:29:57.940 --> 00:29:58.940]   Creepy.
[00:29:58.940 --> 00:30:00.980]   You think it's creepy?
[00:30:00.980 --> 00:30:02.140]   It's kind of like real fakes.
[00:30:02.140 --> 00:30:07.500]   I think the potential for this is pretty disconcerting.
[00:30:07.500 --> 00:30:10.100]   You sound like a Swiss journalist now.
[00:30:10.100 --> 00:30:18.060]   But you'll get used to the idea that sometimes the voices, you know computer voices sound
[00:30:18.060 --> 00:30:19.060]   whacky.
[00:30:19.060 --> 00:30:20.060]   Computer voices don't have to sound whacky.
[00:30:20.060 --> 00:30:21.060]   They can sound human.
[00:30:21.060 --> 00:30:22.780]   And once they do, they can sound like different humans.
[00:30:22.780 --> 00:30:24.860]   And I think we'll just get used to that.
[00:30:24.860 --> 00:30:25.860]   Yeah.
[00:30:25.860 --> 00:30:26.860]   So that's fine.
[00:30:26.860 --> 00:30:34.180]   As long as we build this with an adversarial viewpoint on how it could be used, we're already
[00:30:34.180 --> 00:30:36.620]   talking about things like voice authentication.
[00:30:36.620 --> 00:30:42.420]   There's a thing here last night at Microsoft Build that talked about using voices and authenticator.
[00:30:42.420 --> 00:30:46.140]   And something like this would just destroy that.
[00:30:46.140 --> 00:30:48.260]   Yeah, that's true.
[00:30:48.260 --> 00:30:50.780]   But is Google actually rolling WaveNet out?
[00:30:50.780 --> 00:30:53.340]   Or are they just using it for their own products and services?
[00:30:53.340 --> 00:30:55.980]   Well, that's an interesting question.
[00:30:55.980 --> 00:31:00.700]   Maybe they'll have to keep it to themselves because of like face recognition, the danger.
[00:31:00.700 --> 00:31:01.700]   Right.
[00:31:01.700 --> 00:31:05.980]   Well, but a lot of their AI efforts will be, I mean, they're going to generate reports
[00:31:05.980 --> 00:31:08.060]   and papers based on this.
[00:31:08.060 --> 00:31:11.900]   But in one of the reasons why it probably sounds a bit generic is because their training
[00:31:11.900 --> 00:31:17.580]   data is probably like, it's like the BBC English, except it's not.
[00:31:17.580 --> 00:31:22.780]   It's generic American English because that's the easiest thing for people to understand.
[00:31:22.780 --> 00:31:23.780]   Well, that's what they're going to play.
[00:31:23.780 --> 00:31:24.780]   But yeah, I'm sure.
[00:31:24.780 --> 00:31:29.980]   I mean, Sundar Pichai could do his voice and it could sound here.
[00:31:29.980 --> 00:31:30.980]   Let me play.
[00:31:30.980 --> 00:31:35.180]   This is the thing that I don't know if this is creepy, but this is very impressive.
[00:31:35.180 --> 00:31:42.460]   So this is now another technology that Google announced that is going to allow the assistant
[00:31:42.460 --> 00:31:44.500]   to actually place calls on your behalf.
[00:31:44.500 --> 00:31:45.740]   Watch this.
[00:31:45.740 --> 00:31:48.460]   This one is wild.
[00:31:48.460 --> 00:31:51.140]   Is to help you get things done.
[00:31:51.140 --> 00:31:56.660]   It turns out a big part of getting things done is making a phone call.
[00:31:56.660 --> 00:32:00.460]   You may want to get an oil change schedule, maybe call a plumber in the middle of the
[00:32:00.460 --> 00:32:04.460]   week or even schedule a haircut appointment.
[00:32:04.460 --> 00:32:08.260]   You know, we are working hard to help users through those moments.
[00:32:08.260 --> 00:32:13.380]   We want to connect users to businesses in a good way.
[00:32:13.380 --> 00:32:20.020]   Businesses actually rely a lot on this, but even in the US, 60% of small businesses don't
[00:32:20.020 --> 00:32:23.180]   have an online booking system, et cetera.
[00:32:23.180 --> 00:32:26.820]   We think AI can help with this problem.
[00:32:26.820 --> 00:32:28.900]   So let's go back to this example.
[00:32:28.900 --> 00:32:34.300]   Let's say you want to ask Google to make your haircut appointment on Tuesday between 10
[00:32:34.300 --> 00:32:35.940]   and noon.
[00:32:35.940 --> 00:32:41.420]   What happens is the Google assistant makes the call seamlessly in the background for
[00:32:41.420 --> 00:32:42.860]   you.
[00:32:42.860 --> 00:32:48.500]   So what you're going to hear is the Google assistant actually calling a real salon to
[00:32:48.500 --> 00:32:50.740]   schedule the appointment for you.
[00:32:50.740 --> 00:32:51.740]   Let's listen.
[00:32:51.740 --> 00:32:54.260]   Now, this is wild.
[00:32:54.260 --> 00:32:57.340]   You'll hear, by the way, this sounds like a real assistant.
[00:32:57.340 --> 00:33:01.620]   They intentionally have her sticking in arms and pauses.
[00:33:01.620 --> 00:33:06.300]   But the thing that's most impressive to me is not that, but how she responds to a real
[00:33:06.300 --> 00:33:12.780]   human, and this is like the self-driving car problem, and sometimes humans steer you wrong
[00:33:12.780 --> 00:33:17.260]   or say, and it seemed to do, and of course these are canned demos, but it seemed to do
[00:33:17.260 --> 00:33:20.300]   a very good job of handling real world situations.
[00:33:20.300 --> 00:33:22.700]   Listen to this.
[00:33:22.700 --> 00:33:23.860]   This is the assistant.
[00:33:23.860 --> 00:33:27.300]   How's it happening out here?
[00:33:27.300 --> 00:33:28.300]   That's a real salon.
[00:33:28.300 --> 00:33:30.940]   Hi, I'm calling the Google Women's haircut for our client.
[00:33:30.940 --> 00:33:33.700]   I'm looking for something on May 3rd.
[00:33:33.700 --> 00:33:34.700]   That's the assistant.
[00:33:34.700 --> 00:33:37.220]   I'm looking for something on May 3rd.
[00:33:37.220 --> 00:33:38.220]   That's a computer.
[00:33:38.220 --> 00:33:40.460]   Do I get me one second?
[00:33:40.460 --> 00:33:41.460]   This is a human.
[00:33:41.460 --> 00:33:42.860]   She's up talking too.
[00:33:42.860 --> 00:33:43.860]   She is.
[00:33:43.860 --> 00:33:44.860]   She's up talking.
[00:33:44.860 --> 00:33:45.860]   Sure.
[00:33:45.860 --> 00:33:46.860]   What time are you looking for?
[00:33:46.860 --> 00:33:47.860]   She said, "Mm-hmm."
[00:33:47.860 --> 00:33:49.780]   At 12 p.m.
[00:33:49.780 --> 00:33:51.260]   We do not have a 12 p.m.
[00:33:51.260 --> 00:33:52.260]   Available.
[00:33:52.260 --> 00:33:55.380]   The closest we have to that is a 1/15.
[00:33:55.380 --> 00:33:58.100]   Now how is the assistant going to deal with that one, right?
[00:33:58.100 --> 00:34:03.020]   Do you have anything between 10 a.m. and 12 p.m.?
[00:34:03.020 --> 00:34:07.940]   Depending on what service she would like, what service is she looking for?
[00:34:07.940 --> 00:34:09.540]   Just a woman's haircut for now.
[00:34:09.540 --> 00:34:11.500]   Okay, we have a 10 o'clock.
[00:34:11.500 --> 00:34:13.340]   10 a.m. is fine.
[00:34:13.340 --> 00:34:16.180]   Okay, what's her first name?
[00:34:16.180 --> 00:34:17.980]   The first name is Lisa.
[00:34:17.980 --> 00:34:18.980]   Okay, perfect.
[00:34:18.980 --> 00:34:22.780]   So I will see you at 10 o'clock on May 3rd.
[00:34:22.780 --> 00:34:23.780]   Okay, great.
[00:34:23.780 --> 00:34:24.780]   Thanks.
[00:34:24.780 --> 00:34:28.460]   One of the things that scares me about that is that it's clear they're trying to make
[00:34:28.460 --> 00:34:29.620]   it full.
[00:34:29.620 --> 00:34:30.620]   The receptionist.
[00:34:30.620 --> 00:34:31.620]   What did you hear?
[00:34:31.620 --> 00:34:32.620]   It's just turned around the robocall now.
[00:34:32.620 --> 00:34:39.060]   Yeah, I mean, I can imagine this technology in the wrong hands is terrifying.
[00:34:39.060 --> 00:34:42.140]   Well, it also, I get those all the time.
[00:34:42.140 --> 00:34:45.100]   Don't you make it those whether they're trying to fake that they're human.
[00:34:45.100 --> 00:34:46.100]   Yeah.
[00:34:46.100 --> 00:34:47.100]   And you ask them, "Are you a robot?"
[00:34:47.100 --> 00:34:49.660]   And they have some kind to answer and they're not.
[00:34:49.660 --> 00:34:50.660]   Sorry.
[00:34:50.660 --> 00:34:51.660]   Go ahead, Stacy.
[00:34:51.660 --> 00:34:54.420]   No, I was very excited from a business perspective.
[00:34:54.420 --> 00:34:58.740]   Michael's talking about using this for calling restaurants to see if they're open for holiday
[00:34:58.740 --> 00:35:00.540]   hours, for example.
[00:35:00.540 --> 00:35:07.220]   And that, if you can imagine the automation of data collection at that kind of scale,
[00:35:07.220 --> 00:35:11.700]   that is mind boggling how quickly Google could gain an edge.
[00:35:11.700 --> 00:35:14.060]   Well, and this is really an example.
[00:35:14.060 --> 00:35:18.500]   To me, I'm kind of lately on a kick saying, "Stop calling it AI.
[00:35:18.500 --> 00:35:19.860]   That's a marketing term.
[00:35:19.860 --> 00:35:23.700]   Machine learning I can live with, but AI is a marketing term, except that this is
[00:35:23.700 --> 00:35:24.700]   AI now.
[00:35:24.700 --> 00:35:25.700]   This is AI now.
[00:35:25.700 --> 00:35:27.700]   This is actually here.
[00:35:27.700 --> 00:35:28.700]   Listen to this one.
[00:35:28.700 --> 00:35:31.180]   This is a much more challenging call.
[00:35:31.180 --> 00:35:32.540]   I couldn't have done this phone call.
[00:35:32.540 --> 00:35:33.540]   I couldn't have done this phone call.
[00:35:33.540 --> 00:35:34.540]   I couldn't have done this one.
[00:35:34.540 --> 00:35:35.540]   I couldn't have done this phone call.
[00:35:35.540 --> 00:35:36.540]   I couldn't have done this one.
[00:35:36.540 --> 00:35:38.580]   But it's a small restaurant which is not easily available to book online.
[00:35:38.580 --> 00:35:41.820]   The call actually goes a bit differently than expected.
[00:35:41.820 --> 00:35:43.820]   So take a listen.
[00:35:43.820 --> 00:35:49.220]   See, how many are you?
[00:35:49.220 --> 00:35:50.220]   Hi.
[00:35:50.220 --> 00:35:53.220]   I'd like to reserve a table for Wednesday, the seven.
[00:35:53.220 --> 00:35:54.220]   Okay.
[00:35:54.220 --> 00:36:01.820]   So the human misunderstood the assistant.
[00:36:01.820 --> 00:36:06.100]   He was saying at 7 p.m. she thought he said for seven people.
[00:36:06.100 --> 00:36:07.620]   Maybe she's a robot.
[00:36:07.620 --> 00:36:08.620]   That would be awesome.
[00:36:08.620 --> 00:36:11.500]   She did a worse job of understanding him than she does.
[00:36:11.500 --> 00:36:14.500]   He doesn't understand or it does of understanding her.
[00:36:14.500 --> 00:36:16.100]   She's actually pretty difficult to understand.
[00:36:16.100 --> 00:36:17.540]   Listen to how this continues.
[00:36:17.540 --> 00:36:19.780]   So she's already throwing him throwing it.
[00:36:19.780 --> 00:36:21.060]   I got to say it.
[00:36:21.060 --> 00:36:22.980]   It's really tempting to say him, isn't it?
[00:36:22.980 --> 00:36:23.980]   A curve.
[00:36:23.980 --> 00:36:26.620]   It's for four people.
[00:36:26.620 --> 00:36:28.620]   For people when?
[00:36:28.620 --> 00:36:29.620]   Monday?
[00:36:29.620 --> 00:36:32.380]   Wednesday at 6 p.m.
[00:36:32.380 --> 00:36:38.180]   Oh, actually we leave here for like opera, like a five people.
[00:36:38.180 --> 00:36:44.300]   Now if you are not watching the video and didn't see the caption, what the human just
[00:36:44.300 --> 00:36:50.620]   said is we only allow reservations for more than five people.
[00:36:50.620 --> 00:36:52.380]   So before you can come.
[00:36:52.380 --> 00:36:54.180]   For four people you can come.
[00:36:54.180 --> 00:36:56.580]   How long is the wait usually to be seated?
[00:36:56.580 --> 00:37:01.740]   Now that's actually the right response to that which is, oh, I don't need a reservation.
[00:37:01.740 --> 00:37:03.300]   Well, how long will I have to wait?
[00:37:03.300 --> 00:37:07.060]   That's a very sophisticated, I think, response.
[00:37:07.060 --> 00:37:12.300]   When tomorrow or weekend?
[00:37:12.300 --> 00:37:14.500]   For next Wednesday, the seven.
[00:37:14.500 --> 00:37:17.420]   Oh, no, it's not too busy.
[00:37:17.420 --> 00:37:22.420]   You can call people.
[00:37:22.420 --> 00:37:24.420]   Oh, I got you.
[00:37:24.420 --> 00:37:25.420]   Thanks.
[00:37:25.420 --> 00:37:26.420]   Wow.
[00:37:26.420 --> 00:37:27.420]   This is Turing test level.
[00:37:27.420 --> 00:37:28.420]   Turing test level.
[00:37:28.420 --> 00:37:30.660]   This is, yeah, this is very impressive.
[00:37:30.660 --> 00:37:32.420]   I want to know something here for a moment.
[00:37:32.420 --> 00:37:36.060]   It's one o'clock in Sundar's calendar and they just turned serving margaritas.
[00:37:36.060 --> 00:37:38.060]   Actually serving margaritas?
[00:37:38.060 --> 00:37:41.300]   I don't really, I shouldn't break it.
[00:37:41.300 --> 00:37:42.300]   There's string in it.
[00:37:42.300 --> 00:37:43.300]   Enjoy.
[00:37:43.300 --> 00:37:46.900]   I don't get any beverages until like six o'clock.
[00:37:46.900 --> 00:37:48.900]   Microsoft does not serve margaritas.
[00:37:48.900 --> 00:37:51.420]   It's good and it has tequila in it.
[00:37:51.420 --> 00:37:53.420]   There's actually tequila.
[00:37:53.420 --> 00:37:55.220]   Oh my God.
[00:37:55.220 --> 00:37:56.220]   Nice.
[00:37:56.220 --> 00:37:59.660]   So when are they announcing this?
[00:37:59.660 --> 00:38:03.380]   Because this reminds me, remember Facebook M, but they put the person in the loop for
[00:38:03.380 --> 00:38:05.220]   as things got more complicated.
[00:38:05.220 --> 00:38:08.100]   Facebook actually killed that project with the people.
[00:38:08.100 --> 00:38:11.860]   They kept the M AI efforts and they're bringing that back for their smart speaker, it sounds
[00:38:11.860 --> 00:38:12.860]   like.
[00:38:12.860 --> 00:38:20.020]   This is, and I've got a lot of questions about was this, obviously it's not a live demo.
[00:38:20.020 --> 00:38:21.420]   So.
[00:38:21.420 --> 00:38:23.460]   They said they had a lot.
[00:38:23.460 --> 00:38:29.140]   I don't remember the number of calls in the can, but my impression is this is an actual
[00:38:29.140 --> 00:38:31.340]   project, right?
[00:38:31.340 --> 00:38:35.020]   So that means it's not, it's something they're working on.
[00:38:35.020 --> 00:38:36.020]   It's Google duplex.
[00:38:36.020 --> 00:38:37.020]   It's Google duplex.
[00:38:37.020 --> 00:38:41.260]   It's my sense that this is something they're working on.
[00:38:41.260 --> 00:38:44.060]   They didn't say coming soon to a phone near you.
[00:38:44.060 --> 00:38:50.260]   Well, well, but about almost a year ago, I talked on the show about beta works had a
[00:38:50.260 --> 00:38:54.340]   voice accelerator and demo session.
[00:38:54.340 --> 00:38:58.860]   And they had showed exactly this where it calls off and makes it a webinar by flowers
[00:38:58.860 --> 00:38:59.860]   or whatever.
[00:38:59.860 --> 00:39:02.260]   So they, you know, these are companies outside Google working on this.
[00:39:02.260 --> 00:39:07.820]   The other interesting thing is seriously, you also get the opportunity at a really wacky
[00:39:07.820 --> 00:39:10.620]   level to have robot to robot in voice.
[00:39:10.620 --> 00:39:11.620]   Right.
[00:39:11.620 --> 00:39:13.620]   The rather than having to have any voice becomes the API.
[00:39:13.620 --> 00:39:15.420]   But you don't even need voice at that point.
[00:39:15.420 --> 00:39:20.620]   If you get robot to robot, they could go, but they could, well, we're going to see that
[00:39:20.620 --> 00:39:22.980]   with AI does kind of make up its own language.
[00:39:22.980 --> 00:39:27.380]   They could make up the island language, but you don't need a, you don't need it.
[00:39:27.380 --> 00:39:28.380]   Get a communication.
[00:39:28.380 --> 00:39:29.380]   You don't need an API.
[00:39:29.380 --> 00:39:31.380]   Audio becomes that connection, which is fascinating.
[00:39:31.380 --> 00:39:35.540]   Well, but that, that's the way we understand it, but it's not the way computers understand
[00:39:35.540 --> 00:39:36.540]   it.
[00:39:36.540 --> 00:39:39.660]   So really what they'll do is create an analog equivalent to voice that computers can understand.
[00:39:39.660 --> 00:39:46.340]   So basically, I know this is esoteric, but I hate when people try to think about humans
[00:39:46.340 --> 00:39:51.620]   in, or computers in human terms because it, it changed it.
[00:39:51.620 --> 00:39:53.420]   It's ridiculously inefficient.
[00:39:53.420 --> 00:39:54.420]   Yes.
[00:39:54.420 --> 00:39:55.420]   Thank you.
[00:39:55.420 --> 00:39:57.140]   It's like, we're not good at the things computers are good at it.
[00:39:57.140 --> 00:39:58.140]   They're good at things.
[00:39:58.140 --> 00:39:59.140]   We're not good at it.
[00:39:59.140 --> 00:40:02.060]   There's no reason to have a computer translated into human mode so that it could talk to another
[00:40:02.060 --> 00:40:04.300]   computer who has to un-translate it.
[00:40:04.300 --> 00:40:08.140]   It's a computer computer can call not knowing whether it's going to get a human or a computer.
[00:40:08.140 --> 00:40:09.140]   Right.
[00:40:09.140 --> 00:40:10.140]   So it's going to be neither.
[00:40:10.140 --> 00:40:11.140]   Right.
[00:40:11.140 --> 00:40:14.460]   So more down to earth.
[00:40:14.460 --> 00:40:17.540]   Kevin, I'll, actually before I do that, Kevin, it sounded like you wanted to say something
[00:40:17.540 --> 00:40:18.540]   about this technology.
[00:40:18.540 --> 00:40:19.780]   No, no, I'm good.
[00:40:19.780 --> 00:40:22.140]   I'm kind of watching the keynote and listening along here.
[00:40:22.140 --> 00:40:23.140]   Okay, go right ahead.
[00:40:23.140 --> 00:40:24.140]   Yeah, go ahead.
[00:40:24.140 --> 00:40:25.140]   Be my guest.
[00:40:25.140 --> 00:40:30.500]   They did mention that the Google Assistant will have a feature that Amazon added to its
[00:40:30.500 --> 00:40:35.140]   echo a couple of months ago, the ability to continue on, you trigger it with, you know,
[00:40:35.140 --> 00:40:36.860]   hey, Google, but then it would continue on.
[00:40:36.860 --> 00:40:39.060]   You like Stacey's doing a little Stacy dance.
[00:40:39.060 --> 00:40:42.900]   I'm like, I'm excited about the customize smart routines, even though I spent, you know,
[00:40:42.900 --> 00:40:46.980]   hours programming the mid-the Amazon Echo, but now I can spend hours programming them
[00:40:46.980 --> 00:40:49.940]   into the Google Assistant or, yeah, Google Home.
[00:40:49.940 --> 00:40:53.500]   Custom and scheduled routines coming to this isn't this is very much like the blueprint
[00:40:53.500 --> 00:40:55.580]   feature of Amazon Echo.
[00:40:55.580 --> 00:40:59.020]   No, it's more like the, what do they call it on?
[00:40:59.020 --> 00:41:03.700]   It's the same, what you say, like a good night command or watch a movie and it does
[00:41:03.700 --> 00:41:04.700]   all that.
[00:41:04.700 --> 00:41:09.380]   Oh, the scheduled aspect of it, I don't believe Amazon has a schedule.
[00:41:09.380 --> 00:41:10.820]   So that's actually new.
[00:41:10.820 --> 00:41:12.420]   Okay, good.
[00:41:12.420 --> 00:41:15.860]   That's the missing piece because yeah, we talk about this on our show.
[00:41:15.860 --> 00:41:20.940]   Stacy, you need a hub to kind of create these automations and schedules.
[00:41:20.940 --> 00:41:23.540]   And I had said back in March, it's time for Google to make a hub.
[00:41:23.540 --> 00:41:24.900]   Well, it looks like they're not doing that.
[00:41:24.900 --> 00:41:29.980]   It looks like they're bringing hub features to a voice user interface.
[00:41:29.980 --> 00:41:31.380]   They also had smart.
[00:41:31.380 --> 00:41:36.900]   They also had something which I think is I have mixed feelings about pretty please.
[00:41:36.900 --> 00:41:37.900]   I like this.
[00:41:37.900 --> 00:41:42.900]   I'm not just for kids, but I think I think it encourages civility.
[00:41:42.900 --> 00:41:48.340]   The idea is the assistant can be taught to sit to wait until you ask it politely.
[00:41:48.340 --> 00:41:50.420]   And then can thank you for doing so.
[00:41:50.420 --> 00:41:52.180]   And then says thank you for being so polite.
[00:41:52.180 --> 00:41:55.700]   I've heard all kinds of parents say that they see their kids burning into dictators,
[00:41:55.700 --> 00:41:56.940]   yelling at Siri.
[00:41:56.940 --> 00:42:02.860]   Well, my view on this maybe is that the kid could learn a more nuanced approach, which
[00:42:02.860 --> 00:42:05.780]   is that machines do not need to be talked to politely.
[00:42:05.780 --> 00:42:06.780]   They're machines.
[00:42:06.780 --> 00:42:11.980]   I don't like the idea of mixing the distinction between humans and machines.
[00:42:11.980 --> 00:42:12.980]   I think it's important.
[00:42:12.980 --> 00:42:17.220]   So encouraging anthropomorphization is not a good thing.
[00:42:17.220 --> 00:42:21.860]   I will disagree with you just because it is impossible for us not to anthropomorphize
[00:42:21.860 --> 00:42:23.660]   things.
[00:42:23.660 --> 00:42:26.140]   That is what we're wired to do.
[00:42:26.140 --> 00:42:33.260]   And so training people to be polite and civil in their interactions with technology.
[00:42:33.260 --> 00:42:35.180]   It just becomes a habit in general.
[00:42:35.180 --> 00:42:37.260]   Yeah, you become less abrasive.
[00:42:37.260 --> 00:42:39.380]   I think I'm okay with that.
[00:42:39.380 --> 00:42:45.740]   As long as there is also an effort to really help kids understand that this is not a human,
[00:42:45.740 --> 00:42:49.540]   there's not a person in there and that you do treat machines differently than you treat
[00:42:49.540 --> 00:42:50.540]   humans.
[00:42:50.540 --> 00:42:57.020]   I don't think that there are any danger of that children recognize machines and people.
[00:42:57.020 --> 00:42:59.380]   And I think this isn't actually just for kids too.
[00:42:59.380 --> 00:43:04.420]   I think part of you, if you just get used to belting out commands at things, you do that
[00:43:04.420 --> 00:43:05.420]   as a default habit.
[00:43:05.420 --> 00:43:06.420]   It could become habit-evigement.
[00:43:06.420 --> 00:43:07.420]   I understand that.
[00:43:07.420 --> 00:43:08.420]   Yeah.
[00:43:08.420 --> 00:43:13.020]   Even before this rolls out or rolled out, the echo has a way now where it will listen for
[00:43:13.020 --> 00:43:17.300]   an extra five seconds and you can say thank you and she will say you're welcome and so
[00:43:17.300 --> 00:43:18.300]   on.
[00:43:18.300 --> 00:43:19.300]   And I've been using that feature.
[00:43:19.300 --> 00:43:22.580]   I mean, I was saying thank you a year ago on that thing.
[00:43:22.580 --> 00:43:26.340]   So I think it depends on the person and kids.
[00:43:26.340 --> 00:43:27.340]   That's a tough one.
[00:43:27.340 --> 00:43:28.500]   I mean, I know lots of little kids.
[00:43:28.500 --> 00:43:30.660]   How do they get the people in the TV?
[00:43:30.660 --> 00:43:36.580]   They don't think they don't realize that it is a device sometimes.
[00:43:36.580 --> 00:43:41.220]   And I like it's always nice because sometimes I say thank you to people regardless.
[00:43:41.220 --> 00:43:45.420]   I say thank you to my washing machine, not because I feel like it's doing anything.
[00:43:45.420 --> 00:43:46.580]   It's just a habit.
[00:43:46.580 --> 00:43:47.580]   That's a big one.
[00:43:47.580 --> 00:43:49.340]   Yeah, we learned something there.
[00:43:49.340 --> 00:43:51.500]   I just I'm like, oh, thank you.
[00:43:51.500 --> 00:43:52.500]   You know, Stacy.
[00:43:52.500 --> 00:43:54.100]   Stacy, Stacy, I got to know.
[00:43:54.100 --> 00:43:57.100]   What's the conversation you have with your June oven?
[00:43:57.100 --> 00:43:58.940]   Oh, we have long deep.
[00:43:58.940 --> 00:44:02.860]   There's no voice interaction in the June oven.
[00:44:02.860 --> 00:44:03.860]   So it's just.
[00:44:03.860 --> 00:44:05.460]   And you know what?
[00:44:05.460 --> 00:44:10.540]   My most common phrase actually to all of my devices, you are disappointing me.
[00:44:10.540 --> 00:44:11.540]   I'm just.
[00:44:11.540 --> 00:44:14.700]   You see, she's so polite.
[00:44:14.700 --> 00:44:21.500]   I say, you Siri is what I literally say about three times a day.
[00:44:21.500 --> 00:44:26.660]   Kevin, is there anything I see it, but I can't if they talk about anything interesting that
[00:44:26.660 --> 00:44:31.580]   you can tell them in topics on the it's all it's all Android so far.
[00:44:31.580 --> 00:44:32.580]   So I haven't.
[00:44:32.580 --> 00:44:33.580]   Okay.
[00:44:33.580 --> 00:44:34.580]   And I'm not listening.
[00:44:34.580 --> 00:44:35.580]   I'm only watching.
[00:44:35.580 --> 00:44:40.140]   So I by the way, I thanks to the chat room and found the Google AI blog post, which just
[00:44:40.140 --> 00:44:46.300]   went up on the Google duplex system, and it has some more examples and talks about how
[00:44:46.300 --> 00:44:47.460]   it works.
[00:44:47.460 --> 00:44:52.460]   And they're saying this is using something called a recurrent neural network designed
[00:44:52.460 --> 00:44:57.100]   to cope with challenging environments using extended TensorFlow or 10 or 10 or 10 or
[00:44:57.100 --> 00:45:00.100]   10 or 10 or 10 or 10 or 10 or 10 or 10 or 10 or 10 or 10 or 10 or 10 or 10 or 10 or
[00:45:00.100 --> 00:45:01.100]   10 extended TFX.
[00:45:01.100 --> 00:45:05.660]   And let me just play a little a couple more of these clips that will give you an idea
[00:45:05.660 --> 00:45:08.380]   of this is duplex handling interruptions.
[00:45:08.380 --> 00:45:11.620]   Okay, what's your phone number?
[00:45:11.620 --> 00:45:15.820]   The phone number is six oh seven.
[00:45:15.820 --> 00:45:19.820]   Wait, where the case got over?
[00:45:19.820 --> 00:45:24.300]   The number is six oh seven.
[00:45:24.300 --> 00:45:30.700]   It's a little robotic two, three, but two, two, what?
[00:45:30.700 --> 00:45:33.580]   Two, two, three.
[00:45:33.580 --> 00:45:34.580]   That's impressive.
[00:45:34.580 --> 00:45:35.580]   Okay, two, two, three.
[00:45:35.580 --> 00:45:36.580]   These are real conversations.
[00:45:36.580 --> 00:45:37.580]   Let me give you another one.
[00:45:37.580 --> 00:45:41.180]   Hi, I would like to serve a table for May 25.
[00:45:41.180 --> 00:45:43.940]   Sorry, what day?
[00:45:43.940 --> 00:45:46.060]   For Friday, May 25.
[00:45:46.060 --> 00:45:50.620]   It added Friday to help the human.
[00:45:50.620 --> 00:45:52.700]   Here's duplex responding to a sink.
[00:45:52.700 --> 00:45:54.700]   Are you here?
[00:45:54.700 --> 00:45:57.140]   Yeah, I'm here.
[00:45:57.140 --> 00:45:58.140]   Are you here?
[00:45:58.140 --> 00:45:59.620]   Yeah, I'm here.
[00:45:59.620 --> 00:46:04.020]   So imagine how this could be used in all kinds of other ways and not just making appointments
[00:46:04.020 --> 00:46:09.500]   at first, but turning journalism and DuVois and asking questions and being able to call
[00:46:09.500 --> 00:46:12.460]   brands and stores and get help.
[00:46:12.460 --> 00:46:14.580]   The opportunities just are just endless.
[00:46:14.580 --> 00:46:15.580]   Here's a new word.
[00:46:15.580 --> 00:46:18.940]   Have your AI negotiate on your behalf for insurance companies.
[00:46:18.940 --> 00:46:20.580]   Here's a new word.
[00:46:20.580 --> 00:46:24.580]   Here's a new word that you like for you writers.
[00:46:24.580 --> 00:46:25.780]   Disfluencies.
[00:46:25.780 --> 00:46:31.700]   The us and us that they add, they call those speech disfluencies.
[00:46:31.700 --> 00:46:36.320]   They're added when combining widely different sound units in the concaten of TTS or adding
[00:46:36.320 --> 00:46:40.940]   synthetic weights, which allows the system to signal in a natural way that is still
[00:46:40.940 --> 00:46:41.940]   processing.
[00:46:41.940 --> 00:46:44.940]   In other words, when you hear those umbs, it's actually thinking.
[00:46:44.940 --> 00:46:46.420]   Oh, it's the spinning wheel.
[00:46:46.420 --> 00:46:49.180]   It's doing what humans do when you hear an um.
[00:46:49.180 --> 00:46:50.180]   Mm hmm.
[00:46:50.180 --> 00:46:54.380]   Well, I see the future very close when a parent says to a child saying, "I'm all the time."
[00:46:54.380 --> 00:46:58.620]   You sound like a rat.
[00:46:58.620 --> 00:47:00.620]   It's better than working.
[00:47:00.620 --> 00:47:03.740]   Oh, someone in the chat room did this.
[00:47:03.740 --> 00:47:04.740]   But this is true.
[00:47:04.740 --> 00:47:07.220]   Sales pitches to old pensioners from Toad's Law.
[00:47:07.220 --> 00:47:08.220]   Oh, oh yeah.
[00:47:08.220 --> 00:47:10.020]   The idea of scammers using this is like.
[00:47:10.020 --> 00:47:11.020]   Oh, it's going to be terrible.
[00:47:11.020 --> 00:47:12.020]   Oh, God.
[00:47:12.020 --> 00:47:13.020]   It's going to be terrible.
[00:47:13.020 --> 00:47:14.020]   What's that?
[00:47:14.020 --> 00:47:15.020]   What's that Jolly Roger site?
[00:47:15.020 --> 00:47:19.020]   I think it is where you can turn it on to a ripple collar.
[00:47:19.020 --> 00:47:20.940]   We've had him on a couple of times.
[00:47:20.940 --> 00:47:21.940]   He's great.
[00:47:21.940 --> 00:47:22.940]   And yeah.
[00:47:22.940 --> 00:47:23.940]   Here's another.
[00:47:23.940 --> 00:47:25.500]   This is again from this blog post.
[00:47:25.500 --> 00:47:29.340]   It's important for latency to match people's expectations.
[00:47:29.340 --> 00:47:34.220]   For example, after people say something simple, for example, hello, they expect an instant
[00:47:34.220 --> 00:47:37.260]   response and are more sensitive to latency.
[00:47:37.260 --> 00:47:42.540]   When we detect that low latency is required, we use faster, low confidence models.
[00:47:42.540 --> 00:47:47.700]   In other words, they'd like to be more precise in their responses, but they can sense that
[00:47:47.700 --> 00:47:50.300]   the human is impatient.
[00:47:50.300 --> 00:47:55.420]   So they'll give him something that may be a little wrong, but at least it's something.
[00:47:55.420 --> 00:47:57.140]   This is just wild.
[00:47:57.140 --> 00:47:58.340]   That's actually how I talk to people.
[00:47:58.340 --> 00:47:59.340]   Yeah.
[00:47:59.340 --> 00:48:01.580]   I'm like, uh, pot tickles.
[00:48:01.580 --> 00:48:03.740]   See, this is, this is AI.
[00:48:03.740 --> 00:48:05.580]   This is AI.
[00:48:05.580 --> 00:48:07.380]   This is really.
[00:48:07.380 --> 00:48:11.220]   It's making a computer look human is what it is.
[00:48:11.220 --> 00:48:12.220]   Okay.
[00:48:12.220 --> 00:48:13.220]   All right.
[00:48:13.220 --> 00:48:14.220]   All right.
[00:48:14.220 --> 00:48:15.220]   It's not really thinking.
[00:48:15.220 --> 00:48:21.540]   Leo, since you do radio years ago when I used to be on Ron Owen's show, Ron would let
[00:48:21.540 --> 00:48:24.300]   a call go on and he started talking to me.
[00:48:24.300 --> 00:48:27.340]   And I think what the hell he started talking to me.
[00:48:27.340 --> 00:48:28.340]   I think we're only doing it.
[00:48:28.340 --> 00:48:32.780]   Then I was like, oh, he would just put his hand up, hit the button and say, why do you say
[00:48:32.780 --> 00:48:33.780]   that?
[00:48:33.780 --> 00:48:35.780]   Oh, he was just showing off.
[00:48:35.780 --> 00:48:37.940]   He was just showing off.
[00:48:37.940 --> 00:48:42.220]   I have to say though, John Seed of Warac tells the story of being on the Larry King show
[00:48:42.220 --> 00:48:44.780]   and Larry actually falling asleep.
[00:48:44.780 --> 00:48:51.020]   Because remember, it used to be an all night late night show falling asleep during a call.
[00:48:51.020 --> 00:48:52.340]   John didn't know what to do.
[00:48:52.340 --> 00:48:56.820]   So he answered and he kind of extended the answer hoping Larry would wake up.
[00:48:56.820 --> 00:48:58.700]   Finally, he just ran out of things to say.
[00:48:58.700 --> 00:49:06.180]   So he stopped and King goes, Minneapolis next.
[00:49:06.180 --> 00:49:07.860]   So that is awesome.
[00:49:07.860 --> 00:49:12.540]   And that's so humans have all sorts of tricks for handling these kinds of things.
[00:49:12.540 --> 00:49:17.180]   I'm really impressed by this Google duplex.
[00:49:17.180 --> 00:49:20.100]   I know I'm not sure how it's going to get used.
[00:49:20.100 --> 00:49:24.420]   Obviously, it says this summer we'll start testing the duplex technology within Google
[00:49:24.420 --> 00:49:30.780]   Assistant this summer to help users make reservations for restaurants to schedule hair
[00:49:30.780 --> 00:49:33.460]   salon appointments and get holiday hours over the phone.
[00:49:33.460 --> 00:49:36.580]   They feel apparently confident enough.
[00:49:36.580 --> 00:49:42.900]   Here's a picture of Janie Vlevaeth and Google duplex lead and Mattan Coleman engineering
[00:49:42.900 --> 00:49:48.180]   manager enjoying a meal booked through a call from duplex.
[00:49:48.180 --> 00:49:50.220]   Here's the call.
[00:49:50.220 --> 00:49:51.220]   They're actually.
[00:49:51.220 --> 00:49:54.380]   Hey, I'm calling to make reservation.
[00:49:54.380 --> 00:49:57.540]   I'm looking for a table on Friday the fourth.
[00:49:57.540 --> 00:49:59.180]   This Friday?
[00:49:59.180 --> 00:50:00.180]   Yeah.
[00:50:00.180 --> 00:50:01.180]   Wow.
[00:50:01.180 --> 00:50:04.820]   So how many people?
[00:50:04.820 --> 00:50:05.820]   Five.
[00:50:05.820 --> 00:50:08.580]   It's for two people.
[00:50:08.580 --> 00:50:09.820]   For two?
[00:50:09.820 --> 00:50:10.820]   Yes.
[00:50:10.820 --> 00:50:13.740]   And what time do you work at?
[00:50:13.740 --> 00:50:15.500]   At seven p.m.
[00:50:15.500 --> 00:50:16.500]   Seven?
[00:50:16.500 --> 00:50:20.300]   Am I your name?
[00:50:20.300 --> 00:50:23.740]   His name is Janie V. Y-A-N-I-V.
[00:50:23.740 --> 00:50:24.740]   Okay.
[00:50:24.740 --> 00:50:25.740]   I got it.
[00:50:25.740 --> 00:50:34.180]   So it will be two people at seven or four?
[00:50:34.180 --> 00:50:35.180]   Yes.
[00:50:35.180 --> 00:50:36.180]   Okay.
[00:50:36.180 --> 00:50:37.180]   Got it.
[00:50:37.180 --> 00:50:38.180]   Thank you.
[00:50:38.180 --> 00:50:39.180]   Okay.
[00:50:39.180 --> 00:50:41.180]   You sound kind of like a jerk there.
[00:50:41.180 --> 00:50:42.180]   Yeah.
[00:50:42.180 --> 00:50:43.180]   Yeah.
[00:50:43.180 --> 00:50:47.100]   A little robotic jerk like a Google bro.
[00:50:47.100 --> 00:50:48.580]   Maybe that's exactly how Janie sounds.
[00:50:48.580 --> 00:50:49.580]   I don't know.
[00:50:49.580 --> 00:50:55.540]   So listening to these calls, which apparently are made to mostly like four ethnic restaurants,
[00:50:55.540 --> 00:50:56.540]   I guess.
[00:50:56.540 --> 00:50:58.500]   Well, those are challenging.
[00:50:58.500 --> 00:50:59.500]   Those are.
[00:50:59.500 --> 00:51:00.500]   They are.
[00:51:00.500 --> 00:51:03.580]   Well, they're challenging if you're trained on certain accents and data.
[00:51:03.580 --> 00:51:08.140]   So that's it kind of emphasizes this weird class divide.
[00:51:08.140 --> 00:51:10.900]   It's actually uncomfortable listening to a bunch of those in a row.
[00:51:10.900 --> 00:51:11.900]   Yeah, I know what you mean.
[00:51:11.900 --> 00:51:12.900]   Yeah, because it's this white.
[00:51:12.900 --> 00:51:14.700]   Yeah, that's really true Stacy.
[00:51:14.700 --> 00:51:15.700]   Yeah.
[00:51:15.700 --> 00:51:21.220]   Well, I think they're showing Linux on the Chromebooks in the developer conference.
[00:51:21.220 --> 00:51:24.020]   Oh, how exciting.
[00:51:24.020 --> 00:51:25.420]   Crostini.
[00:51:25.420 --> 00:51:29.420]   Crouton, Crostini is the next one, Brusqueda.
[00:51:29.420 --> 00:51:30.420]   Brusqueda?
[00:51:30.420 --> 00:51:31.420]   Brusqueda.
[00:51:31.420 --> 00:51:35.780]   C-H is a c in Italian.
[00:51:35.780 --> 00:51:41.620]   Oh, so as impressive as all this is, I mean, to me, it's almost like...
[00:51:41.620 --> 00:51:48.940]   By the way, that's exactly how bros talk to people at where it gets restaurants.
[00:51:48.940 --> 00:51:50.580]   I think you mean Brusqueda.
[00:51:50.580 --> 00:51:51.580]   Go ahead, Kevin.
[00:51:51.580 --> 00:51:52.580]   I'm sorry.
[00:51:52.580 --> 00:51:53.580]   That's okay.
[00:51:53.580 --> 00:51:56.700]   I'm just trying to get Stacy to spit that perrier out through her nose.
[00:51:56.700 --> 00:51:58.660]   That happening.
[00:51:58.660 --> 00:52:01.820]   It almost feels like a next generation chatbot in a sense.
[00:52:01.820 --> 00:52:03.900]   Only you're not doing the chatting at this point.
[00:52:03.900 --> 00:52:10.260]   Well, somebody in the chat room said, and I think this is accurate, it sounds depressed.
[00:52:10.260 --> 00:52:12.460]   The bot doesn't sound...
[00:52:12.460 --> 00:52:15.340]   It sounds like it's something wrong with it.
[00:52:15.340 --> 00:52:17.860]   No, it's trying to say, okay.
[00:52:17.860 --> 00:52:18.860]   This is the...
[00:52:18.860 --> 00:52:24.820]   This is where Uncanny Valley happens in audio as well as in visual elements, right?
[00:52:24.820 --> 00:52:32.140]   We're so well attuned to detecting emotions and feelings in other people that I think
[00:52:32.140 --> 00:52:36.700]   when we hear these voices, that we do, we come and we say, "Well, you sound like you're
[00:52:36.700 --> 00:52:37.700]   covering up depression."
[00:52:37.700 --> 00:52:40.420]   It sounds like to me.
[00:52:40.420 --> 00:52:42.820]   It's trying to offer up its own emotions and not react to yours.
[00:52:42.820 --> 00:52:43.820]   I get that.
[00:52:43.820 --> 00:52:44.820]   Yeah.
[00:52:44.820 --> 00:52:47.900]   So it makes me wonder, what would it like to redo Eliza today?
[00:52:47.900 --> 00:52:49.620]   Wow.
[00:52:49.620 --> 00:52:51.820]   You're sounding depressed, Jeff.
[00:52:51.820 --> 00:52:56.180]   Are you wondering what it would have been like to redo Eliza today?
[00:52:56.180 --> 00:52:58.140]   Why do you say that?
[00:52:58.140 --> 00:53:00.980]   I say that because you said this.
[00:53:00.980 --> 00:53:04.420]   Guys, we might be building Marvin.
[00:53:04.420 --> 00:53:07.620]   Marvin, the paranoid Android.
[00:53:07.620 --> 00:53:12.900]   I have this pain all up and down the diodes on my back.
[00:53:12.900 --> 00:53:16.980]   Well, the next animal really preached by European neighbor.
[00:53:16.980 --> 00:53:18.340]   It was arranged in...
[00:53:18.340 --> 00:53:19.340]   Sorry, sorry.
[00:53:19.340 --> 00:53:21.340]   No, go ahead, Jeff.
[00:53:21.340 --> 00:53:22.940]   No, and step out your punchline.
[00:53:22.940 --> 00:53:23.940]   Sorry.
[00:53:23.940 --> 00:53:28.180]   A grain the size of a planet.
[00:53:28.180 --> 00:53:31.540]   So who else is Google competing with here?
[00:53:31.540 --> 00:53:32.540]   Is anybody else who can close the deal?
[00:53:32.540 --> 00:53:33.540]   No one's even close.
[00:53:33.540 --> 00:53:37.540]   Amazon's the next closest, right?
[00:53:37.540 --> 00:53:43.380]   And they've been winning because of Amazon's ubiquity, because people like to shop.
[00:53:43.380 --> 00:53:46.340]   That seems to be a good useful tool on this stuff.
[00:53:46.340 --> 00:53:50.300]   But ultimately, if Google can make this stuff real, it's all over.
[00:53:50.300 --> 00:53:54.580]   Frankly, one of the problems I have with a lot of voice assistants is I use the...
[00:53:54.580 --> 00:53:58.180]   I use the echoes in the homes because they're ambient.
[00:53:58.180 --> 00:53:59.180]   I rarely...
[00:53:59.180 --> 00:54:00.180]   I don't know about you guys.
[00:54:00.180 --> 00:54:01.700]   Do you use voice assistants on your phones?
[00:54:01.700 --> 00:54:02.700]   I do not.
[00:54:02.700 --> 00:54:03.700]   I eat driving.
[00:54:03.700 --> 00:54:04.700]   Yeah.
[00:54:04.700 --> 00:54:05.700]   Driving maybe.
[00:54:05.700 --> 00:54:06.700]   Okay.
[00:54:06.700 --> 00:54:07.700]   I don't use...
[00:54:07.700 --> 00:54:09.660]   I'm sitting around up up around people.
[00:54:09.660 --> 00:54:11.540]   They say, "Well, to what time is up will you start?"
[00:54:11.540 --> 00:54:12.540]   I'll say it.
[00:54:12.540 --> 00:54:15.020]   They all hate me for doing it because it looks obnoxious, but I'll do it.
[00:54:15.020 --> 00:54:16.020]   Okay, Google.
[00:54:16.020 --> 00:54:17.420]   Yeah, I do it sometimes like that.
[00:54:17.420 --> 00:54:19.060]   But I know I don't use it an awful lot.
[00:54:19.060 --> 00:54:21.580]   And I think these kinds of things might make you use it more.
[00:54:21.580 --> 00:54:22.580]   I...
[00:54:22.580 --> 00:54:23.580]   You know...
[00:54:23.580 --> 00:54:24.580]   By customizing it.
[00:54:24.580 --> 00:54:25.580]   You got to think of it.
[00:54:25.580 --> 00:54:26.580]   You got to remember you can do it.
[00:54:26.580 --> 00:54:27.580]   What are they...
[00:54:27.580 --> 00:54:29.580]   Is they saying anything interesting about Christina, Kevin?
[00:54:29.580 --> 00:54:31.420]   No, they've zipped around.
[00:54:31.420 --> 00:54:33.660]   They went through PWAs quickly.
[00:54:33.660 --> 00:54:38.460]   Talked about uptake on that, how to get more people using them.
[00:54:38.460 --> 00:54:39.460]   And then...
[00:54:39.460 --> 00:54:43.580]   Yeah, it looks like they're just showing Android UI stuff right now.
[00:54:43.580 --> 00:54:44.580]   Wow.
[00:54:44.580 --> 00:54:45.580]   They moved on fast.
[00:54:45.580 --> 00:54:46.580]   Wow.
[00:54:46.580 --> 00:54:47.580]   Yeah.
[00:54:47.580 --> 00:54:48.580]   Okay, wait.
[00:54:48.580 --> 00:54:50.300]   I want to go back to this duplex thing and other companies.
[00:54:50.300 --> 00:54:52.540]   Not in voice, but in...
[00:54:52.540 --> 00:54:53.900]   Actually, no in voice.
[00:54:53.900 --> 00:54:57.620]   And then translating voice into an action item or service.
[00:54:57.620 --> 00:55:00.100]   Microsoft actually yesterday ran a really cool demo.
[00:55:00.100 --> 00:55:02.660]   I don't know if they talked about the future of work demo that they did.
[00:55:02.660 --> 00:55:03.660]   Oh, the team's demo.
[00:55:03.660 --> 00:55:05.060]   Yeah, it wasn't that amazing.
[00:55:05.060 --> 00:55:06.060]   Yeah.
[00:55:06.060 --> 00:55:07.060]   So they showed off...
[00:55:07.060 --> 00:55:08.820]   Basically, they had this cone.
[00:55:08.820 --> 00:55:12.180]   I don't really know what all is in there with a camera and some voice stuff.
[00:55:12.180 --> 00:55:17.420]   So when you came into a meeting, it was transcribing your meeting as people talked and it knew
[00:55:17.420 --> 00:55:21.660]   who the people were based on like, I don't know, their face and their employee ID badge.
[00:55:21.660 --> 00:55:23.140]   I'm not sure which one.
[00:55:23.140 --> 00:55:26.900]   But then it pulled out things that it thought were action items.
[00:55:26.900 --> 00:55:30.740]   So when you say where it's like, "Oh, I'll follow up on this or I'll send you an email."
[00:55:30.740 --> 00:55:35.140]   It actually pulled that into notes that you could then pull into your outlook or whatever.
[00:55:35.140 --> 00:55:41.540]   But I actually thought that was a really good way of augmenting human intelligence and to
[00:55:41.540 --> 00:55:43.340]   spare you what is it?
[00:55:43.340 --> 00:55:46.220]   Just a crappy robotic task.
[00:55:46.220 --> 00:55:52.940]   So I know it's not like an interaction with a real human at a restaurant.
[00:55:52.940 --> 00:55:56.620]   But another question is, how long are you going to have humans answering the phone as
[00:55:56.620 --> 00:55:59.140]   opposed to computers querying each other?
[00:55:59.140 --> 00:56:09.580]   Well, I mean, to be fair, I know Sundar said only 60% of what businesses don't have online
[00:56:09.580 --> 00:56:10.580]   booking.
[00:56:10.580 --> 00:56:15.420]   But most of the restaurants I go to, my hair salon, most of the places I go are online
[00:56:15.420 --> 00:56:16.420]   booking.
[00:56:16.420 --> 00:56:17.740]   So I never interact with a human.
[00:56:17.740 --> 00:56:18.980]   I don't want to interact with a human.
[00:56:18.980 --> 00:56:19.980]   It's much easier to do it.
[00:56:19.980 --> 00:56:22.460]   I hate doing online booking because you're like, "Oh, really?"
[00:56:22.460 --> 00:56:25.900]   Well, maybe it's because, you know, I'm like, "Oh, is it going to be a haircut?
[00:56:25.900 --> 00:56:27.100]   Is it going to be the dye for the blue?
[00:56:27.100 --> 00:56:28.100]   Is it going to be a bleach?
[00:56:28.100 --> 00:56:30.340]   There's a lot of information that has to happen."
[00:56:30.340 --> 00:56:32.940]   And then I can't ever remember my passwords.
[00:56:32.940 --> 00:56:36.100]   It's just drama.
[00:56:36.100 --> 00:56:38.980]   And I have to remember to do it.
[00:56:38.980 --> 00:56:43.540]   So like when I remember something right away and say, "Oh, hey, Google, just make me an
[00:56:43.540 --> 00:56:49.460]   appointment for redying my hair," that would be super awesome if it just went and did it.
[00:56:49.460 --> 00:56:51.900]   But that's kind of a lot of things to join.
[00:56:51.900 --> 00:56:53.500]   I know we'll hopefully we'll get there.
[00:56:53.500 --> 00:56:55.700]   You have different needs than I do, perhaps.
[00:56:55.700 --> 00:56:56.700]   I do.
[00:56:56.700 --> 00:57:00.980]   You communicate, you continue with communication, you continue with conversation.
[00:57:00.980 --> 00:57:04.700]   I touched on it briefly just the idea that your Amazon did this a few months ago where
[00:57:04.700 --> 00:57:08.180]   you trigger it once and then you can have an ongoing conversation.
[00:57:08.180 --> 00:57:12.140]   I was just going to say, "That's what frequent European neighbor more than anything."
[00:57:12.140 --> 00:57:13.580]   I said, "You constantly."
[00:57:13.580 --> 00:57:14.580]   And I said, "You opt in.
[00:57:14.580 --> 00:57:15.580]   Continue.
[00:57:15.580 --> 00:57:16.580]   Continue constantly."
[00:57:16.580 --> 00:57:19.620]   No, it doesn't listen to you until you say the trigger word.
[00:57:19.620 --> 00:57:20.620]   And then it says, "All right."
[00:57:20.620 --> 00:57:21.620]   When does it go to shut off?
[00:57:21.620 --> 00:57:22.620]   When do you know it shuts off?
[00:57:22.620 --> 00:57:24.420]   Well, that's a good question.
[00:57:24.420 --> 00:57:25.900]   I guess it's waiting for a pause.
[00:57:25.900 --> 00:57:26.900]   I don't know.
[00:57:26.900 --> 00:57:30.140]   I think it's supposed to be seven in a normal thing.
[00:57:30.140 --> 00:57:32.220]   It's seven seconds after you're done.
[00:57:32.220 --> 00:57:33.220]   Okay.
[00:57:33.220 --> 00:57:35.220]   And there's a total limit as well, right?
[00:57:35.220 --> 00:57:36.220]   I don't know.
[00:57:36.220 --> 00:57:37.220]   There's a total limit.
[00:57:37.220 --> 00:57:38.220]   I think there's a total limit.
[00:57:38.220 --> 00:57:47.580]   So to be interested, Howard Stern this week had a radio derby and he tried to get to see
[00:57:47.580 --> 00:57:50.940]   how long you could keep a call going to a restaurant or something.
[00:57:50.940 --> 00:57:54.780]   It was exactly what your points to see about a class thing here.
[00:57:54.780 --> 00:57:58.380]   How you could keep going by never answering somebody's question.
[00:57:58.380 --> 00:58:02.180]   You could imagine doing derbies with this stuff and how long you could keep going.
[00:58:02.180 --> 00:58:04.500]   In the hands of trolls, this stuff is a nightmare.
[00:58:04.500 --> 00:58:06.100]   Oh, be amazing.
[00:58:06.100 --> 00:58:08.420]   Well, 40 phone calls.
[00:58:08.420 --> 00:58:10.980]   And that's why we need this.
[00:58:10.980 --> 00:58:17.220]   I feel like it was a wired story or a R's or story that this adversarial model for building
[00:58:17.220 --> 00:58:23.020]   this stuff and testing it, recognizing that humans, when we program them, we program them
[00:58:23.020 --> 00:58:25.820]   to behave one way and expect them to behave one way.
[00:58:25.820 --> 00:58:31.380]   But when you start interacting more closely with people, people are going to do just mean
[00:58:31.380 --> 00:58:32.780]   awful things to them.
[00:58:32.780 --> 00:58:35.780]   And on the programming side, we actually have to start thinking about that and building
[00:58:35.780 --> 00:58:36.780]   for that.
[00:58:36.780 --> 00:58:41.340]   I'm more worried about the robots moving, doing mean awful things to the humans.
[00:58:41.340 --> 00:58:45.340]   I don't think, I mean, yes, that's one way we could go.
[00:58:45.340 --> 00:58:49.860]   But I really think it's important as we put our faith in this stuff.
[00:58:49.860 --> 00:58:55.220]   So think about things like people are talking about putting little tiny pixels on stop signs
[00:58:55.220 --> 00:58:57.860]   just to screw with self-driving cars.
[00:58:57.860 --> 00:59:01.180]   Those are the kind of things people will consider and think of.
[00:59:01.180 --> 00:59:04.220]   And we need the people building this stuff to think of that too.
[00:59:04.220 --> 00:59:05.220]   So it goes both ways.
[00:59:05.220 --> 00:59:10.180]   And of course, the other side of that is the ethics conversation that I think such in Adela
[00:59:10.180 --> 00:59:14.260]   spent much more time on appropriately so yesterday.
[00:59:14.260 --> 00:59:16.380]   But then soon, I was touched on a little bit.
[00:59:16.380 --> 00:59:24.180]   But I think that all programmers now have to, for so many years, you're programming, Kevin,
[00:59:24.180 --> 00:59:29.220]   you know this, your whole mindset was just let me get the machine to do this thing.
[00:59:29.220 --> 00:59:33.860]   Now they really have to think about how will it be used and how will it be abused.
[00:59:33.860 --> 00:59:39.180]   Both of those things are now very, very important and we're missing and have been missing in
[00:59:39.180 --> 00:59:40.180]   the past.
[00:59:40.180 --> 00:59:41.180]   Right.
[00:59:41.180 --> 00:59:45.380]   Design and user experience, user interface are all obviously things that you have to consider.
[00:59:45.380 --> 00:59:51.700]   But it's not just the people that are going to use your service, your app, your device.
[00:59:51.700 --> 00:59:55.180]   It's one of the other people around it who aren't going to use it, what might they do
[00:59:55.180 --> 00:59:58.020]   and how do you mitigate for things you don't even know.
[00:59:58.020 --> 01:00:02.260]   Well, and then abuse it with like hit your LIDAR with a baseball bat.
[01:00:02.260 --> 01:00:05.900]   Hit your $11,000 LIDAR with a baseball bat.
[01:00:05.900 --> 01:00:07.460]   So yeah, it's unfortunate.
[01:00:07.460 --> 01:00:12.700]   But I think it's actually a maturation process in coding that you have to start thinking
[01:00:12.700 --> 01:00:14.740]   about human factors much more.
[01:00:14.740 --> 01:00:15.940]   Stacey, back to your point.
[01:00:15.940 --> 01:00:22.500]   I think there's, I think there's, now you have the ethic and in some cases, law that
[01:00:22.500 --> 01:00:26.060]   if you're being recorded, you have to hear a beep.
[01:00:26.060 --> 01:00:27.060]   Right.
[01:00:27.060 --> 01:00:28.060]   Yeah.
[01:00:28.060 --> 01:00:30.820]   And so I wonder if a robot speaking, do they have to start with a beep?
[01:00:30.820 --> 01:00:31.820]   They should.
[01:00:31.820 --> 01:00:32.820]   I think it would be appropriate.
[01:00:32.820 --> 01:00:37.300]   Don't you that the robots say, I am an assistant, an automated assistant calling on behalf of
[01:00:37.300 --> 01:00:39.260]   Leo Laporte before it does that?
[01:00:39.260 --> 01:00:40.500]   That would make sense.
[01:00:40.500 --> 01:00:41.500]   Yeah.
[01:00:41.500 --> 01:00:45.580]   Yeah, but that's, I mean, that's something that good people will do and other people won't.
[01:00:45.580 --> 01:00:46.580]   Right.
[01:00:46.580 --> 01:00:47.580]   But yeah, I agree.
[01:00:47.580 --> 01:00:52.740]   There's probably the polite thing to, just like the polite thing to do, even if in like
[01:00:52.740 --> 01:00:56.980]   a two party consent state, I have to say that I'm recording, but I'm recording from Texas,
[01:00:56.980 --> 01:00:57.980]   which is not.
[01:00:57.980 --> 01:01:01.820]   So I could just record the phone call, but because I'm not a jerk.
[01:01:01.820 --> 01:01:02.820]   Yeah.
[01:01:02.820 --> 01:01:03.820]   I say I'm recording this.
[01:01:03.820 --> 01:01:04.820]   Is it okay?
[01:01:04.820 --> 01:01:05.820]   Hello.
[01:01:05.820 --> 01:01:11.620]   I am an automated assistant calling for Leo Laporte.
[01:01:11.620 --> 01:01:15.620]   So then the Harris salon, when they get upset, they say, take me to your master.
[01:01:15.620 --> 01:01:17.940]   Oh, can I speak to a supervisor?
[01:01:17.940 --> 01:01:21.020]   Can I speak to your owner?
[01:01:21.020 --> 01:01:22.620]   What about when this becomes commonplace?
[01:01:22.620 --> 01:01:25.180]   So let's think five, 10 years down the road, assuming it does.
[01:01:25.180 --> 01:01:26.540]   I mean, right now it makes sense.
[01:01:26.540 --> 01:01:27.620]   Yes, I agree.
[01:01:27.620 --> 01:01:32.180]   The bot, the AI should identify itself because it's a new thing and people have to get used
[01:01:32.180 --> 01:01:33.180]   to this.
[01:01:33.180 --> 01:01:34.860]   But what if it becomes widespread?
[01:01:34.860 --> 01:01:36.780]   Will we not want to know that?
[01:01:36.780 --> 01:01:37.780]   Will we care?
[01:01:37.780 --> 01:01:38.780]   I don't know.
[01:01:38.780 --> 01:01:42.860]   Well, there's some really cool things like what if you got a robot therapist in the future,
[01:01:42.860 --> 01:01:43.860]   right?
[01:01:43.860 --> 01:01:48.460]   Like an AI based therapist, would you that in a way could open up a lot of opportunities
[01:01:48.460 --> 01:01:55.340]   for people who can't afford a like $150 or $200 therapy session, right?
[01:01:55.340 --> 01:02:00.780]   You could buy a robot one for 25 and see if it works for you.
[01:02:00.780 --> 01:02:02.780]   So I think you should have...
[01:02:02.780 --> 01:02:03.780]   That really...
[01:02:03.780 --> 01:02:04.780]   It's not crazy.
[01:02:04.780 --> 01:02:10.300]   Yeah, but it really kind of diminishes the importance of the skill of a therapist.
[01:02:10.300 --> 01:02:12.100]   That is where we're...
[01:02:12.100 --> 01:02:14.220]   Listen, you're just here to listen.
[01:02:14.220 --> 01:02:16.420]   Shut up and I'm going to tell you, my dreams.
[01:02:16.420 --> 01:02:20.260]   Well, no, I mean, there are definitely questions it could ask to help you.
[01:02:20.260 --> 01:02:21.260]   I guess so.
[01:02:21.260 --> 01:02:26.900]   Anyway, the point is, I think you will have to have a truth in labeling kind of when you're
[01:02:26.900 --> 01:02:28.620]   dealing with an AI at all times.
[01:02:28.620 --> 01:02:29.780]   It's going to be very interesting.
[01:02:29.780 --> 01:02:31.820]   Yeah, it's going to be very interesting.
[01:02:31.820 --> 01:02:32.820]   I agree with you.
[01:02:32.820 --> 01:02:33.820]   I think that absolutely.
[01:02:33.820 --> 01:02:34.820]   Let's take a little break.
[01:02:34.820 --> 01:02:35.820]   Lots more to talk about.
[01:02:35.820 --> 01:02:40.980]   Stacey Higginbotham is here while she's actually in Seattle where she's covering Microsoft's
[01:02:40.980 --> 01:02:44.140]   developer conference build for Stacey on IOT.com.
[01:02:44.140 --> 01:02:46.140]   It's nice to have you.
[01:02:46.140 --> 01:02:48.020]   Thank you for taking time out from your day to do this.
[01:02:48.020 --> 01:02:49.380]   I appreciate it.
[01:02:49.380 --> 01:02:56.220]   Also here, Kevin Tofol, Stacey's partner in arms in the IOT podcast, he also has his own
[01:02:56.220 --> 01:03:04.860]   site about Chromebooks.com, a new site dedicated to our favorite computers, the Chromebooks.
[01:03:04.860 --> 01:03:10.980]   And he's at Google I/O, Mr. Jeff Jarvis and the press tent, Buzzmachine.com, professor
[01:03:10.980 --> 01:03:11.980]   of journalism.
[01:03:11.980 --> 01:03:17.060]   Actually, our next segment is going to be all about you, Jeff, because I actually have
[01:03:17.060 --> 01:03:21.060]   been playing with the new Google News and I'm very excited about it.
[01:03:21.060 --> 01:03:23.180]   We'll talk about that in just a second.
[01:03:23.180 --> 01:03:26.460]   The show today brought to you by Rocket Mortgage from Quick and Loans.
[01:03:26.460 --> 01:03:31.300]   If the time has come to buy a new house or refinance your own home, you ought to think
[01:03:31.300 --> 01:03:32.300]   about Rocket Mortgage.
[01:03:32.300 --> 01:03:36.300]   Quick and Loans, the best lender in the country by far, number one in customer satisfaction,
[01:03:36.300 --> 01:03:43.540]   eight years in a row, thanks to the good folks at JD Power and all the people who voted for
[01:03:43.540 --> 01:03:44.540]   them.
[01:03:44.540 --> 01:03:47.780]   But it's much more than just that.
[01:03:47.780 --> 01:03:52.340]   Rocket Mortgage is a completely online loan approval process.
[01:03:52.340 --> 01:03:54.340]   How about that?
[01:03:54.340 --> 01:03:56.740]   Forget, now this is good.
[01:03:56.740 --> 01:03:58.980]   You don't need, you don't need to be your bot to call on the bank.
[01:03:58.980 --> 01:04:03.820]   That would be nice to have a bot call the bank and say, I would like a home loan.
[01:04:03.820 --> 01:04:06.940]   Instead, you don't even have to do the bank.
[01:04:06.940 --> 01:04:08.340]   Forget the bank thing.
[01:04:08.340 --> 01:04:10.420]   Just go to rocketmortgage.com/twig.
[01:04:10.420 --> 01:04:14.540]   You answer a few questions, simple things you know the answer to and then it takes it
[01:04:14.540 --> 01:04:16.660]   over and does the rest.
[01:04:16.660 --> 01:04:19.940]   There are trusted partners that allow you to share your financial information at the
[01:04:19.940 --> 01:04:23.940]   touch of a button so you don't have to go through boxes of paperwork.
[01:04:23.940 --> 01:04:27.540]   In fact, all of this happens so fast you could do it at an open house.
[01:04:27.540 --> 01:04:28.860]   You could be at an open house.
[01:04:28.860 --> 01:04:33.660]   Say, I want to buy this house, fire up rocketmortgage.com/twig and within a few minutes,
[01:04:33.660 --> 01:04:37.660]   it's under 10 minutes, you would have a home loan.
[01:04:37.660 --> 01:04:42.460]   Based on income assets and credit, Rocket Mortgage will analyze all the home loan options
[01:04:42.460 --> 01:04:43.700]   for which you qualify.
[01:04:43.700 --> 01:04:45.500]   Find the one that's just right for you.
[01:04:45.500 --> 01:04:48.860]   You specify the term, the rate, the down payment.
[01:04:48.860 --> 01:04:49.860]   You get the loan.
[01:04:49.860 --> 01:04:52.980]   In it's no paperwork.
[01:04:52.980 --> 01:04:53.980]   No paper.
[01:04:53.980 --> 01:04:57.540]   Well, I guess when you get the loan you have to sign the papers.
[01:04:57.540 --> 01:04:59.340]   But actually they even make that easy.
[01:04:59.340 --> 01:05:04.780]   A notary comes to you with all the paperwork, comes to your house and you don't even have
[01:05:04.780 --> 01:05:06.460]   to leave the house.
[01:05:06.460 --> 01:05:11.620]   Rocket mortgage from quick and loans apply simply, understand fully and mortgage confidently
[01:05:11.620 --> 01:05:13.620]   at rocketmortgage.com/twig.
[01:05:13.620 --> 01:05:18.620]   They're an equal housing lender licensed in all 50 states and MLS consumer access.org
[01:05:18.620 --> 01:05:21.180]   at number 3030.
[01:05:21.180 --> 01:05:23.860]   Rocketmortgage.com/twig.
[01:05:23.860 --> 01:05:25.700]   We thank them for their support of this week in Google.
[01:05:25.700 --> 01:05:30.780]   I realize I have some disfluencies myself, but we've actually built those into the Leo
[01:05:30.780 --> 01:05:34.020]   bot 9000 so that I sound more human.
[01:05:34.020 --> 01:05:35.020]   It's working.
[01:05:35.020 --> 01:05:36.820]   I fool all of you.
[01:05:36.820 --> 01:05:41.020]   I think you should get them to train a Leo bot and have it do a show.
[01:05:41.020 --> 01:05:42.020]   Oh, I would.
[01:05:42.020 --> 01:05:45.420]   You know, by the way, a huge business opportunity.
[01:05:45.420 --> 01:05:48.980]   I think by the time I retire, because I'm not going to retire for another eight to 10 years,
[01:05:48.980 --> 01:05:49.980]   right?
[01:05:49.980 --> 01:05:56.500]   By the time I retire, I think this show could be hosted by Google duplex.
[01:05:56.500 --> 01:06:01.300]   If anybody's going to do it, if anybody's going to the first show hosted by a robot.
[01:06:01.300 --> 01:06:02.540]   I want to do this.
[01:06:02.540 --> 01:06:03.540]   Better be twin.
[01:06:03.540 --> 01:06:04.540]   Yeah.
[01:06:04.540 --> 01:06:06.500]   Yeah, bring it, bring it on.
[01:06:06.500 --> 01:06:07.500]   They can fact check us.
[01:06:07.500 --> 01:06:10.140]   They actually would replace me because they would fact check me.
[01:06:10.140 --> 01:06:11.140]   They'd out-
[01:06:11.140 --> 01:06:12.140]   I'm sorry, Stacey.
[01:06:12.140 --> 01:06:13.140]   I'm sorry, Stacey.
[01:06:13.140 --> 01:06:15.340]   You have already looked that up.
[01:06:15.340 --> 01:06:18.140]   Actually, Leo, this is how it works.
[01:06:18.140 --> 01:06:19.140]   Rose Getta.
[01:06:19.140 --> 01:06:24.140]   Let's let's now imagine this Stacy Bothelio bot, the Jeff Bot, on the regular show.
[01:06:24.140 --> 01:06:26.420]   Yeah, we could all just go home.
[01:06:26.420 --> 01:06:27.860]   Collect the residuals.
[01:06:27.860 --> 01:06:37.140]   Technology being OK and techno, techno panic and Stacey will be and Leo will play devil's
[01:06:37.140 --> 01:06:38.140]   advocate.
[01:06:38.140 --> 01:06:39.140]   Yeah, it's easy.
[01:06:39.140 --> 01:06:40.140]   It's easy.
[01:06:40.140 --> 01:06:41.140]   Easy peasy.
[01:06:41.140 --> 01:06:42.140]   Let's see.
[01:06:42.140 --> 01:06:43.540]   Let's talk about this news.
[01:06:43.540 --> 01:06:50.380]   By the way, they did announce as they did last year, the public beta for Android P is
[01:06:50.380 --> 01:06:51.380]   now available.
[01:06:51.380 --> 01:06:56.260]   I immediately installed it on my Pixel 2 XL and I'm liking it.
[01:06:56.260 --> 01:06:57.660]   There's some nice new features.
[01:06:57.660 --> 01:07:02.580]   You probably have heard all about them, though, already from Jason Hell in the All About Android
[01:07:02.580 --> 01:07:03.580]   crew.
[01:07:03.580 --> 01:07:04.820]   And I'm sure they'll talk more about that tonight.
[01:07:04.820 --> 01:07:07.460]   Jason actually is going to do all about Android.
[01:07:07.460 --> 01:07:10.260]   I believe he's doing it from Google I/O.
[01:07:10.260 --> 01:07:11.260]   Am I right?
[01:07:11.260 --> 01:07:12.260]   Yes.
[01:07:12.260 --> 01:07:14.260]   And we're going to run to the Internet.
[01:07:14.260 --> 01:07:15.260]   Oh, nice.
[01:07:15.260 --> 01:07:18.300]   He's going to interview two top Android people tomorrow morning at 10.
[01:07:18.300 --> 01:07:24.300]   Yeah, Google loves all about Android and they're giving him access to some big shots,
[01:07:24.300 --> 01:07:25.220]   executives and so forth.
[01:07:25.220 --> 01:07:30.500]   So there will be, I'll leave that Android P discussion to all about Android.
[01:07:30.500 --> 01:07:34.620]   But I have to say, I'm liking it quite a bit already.
[01:07:34.620 --> 01:07:39.940]   And one of the newest features that I put a widget already, Google has replaced their
[01:07:39.940 --> 01:07:46.020]   news to Google Play News Stand app with something simply called Google News, which is confusing
[01:07:46.020 --> 01:07:50.060]   because they do have Google News and whether I'm guessing that has been deprecated.
[01:07:50.060 --> 01:07:56.780]   But Google News can still has the Google News Stand subscriptions as a tab on it.
[01:07:56.780 --> 01:07:57.940]   But I'm very impressed.
[01:07:57.940 --> 01:07:59.500]   How do I get?
[01:07:59.500 --> 01:08:02.180]   I can't figure out where to get to it, is it?
[01:08:02.180 --> 01:08:05.060]   So if you have newsstand installed, it'll update.
[01:08:05.060 --> 01:08:06.060]   That's all I did.
[01:08:06.060 --> 01:08:10.220]   If you don't have newsstand installed, you probably have to just go to the Android App Store
[01:08:10.220 --> 01:08:11.220]   and download.
[01:08:11.220 --> 01:08:13.020]   Okay, got it.
[01:08:13.020 --> 01:08:14.020]   Thank you.
[01:08:14.020 --> 01:08:18.460]   So I'll show you, it's hard to see, I know, but front page, first thing that shows up
[01:08:18.460 --> 01:08:21.740]   is Leo's briefing, top five stories right now.
[01:08:21.740 --> 01:08:26.700]   And they implied that those were algorithmically chosen based on my interests.
[01:08:26.700 --> 01:08:33.180]   In my case, the big story is the big story of the day, the Iran nuclear deal.
[01:08:33.180 --> 01:08:41.340]   And there's another Trump story, Kellyanne Conway, the investigators back at rural Petaluma
[01:08:41.340 --> 01:08:43.420]   home for death investigation.
[01:08:43.420 --> 01:08:45.220]   That's obviously something I'm interested in.
[01:08:45.220 --> 01:08:47.460]   That's a local story.
[01:08:47.460 --> 01:08:48.860]   Here's a New York post story.
[01:08:48.860 --> 01:08:51.780]   I probably would not use that as a source in future.
[01:08:51.780 --> 01:08:54.460]   I'm sure there's a way to turn that off.
[01:08:54.460 --> 01:08:57.420]   And nine to five max, so it knows I like technology.
[01:08:57.420 --> 01:09:00.300]   And then below that, there are additional stories.
[01:09:00.300 --> 01:09:06.260]   And those you can categorize in the favorites section, you can choose the topics you're
[01:09:06.260 --> 01:09:09.300]   interested in, the sources you're interested in.
[01:09:09.300 --> 01:09:10.580]   It has my locations.
[01:09:10.580 --> 01:09:13.660]   So that's why it knew about the Petaluma story.
[01:09:13.660 --> 01:09:18.660]   And then magazines that I can or should subscribe to, this is kind of Apple's going to do this.
[01:09:18.660 --> 01:09:19.660]   They bought texture.
[01:09:19.660 --> 01:09:22.860]   So Apple's obviously going to have something like that in the Apple news.
[01:09:22.860 --> 01:09:24.220]   So there's the four you.
[01:09:24.220 --> 01:09:29.300]   One thing I think is really important in all of the stories is there's a button.
[01:09:29.300 --> 01:09:34.460]   It's a little one on here that gives you full, what they call it, Google called full coverage.
[01:09:34.460 --> 01:09:37.420]   And in that case, it's not algorithmically chosen.
[01:09:37.420 --> 01:09:43.620]   They were very careful to say, this is all the reporting from all the best sources.
[01:09:43.620 --> 01:09:48.100]   I don't know how they choose those on that particular topic.
[01:09:48.100 --> 01:09:50.140]   They do include tweets.
[01:09:50.140 --> 01:09:54.420]   Now somebody was saying, and we weren't sure, is it people you follow on Twitter?
[01:09:54.420 --> 01:09:59.980]   I don't follow the president, so it isn't just people I follow on Twitter.
[01:09:59.980 --> 01:10:02.980]   I'm not sure that this wasn't algorithmically done.
[01:10:02.980 --> 01:10:05.740]   They said that everybody would get the same thing.
[01:10:05.740 --> 01:10:06.740]   That's what they meant.
[01:10:06.740 --> 01:10:07.740]   That's what I meant.
[01:10:07.740 --> 01:10:08.740]   Yes, right.
[01:10:08.740 --> 01:10:14.540]   Of course it's algorithmically done, but it's not in the sense that it's tailored to my interest.
[01:10:14.540 --> 01:10:15.540]   Everybody gets the same thing.
[01:10:15.540 --> 01:10:20.540]   When you get to that story, everybody gets the same full coverage of that story.
[01:10:20.540 --> 01:10:22.220]   And all of that they're talking about quality.
[01:10:22.220 --> 01:10:24.780]   They're talking about the most trusted sources.
[01:10:24.780 --> 01:10:27.380]   We have both Facebook and Google working on that now.
[01:10:27.380 --> 01:10:28.660]   So latest updates.
[01:10:28.660 --> 01:10:32.380]   Now in the full coverage page, latest updates, top coverage.
[01:10:32.380 --> 01:10:33.380]   It's got social stuff.
[01:10:33.380 --> 01:10:36.860]   It's got video, probably not all from YouTube, by the way.
[01:10:36.860 --> 01:10:38.020]   There's CNN.
[01:10:38.020 --> 01:10:39.780]   There's CNBC.
[01:10:39.780 --> 01:10:41.220]   There's the CBS Evening News.
[01:10:41.220 --> 01:10:48.380]   A fact check section, I think that's telling, an opinion section, which is interesting, and
[01:10:48.380 --> 01:10:51.940]   then analysis and frequently asked questions.
[01:10:51.940 --> 01:10:55.900]   This is, I think, really nicely done.
[01:10:55.900 --> 01:10:58.420]   This is better than Twitter moments.
[01:10:58.420 --> 01:11:01.980]   Look how long this is all on that one story.
[01:11:01.980 --> 01:11:03.900]   It almost feels like an infinite scroll.
[01:11:03.900 --> 01:11:07.700]   I think it might be on that one story for full coverage.
[01:11:07.700 --> 01:11:12.380]   So I think this is, I've only played with it a little bit.
[01:11:12.380 --> 01:11:18.060]   There's headlines, and then the headlines you can look at latest US world business, technology
[01:11:18.060 --> 01:11:19.580]   entertainment, and so forth.
[01:11:19.580 --> 01:11:21.740]   My favorites, as I said, was where you customize.
[01:11:21.740 --> 01:11:26.100]   They have kept the subscription feature, the newsstand feature, but it's just a tab.
[01:11:26.100 --> 01:11:30.420]   And I think, I'll defer to you in a moment, Jeff, but I think that this is exactly the
[01:11:30.420 --> 01:11:35.780]   kind of thing publications with paywalls want, because it's very, and they mentioned this
[01:11:35.780 --> 01:11:39.460]   on the stage, very easy to, oh, I want to subscribe to Los Angeles Times.
[01:11:39.460 --> 01:11:42.540]   It's a one button push and you're in using your Google Play.
[01:11:42.540 --> 01:11:44.620]   And then there's a lot of free stuff as well.
[01:11:44.620 --> 01:11:46.180]   There's a subscription sale here.
[01:11:46.180 --> 01:11:54.700]   So this is the old newsstand, but I think nicely done mixing both free and paywall content
[01:11:54.700 --> 01:11:56.180]   in together.
[01:11:56.180 --> 01:11:57.660]   Vanity Fair, $1.99 a month.
[01:11:57.660 --> 01:12:01.140]   If I wanted to subscribe to that, I push a button.
[01:12:01.140 --> 01:12:05.340]   Could Google take that same infrastructure and do it for a per-article basis?
[01:12:05.340 --> 01:12:07.340]   Because that would be really interesting.
[01:12:07.340 --> 01:12:08.340]   I'd love to see that.
[01:12:08.340 --> 01:12:09.340]   Right.
[01:12:09.340 --> 01:12:10.340]   Just micropans.
[01:12:10.340 --> 01:12:20.740]   I don't think the business demand is there for that.
[01:12:20.740 --> 01:12:22.140]   Yeah, they could.
[01:12:22.140 --> 01:12:26.660]   But by adding a drag, by adding a step in, and no matter if it's a nickel or whatever
[01:12:26.660 --> 01:12:32.380]   it is, the cost of closing the transaction is much higher than the value of the transaction.
[01:12:32.380 --> 01:12:34.340]   That's the problem with microtransactions right now.
[01:12:34.340 --> 01:12:36.180]   You've got to get less expensive.
[01:12:36.180 --> 01:12:41.940]   If I clicked on an article and it was something that I could only get there, and I clicked
[01:12:41.940 --> 01:12:46.420]   through and Google threw up a thing saying, "Hey, it's going to cost you a quarter.
[01:12:46.420 --> 01:12:47.420]   They've already got my credit card.
[01:12:47.420 --> 01:12:52.420]   If I just hit yes, it'll run through my credit card and my Google Pay or whatever the hell
[01:12:52.420 --> 01:12:53.420]   I've got."
[01:12:53.420 --> 01:12:55.700]   That would actually not be super friction-y.
[01:12:55.700 --> 01:12:57.860]   They could do what Apple does.
[01:12:57.860 --> 01:12:58.860]   They could bundle it.
[01:12:58.860 --> 01:12:59.860]   Yeah.
[01:12:59.860 --> 01:13:00.860]   Right.
[01:13:00.860 --> 01:13:01.860]   You take, yeah, Apple does this with iTunes.
[01:13:01.860 --> 01:13:06.140]   You take all of the payments and you do them all at once to reduce the total cost.
[01:13:06.140 --> 01:13:11.700]   The other issue is that if you're going to have a $400 subscription to the Wall Street
[01:13:11.700 --> 01:13:19.380]   Journal and I can get by at the five times I care to in a quarter for $0.25 each, then
[01:13:19.380 --> 01:13:23.660]   the publisher is unlikely to want to offer up their stuff in that way.
[01:13:23.660 --> 01:13:29.300]   It's always been one of the many messiah we keep waiting for, but it's not quite there.
[01:13:29.300 --> 01:13:34.580]   This is to me what Apple probably wants to do with Apple News didn't do.
[01:13:34.580 --> 01:13:35.900]   By the way, the widget is great.
[01:13:35.900 --> 01:13:38.220]   Every time I go to the widget, it's a slideshow.
[01:13:38.220 --> 01:13:39.420]   It says Leo's briefing.
[01:13:39.420 --> 01:13:43.380]   It's got a brief weather and a headline, but every time I go to the slideshow, it has a
[01:13:43.380 --> 01:13:45.580]   new slide.
[01:13:45.580 --> 01:13:50.380]   They're just kind of updating that, which I think is this is really nicely done.
[01:13:50.380 --> 01:13:53.340]   I have to say I'm very, very impressed.
[01:13:53.340 --> 01:13:57.460]   In fact, immediately put it on a widget on my homepage because I think it's the kind
[01:13:57.460 --> 01:14:01.100]   of thing I will probably use as a main news source.
[01:14:01.100 --> 01:14:10.260]   Jeff, you haven't seen it yet, so you probably don't have the ability to review it, but unmute.
[01:14:10.260 --> 01:14:11.260]   Thank you.
[01:14:11.260 --> 01:14:12.260]   Sorry.
[01:14:12.260 --> 01:14:13.260]   I have it now.
[01:14:13.260 --> 01:14:15.740]   I don't even know how to get a widget anymore, but I haven't done it in so long.
[01:14:15.740 --> 01:14:17.740]   You press and long press.
[01:14:17.740 --> 01:14:19.740]   I think it's very nice.
[01:14:19.740 --> 01:14:26.540]   One thing I wonder is, just the first off is why didn't they replace the swipe right
[01:14:26.540 --> 01:14:29.500]   on the end from the Android home with that?
[01:14:29.500 --> 01:14:33.380]   Yeah, because swipe right still gives you Google Now cards, which are clearly getting
[01:14:33.380 --> 01:14:38.420]   deprecated and are basically news stuff and not very good news.
[01:14:38.420 --> 01:14:39.420]   It's okay.
[01:14:39.420 --> 01:14:41.420]   I go through it once a day, but...
[01:14:41.420 --> 01:14:44.060]   No, I agree with you.
[01:14:44.060 --> 01:14:45.460]   I'm hoping they'll do that.
[01:14:45.460 --> 01:14:46.460]   You have news.
[01:14:46.460 --> 01:14:50.980]   Google.com, which is different from Google News Playstand, which is different from Now
[01:14:50.980 --> 01:14:51.980]   Cards news.
[01:14:51.980 --> 01:14:57.300]   There's a need for some consolidation here, but I've been complained about that.
[01:14:57.300 --> 01:14:58.540]   Nothing new with Google, right?
[01:14:58.540 --> 01:15:00.540]   How many messaging apps do they have?
[01:15:00.540 --> 01:15:01.540]   Exactly.
[01:15:01.540 --> 01:15:03.380]   I think it is very nice.
[01:15:03.380 --> 01:15:05.180]   I think that they're trying to...
[01:15:05.180 --> 01:15:12.340]   What I want to know, what I'll be talking about tomorrow, is how they're making their
[01:15:12.340 --> 01:15:15.060]   decisions about quality.
[01:15:15.060 --> 01:15:17.780]   I've talked before in the show about the flight to quality that's occurring.
[01:15:17.780 --> 01:15:20.220]   Google says they're...
[01:15:20.220 --> 01:15:23.660]   Google News, let's basically any site in.
[01:15:23.660 --> 01:15:27.380]   Now how does something rise up in this?
[01:15:27.380 --> 01:15:28.380]   There's a quality factor in there.
[01:15:28.380 --> 01:15:29.540]   What does that quality factor look like?
[01:15:29.540 --> 01:15:30.540]   I'm glad it's there.
[01:15:30.540 --> 01:15:32.540]   It'll be controversial in some cases.
[01:15:32.540 --> 01:15:35.340]   Same thing going on with Facebook.
[01:15:35.340 --> 01:15:36.340]   So we'll see where this goes.
[01:15:36.340 --> 01:15:38.620]   But yeah, I think it's...
[01:15:38.620 --> 01:15:47.020]   I can't really know until I use it for day after day after day, but it looks very nice.
[01:15:47.020 --> 01:15:49.380]   I think that the ideas behind it are good.
[01:15:49.380 --> 01:15:50.860]   Yeah, I think Google's right on.
[01:15:50.860 --> 01:15:54.100]   Google mentioned that they have now this new initiative.
[01:15:54.100 --> 01:15:56.140]   Tell us about this initiative.
[01:15:56.140 --> 01:15:58.820]   They're putting several million dollars into it.
[01:15:58.820 --> 01:16:01.340]   Kind of like the Facebook News initiative or different.
[01:16:01.340 --> 01:16:02.340]   They're different.
[01:16:02.340 --> 01:16:06.100]   Google is investing, not giving away, but investing $300 million in the Google News
[01:16:06.100 --> 01:16:07.300]   initiative.
[01:16:07.300 --> 01:16:12.740]   After Europe went after them, the Germans went after them with the "Nestung Schutzrecht"
[01:16:12.740 --> 01:16:18.260]   and Google did create a fund of $150 million in Europe that they gave away to news innovators
[01:16:18.260 --> 01:16:19.660]   for certain projects.
[01:16:19.660 --> 01:16:21.420]   They're about to get to the last...
[01:16:21.420 --> 01:16:23.940]   Giving away the last euros of that.
[01:16:23.940 --> 01:16:27.100]   They also have Google News guys brought to Europe.
[01:16:27.100 --> 01:16:31.060]   And this week I'm going to News Guys La Am and they had the Google News Lab around the
[01:16:31.060 --> 01:16:32.060]   world.
[01:16:32.060 --> 01:16:33.780]   So Google did a lot of things around news.
[01:16:33.780 --> 01:16:36.900]   The European initiative was so successfully now made it global.
[01:16:36.900 --> 01:16:40.180]   And in every way that we wanted in news except that they're not giving away money to news
[01:16:40.180 --> 01:16:44.580]   organizations, but I also think that that's kind of foolish to wish for.
[01:16:44.580 --> 01:16:45.980]   They are investing a lot of money.
[01:16:45.980 --> 01:16:46.980]   They care about it.
[01:16:46.980 --> 01:16:49.860]   This was the first time that Google News was on the stage at I/O.
[01:16:49.860 --> 01:16:53.860]   It was a big deal for Richard J. Engerson company.
[01:16:53.860 --> 01:16:58.300]   And part of their initiative is also to help business models with news.
[01:16:58.300 --> 01:17:01.900]   The way they're doing it at first, which is the way they have the highest pressure for
[01:17:01.900 --> 01:17:04.780]   especially German publishers, is around subscriptions.
[01:17:04.780 --> 01:17:09.220]   So they announced a few weeks ago I talked about a brief on the show that they've now
[01:17:09.220 --> 01:17:11.860]   made it extremely easy to subscribe.
[01:17:11.860 --> 01:17:16.100]   Using your Google ID, using your Google payment, you come across a challenge, you've seen
[01:17:16.100 --> 01:17:17.100]   five articles.
[01:17:17.100 --> 01:17:18.500]   Do you want to subscribe?
[01:17:18.500 --> 01:17:19.780]   Yes, I do.
[01:17:19.780 --> 01:17:24.220]   One click, your signing is the same, your payment is done, and you're there.
[01:17:24.220 --> 01:17:29.900]   Then further, when you do a search on a topic, you will get the carousel of the most important
[01:17:29.900 --> 01:17:30.900]   things on top.
[01:17:30.900 --> 01:17:34.940]   But below that, you'll get a carousel of your news sources.
[01:17:34.940 --> 01:17:39.820]   And that includes the sources that are made up of the sources that you subscribe to.
[01:17:39.820 --> 01:17:43.460]   Now my affection about that is I also want them to do this around a model of membership
[01:17:43.460 --> 01:17:44.900]   and contribution.
[01:17:44.900 --> 01:17:48.180]   And when it comes to my sources, I should be able to put in sources there that are
[01:17:48.180 --> 01:17:52.540]   free that I like, not just things I pay for.
[01:17:52.540 --> 01:17:56.420]   But those things can be done easily and they don't have their heads at that.
[01:17:56.420 --> 01:18:00.140]   So yeah, I think this is a good thing for news.
[01:18:00.140 --> 01:18:02.060]   They're making friends with news.
[01:18:02.060 --> 01:18:05.380]   It's also their way to poke their eyes, poke the eyes of Facebook a little bit to say,
[01:18:05.380 --> 01:18:07.340]   you know, look how friendly we can be.
[01:18:07.340 --> 01:18:11.740]   And the fact that it came from the stage and from Sundar personally endorsing news, he
[01:18:11.740 --> 01:18:16.220]   has met with many, many news executives in the industry over the last two years.
[01:18:16.220 --> 01:18:18.900]   And I think they're proving that they mean what they say.
[01:18:18.900 --> 01:18:20.380]   Good.
[01:18:20.380 --> 01:18:24.980]   One of the things that everybody's talking about these days, Apple in fact got challenged
[01:18:24.980 --> 01:18:30.380]   by its shareholders, a couple of activist shareholders a few months ago told Apple,
[01:18:30.380 --> 01:18:34.100]   you need to do something about the addictiveness of your iPhone.
[01:18:34.100 --> 01:18:40.060]   So Apple, Google and yesterday Microsoft are all looking at ways to get people to use their
[01:18:40.060 --> 01:18:43.460]   products less, or at least more healthfully.
[01:18:43.460 --> 01:18:45.860]   Google calls it digital well-being.
[01:18:45.860 --> 01:18:49.340]   And this will be a feature also of Android P.
[01:18:49.340 --> 01:18:54.740]   Now you want to talk about creepy, this stuff creepy out.
[01:18:54.740 --> 01:18:59.340]   I don't want anybody nannying me and telling you what I can turn it on.
[01:18:59.340 --> 01:19:02.980]   You don't have to opt into it.
[01:19:02.980 --> 01:19:05.380]   I think it's like, no, I think it's great.
[01:19:05.380 --> 01:19:08.180]   You can mandatorily make it for your kids, right?
[01:19:08.180 --> 01:19:09.180]   Yes.
[01:19:09.180 --> 01:19:10.420]   I think that'll train them into good habits.
[01:19:10.420 --> 01:19:15.020]   And if you're the type of person who knows that, I mean, if you know that if you get
[01:19:15.020 --> 01:19:18.660]   on your phone after 10, you're going to sit there and scroll through and do worthless
[01:19:18.660 --> 01:19:22.940]   crap on it, then you can set this up and it'll help you.
[01:19:22.940 --> 01:19:27.020]   There's a dashboard that is kind of, I already used something called instant that does the
[01:19:27.020 --> 01:19:28.340]   same thing as dashboard does.
[01:19:28.340 --> 01:19:32.220]   It shows you how long you've used the phone and what apps you use the most and how long
[01:19:32.220 --> 01:19:34.300]   you've used them, that kind of thing.
[01:19:34.300 --> 01:19:37.980]   But there's also a very odd thing.
[01:19:37.980 --> 01:19:43.020]   I think this is what you were talking about, Stacey, that you tell it that you go to bed
[01:19:43.020 --> 01:19:48.580]   at 10, for instance, and it will kind of shut down at 10, turn off all notifications
[01:19:48.580 --> 01:19:56.060]   and go and fade to monochrome, fade the black and white, making it less attractive.
[01:19:56.060 --> 01:19:57.060]   I thought that was kind of cool.
[01:19:57.060 --> 01:20:02.460]   I love this because I spend, you're already like, you're sacked out on the couch or something.
[01:20:02.460 --> 01:20:05.100]   You're like, I really should get off my phone.
[01:20:05.100 --> 01:20:10.740]   I don't need to watch, you know, the latest celebrity fight, but you're still there scrolling.
[01:20:10.740 --> 01:20:11.740]   Yep.
[01:20:11.740 --> 01:20:15.100]   Okay, here's the analog.
[01:20:15.100 --> 01:20:18.260]   Imagine that the Samsung motor roll up in the day.
[01:20:18.260 --> 01:20:22.940]   Zee-diff, Zee-diff sold you a TV that said, after you've watched so many hours, the TV
[01:20:22.940 --> 01:20:25.460]   won't go back on and you can't turn it on.
[01:20:25.460 --> 01:20:27.100]   America, what a real thing.
[01:20:27.100 --> 01:20:30.220]   So, they would offer that as an option.
[01:20:30.220 --> 01:20:35.100]   And you're talking, Jeff, to someone who actually programmed a smart plug to turn off.
[01:20:35.100 --> 01:20:36.260]   Of course, you do.
[01:20:36.260 --> 01:20:38.060]   It's like the TV in a smart plug.
[01:20:38.060 --> 01:20:41.460]   So if I didn't take 10,000 steps today, the TV went forward.
[01:20:41.460 --> 01:20:43.260]   Good for you.
[01:20:43.260 --> 01:20:45.860]   Now, how long before you turned that back on?
[01:20:45.860 --> 01:20:46.860]   It's deep off.
[01:20:46.860 --> 01:20:48.660]   Yeah, it lasted like a day or two.
[01:20:48.660 --> 01:20:49.660]   Yeah, okay.
[01:20:49.660 --> 01:20:50.660]   I just want to point out.
[01:20:50.660 --> 01:20:51.660]   What is this drop?
[01:20:51.660 --> 01:20:53.300]   It's a good idea.
[01:20:53.300 --> 01:20:54.300]   We do have, though.
[01:20:54.300 --> 01:20:55.500]   No, we use our router.
[01:20:55.500 --> 01:20:58.140]   The Euro turns off at 10 p.m.
[01:20:58.140 --> 01:21:02.500]   Not for us, but for Michael, because kids will stay up all night.
[01:21:02.500 --> 01:21:04.700]   And so there's no internet after 10 p.m.
[01:21:04.700 --> 01:21:06.780]   I think these are all appropriate.
[01:21:06.780 --> 01:21:08.900]   Google calls that feature "Wines Down", by the way.
[01:21:08.900 --> 01:21:11.020]   I was trying to remember the name.
[01:21:11.020 --> 01:21:15.060]   Is the equivalent of pre-prepping vegetables sticking them in your fridge when you're on
[01:21:15.060 --> 01:21:16.060]   a diet?
[01:21:16.060 --> 01:21:17.060]   That is what this is.
[01:21:17.060 --> 01:21:18.060]   We do that.
[01:21:18.060 --> 01:21:20.460]   I have carrots, sticks, and celery sticks in my fridge.
[01:21:20.460 --> 01:21:24.980]   No, my mother looks at me and says, "Jeff, you've been on that YouTube a very long time
[01:21:24.980 --> 01:21:25.980]   today."
[01:21:25.980 --> 01:21:26.980]   You don't have to do it.
[01:21:26.980 --> 01:21:28.740]   YouTube will also have that, by the way.
[01:21:28.740 --> 01:21:31.620]   Yes, YouTube will have the "How long you've watched" feature.
[01:21:31.620 --> 01:21:32.620]   Oh, God.
[01:21:32.620 --> 01:21:35.860]   And I don't want to end the dashboard telling what I've done all day.
[01:21:35.860 --> 01:21:36.860]   I don't want to know.
[01:21:36.860 --> 01:21:37.860]   You don't have to.
[01:21:37.860 --> 01:21:38.860]   But you don't have to.
[01:21:38.860 --> 01:21:39.860]   This is why I'm not going to do it.
[01:21:39.860 --> 01:21:40.860]   You don't have to.
[01:21:40.860 --> 01:21:41.860]   This is why I like...
[01:21:41.860 --> 01:21:45.700]   It's good for you, Jeff.
[01:21:45.700 --> 01:21:46.700]   Just eat your spinach.
[01:21:46.700 --> 01:21:49.140]   All this data was already there, Jeff.
[01:21:49.140 --> 01:21:50.140]   It's just a chemistry.
[01:21:50.140 --> 01:21:51.500]   It's been on there.
[01:21:51.500 --> 01:21:52.500]   It's on Windows machines.
[01:21:52.500 --> 01:21:56.020]   It's all the information that they're just making it useful if you want to use it, if
[01:21:56.020 --> 01:21:57.020]   you don't want to.
[01:21:57.020 --> 01:21:58.020]   Not a problem.
[01:21:58.020 --> 01:22:00.300]   Sundar Pachai coined a new acronym.
[01:22:00.300 --> 01:22:06.060]   Instead of FOMO, fear of missing out, he calls it "Joe-mo", the joy of missing out, Jeff.
[01:22:06.060 --> 01:22:08.100]   You need to get Joe-mo'd.
[01:22:08.100 --> 01:22:09.100]   You need more joy.
[01:22:09.100 --> 01:22:11.100]   Miriam Jwara, when she's on it...
[01:22:11.100 --> 01:22:13.100]   Don't you tell me to have Miriam Jwara.
[01:22:13.100 --> 01:22:16.100]   I don't want to hear it, Dan.
[01:22:16.100 --> 01:22:19.100]   I see the vitamin D from Shoreline.
[01:22:19.100 --> 01:22:22.100]   It's not enough sunshine.
[01:22:22.100 --> 01:22:28.220]   He's not even still pale as a ghost.
[01:22:28.220 --> 01:22:31.380]   One of the things Miriam Jwara said she liked was the new maps feature.
[01:22:31.380 --> 01:22:33.380]   I really like this.
[01:22:33.380 --> 01:22:36.140]   This was a show winner.
[01:22:36.140 --> 01:22:37.140]   Was the walking.
[01:22:37.140 --> 01:22:38.140]   Really?
[01:22:38.140 --> 01:22:43.180]   Every time I'm a New Yorker, every time I get out, and I try to figure out the blue thing,
[01:22:43.180 --> 01:22:44.180]   where am I going?
[01:22:44.180 --> 01:22:46.180]   This way, where am I going that way?
[01:22:46.180 --> 01:22:47.420]   This is the thing I applauded.
[01:22:47.420 --> 01:22:49.460]   This is the show winner.
[01:22:49.460 --> 01:22:50.460]   Is visual.
[01:22:50.460 --> 01:22:51.620]   Here's where you are.
[01:22:51.620 --> 01:22:52.620]   Go this way.
[01:22:52.620 --> 01:22:53.620]   I agree 100%.
[01:22:53.620 --> 01:22:54.620]   I love that.
[01:22:54.620 --> 01:22:55.620]   I love that.
[01:22:55.620 --> 01:22:56.620]   It's really...
[01:22:56.620 --> 01:22:59.780]   I guess if you drive, you're giving it enough direction information that that's not an issue.
[01:22:59.780 --> 01:23:05.580]   It's when you're walking, and I invariably go the wrong way down the street.
[01:23:05.580 --> 01:23:10.140]   People see me, if you see me around town, I'm always going like this.
[01:23:10.140 --> 01:23:17.540]   This is the figure eight gesture you do to supposedly calibrate the compass.
[01:23:17.540 --> 01:23:18.980]   Then I go like this.
[01:23:18.980 --> 01:23:20.500]   Which way am I going?
[01:23:20.500 --> 01:23:22.300]   What they're doing with Google Maps, I think, is great.
[01:23:22.300 --> 01:23:24.500]   They're going to use augmented reality.
[01:23:24.500 --> 01:23:28.140]   They're going to actually turn on the camera when you're looking at the maps and you're
[01:23:28.140 --> 01:23:29.420]   walking around.
[01:23:29.420 --> 01:23:30.700]   Show the street.
[01:23:30.700 --> 01:23:32.180]   Show where you're looking.
[01:23:32.180 --> 01:23:33.500]   I got to hear a car.
[01:23:33.500 --> 01:23:34.500]   You got it.
[01:23:34.500 --> 01:23:35.500]   There you go.
[01:23:35.500 --> 01:23:37.620]   Superimpose the map on top of that.
[01:23:37.620 --> 01:23:40.660]   It's very clear that it's time to turn right.
[01:23:40.660 --> 01:23:43.820]   They're playing with the idea you can even follow a nice little animal.
[01:23:43.820 --> 01:23:44.980]   They had a fox.
[01:23:44.980 --> 01:23:45.980]   Yeah.
[01:23:45.980 --> 01:23:48.260]   I don't mind that.
[01:23:48.260 --> 01:23:51.620]   I will turn on the fox if they turn that on.
[01:23:51.620 --> 01:23:53.460]   I think that's nice.
[01:23:53.460 --> 01:23:54.460]   Lens.
[01:23:54.460 --> 01:23:56.300]   I was excited about the fox.
[01:23:56.300 --> 01:23:58.300]   Kids will love that.
[01:23:58.300 --> 01:24:00.300]   Yeah, anybody would.
[01:24:00.300 --> 01:24:02.300]   I think this will be a nice one.
[01:24:02.300 --> 01:24:06.460]   It's also a wonderful place to do merchandising tie-in.
[01:24:06.460 --> 01:24:10.380]   Imagine following BDA or never.
[01:24:10.380 --> 01:24:11.380]   Yeah.
[01:24:11.380 --> 01:24:15.180]   And once again, they showed this last year and it never really came out.
[01:24:15.180 --> 01:24:19.260]   The idea that as you're looking on this augmented reality, signs pop up.
[01:24:19.260 --> 01:24:21.580]   You see the street and signs pop up where businesses are.
[01:24:21.580 --> 01:24:23.100]   They didn't implement it.
[01:24:23.100 --> 01:24:27.140]   They showed it again, maybe they'll implement it this year.
[01:24:27.140 --> 01:24:28.140]   That's all kind of part of lens.
[01:24:28.140 --> 01:24:36.260]   In fact, lens will now, if you show it an outfit, you can tap parts of the outfit and say,
[01:24:36.260 --> 01:24:42.860]   how do I get that bag or bags like it?
[01:24:42.860 --> 01:24:44.780]   Those features, I've never been pleased with those features.
[01:24:44.780 --> 01:24:46.220]   I've seen a lot of them.
[01:24:46.220 --> 01:24:47.220]   Yeah.
[01:24:47.220 --> 01:24:48.220]   They remain.
[01:24:48.220 --> 01:24:50.060]   I'll test it and see if it's any good.
[01:24:50.060 --> 01:24:52.300]   It's like image search, but it's built in, right?
[01:24:52.300 --> 01:24:54.940]   Yeah, but most of them suck.
[01:24:54.940 --> 01:24:56.100]   Yeah, I agree.
[01:24:56.100 --> 01:25:04.420]   I do like, again, remember last year they demoed the instantaneous translation using PixelBuds?
[01:25:04.420 --> 01:25:08.140]   And then when we got the PixelBuds, it was nothing like the demo.
[01:25:08.140 --> 01:25:11.220]   Yeah, this is what I was worried about with duplex.
[01:25:11.220 --> 01:25:12.220]   Yeah.
[01:25:12.220 --> 01:25:13.220]   So we'll see.
[01:25:13.220 --> 01:25:18.500]   It's always got to remember demos are a different animal, although this Google lens demo where
[01:25:18.500 --> 01:25:20.340]   you could take a picture.
[01:25:20.340 --> 01:25:29.940]   She's aiming her camera at a recipe book using lens to, not even taking a picture, show the
[01:25:29.940 --> 01:25:32.740]   text and then copying and pasting it.
[01:25:32.740 --> 01:25:35.580]   Let me just send the picture to someone.
[01:25:35.580 --> 01:25:37.220]   I send people recipes this way.
[01:25:37.220 --> 01:25:39.860]   I do too, but then you can paste it into something.
[01:25:39.860 --> 01:25:41.420]   I don't know.
[01:25:41.420 --> 01:25:44.500]   I'd love to do this for researching and playing quotes out of books.
[01:25:44.500 --> 01:25:45.500]   Sure.
[01:25:45.500 --> 01:25:46.500]   Sure.
[01:25:46.500 --> 01:25:50.300]   Again, we'll wait and see how well that works.
[01:25:50.300 --> 01:25:56.220]   You can point your picture at a poster of a musician or an ad and get his music automatically.
[01:25:56.220 --> 01:25:59.900]   This is like just image search built in mobile and doing a lot more.
[01:25:59.900 --> 01:26:06.140]   They wrapped up talking about self-driving cars and Waymo and they're claiming a 100
[01:26:06.140 --> 01:26:10.940]   times reduction in error rate using deep learning.
[01:26:10.940 --> 01:26:15.860]   Now they didn't mention the fact that there was a Waymo accident in Arizona just last
[01:26:15.860 --> 01:26:17.340]   week.
[01:26:17.340 --> 01:26:22.140]   I wasn't around for it, but I understand it wasn't the car's fault that it was hit by
[01:26:22.140 --> 01:26:24.140]   something.
[01:26:24.140 --> 01:26:28.580]   Yeah, I think something veered over and hit it.
[01:26:28.580 --> 01:26:29.580]   Right.
[01:26:29.580 --> 01:26:34.220]   They showed a picture of people in inflatable dinosaur suits and how the car knew that
[01:26:34.220 --> 01:26:36.380]   was people or something or something like that.
[01:26:36.380 --> 01:26:38.180]   I guess something you shouldn't hit.
[01:26:38.180 --> 01:26:39.180]   Maybe that.
[01:26:39.180 --> 01:26:44.620]   However, if it can recognize a mine and hit that, that's fine.
[01:26:44.620 --> 01:26:45.620]   Yeah.
[01:26:45.620 --> 01:26:48.300]   You can hit it.
[01:26:48.300 --> 01:26:49.860]   Right.
[01:26:49.860 --> 01:26:54.580]   It showed, oh, I thought this was good how it can handle inclement weather.
[01:26:54.580 --> 01:26:59.140]   They showed a snowy, snowy street and this is all the noise caused by the snow, which
[01:26:59.140 --> 01:27:03.820]   makes it very difficult to drive, but they showed how machine learning can strip the
[01:27:03.820 --> 01:27:10.500]   noise out and actually give you a clearer view than you have as a human.
[01:27:10.500 --> 01:27:11.860]   I think that's exciting.
[01:27:11.860 --> 01:27:13.700]   I think that's very cool.
[01:27:13.700 --> 01:27:17.980]   Some waymo, they're moving along.
[01:27:17.980 --> 01:27:24.100]   They act as if it's just around the corner that you're going to be able to get a waymo.
[01:27:24.100 --> 01:27:25.860]   You don't think so, Kevin?
[01:27:25.860 --> 01:27:27.100]   I don't think so.
[01:27:27.100 --> 01:27:30.820]   I mean, they did say that in Phoenix where they've been doing the trials, they're going
[01:27:30.820 --> 01:27:34.340]   to have by the end of the year, I believe, a transportation center for this.
[01:27:34.340 --> 01:27:37.100]   But I mean, that's one very large area.
[01:27:37.100 --> 01:27:38.820]   Yes, but just one area.
[01:27:38.820 --> 01:27:40.220]   This is going to take a long time to roll out.
[01:27:40.220 --> 01:27:44.660]   There's still a lot of laws to deal with amongst different states.
[01:27:44.660 --> 01:27:46.500]   This is baby steps.
[01:27:46.500 --> 01:27:47.500]   Yeah.
[01:27:47.500 --> 01:27:51.380]   So that, in a nutshell, is Google I/O.
[01:27:51.380 --> 01:27:52.620]   Of course, it continues on.
[01:27:52.620 --> 01:27:55.460]   Right now, they're doing the developer talk.
[01:27:55.460 --> 01:27:56.940]   Anything more to be said there?
[01:27:56.940 --> 01:28:00.300]   I remember how excited everybody was last year when they said the word "Kotlin."
[01:28:00.300 --> 01:28:02.900]   Oh, yeah.
[01:28:02.900 --> 01:28:04.460]   People are still excited about Kotlin.
[01:28:04.460 --> 01:28:05.460]   Yeah, that's it.
[01:28:05.460 --> 01:28:06.460]   They have Kotlin.
[01:28:06.460 --> 01:28:07.460]   They have Kotlin.
[01:28:07.460 --> 01:28:08.460]   They have Kotlin.
[01:28:08.460 --> 01:28:12.020]   Programming language that you can write and write apps in.
[01:28:12.020 --> 01:28:17.780]   It's interesting how they're changing all of the applications that to support creating
[01:28:17.780 --> 01:28:18.780]   apps for their platform.
[01:28:18.780 --> 01:28:24.140]   They've got Flutter, they've got Kotlin, they've got Dart, they still use Go.
[01:28:24.140 --> 01:28:27.500]   I don't know if this all still has to do with Oracle and Java or what the deal is, but I
[01:28:27.500 --> 01:28:28.500]   mean, this is like--
[01:28:28.500 --> 01:28:29.940]   I think it started that way.
[01:28:29.940 --> 01:28:35.620]   But I think it's also an occupational hazard amongst engineers, computer scientists.
[01:28:35.620 --> 01:28:37.820]   They just love new languages and everybody's got to write.
[01:28:37.820 --> 01:28:38.820]   You struggle with one language.
[01:28:38.820 --> 01:28:39.820]   Right.
[01:28:39.820 --> 01:28:40.820]   You struggle with five.
[01:28:40.820 --> 01:28:42.500]   Yeah, yeah, absolutely.
[01:28:42.500 --> 01:28:46.220]   They'll always be writing new languages.
[01:28:46.220 --> 01:28:47.420]   That's why I'm a Lisp fan.
[01:28:47.420 --> 01:28:49.940]   It's a language that you can write languages in.
[01:28:49.940 --> 01:28:50.940]   Perfect.
[01:28:50.940 --> 01:28:51.940]   Perfect.
[01:28:51.940 --> 01:29:00.420]   You basically write a application-specific language for every program you write.
[01:29:00.420 --> 01:29:01.900]   Anything else anybody wants to say?
[01:29:01.900 --> 01:29:02.900]   Anything going on?
[01:29:02.900 --> 01:29:03.900]   Just a little bit of news.
[01:29:03.900 --> 01:29:09.860]   Yes, I just saw that Facebook, David Marcus, is leaving Messenger to have a new team dedicated
[01:29:09.860 --> 01:29:11.460]   to blockchain.
[01:29:11.460 --> 01:29:13.460]   What that means, I have no idea.
[01:29:13.460 --> 01:29:15.740]   Every Meatloaf has to have a blockchain gravy.
[01:29:15.740 --> 01:29:16.740]   Oh, Lord.
[01:29:16.740 --> 01:29:19.860]   Well, of course, WhatsApp's founders are moving on.
[01:29:19.860 --> 01:29:24.420]   I wonder what that all means.
[01:29:24.420 --> 01:29:29.820]   Facebook is making its biggest executive shuffle in company history, according to Kurt
[01:29:29.820 --> 01:29:31.220]   Wagner and Recode.
[01:29:31.220 --> 01:29:32.220]   Wow.
[01:29:32.220 --> 01:29:33.220]   WhatsApp.
[01:29:33.220 --> 01:29:36.140]   And Facebook's core apps are splitting.
[01:29:36.140 --> 01:29:44.260]   I'm sorry, are getting new leaders as part of a massive executive shakeup.
[01:29:44.260 --> 01:29:46.260]   Reorg.
[01:29:46.260 --> 01:29:47.260]   We'll have more on that.
[01:29:47.260 --> 01:29:49.460]   I'm sure next week.
[01:29:49.460 --> 01:29:50.940]   Anybody, anything?
[01:29:50.940 --> 01:29:51.940]   Anybody?
[01:29:51.940 --> 01:29:52.940]   Anybody?
[01:29:52.940 --> 01:29:53.940]   Blockchain?
[01:29:53.940 --> 01:29:54.940]   Anybody?
[01:29:54.940 --> 01:29:57.780]   On billions on Sunday night- I'm all mellow now.
[01:29:57.780 --> 01:30:02.660]   And then billions Sunday night, they gave a guy a USB key.
[01:30:02.660 --> 01:30:06.140]   He said, "There's a million dollars in cryptocurrency on here."
[01:30:06.140 --> 01:30:07.140]   A stick.
[01:30:07.140 --> 01:30:08.140]   They call it.
[01:30:08.140 --> 01:30:10.420]   And like you, he didn't river the password.
[01:30:10.420 --> 01:30:11.420]   Yeah.
[01:30:11.420 --> 01:30:12.420]   Yeah.
[01:30:12.420 --> 01:30:15.620]   I have to say, I see stuff like that and go, "Are you going to give him the password?
[01:30:15.620 --> 01:30:16.980]   Are you going to give him the password?
[01:30:16.980 --> 01:30:18.220]   Are you going to give him the...
[01:30:18.220 --> 01:30:19.620]   He's a kid in the stick.
[01:30:19.620 --> 01:30:22.900]   Give him the password.
[01:30:22.900 --> 01:30:24.180]   Westworld, we like it.
[01:30:24.180 --> 01:30:25.460]   I haven't seen it yet.
[01:30:25.460 --> 01:30:26.460]   Should I?
[01:30:26.460 --> 01:30:27.460]   I haven't seen it.
[01:30:27.460 --> 01:30:30.340]   I don't watch current shows.
[01:30:30.340 --> 01:30:31.340]   Yeah.
[01:30:31.340 --> 01:30:34.260]   You know what I was thinking now that I'm like two or three episodes behind because we were
[01:30:34.260 --> 01:30:35.460]   out of the country?
[01:30:35.460 --> 01:30:40.660]   Maybe if I could just force myself, I have to do that switch thing you did that until
[01:30:40.660 --> 01:30:42.940]   the series is over, I can't start watching it.
[01:30:42.940 --> 01:30:46.340]   I'm catching up on Silicon Valley now, which is just stellar this season.
[01:30:46.340 --> 01:30:47.340]   Is it?
[01:30:47.340 --> 01:30:48.340]   Okay, good.
[01:30:48.340 --> 01:30:50.340]   Because it went through a dip.
[01:30:50.340 --> 01:30:51.340]   Yeah, but it's...
[01:30:51.340 --> 01:30:52.860]   I think it's amazing this season.
[01:30:52.860 --> 01:30:53.860]   Just amazing.
[01:30:53.860 --> 01:30:54.860]   All right.
[01:30:54.860 --> 01:30:55.860]   Good.
[01:30:55.860 --> 01:30:56.860]   I stopped watching.
[01:30:56.860 --> 01:31:00.540]   - I, the dystopia of Handmade Sale was...
[01:31:00.540 --> 01:31:01.760]   - Handmade Sale, I couldn't watch it.
[01:31:01.760 --> 01:31:02.600]   - Good much.
[01:31:02.600 --> 01:31:03.460]   - Yeah, too much.
[01:31:03.460 --> 01:31:04.900]   - Same thing, I watched it the first episode
[01:31:04.900 --> 01:31:06.820]   of it was like, oh God.
[01:31:06.820 --> 01:31:12.120]   - Yeah, I, it felt more torture porny and not...
[01:31:12.120 --> 01:31:12.960]   - Yeah.
[01:31:12.960 --> 01:31:14.800]   - I was like, yeah, I don't need to see this, at all.
[01:31:14.800 --> 01:31:16.660]   - Yeah, yeah.
[01:31:16.660 --> 01:31:19.140]   I, I, I pretty much only watch comedies now.
[01:31:19.140 --> 01:31:22.640]   - That's the way of the world now, yeah.
[01:31:22.640 --> 01:31:24.300]   - Only one of watch comedies.
[01:31:24.300 --> 01:31:26.660]   Kevin Tofel's got a great new website
[01:31:26.660 --> 01:31:29.060]   you might want to spend some time checking on that.
[01:31:29.060 --> 01:31:30.460]   - I'm gonna spend forever there.
[01:31:30.460 --> 01:31:32.580]   Kevin is doing God's work.
[01:31:32.580 --> 01:31:33.420]   - About.
[01:31:33.420 --> 01:31:36.180]   - Working the most important kind of computer technology
[01:31:36.180 --> 01:31:37.500]   any shock can use today.
[01:31:37.500 --> 01:31:39.740]   - I agree about Chromebooks.com
[01:31:39.740 --> 01:31:43.260]   and I'm sure we'll see more crostini coverage there.
[01:31:43.260 --> 01:31:44.740]   - You will, you will.
[01:31:44.740 --> 01:31:47.220]   - Get ready for a brisket or bros skita
[01:31:47.220 --> 01:31:48.340]   as the chatroom calls it.
[01:31:48.340 --> 01:31:51.340]   - It's, it's a shame that the Chrome, Chrome OS Chromebooks
[01:31:51.340 --> 01:31:53.740]   just don't get the coverage and treatment
[01:31:53.740 --> 01:31:54.780]   from all the major tech sites.
[01:31:54.780 --> 01:31:55.700]   It's always an afterthought.
[01:31:55.700 --> 01:31:58.740]   So that's why I launched the site to focus just on them.
[01:31:58.740 --> 01:32:00.620]   - I'm seriously grateful you did, Kevin.
[01:32:00.620 --> 01:32:02.820]   - And you, as you said an interesting thing on this site,
[01:32:02.820 --> 01:32:05.540]   you said that you thought that the Chrome tablets
[01:32:05.540 --> 01:32:07.580]   are gonna be a game changer.
[01:32:07.580 --> 01:32:12.420]   - I do because they're basically the first mobile device
[01:32:12.420 --> 01:32:15.460]   that has a full desktop browser experience
[01:32:15.460 --> 01:32:18.020]   which is a big deal if you use web apps
[01:32:18.020 --> 01:32:20.580]   which still has the mobile app experience of,
[01:32:20.580 --> 01:32:21.740]   in this case, Android apps.
[01:32:21.740 --> 01:32:23.980]   So nothing else does that.
[01:32:23.980 --> 01:32:26.740]   So this is, this is why I sold the iPad Pro that I love.
[01:32:26.740 --> 01:32:28.220]   Great device and got the Pixelbook.
[01:32:28.220 --> 01:32:29.140]   - Interesting.
[01:32:29.140 --> 01:32:34.140]   - Well, I have this my Skype app, my Netflix, my Showtime.
[01:32:34.140 --> 01:32:37.780]   They are very, very flaky and don't work
[01:32:37.780 --> 01:32:41.700]   half the time on, um, through Chrome.
[01:32:41.700 --> 01:32:44.940]   - Yeah, I, I, I know that in certain cases,
[01:32:44.940 --> 01:32:46.340]   some of the apps aren't 100%.
[01:32:46.340 --> 01:32:47.180]   It's ironic.
[01:32:47.180 --> 01:32:48.940]   I was literally just talking to somebody on my old team
[01:32:48.940 --> 01:32:52.180]   at Google yesterday and we were chatting about Android apps
[01:32:52.180 --> 01:32:53.820]   and how they're working for me and how they're not.
[01:32:53.820 --> 01:32:56.700]   So I try and provide feedback and that's one of the things
[01:32:56.700 --> 01:32:57.540]   that I do.
[01:32:57.540 --> 01:32:59.900]   - I think also there's not a lot of incentive
[01:32:59.900 --> 01:33:02.940]   for those companies because they don't feel copy protected
[01:33:02.940 --> 01:33:05.100]   on platforms that they don't like,
[01:33:05.100 --> 01:33:07.260]   not that aren't Windows and Macintosh.
[01:33:07.260 --> 01:33:10.060]   I'm sure they're just as copy protected.
[01:33:10.060 --> 01:33:14.940]   - Yeah, you cannot use Netflix or any DRM type content app
[01:33:14.940 --> 01:33:17.740]   on, on a Chromebook if you're in developer mode for example.
[01:33:17.740 --> 01:33:19.060]   A lot of those things won't work.
[01:33:19.060 --> 01:33:21.460]   Same if you, you know, unlock the bootloader
[01:33:21.460 --> 01:33:24.180]   of your Android phone, same, same deal for the DRM key.
[01:33:24.180 --> 01:33:25.660]   So I totally get that.
[01:33:25.660 --> 01:33:27.580]   - I think, yeah, I think they're, they're kind of,
[01:33:27.580 --> 01:33:29.900]   they're, they're on the fence a little bit about these.
[01:33:29.900 --> 01:33:30.820]   - Yeah.
[01:33:30.820 --> 01:33:31.660]   - Yeah.
[01:33:31.660 --> 01:33:33.140]   - Go Stacy, they're right now.
[01:33:33.140 --> 01:33:36.620]   - We're starting the IO, Android Things scavenger hunt.
[01:33:36.620 --> 01:33:38.300]   - Get in there, get in there.
[01:33:38.300 --> 01:33:39.220]   - Go, go, go.
[01:33:39.220 --> 01:33:40.060]   - That's right.
[01:33:40.060 --> 01:33:41.460]   We didn't talk about Android Things.
[01:33:41.460 --> 01:33:42.940]   Well, Kevin and I'll talk about it on our show.
[01:33:42.940 --> 01:33:43.780]   That's fine.
[01:33:43.780 --> 01:33:47.140]   - Well, anything you want to say briefly about it?
[01:33:47.140 --> 01:33:48.820]   Is it a big, a big deal?
[01:33:49.940 --> 01:33:53.500]   - Yeah, it's a big deal that it's finally out, but.
[01:33:53.500 --> 01:33:54.340]   - It's a big deal.
[01:33:54.340 --> 01:33:55.900]   - So it was a quick glance, yeah, the quick glance I had,
[01:33:55.900 --> 01:33:58.340]   it was like, it's a big deal for Google,
[01:33:58.340 --> 01:33:59.980]   but I don't think they go far enough yet.
[01:33:59.980 --> 01:34:02.700]   So I'm kind of like, hey, but that's just me.
[01:34:02.700 --> 01:34:03.540]   - Okay.
[01:34:03.540 --> 01:34:04.380]   - Kevin.
[01:34:04.380 --> 01:34:05.660]   - It helps, it helps with the security aspect
[01:34:05.660 --> 01:34:08.580]   and updating devices because Google will,
[01:34:08.580 --> 01:34:10.620]   will keep these devices updated for at least three years.
[01:34:10.620 --> 01:34:12.580]   And in IOT, keeping devices updated,
[01:34:12.580 --> 01:34:14.660]   there's no incentive for companies to do that either.
[01:34:14.660 --> 01:34:15.500]   So.
[01:34:15.500 --> 01:34:17.740]   - So Microsoft just announced 10 years.
[01:34:17.740 --> 01:34:18.580]   - Yeah.
[01:34:18.580 --> 01:34:21.740]   - And Qualcomm actually has announced seven years
[01:34:21.740 --> 01:34:25.940]   and 10 years respectively on their Snapdragon for Embedded.
[01:34:25.940 --> 01:34:28.980]   So I actually looked at this and Kevin and I
[01:34:28.980 --> 01:34:30.060]   were I aming beforehand.
[01:34:30.060 --> 01:34:32.100]   I was like, Google, this sucks.
[01:34:32.100 --> 01:34:35.180]   I think I did WTF Google, but can you pay
[01:34:35.180 --> 01:34:36.500]   to get additional support?
[01:34:36.500 --> 01:34:40.460]   'Cause honestly, that's not a game changer at all to me.
[01:34:40.460 --> 01:34:42.380]   - My understanding is yes, that you can,
[01:34:42.380 --> 01:34:45.380]   if you're a device maker, yes, after three years.
[01:34:45.380 --> 01:34:47.940]   - You probably talked about it on the show last week
[01:34:47.940 --> 01:34:50.700]   or the week before, but didn't Microsoft announce
[01:34:50.700 --> 01:34:52.660]   Azure, what is it, Sphere?
[01:34:52.660 --> 01:34:54.500]   - Sphere, Project Sphere.
[01:34:54.500 --> 01:34:55.340]   - Project Sphere?
[01:34:55.340 --> 01:34:58.220]   - Yes, we did a hold thing on that.
[01:34:58.220 --> 01:34:59.060]   - I'm sure you did.
[01:34:59.060 --> 01:35:01.420]   - Okay, 'cause that I think is very, it's timely.
[01:35:01.420 --> 01:35:03.300]   I'd like to see Google do the same thing where,
[01:35:03.300 --> 01:35:05.660]   and I'm sure Qualcomm will, where they kind of
[01:35:05.660 --> 01:35:10.580]   can reach a low-end Chinese IOT manufacturer
[01:35:10.580 --> 01:35:14.020]   can use something like that to make a device that is in fact
[01:35:14.020 --> 01:35:17.460]   updatable, secure, and reliable, and consumers could buy it
[01:35:17.460 --> 01:35:18.740]   with some confidence, right?
[01:35:18.740 --> 01:35:19.580]   Is that?
[01:35:19.580 --> 01:35:20.660]   - Right, that's the idea.
[01:35:20.660 --> 01:35:23.980]   Now, wait, I have to ask, because I saw that they showed
[01:35:23.980 --> 01:35:26.900]   the Lenovo Smart Display, because they announced
[01:35:26.900 --> 01:35:27.740]   availability.
[01:35:27.740 --> 01:35:28.580]   - No.
[01:35:28.580 --> 01:35:29.900]   - Oh, they said Summer.
[01:35:29.900 --> 01:35:31.220]   - I think July, I think.
[01:35:31.220 --> 01:35:32.180]   - July, yeah.
[01:35:32.180 --> 01:35:34.540]   And there'll be three to choose from, not just LGs,
[01:35:34.540 --> 01:35:36.820]   although, or Lenovo, and I think you and I both
[01:35:36.820 --> 01:35:38.060]   want the Lenovo one.
[01:35:38.060 --> 01:35:38.900]   - Yeah.
[01:35:38.900 --> 01:35:41.220]   - Well, one of us should probably get a different one.
[01:35:41.220 --> 01:35:42.060]   - Sorry, I'll probably deliver one.
[01:35:42.060 --> 01:35:42.900]   - Super scissors.
[01:35:42.900 --> 01:35:44.260]   (laughing)
[01:35:44.260 --> 01:35:45.100]   - Yeah, it's got--
[01:35:45.100 --> 01:35:46.820]   - I'll have my bought order at Lenovo one.
[01:35:46.820 --> 01:35:48.700]   It's got a bigger screen than the Echo Show.
[01:35:48.700 --> 01:35:49.900]   I'm a big fan of the Echo Show.
[01:35:49.900 --> 01:35:51.860]   They have a 10 inch screen available.
[01:35:51.860 --> 01:35:54.060]   I think this is gonna be, and they did do a demo on it.
[01:35:54.060 --> 01:35:57.180]   I think this is gonna be a very interesting product.
[01:35:57.180 --> 01:35:59.020]   - You know what was awesome about the Echo Show?
[01:35:59.020 --> 01:36:01.020]   I'm finding, I'm liking it more and more.
[01:36:01.020 --> 01:36:01.860]   - Oh, I love it.
[01:36:01.860 --> 01:36:04.340]   - And I think it's actually 160 right now for a Mother's Day,
[01:36:04.340 --> 01:36:07.140]   but I actually, last night, we were, Microsoft did a thing
[01:36:07.140 --> 01:36:09.820]   at the Chiluis Gardens, which is lovely.
[01:36:09.820 --> 01:36:13.340]   And I called my daughter on the Echo Show,
[01:36:13.340 --> 01:36:15.420]   and I just took her on a little tour with me.
[01:36:15.420 --> 01:36:16.820]   - They're awesome.
[01:36:16.820 --> 01:36:17.900]   - And that was really fun.
[01:36:17.900 --> 01:36:20.020]   - Yeah, I think that's awesome.
[01:36:20.020 --> 01:36:23.660]   I might have to get an Echo Show for my daughter,
[01:36:23.660 --> 01:36:27.180]   because my mother will only talk to her on FaceTime,
[01:36:27.180 --> 01:36:29.340]   and Abby no longer has an Apple device of any kind.
[01:36:29.340 --> 01:36:33.620]   She's got a Pixel, and she's got a Note 8,
[01:36:33.620 --> 01:36:37.180]   a Samsung Note 8, and so she can't use FaceTime,
[01:36:37.180 --> 01:36:41.860]   and Mom won't use Duo, but Mom does have an Echo Show.
[01:36:41.860 --> 01:36:43.380]   I think if I got an Echo Show for Abby,
[01:36:43.380 --> 01:36:45.220]   then they could still talk to each other.
[01:36:45.220 --> 01:36:46.500]   - That's the solution.
[01:36:46.500 --> 01:36:49.660]   I just answered my own question.
[01:36:49.660 --> 01:36:51.380]   - God, that we have to even think of that hard,
[01:36:51.380 --> 01:36:52.980]   is just wrong.
[01:36:52.980 --> 01:36:57.180]   - It just annoys me that Apple is so siloed, it's really sad.
[01:36:57.180 --> 01:36:58.780]   Hello, Bijon!
[01:36:58.780 --> 01:37:00.740]   How are you today?
[01:37:00.740 --> 01:37:02.780]   Oh, Normie is such a cute little doggy,
[01:37:02.780 --> 01:37:04.540]   and you keep him so clean.
[01:37:04.540 --> 01:37:06.020]   - I'm making it clean.
[01:37:06.020 --> 01:37:07.060]   - Oh, he does that himself.
[01:37:07.060 --> 01:37:08.580]   - Oh, he does?
[01:37:08.580 --> 01:37:09.700]   - He needs a haircut, though.
[01:37:09.700 --> 01:37:10.660]   - They don't shed, right?
[01:37:10.660 --> 01:37:12.220]   Bijons don't shed.
[01:37:12.220 --> 01:37:13.060]   - That's correct.
[01:37:13.060 --> 01:37:14.500]   They're hypoallergenic, they do not shed.
[01:37:14.500 --> 01:37:17.060]   - Oh, but so it's good for people without allergies,
[01:37:17.060 --> 01:37:17.980]   but would you believe it?
[01:37:17.980 --> 01:37:19.060]   He has allergies.
[01:37:19.060 --> 01:37:20.260]   (laughs)
[01:37:20.260 --> 01:37:21.100]   - I believe it.
[01:37:21.100 --> 01:37:22.340]   - He has to get in medicine every day to pull it.
[01:37:22.340 --> 01:37:23.980]   - That's a problem with pure breast.
[01:37:23.980 --> 01:37:26.300]   Hey, does he smell like a dog?
[01:37:26.300 --> 01:37:29.020]   - Not really.
[01:37:29.020 --> 01:37:31.820]   - See, 'cause that's the objection Lisa has,
[01:37:31.820 --> 01:37:33.780]   is that dogs have a doggy smell.
[01:37:33.780 --> 01:37:35.780]   They smell like dogs.
[01:37:35.780 --> 01:37:36.620]   - No, he don't.
[01:37:36.620 --> 01:37:40.180]   - But I remember Arpeggion always smelled like ivory soap.
[01:37:40.180 --> 01:37:42.140]   And looks-- - That's weird.
[01:37:42.140 --> 01:37:43.700]   (laughs)
[01:37:43.700 --> 01:37:45.500]   - Like, like, like.
[01:37:45.500 --> 01:37:46.660]   All right, thank you.
[01:37:46.660 --> 01:37:48.180]   Thank you so much, Tacy.
[01:37:48.180 --> 01:37:50.060]   Thank you so much, Kevin.
[01:37:50.060 --> 01:37:54.100]   You guys have fun on your All About IOT podcast.
[01:37:54.100 --> 01:37:55.700]   You can find that as-- - Aw, that's like the end
[01:37:55.700 --> 01:37:57.620]   of Lassie when Lassie used to wave.
[01:37:57.620 --> 01:37:59.860]   (vocalizing)
[01:37:59.860 --> 01:38:05.660]   - Bye bye, Lassie.
[01:38:05.660 --> 01:38:07.700]   Jeff Jarvis. - Are you crying?
[01:38:07.700 --> 01:38:09.380]   I used to cry at the end of Lassie.
[01:38:09.380 --> 01:38:10.380]   - It's tequila time. - And you're gonna
[01:38:10.380 --> 01:38:11.300]   be funny for it.
[01:38:11.300 --> 01:38:13.420]   - You know where I cried at the end of Shane?
[01:38:14.260 --> 01:38:15.740]   That's where I cried.
[01:38:15.740 --> 01:38:19.340]   So, Jeff, go enjoy your margaritas at Google I/O.
[01:38:19.340 --> 01:38:20.700]   Thank you for-- - God, I'm done with that.
[01:38:20.700 --> 01:38:23.020]   - For making it happen here.
[01:38:23.020 --> 01:38:23.980]   We appreciate it.
[01:38:23.980 --> 01:38:26.420]   Thanks to all of you for joining us.
[01:38:26.420 --> 01:38:29.660]   We do have, of course, a complete live coverage
[01:38:29.660 --> 01:38:31.180]   of the Google I/O keynote.
[01:38:31.180 --> 01:38:33.020]   Father Robert Ballisar and I did earlier today,
[01:38:33.020 --> 01:38:34.900]   and you can find that on the Twit Live specials feed
[01:38:34.900 --> 01:38:36.380]   any minute now.
[01:38:36.380 --> 01:38:37.860]   We are gonna continue on with security now,
[01:38:37.860 --> 01:38:38.620]   a little bit delayed.
[01:38:38.620 --> 01:38:40.940]   Tomorrow is Apple Day.
[01:38:40.940 --> 01:38:42.380]   We've moved everything out of the way
[01:38:42.380 --> 01:38:44.220]   so that we could do our coverage of Microsoft Build
[01:38:44.220 --> 01:38:45.300]   and Google I/O.
[01:38:45.300 --> 01:38:50.300]   So, I/O us today begins at 9 a.m. Pacific on Wednesday,
[01:38:50.300 --> 01:38:52.500]   and Macbrick weekly will follow immediately
[01:38:52.500 --> 01:38:54.660]   after around around 11 a.m. Pacific
[01:38:54.660 --> 01:38:58.500]   on most of these same Twit stations.
[01:38:58.500 --> 01:39:01.740]   We do this week in Google normally on Wednesdays,
[01:39:01.740 --> 01:39:06.260]   about 1.30 p.m. Pacific, 4.30 Eastern, 28.30 UTC.
[01:39:06.260 --> 01:39:07.540]   Stop by, say hi.
[01:39:07.540 --> 01:39:10.460]   You can watch live feed at twit.tv/live.
[01:39:10.460 --> 01:39:12.940]   Join the chat at IRC.twit.tv,
[01:39:12.940 --> 01:39:16.780]   or even join us in studio by emailing tickets at twit.tv.
[01:39:16.780 --> 01:39:18.700]   There's no charge, we'd love to have you.
[01:39:18.700 --> 01:39:22.180]   And if you wanna download any show we do,
[01:39:22.180 --> 01:39:25.180]   we have on demand audio and video
[01:39:25.180 --> 01:39:27.180]   for this show at twit.tv/twit.
[01:39:27.180 --> 01:39:30.020]   It's all on the website, twit.tv.
[01:39:30.020 --> 01:39:31.740]   Thanks for joining us, we'll see you next time
[01:39:31.740 --> 01:39:33.300]   on This Week in Google.
[01:39:33.300 --> 01:39:34.140]   Bye bye.
[01:39:34.140 --> 01:39:36.720]   (upbeat music)
[01:39:36.720 --> 01:39:39.300]   (upbeat music)
[01:39:39.300 --> 01:39:41.880]   (upbeat music)

