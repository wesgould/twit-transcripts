;FFMETADATA1
title=Grackles, Nuthatches, and Swifts, Oh My!
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=449
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf58.76.100
Failed to align segment (" Nothing. It's been really boring. Nothing's happened."): backtrack failed, resorting to original...
Failed to align segment (" 270,000."): no characters in this segment found in model dictionary, resorting to original...
Failed to align segment (" 2000."): no characters in this segment found in model dictionary, resorting to original...
Failed to align segment (" You could just ban wholesale."): backtrack failed, resorting to original...
Failed to align segment (" Thank you."): backtrack failed, resorting to original...
Failed to align segment (" And so no one-"): backtrack failed, resorting to original...
Speaker: SPEAKER_01
Transcript:  It's time for Twig this week at Google. Jeff Jarvis is here. Stacey Higginbotham.  We'll talk about the self-driving car from Uber.  Is it the end of the line? We'll talk about Facebook. It is the end of the line.  And Congress has done a shameful thing. It's all coming up next.  Are you surprised on Twig?  Netcasts you love.

Speaker: SPEAKER_03
Transcript:  From people you trust.

Speaker: SPEAKER_01
Transcript:  This is Twig.  The bandwidth for this week in Google is provided by Cashfly.  C-A-C-H-E-F-L-Y dot com.  This is Twig. This week in Google, episode 449, recorded Wednesday, March 21st, 2018.  Grackles, nut hatches, and swifts. Oh my.  This week in Google is brought to you by WordPress.  Reach more customers when you build your business website on WordPress.com.  Plans start at just $4 a month.  Get 15% off any new plan at WordPress.com slash Twig.  It's time for Twig this week in Google.  Although, we should call it Twif this week. Or Twicca.  With us, Jeff Jarvis, City University of New York, professor of journalism,  BuzzMachine.com, author of What Would Google Do? Public Parts, Geeks Sparing Gifts.  First day of spring.  First day of, full day of the vernal equinox in the snow.  Came a-tumbling down.  You got Toby out your window.

Speaker: SPEAKER_00
Transcript:  It's snowing.  Hello, Toby.  Jeff, you want to see out my window?

Speaker: SPEAKER_03
Transcript:  Yeah, let's see out Stacey Higginbotham's window.  Here we go.

Speaker: SPEAKER_01
Transcript:  She's in the beautiful Austin.  Where, oh. Shush.  Hey, I didn't know you looked out at that beautiful view while you're doing the show.  It is.  No wonder you can tolerate us.

Speaker: SPEAKER_00
Transcript:  Wow, do you have those strange little Austin birds on the tree?  The grackles?  Yeah.

Speaker: SPEAKER_01
Transcript:  Strange for their name only. I mean, I don't think.

Speaker: SPEAKER_03
Transcript:  They're ginormous, creepy, evil birds.

Speaker: SPEAKER_00
Transcript:  Oh, no, there's the little ones. Lots of little ones.  Oh, the little sparrow type birds?

Speaker: SPEAKER_03
Transcript:  Yeah.  The swift.  Nuthatches? Swift? I don't know. Tiny little birds?  Yeah, birds.  I'm just going to make this up. I do have birds.  I do have birds.

Speaker: SPEAKER_01
Transcript:  Grackles and nuthatches and swifts. Oh, my.  It's grackles and nuthatches and swifts.  Well, well, well, well. Did anything happen this week in Tech News?  Nothing.

Speaker: SPEAKER_00
Transcript:  Anything at all?  Everybody's been happy, doing neat things, moving to the future.  Happy go lucky.  Yeah. Yeah. It's a new world, you know. It's a good new world.  And it's OK.  It's fun.

Speaker: SPEAKER_03
Transcript:  I want to do a countdown.  Actually, let's start this show with let's do a countdown until how long Jeff says,  techno panic, because I'm pretty sure we're going to hit it.

Speaker: SPEAKER_00
Transcript:  I'm trying to beat you to it. I thought you were going to say moral panic.

Speaker: SPEAKER_01
Transcript:  Oh, close.  So there was a very good New York Times, The Daily yesterday.

Speaker: SPEAKER_00
Transcript:  It was actually an observer guardian that they gave to the New York Times.  Is that the one you're talking about?  Well, it was in their podcast.

Speaker: SPEAKER_01
Transcript:  Oh, sorry.  It was The Daily. But it may have.  But yes, the observer had the story, of course, broke the story.  And it was whistleblower Christopher Wiley from Cambridge Analytica who went to the observer.  But I thought a very good synopsis of this, which I will kind of attempt to rehash,  because I think before we even talk about Facebook's culpability and all this,  it would be good to understand what exactly transpired.

Speaker: SPEAKER_00
Transcript:  A lot of the coverage is the way wrong.  And it doesn't make a lot of sense for Facebook to parse breach versus not breach,  but it does make sense for us to get the facts right.

Speaker: SPEAKER_01
Transcript:  Yeah.  So this began Christopher Wiley. We'll start with him. He was the whistleblower.  He was a high school dropout from B.C., from Canada, who became fascinated with data,  big data, when he was volunteering at the Obama campaign in 2012  and was very intrigued by what data would tell them about reaching out to voters,  how to advertise to them, who to advertise to and all of that stuff.  He was following an interesting study, and I'm trying to remember if that was from Cambridge  or somewhere else. I think not, where some researchers found out that if we get 50 of your likes,  50, 50 of your likes, we can tell what race you are.  We can tell what kind of education you have. We can tell a lot about you.  Give us 500 of your likes. We'll know more about you than your spouse does.  And they were able to do that by correlating. It was a kind of imperfect system, correlating.  Likes were scrapable. I think they still are scrapable from Facebook.  You don't have to have permission to see what people like. It's right there on their profile.  So they basically get as much of that like data as they could and see if they could find correlations.  And this is very different from you or me. One of the things they found out, for instance,  if you like Hello Kitty, I don't even remember, but it said something about you.  It was that you were a nice person but not a rule follower, something like that.  So you could deduce all of these things. And they did this again by taking great lumps of data  and making correlations and then projecting out.  Wiley was fascinated by this, thought this was very interesting, started doing some of this himself  for the Obama campaign and then went to work for a British company and a group in the British company  run by this, it was Andrew Nix, the CEO now ousted of Cambridge Analytica.  They realized that they could do a lot more if they could get more access to the Facebook data.  And this is where the story became very interesting.  Nix ran into somebody from the, I think it was the Cruz campaign, the Ted Cruz campaign,  and decided that it would be great to get into the American market.  And decided that if they could do a little bit better targeting,  that they might be able to do this kind of targeting they were already doing in the U.K., in the American market.  Got the attention of billionaire Robert Mercer who funded it,  Steve Bannon who became the chairman and created Cambridge Analytica.  And Cambridge Analytica did, this is the thing that they did that really has gotten them in trouble with Facebook,  is they found a researcher at Cambridge who created a psychological quiz.  And they put out on Facebook an offer, if you take this quiz, it was a real quiz,  it was not a Game of Thrones, which vegetable are you kind of a quiz,  it was a real psychological profile, standard kind of profile.  If you take this quiz, you'll have to download an app and take it offline, off Facebook.  We'll give you some money, we'll pay you.  And they got a significant number of people to do that through Facebook.  So they had their Facebook login and they had the quiz.  They were able to use the data they gained from that quiz to, it was about a quarter, I think 275,000 people.  Yeah, to create this, what you really need in this, a machine learning database in effect,  making correlations between what the Facebook profile told you about this person, what their psychological profile was.  And they also got something which Facebook shut down a few years ago, which is this friends of friends stuff,  which I always thought was reprehensible.  So if you take a quiz or give somebody access to your Facebook account,  they not only get your information, your likes, but they get all your friends' information, all your friends' likes.

Speaker: SPEAKER_00
Transcript:  Well, Leo, just to clarify here, that's your public information, they don't get anything behind private.  No, no, no, no. Not credit card numbers, it's stuff you've posted.

Speaker: SPEAKER_01
Transcript:  They get access to their accounts though. They don't have to be, you don't have to have your own relationship.

Speaker: SPEAKER_00
Transcript:  At that time, you didn't have to have your own business relationship or relationship with that.

Speaker: SPEAKER_01
Transcript:  And that's how people thought Facebook worked. I mean, I think everybody thought Facebook worked is people can see my page if I allow them to.  You know, if they friend me and I friend them back, then we are sharing data.  But as it turns out, if they friend you and you friend them back, then anybody who friends them can see your data, which was at the time that was something Facebook stopped a couple of years.

Speaker: SPEAKER_00
Transcript:  And by the way, just real quickly, I worked with lots of media companies and said goody goody and did that like crazy.  And then the Obama's campaign had a million users and did the same thing.  And so it was very common at the time, but it was all part of that kerfuffle that we talked about the show at the time about people not understanding the circles of privacy.

Speaker: SPEAKER_01
Transcript:  We have been talking about this for years and trying to raise people's awareness of this for years.  So Cambridge Analytica got all of this data for it turns out once they got the 275,000 people to take the quiz, they got their friends of friends.  And that ended up being a 50 million person database, a very, very should have been handed over to Cambridge Analytica.  Well, it's unclear. It isn't that they shouldn't have access to it. They got legitimate access to it saying it was a research.

Speaker: SPEAKER_00
Transcript:  And they kept it for too long.  Said no, the rules said you couldn't pass that on to say a commercial entity.  Right. And it was passed on.

Speaker: SPEAKER_01
Transcript:  So the research or who had ties to Russia, by the way, was actually passing along to a commercial entity illegally.  Facebook against Facebook's against their is not illegal, but against their rules.  And then Cambridge Analytica used it apparently quite effectively to target advertising.  And and and you know, Wiley was so they say, which is there's been a lot of question about what we've got.  We've seen now the videos of its CEO admitting to all sorts of chicanery.  But again, they were boasting about stuff they probably didn't do in the past.  So there's no way of knowing if they actually do hire prostitutes to subordinate candidates and things like that.  And it's also there's debate among researchers.

Speaker: SPEAKER_00
Transcript:  I was at a Annenberg event at Penn some weeks ago where most of the researchers in the room said that they did not believe for a second that Cambridge Analytica had any near the skills that they say they have.  Again, that's irrelevant to this discussion insofar as things that shouldn't have happened happened.  But how much impact it then had is something that could be up for debate.

Speaker: SPEAKER_01
Transcript:  A lot of people say, hey, Obama did the same thing.  The only reason this is a hot button is because Trump used it to win and not not a Democrat.  Fair enough. That's fair criticism.  Facebook said we it was we understand it's a breach of trust and we expected this is Mark Zuckerberg's comment earlier.  He said we have a responsibility to protect your data.  If we can't, we don't deserve to serve you.  I've been working to understand exactly what happened and how to make sure this doesn't happen again.  He's going to be on CNN at 6 p.m. Pacific 9 p.m. Eastern tonight on the Anderson Cooper.  Let's not stay on until then. No, we'll be off by then.  So Cambridge Analytica, which was first of all was not supposed to have that data and then told Facebook they had deleted that data, apparently did not delete the data.  And until Wiley stepped forward, we didn't really understand exactly what had happened.  But the whistleblower, who was one of the principals, explained the whole process.  So a number of people, including Brian Acton, who was one of the founders of WhatsApp and was paid 16 billion dollars along with Yon Kuhn, his co-founder for the app by Facebook, said it's time.  Hashtag delete Facebook. A lot of people, that hashtag has gotten very popular.  I deleted Facebook not so much because I've I mean, everybody knows all that information.  I just don't like it really brought home to me, you know, something I've been thinking for a long time, which is I don't trust them.  Now, we should mention and I'm sure Jeff will mention that you actually work with Facebook and the foundation that they do.

Speaker: SPEAKER_00
Transcript:  Just to get the full clear disclosure, I raised 14 million dollars from Facebook, Craig Newmark, the Ford Foundation, AppNexus and others to start the News Integrity Initiative.  We run it independently of Facebook and I am not paid by and never have been paid by any of the platforms. End disclosure.

Speaker: SPEAKER_01
Transcript:  The parent company of Cambridge Analytica has also been banned as has Cambridge Analytica for all of this.  So there's there was clearly a violation of Facebook's terms.  And I think Mark Zuckerberg is taking some culpability, taking some responsibility for this, saying, yeah, we we didn't do a great job.  Although there's also the sense that Facebook is as much a victim as the as the end users that Cambridge Analytica lied to.

Speaker: SPEAKER_03
Transcript:  You disagree. That's like that's like saying Equifax is as much a victim as the end users.  I mean, this is a company that's dedicated to collecting data about people.

Speaker: SPEAKER_01
Transcript:  Well, Facebook's paying the price. They lost 50 billion dollars in stock market value of the last.

Speaker: SPEAKER_03
Transcript:  Are they really paying the price? I mean, I don't know.

Speaker: SPEAKER_01
Transcript:  It depends. I mean, this is a really interesting nexus. It's kind of a turning point.  It could be the end of Facebook in some ways. Right.

Speaker: SPEAKER_03
Transcript:  What do you think? You know, I I'm not a huge Facebook user.  I've never been. But I wouldn't be.  We need to have something like Facebook. Do we do we? Yeah, I do think we do.  I think we should have connects people.

Speaker: SPEAKER_01
Transcript:  Yeah, but I mean, I have a phone and an email and mail. I could connect to it.

Speaker: SPEAKER_03
Transcript:  It connects people superficially. It makes superficial connections feel more accessible.  So it helps you scale your connections, I would say.

Speaker: SPEAKER_01
Transcript:  So that may be the fault. That may be one of its flaws, though, by the way, because they're not genuine connections.

Speaker: SPEAKER_03
Transcript:  Maybe I don't know. I think they can be genuine connections.  I mean, if you use it well, you can create communities.  You just have to you have to curate them in a way that a lot of people aren't very disciplined at.  I mean, think about think about how sophisticated people are and different types of people.  I don't have a lot of close friends, for example.  I am not a huge Facebook user, not because of that, but I think those things are correlated.  And there are lots of people who seek others' approval.  And those people tend to be on Facebook and establish, I would say, to them meaningful relationships.  And I'm not going to denigrate that.  And I think it's something a lot of people need and want.  And so I think there's a real value there. I do think it's abused.  But those people are also abused in real life by, quote unquote, friends, frenemies, people who treat them like crap.

Speaker: SPEAKER_01
Transcript:  So you're saying that Facebook had a higher obligation to, I mean, to protect our data.  Certainly, they do have a if we're going to give them that data, very high obligation to keep it safe.  Yet, after all, all this really was this kind of Facebook's business model.  But somebody took advantage of it.

Speaker: SPEAKER_00
Transcript:  But yes, all true.  I mean, I think a couple of observations.  One and Stacey, I agree.  I think that society needs something like Facebook.  And as I wrote some weeks ago, I think that their definition of community and their definition of connections is still too shallow.  And so I agree with both of you.  But the data that was there was data that people shared publicly in the definition of public at the time on Facebook.  It wasn't as if it was private data that was stolen or breached.  That's one point. Second point is that at the time it was I would say on that first point real quickly, it was weaponized.  And I think that's different. That's different. That's different.  Yes. How it was used is absolutely different.  But and how it was used by those players, by Cambridge Analytica or other players was different.  But people said, I like the Beatles and there was the rule of the Seattle Beatles.

Speaker: SPEAKER_01
Transcript:  But that's a larger story that's going on right now, which is what we thought was innocuous sharing of information turns out, thanks to big data, massive databases, multiple massive databases and very powerful number crunching machine learning to not be as innocuous as we thought it was.  In other words, the kind of sharing I do in real life is innocuous.

Speaker: SPEAKER_00
Transcript:  But as soon as it gets attached to these big databases, because I'm not sure that you really I don't think I don't think we've really seen the full picture.  We've really seen the full extent of the definition of weaponization.  Nonetheless, was it misused by Cambridge Analytica? Yes.  Stipulated, Your Honor. Let me make a second point real quick, which is that at the time a lot of this was going on.  I think it was a general belief.  This goes after what Stacey was saying that that having a social media platform and using it to involve people and to get them involved civically in politics was a good thing.  Right. What changed in that?  One is this researcher misused the data and miss sent it to somebody who in turn misused it and so on.  The second thing that happened, let's be honest, was Donald Trump.  Because I think that you have to ask yourself, you know, do we say now that no political campaign should ever use social media?  I don't think we would have said that three years ago.

Speaker: SPEAKER_01
Transcript:  Five years ago. Did the way Obama used Facebook, was it was it similar to the way it was highly targeted?  It was highly targeted, wasn't it? And they had even more.  They had a million people.

Speaker: SPEAKER_00
Transcript:  And let's not forget that if you want to start a movement like Black Lives Matter or something else, the ability to target has benefit.  Targeting is not bad and evil.  The sameness of mass media is what some people are trying to defend right now.  And I'm trying to say, no, that's not the future. That's the past.  So bad things happened. Bad people did bad things.  I'm not arguing with any of that, but that doesn't make the intent or the platform venal.  And let's not forget the other things that changed that we're trying to blame Facebook for that mass media hold a huge dose of responsibility for,  like polarization that fertilized this field that allowed Cambridge Analytica and Steve Bannon to come in and plant their seeds of hatred.  So, yeah, I'm disappointed.  I mean, we get to where Zuckerberg's statement was.  I'm disappointed that all he did was deal with the tactical details of Cambridge Analytica.  There's a much bigger issue here about what the public responsibility of these platforms is.  They didn't ask for it. They didn't want it. There's no law about it.  But in their power, they have to take on responsibility for a civil informed conversation for for the health of the public sphere.  That's the bigger conversation that's not being held that I wish we should hold.  And I'll hold Facebook and company to high responsibility.

Speaker: SPEAKER_03
Transcript:  Thank you for doing we need. So along those lines, Jeff, do we need something like.  And you lie in because part of this is it's targeted, but it also targeted lies designed to get people to believe a certain way.  So do we need rules and regulations about campaigning that these sorts of things would have to follow?  That way, it's not the platform's responsibility as much as it's the path.

Speaker: SPEAKER_00
Transcript:  Stacy, I know this, but every single politician lies in half their commercials.  They lie when they appear on TV. They lie when they're on Twitter.  Who is to become the judge of that? Who is to who is to forbid the lie at the level of First Amendment?  And this is not a first amendment issue because a private company, but at the level of First Amendment, you have a right to lie.  You have a you have a constitutional right to lie in this country. You can do it.  People can think of what they will think to be lying is not all things the right standard.

Speaker: SPEAKER_01
Transcript:  I think to me, it's a matter of scale that stuff like this has always gone on.  We've always had targeted advertising. People always lied.  But I keep using that word weaponized.  Modern technology has changed the conversation because we have so much more data and we have so much better ways to correlate data.  You know, that it is now to me, it strikes me as more dangerous.  And I think the really the real the benefit of this Facebook moment is that everybody will start to think a little bit harder about what all these companies are doing, about what information we're giving up and how about how it's being misused.  I think, you know, I mean, there's I would say how it's being used period.

Speaker: SPEAKER_03
Transcript:  This is actually what Kevin and I talked about.  And I think let's let's move beyond just talking about this and let's talk about ways to solve for this.  Like there are things that we know work. Things like having data retention policies, getting rid of people's data after 30 days.

Speaker: SPEAKER_01
Transcript:  That's like, well, I think that's one of the things that's going to happen, right?  This that what you're now going to see is GDPR style regulation in the US.

Speaker: SPEAKER_03
Transcript:  It needs to happen. It has to happen because people aren't.  We are being asked to give up so much information.  We have no insights as users what's going to happen to that information and how it could be used.  We have an amazing array of frightening stories about how it could be used against us.  And that's turning people off from life, life saving benefits like like precision medicine or I'm trying to think of things.

Speaker: SPEAKER_01
Transcript:  But that's where you're in agreement with Jeff, because that's the point.  I think the point of techno panic is that don't throw the baby out with the bath water.

Speaker: SPEAKER_03
Transcript:  Right. So that's why I think regulations like GDPR are actually reasonable.  They're measured. What the challenge is with things like GDPR is we don't actually know how we can implement that yet,  because so far our technology that we've developed has all been about aggregating stuff, sifting through it for correlations over a long period of time and pulling new stuff in.  And now we're going to have to have a technology and no responsibility for it.  Now we need to develop a technology framework for pulling in data for a limited amount of time,  understanding exactly and communicating back how different entities are going to use that data and also give the user the ability to pull that back.  That's a really tough technical order. And we don't actually know how to even do that yet.  So it's one thing to be like, yeah, implement GDPR.  But we also need to be like, OK, how are we going to actually build a tech infrastructure for those rules?

Speaker: SPEAKER_00
Transcript:  Because we don't have it today. Amen. Amen. And two sides of that coin.  I think that American media companies are going to American companies in general brands.  Everybody are going to be shocked. Come May 1, they're going to have to consider cutting off Europe until they get their act together.  I don't think anyone's taking that seriously.

Speaker: SPEAKER_03
Transcript:  It's not just cutting off. They have like the big companies actually have to.  Implement GDPR because they have offices and in any company headquartered in Europe, even those that serve Americans will have to buy a buy.

Speaker: SPEAKER_00
Transcript:  I think you made the point before, Stacey, that that's the small companies that can't afford to do that.  They're going to be affected.  The other side of the coin, I just want to make real quick is and I've made this point often on the show, but I believe the future of media is one that gives you greater relevance.  It doesn't treat you like everybody else. That means I have to have data about you.  Now I have to have a good relationship with you under that data. I have to have your permission. I have to do right by it.  I have to give you value for it. But I even fear now that all targeting is going to be seen to be evil and awful.  And then we return to a world of everybody having to. Sorry about that.

Speaker: SPEAKER_01
Transcript:  It's the bad phone. And does you have a phone that rings, Jeff?

Speaker: SPEAKER_00
Transcript:  It's a it's a flack from Facebook.

Speaker: SPEAKER_01
Transcript:  Oh, they're reaching out now, baby. They're reaching out now. Yeah. Yeah.  So where do we stand on GDPR? I take it, Stacey, you think it's reasonable and not burdensome. Jeff?

Speaker: SPEAKER_03
Transcript:  No, I think ideas are good. I don't think we actually can implement it.

Speaker: SPEAKER_01
Transcript:  Well, that's you know, of course, that's the concern is that these rules, I mean, we started with right to be forgotten.  And it looked as if that broke the way the Internet works. GDPR maybe isn't as bad, but it has similar features.  Jeff, where do you think?

Speaker: SPEAKER_00
Transcript:  No, I think GDPR is the heart is in the right place.  But but the implementation is is is wrong on a bunch of levels because as we've discussed before and to Stacey's medical point, you can't anticipate every use of data.  It's knowledge. And that's the problem. I have both right to be forgotten.  And this is that is that is knowledge. And I think that there are ways that we can protect people and inform them and use things well.  And, you know, I I always go to Amazon, which tells me why it's recommending something unless we change that.  Right. That's not hard to do. We can figure that out. We can we can tell people what you know about you.  We can give you some I'm going to say like a sociologist agency over that.  We can we can know what the value is. And I think we've blown it horribly on the net.  We being technology companies and media companies and advertising companies in that we didn't tell you what we were doing.  We didn't tell you why we didn't tell you the benefit you had. And now we're we're we're paying for that.

Speaker: SPEAKER_01
Transcript:  So, you know, so this is a come to Jesus moment for the big tech companies.  They all are faced with not only the GDPR taking.  And by the way, you don't have to have even business in Europe.  You just have to have customers in Europe. So as long as they collect or process data of individuals located inside the EU,  they're subject to GDPR.  And that means there's responsibility and accountability.  There's notice requirements. There's consent requirements.  There's you have to have a data protection officer who manages compliance with all this.  There's a lot. I mean, if you read the Wikipedia page on GDPR, there are a lot of requirements.  I have a book somebody sent me on GDPR compliance for US companies.  What you need to do. You have to worry about it all. No, we don't collect data on people.  I don't think we store any information about you. So I think I think we're OK.  But that's why by night operation. Yeah. But there.  But, you know, isn't that funny that there are very few businesses you can think of that don't, in fact, collect data on their customers.  Advertisers collect data. That's their business. Not through us. So they have to do that.  But not through us. That's right. We don't allow.  We even do things that other people do.  We we we don't we don't allow tracking links, for instance, on our banner ads on our website.  We don't allow them to do that. So I think we're in the clear on this.  But I think that we're an exception, frankly. Most of you are have to face this.

Speaker: SPEAKER_00
Transcript:  So. And there's more strident strident legislation coming coming from the EU and coming to you about about the privacy stuff and about right to be forgotten is going to expand as well.

Speaker: SPEAKER_01
Transcript:  And I have to say this Facebook thing is is probably going to stimulate something in the United States as well.  And I think good consumers are aware of this now.  And I think consumers are concerned and want this.  It is good because the threat to Facebook, which extends to ultimately to Google and Amazon as well and everybody else, is that people will turn away from these services, realizing that they don't control their data and whatever benefit you get from Facebook.  And I would still argue that it is it is a small amount of benefit that can be duplicated in other ways.  I don't think Facebook is a half. You must have utility for anybody.

Speaker: SPEAKER_03
Transcript:  And I think most companies will leave.  Well, well, no, well, yes. OK. But I think most companies are actually OK with a set of rules because those people are actually trying to behave well.  And so this gives you a stick to beat people like Cambridge Analytica, who are misusing data, misusing your terms of service, for example.  It gives you the ability to say, hey, look, we're we're at least abiding by the lowest common denominator that we've set as a country.  And that's actually a really big deal. I mean, that's kind of what democracy is all about.  Right. And and so that's why I think it's good.  And we need it. And it also gives us an incentive to develop better ways to handle data than we have today.  There's no incentive for like all these poor researchers who are trying to research things like differential privacy.  They don't get gobs of funding because nobody cares about this stuff right now.  But this this incentivizes people to actually start caring about this type of technology that can benefit.

Speaker: SPEAKER_01
Transcript:  Great article yesterday on Motherboard by Carl Bodi, who says, given Facebook's privacy backlash, why aren't we angry with the broadband industry?  If if if your if your ISP is Comcast or AT&T and Verizon, they're collecting the same amount, same information.  They and they don't and they don't even admit to it.  They don't they're just hoovering up your data.

Speaker: SPEAKER_03
Transcript:  Yeah, they share it with the government at the top of anybody.  These guys are nuts. There's no trade.  So you know what you know, why yes, but here is reasonable regulation.

Speaker: SPEAKER_00
Transcript:  I would try for self regulation, self governance first and no platforms have not have not let me let me let me finish.  They have criticized and they have not shown that they're going to do that well.  And this problem, but I would prefer that as a rule.  The second problem, you know, I'm going to get to is, OK, so let's say we agree on a reasonable scheme of regulation.  I'm sorry, but I don't trust this government and this Congress and this regulatory environment to do that and do that well.  I just simply don't. I don't know.

Speaker: SPEAKER_03
Transcript:  You know what? It's going to take a lot longer to get to there.  So, you know, by the time we actually I mean, think about how long it takes us to pass legislation in this country.  It is we can't even get a budget.

Speaker: SPEAKER_01
Transcript:  So we're going to shut down the government again on Friday because they still can't get a budget.

Speaker: SPEAKER_03
Transcript:  I'm not as worried about that. I'm not worried about it.

Speaker: SPEAKER_01
Transcript:  I'm just it's just I mean, those are the those are kind of like the basic things that Congress needs to do.  They can't even do that. So, you know, forget any kind of agreement on data protection.  And frankly, I also think that they realize probably correctly that voters are fairly apathetic.  I mean, this is this this, you know, delete Facebook movement is raised some gotten some attention.  But I know, you know, and there'll be not something else in the news cycle in a day and we'll just move on.

Speaker: SPEAKER_00
Transcript:  And I think this is the argument even even Steve about enough and who is writing is publishing a book very soon against Facebook.  You know, he's trying to bring some perspective this as well and say there's bigger issues going on right now.  Other things to worry about. And and I don't think I think Rosmuss Nielsen, who's a great researcher at Oxford, said,  Let's bet here about how much Facebook's going to fall in usage.  I don't think. However, your first question, could this be an existential crisis?  Yes, it could be. They've got to take this very, very, very seriously.

Speaker: SPEAKER_01
Transcript:  And I'm sure they are. Is the liberal would a libertarian approach be effective?  Is there a way to see? No, no, no. Is there any way that we could see that not that, you know, I mean,  I don't honestly, Stacey, I think, you know, in a perfect world, society would get together and agree that no company should keep your data for more than 30 days, that you should have the right to delete, et cetera, et cetera.  But that's not I don't see that happening in that is what Europe has decided. I know it's it's not 30.

Speaker: SPEAKER_03
Transcript:  I don't think it's 30 days. 30 days, please. Yeah, that's a disaster.  But so it but you do have the ability to revoke your consent.  You have to accompany when they're collecting your data. They have to explain what they plan to do with it.  Those are those are not draconian things to do.

Speaker: SPEAKER_01
Transcript:  And I know, yeah, because it's not law yet how that's going to work out and whether it's going to be implemented and what's going to happen.

Speaker: SPEAKER_03
Transcript:  We really haven't seen that. Well, those are those are the regulations. I know.

Speaker: SPEAKER_01
Transcript:  But they don't they're not in effect yet. And when we go into effect in May, I'm sorry, March, March.  No, no way. When they go into effect in May, I think it'll be very interesting when we get back here in June or July to talk about what's changed.  Anything I had a woman.  What if on what if we put the responsibility on the individuals to keep track of everything?

Speaker: SPEAKER_03
Transcript:  No, we're as individuals. We are tracking too much already.  You cannot you cannot fight the power of a Facebook or your Equifax as individuals.  We're hosed. Yeah, that's us.  Like that's a dumb solution. And I hate when people do it.  They're like, oh, and Americans buy into this so much and it drives me absolutely bonkers.  There are some things individuals cannot do.  End of rant. Yeah.

Speaker: SPEAKER_01
Transcript:  I mean, you could not use Facebook, Google, Amazon.  You could not. But then you have a Safeway card that tells Safeway everything you bought or you have.

Speaker: SPEAKER_03
Transcript:  I mean, look at you.  And we go any time you use your credit card.  Yeah. Yeah.  Or any time you go to the doctor.  I mean, the doctor is pulling data about you that does get pulled into like your insurance company.  You can't not share your data with people.

Speaker: SPEAKER_00
Transcript:  Right. And again, we've had this discussion many times.  It was Dana Boyd who taught me this list.  You regulate the use of data, not not the gathering of it.  Gathering of it is knowledge.  Use is the question.  Should your doctor have all this data about you?  Absolutely. Should you be refused insurance under the current law?  No, you should not. Should you refuse employment?  No, you should not.  That's that's a question of the use of data.  That's not the fact that it exists.

Speaker: SPEAKER_01
Transcript:  Isn't this just kind of a concomitant inevitability when you create business models based on advertising?  Weren't we going to get this?  Wasn't this going to happen no matter what?

Speaker: SPEAKER_00
Transcript:  So pay pay walls will will will redline quality and journalism for the elite.  Yeah.  Commerce will bring you every one of them has conflicts.  The question is, do you have a moral North Star in how you operate in those models?

Speaker: SPEAKER_01
Transcript:  So the data is going to be it's going to be out there.

Speaker: SPEAKER_00
Transcript:  Yeah, for good reason in a lot of cases and sometimes.

Speaker: SPEAKER_03
Transcript:  Now, the challenge with litigating or litigating, regulating use is we historically haven't done that well.  Typically, we regulate things like actions and we like bright lines, for example.  So use is really hard to track because we don't.

Speaker: SPEAKER_01
Transcript:  You can't you know, you need a bright line because otherwise it's too hard to legislate and too hard to.

Speaker: SPEAKER_03
Transcript:  And what I like about the GDPR is that it it establishes a right to privacy for individuals.  It establishes for them the ability to control their data.  And I I actually think that's the right way to go about doing it.  It's not it's not just rules about gathering data.  It's a right to how data it's a right to privacy for your data, how your data is used and controlled and taken.  And that's kind of like the First Amendment.  That is that is a right.  It's like one of those unalienable rights that we just didn't know we needed until today, if that makes sense.

Speaker: SPEAKER_01
Transcript:  Well, and I do it.  We may not have needed as desperately as we need it in this in this technological age where you can combine all these databases and you have massive processing power and you can do so much more than you could do.  And let's not forget information.

Speaker: SPEAKER_03
Transcript:  Go ahead.  What?  No, go ahead, Stacy.  No, I was just going to echo Google, echo Leo.

Speaker: SPEAKER_00
Transcript:  But the scale scale cuts both ways, right?  As I said before, scale allows everyone to talk to anyone.  That is incredibly powerful.  It's disruptive and it's powerful.  And it means the trolls can talk to.  But it's powerful.  That's what enables everything we love about about this net scale also brings the things you talked about in data.  And it brings the difficulty of enforcement of whatever standards you have.  But we're we're at the level of scale now.  We're there and wishing we weren't isn't going to do us a lot of good.  So so the question is, how do we have a discussion about the responsibility that these companies have?  How do they have that?  How do we teach them that?  That's to me the bigger question.  You know, when Facebook threw out all the public content because some of it was inconvenient, it had an impact on news that hurts my not only my industry, but it hurts the informed society.  Now they're trying to bring it back now.  And that's a good thing.  But what do we expect of them?  What's their public responsibility?  I had I had this discussion with Jay Rosen this week, who's a brilliant, brilliant guy.  And he's the one who emphasized to me that we don't have the terms of this discussion.  There's no law that says these companies have public responsibility, though we presume they do.  And I certainly believe they do.  Well, how do we have that conversation?  That's the higher level to have.  So when Zuck writes his post today, it's just about what happened with Cambridge Analytica.  We won't let that happen again.  And we already cut off what's what.  So Zuck, because it's not the case.

Speaker: SPEAKER_01
Transcript:  Apologize after the fact, move on.  And then it happens again and then move on and it happens again.

Speaker: SPEAKER_00
Transcript:  And there's bigger issues about about about the the the moral and ethical basis and culture of these companies.

Speaker: SPEAKER_01
Transcript:  If we were to start from scratch, I know this isn't an impossibility,  but would it be possible to design a system that would protect people's privacy  and yet keep us connected and give us the benefits of a technological revolution?  Or is this just inevitable?  It feels like it's inevitable that this is this is what happens.  Could we design a system that wouldn't have these problems?

Speaker: SPEAKER_03
Transcript:  We don't have the incentive structure in place.

Speaker: SPEAKER_01
Transcript:  Yeah, well, I understand it would never have happened and it's not going to happen now.  Well, so maybe it's a completely rhetorical question because there's, you know,  even if you had an answer, it wouldn't mean anything.

Speaker: SPEAKER_03
Transcript:  Right. Well, so if we want to take on this thought exercise,  we actually have to think about what kind of incentives we have in place in this.  I mean, we'd have to go all the way to society, basically.

Speaker: SPEAKER_01
Transcript:  Yeah. So you'd have to have people be altruists and everybody be altruists.  And even if 99 percent of people are altruists, which I believe they probably are,  that's the one percent that kills you.

Speaker: SPEAKER_03
Transcript:  Yeah. OK. We're not with this.  That's a that's a beer and broad kind of conversation.  Yeah. All right.

Speaker: SPEAKER_01
Transcript:  I don't have I don't hold high hopes that we can regulate it either.  So that's what I'm puzzled about is I don't.

Speaker: SPEAKER_00
Transcript:  We can't. Let me pose your question another way.  Because I think this is this is what I've said this week, too.  I think that I was among many and I think you were and Jay Rosen was.  I had this discussion with Jay.  We believed almost religiously in the openness of the net. Sure did.  And we believe that end to end and open was a good and the good would come.  And I think a lot of good has come and I fear we're going to forget that.  But with that, we now know fully inevitably comes manipulation, bad actors  and how much openness are we willing to give up and to whom are we willing to give it up under what standards and rules?  That's the real discussion that we have now with the Internet.  The Internet is the Internet. It's going to be there. It's going to connect to all kinds of things.  I don't want China.  I don't want a place where where anyone decides what we're allowed to say and not allowed to say.  I saw that that that that's going too far on this openness scale.  Pure unadulterated openness is 4chan and H.N.  And that's not what we want.

Speaker: SPEAKER_01
Transcript:  Do you think this kind of encourages an authoritarianism in the long run that it in a way it's promoting?  What's the antecedent it there?

Speaker: SPEAKER_00
Transcript:  This kind of lack of privacy.

Speaker: SPEAKER_01
Transcript:  And I think because I think the reason it works in China is people want order and the tools now technology provides to keep order kind of encourage an authoritarianism that people are willing to accept.

Speaker: SPEAKER_00
Transcript:  Well, the people are told by the authorities that they want order and they have no choice but to go along with that order that the authorities provide.  Right. So I don't think the technology, the technological tools are there.

Speaker: SPEAKER_01
Transcript:  So I just feel like it's it promotes authoritarianism.  By the way, Facebook is culpable for a lot more than just this.  Look at what they did in the Philippines where they really kind of facilitated the election of Duterte and now support him.  He's shut down the free press, but he uses Facebook Live to stream.

Speaker: SPEAKER_00
Transcript:  Right. I go back to what I said before.  We thought that that involving the citizens in an open platform of civic engagement was a good thing.  And oops, Maria is the bravest journalist I know.  She's she's phenomenal. Maria Ressa is on the screen right now.  And and yeah, these are these are huge issues.  So what what are this freedom we gained?  John Perry Barlow, are we willing to give up and to whom does Facebook have no question?

Speaker: SPEAKER_01
Transcript:  So Facebook, in an attempt to be politically agnostic, helped all the candidates in the Philippines.  And then when Duterte won with their help, they support him because he's the leader,  because they're trying to be politically agnostic.  And yet they're supporting authoritarian governments, vicious policies against its people.

Speaker: SPEAKER_00
Transcript:  So OK, so set the rule, set the rule.  What is the rule there? And where is Facebook's culpability there?  I think that's far more offensive.  Then then I'm going to make you set the rule.  So Facebook in the U.S. or the Philippines or wherever should Facebook cut certain candidates off?  Well, I don't know.  As to the tool, because they are bad actors.

Speaker: SPEAKER_03
Transcript:  Now you're talking about use as opposed to an actual to gathering, for the example.  So you're actually right now arguing for a use case thing as opposed and then calling for or sorry.

Speaker: SPEAKER_02
Transcript:  You're pushing back on Leo.

Speaker: SPEAKER_00
Transcript:  Who's saying I can go on, but but but but but we've had that argument before.

Speaker: SPEAKER_03
Transcript:  But it relates here. That's the same sort of mental argument.

Speaker: SPEAKER_00
Transcript:  No, but I'm just simply saying to Leo, if he said if he starts at the U.S., he starts at the U.S.  and says Duterte or Trump and I come back and I say, OK, then write me the rule that you want them to live.

Speaker: SPEAKER_01
Transcript:  Well, yeah, right. What do you do?  That's what I'm saying is this is a structural problem with any company like this because the company is going to say, well, we're not going to choose sides.  So and then they are right now.

Speaker: SPEAKER_00
Transcript:  OK, so it's like three cases where they are.  And this is this is what's really interesting right now and where I think we need to help them.  As I've talked about before, Google now accounts for the reliability, authority and quality of sources in search ranking.  They side with science.  Twitter came out with its with its new thing saying, OK, we realize we can't be fully open and they're asking people in their RFP to say, help us decide the standards that they're going to live by.  Facebook is at least bringing back quality news.  They are making decisions now.  So all I'm saying to you is help them write those rules, those rules you want, help them write them, whether they're written by government or by Facebook.  You, the citizens, should not just be standing back and saying, well, Facebook, you mess this all up, go fix it.  No, OK, you've got to help. So here are my rules.

Speaker: SPEAKER_03
Transcript:  I want to know exactly what data they're collecting about me.  That includes how far that data, my relationship with people and what that means for things.  I want to understand who else gets access to that data and how I want to understand what those companies are trying to do with the data.  I want to understand.  I want the ability to stop my data going from someplace that I think is awful or unethical.  And that may mean when I try to load an app to take a stupid quiz that a little pop up says, hey, by the way, this is run by such and such.  And the data they're going to use from this is going to be used to infer your political leanings.  You know, and that'll be like, weird.  I just thought I was thinking about fruit or liking Hello Kitty.  Right.  And how you so those those are what the GDPR offers.  And I would like an easy to understand platform for that.  But those are the rules, Jeff.

Speaker: SPEAKER_00
Transcript:  I'm cool with that. But going back to Leo's question, how does that stop deterting?  How does that stop these other things? There's other that's the personal data piece.  That's a slice. And it's fine. I think that's a good a very good I could put a little bit of it, but I generally agree with it.  Set of rules and standards.  Now take that across a lot of other areas like political action.  If a if the Nazi who's running in for Congress in Illinois now comes on should Facebook good example, should Facebook should we expect Facebook to say no, Nazi, you can't use us.  Where's that line?

Speaker: SPEAKER_03
Transcript:  So then I would say Facebook needs to establish a policy about how it shares information on people with their governments.  Part of the issue here is and we're going to encounter this again and again is in America.  We have different standards.  I'm not saying people in the Philippines are really excited about getting killed for having minor drug possessions because I'm pretty sure they're not.  But if Facebook doesn't share data with a regime and maybe it maybe we do something with the OECD or the UN or the what?  The what is it called the World Economic Council where you have a standard of like authoritarianism maybe and you say hey if you fall below this on the scale we're not going to share our data with you period.

Speaker: SPEAKER_00
Transcript:  OK, so let's say let's let Stacey push back. This is good.  So your Twitter now and not an issue of sharing data at all.  It's just providing a platform for the Nazi candidate.  Should they say no, you do not have the right to be here.  I'd be OK with that.  But we have to understand what that standard is.  It has nothing to do with data now at all.  It's about providing allowing that person to have the same service that everybody else has.  But you're going to cut a line.  I'm fine with making that line.  I think we have to make those lines now.  But we have we have to help as norms as society help make those lines.

Speaker: SPEAKER_01
Transcript:  I don't think Facebook did this on purpose, but I think they inadvertently created a platform that is incredibly intrusive.  And I think they've known about this for years.  This is an article from 2013 from almost five years ago in the National Academy of Sciences proceedings.  We show that easily accessible.  This is the by the way the article that started the whole thing.  Easily accessible digital records of behavior.  Facebook likes can be used to automatically and accurately predict a range of highly sensitive personal attributes, including sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age and gender.  They use fifty thousand volunteers.  They they found out things like liking Sephora and curly fries would tell you something about your intelligence.  I mean, it is it is curly fries.  Yeah, it is counterintuitive.  It's extremely powerful.  It's based on big data.  This was fifty eight thousand people now try it with fifty million people.  And the problem with these models is they get better and better and better as we add more and more data.  This is the Facebook's likes platform.  That's it.  Nothing else.  Facebook basically created an incredibly powerful tool which can be used by dictators, demagogues and nice people alike.  And it's not just their responsibility.  It's also do they shut it down?

Speaker: SPEAKER_00
Transcript:  Let me read you a quote from Eve Williams at South by Southwest.  When we built Twitter, he said, we weren't thinking about these things.  We laid down fundamental architectures and that had assumptions that didn't account for bad behavior.  And now we're catching on to that.  It's not just the data is just one tool.  You know, distribution is another tool.  This the social push of groups to lie and harass is another tool.  These are all things that get enabled.  They get enabled by printing presses and telephones to they get enabled at a different speed and scale now.  And we, we, the Internet geeks, we said openness.  So the rules they operated under were our rules.  John Perry Barlow, God bless him.  May he rest in peace.  Those were our rules.  We said openness.  We said that neutrality.  We said all these things.  Now, with experience, we're changing our minds.  OK, we can.  But I don't think we can stand back and say, well, they end up creating this horrible tool.  They created it under our specs.  This is the Internet.  We want it.  The open one.  With Facebook, Facebook is actually more controlled with Facebook is more controlled and more civilized than Twitter right now because it has controls that we balked at.  Why is Facebook choosing what I see?  Why are they censoring people?  They got all that kind of crap, right?  In the day.  And now we're saying, well, now they're doing enough of that.

Speaker: SPEAKER_01
Transcript:  Well, the first thing everybody should do is stop doing likes and delete your likes because with 95 percent accuracy, these you could predict somebody's race.  Ninety three percent their gender with 88 percent accuracy from somebody's likes alone.  You can predict whether they're gay or straight.  Eighty five percent accuracy.  Democrat versus Republican.  These these.  You know, I don't think anybody looked at likes and said, I'm going to be able to get with this kind of accuracy.  And by the way, this is a relatively small sample set.  I can do this kind of stuff.  So well, they don't know.

Speaker: SPEAKER_00
Transcript:  If you go back to the ankle.

Speaker: SPEAKER_01
Transcript:  Did you know, did you know when you were doing likes that this would tell people what your sexual orientation was when you were liking Hello Kitty or Curly Fry?  The same thing happens to you when you buy products at the grocery store and it's in your cart.

Speaker: SPEAKER_00
Transcript:  The exact same thing has for years since the 70s.  This is not a new technology, different scale.  It's hardly new.  It's been used for years.  And I mean, we do it as people.

Speaker: SPEAKER_03
Transcript:  We make inferences all the time.  I'm very curious.  I like both Sephora and Curly Fry's.  I'm like, does that mean I'm intelligent or not intelligent?

Speaker: SPEAKER_01
Transcript:  Well, that's actually they don't say they don't say.  So intelligence, by the way, is one of the harder things to predict.  And, you know, my daughter loves Hello Kitty.

Speaker: SPEAKER_03
Transcript:  Does that make her she is not a rule breaker.  So like so there's a lot of yes in aggregate, but we are the best predictor of high intelligence are.

Speaker: SPEAKER_01
Transcript:  Are you ready?  Yes.  Thunder storms, the Colbert Report, Science and Curly Fry's, Low Intelligence, Sephora, I Love Being a Mom, Harley Davidson and Lady Antebellum, which is a country group.  Good predictors of male homosexuality included, of course, the No Hate campaign.  But really, you would think that that kind of thing would be the most obvious.  But it's not.  Strong predictors of male heterosexuality include Wu-Tang Clan, Shaq and Liking the Group, Being Confused After Waking Up from Naps.  So here's there's there's obviously some bias in there, too, I imagine.

Speaker: SPEAKER_03
Transcript:  So looking at the things that correlate to low intelligence, I see some I see a lot of things that are related to women.  Makeup.  I love being a mom, not Harley Davidson.  But you associate that with possibly country in the south.  No, but this isn't no, no, no.  Nobody's making a qualitative decision.

Speaker: SPEAKER_01
Transcript:  What they did is they took 58,000 people.  They took all of their likes.  They had them take a personality inventory test, answer questions and then match the two and were able to find these correlations.  This is a small set.  Don't rush over Stacey's point.

Speaker: SPEAKER_00
Transcript:  Now, Stacey's making a very good point about the bias that are set up, whether it's an IQ test or whether it's an SAT or whether it's this.  That's possible.  There's huge bias.  That gives me hope.  How we define intelligence society is biased.  Right.

Speaker: SPEAKER_03
Transcript:  Well, and so and that's that's something that we don't talk about when we talk about data.  And that's something I mean, OK, we do talk about it, but we're not talking about it here.  And there's there's an interest.  I mean, there are ways that bias is going to filter into how things are targeted.  And they're going to be mostly accurate.  Could they be more accurate if we didn't have, you know, racist, sexist people sometimes doing these things or making I shouldn't say that people people making maybe an intentional racist or sexist assumptions?  I don't know.  Although.  Yeah, curly fries and Sephora.  Dang it.  I don't know where I fall in intelligence.

Speaker: SPEAKER_01
Transcript:  Now, you want to take the website still up.  My personality project is at mypersonality.org slash Wiki.  I see. Look what you're both doing.

Speaker: SPEAKER_00
Transcript:  Look at mother.  You're going to go take this thing.  You've just been decrypted.  We've just been panning.

Speaker: SPEAKER_03
Transcript:  That is true.

Speaker: SPEAKER_00
Transcript:  I'm like, let me add it.

Speaker: SPEAKER_01
Transcript:  By the way, they now have six million test results paired to four million individual Facebook profiles, which means that their psychometrics are well, given the bias of the within the bias of the test more even more accurate.

Speaker: SPEAKER_03
Transcript:  And this is this is part of like this whole idea of his human beings.  We crave validation and my God, we're just going to go find it wherever we can.  And places like Facebook are awesome for that.  I don't know.  Are we going to talk the whole show about this?  No.  What is this?  Google news, too.  Let's wind it up.

Speaker: SPEAKER_01
Transcript:  What is so I guess we know what the harm could be.  We can think of what the worst things could be out of this.  I don't know if there's a way to fix it.  It's certainly something that we're going to all grapple with, and especially these big companies as they get as they're under fire.

Speaker: SPEAKER_00
Transcript:  I mean, we're in a new society and these are not easy questions.  My only problem with the discussion, not the show, but in general now is this being treated as easy.  Facebook, not a society and they can fix it.  It's not easy. It's not simple.  It's not their fault.  That's dumb.  And these are hard questions.  And I'm as critical as anyone.  And I wrote a post about this, that Facebook is not grappling with these huge questions of public responsibility and that they must they must deal with radical transparency.  They must deal with their own culture and their ethical bases.  They have to do these things and they're not doing them yet.  And and so I'm holding to a very high standard, but just the Cambridge Analytica story is it's an old story actually.  And and there may be better things to talk about.

Speaker: SPEAKER_01
Transcript:  Our show today brought to you by Wordpress.  Now, here's something I can get behind, something that's something that's nice.  It's happy. It's all about you sharing under your control on your website.  It's not your Facebook page.  It's not your Twitter account.  It's your it's your website.  There's a reason why 30 percent of all the websites in the world run on WordPress.  It's the best platform for putting you out there in a way that you control the freedom and flexibility to share your voice, your work, to share it your way.  WordPress.com.  It makes it easy to they do the hosting.  They give you the templates.  They've got the great customer support team there 24 7 Monday through Friday and a weekends to to help answer any questions you might have.  You don't need to be a geek.  You don't need to be a coder or even a designer to do a great looking site that works for you at WordPress.  Go to WordPress.com slash twig for 15 percent off your brand new website.  They've got e-commerce buttons.  Well, options ranging from a simple buy button just next to one thing or donate button to a complete online store.  They can do the whole thing.  Plans start as low as four dollars a month.  And WordPress makes it easy to expand your reach with built in SEO social sharing.  So even if if you know this isn't your Facebook page, your readers, your customers will share your stuff on their Facebook page, which is awesome.  Go to WordPress.com slash twig.  You got to have a place on the web that's yours that you own 100 percent.  That's it right there.  WordPress.com slash twig to save 15 percent off your brand new website.  Find out why 30 percent of all the websites in the world, including mine, including some of the best publications in the world run on WordPress.  WordPress.com slash twig.  We thank them for their support.  Google's buying Lytro or at least that's the rumor.  Reports have it.  Report has it.

Speaker: SPEAKER_00
Transcript:  Didn't we talk last week about how the next version of phones might have two cameras separated?  A lot of them do.  And we speculate about that use.

Speaker: SPEAKER_01
Transcript:  Yeah, Lytro was a camera system that gave you multiple focal planes.  It was a very kind of a weird, expensive, limited in some ways camera.  It was a multi-camera technique.  And according to multiple sources telling TechCrunch, they may have acquired Lytro for a bargain basement price somewhere between twenty five and forty million dollars.  Lytro, which had been struggling with shopping itself to Facebook and Apple.  Not all the according to sources again with TechCrunch, not all employees are going over.  Some have already received severance and part ways with the company.  It sounds like it's an acquisition of Lytro's Lightfield patents and other digital imaging technology.  They have 59 patents.

Speaker: SPEAKER_00
Transcript:  It's a very cool technology, but not something you use every day.

Speaker: SPEAKER_03
Transcript:  Yeah, I was very excited when I mean, when this came out, I was like, cool.  I mean, it's all the cool tech buttons.  So I'm glad, you know, I'm glad that appreciates cool tech and doesn't really care too much about markets.  And, you know, they're going to do something cool with this eventually.

Speaker: SPEAKER_01
Transcript:  Yeah. And there may be talk about there may be other applications besides just camera technology, maybe VR applications, AR applications as well.  So it's interesting stuff.  Go ahead. What did you want to talk about?

Speaker: SPEAKER_00
Transcript:  I just said Google. I was at the Google event yesterday here in New York where they announced tons of things to try to help news, including buying the Chelsea market.  And keeping the scones in place.

Speaker: SPEAKER_01
Transcript:  They're not going to get this. Are they really good scones down there?  Let's go scones there.  The first floor, I guess, is an actual market, right?

Speaker: SPEAKER_00
Transcript:  It's a very it's a little too hip for for normal people.  But it's like me.

Speaker: SPEAKER_03
Transcript:  Is it too hip for you? It must be really hip because I feel like it's really hip.

Speaker: SPEAKER_00
Transcript:  It's really he wears black and everything.

Speaker: SPEAKER_03
Transcript:  I know. I'm like, look at that nice, nicely trimmed beard and, you know, the little poof in his hair.  I feel like, Jeff, you got some hipness.  That's not an insult. I'm just like, you're a put together person.

Speaker: SPEAKER_01
Transcript:  Are they going to?  Now, so I love this statement from Daniel, David Radcliffe.  I wouldn't want to call him Daniel Radcliffe, but that's Harry Potter from David Radcliffe, VP Real Estate and Workplace Services at Google.  It's been 18 years since Google New York first established our single person sales office in a Starbucks on 86th Street.  After moving from 86th Street to a more official space in Times Square.

Speaker: SPEAKER_00
Transcript:  So what year was 18 years ago? Does the math for me first?  That was 2000.  Wow.

Speaker: SPEAKER_02
Transcript:  It's 2018. That's easy math.

Speaker: SPEAKER_03
Transcript:  I can do that.

Speaker: SPEAKER_01
Transcript:  You did it better than us.  Current home is at 111 Eighth Avenue.  They've been there since 2006.  An entire city block.  It's huge.  But now they are buying the they had some space in the Chelsea Market.  They're going to keep the people who are there, I guess, won't be kicked out, including the retail and food hall.

Speaker: SPEAKER_00
Transcript:  But the first floor is all the retail stuff and nobody wants that to go away, including Google employees, even if they can eat for free.  Yeah.  It's very, it's very nice.

Speaker: SPEAKER_01
Transcript:  It's really there are other people leasing parts of that building, but I guess Google can we use it as an opportunity to grow within that building.  Yeah, that's good.  Is it mostly sale?  It's mostly salespeople, though.  It's not it's not program.  No, no, no.

Speaker: SPEAKER_00
Transcript:  The the YouTube performance space is there.  The New York office.  I mean, the thing about Google is they they shift pieces of development around the world.  I mean, Munich and Vienna, so there's development pieces.  Yeah, there's developers in New York.  Yeah.  All right.  Absolutely.

Speaker: SPEAKER_01
Transcript:  Large teams focusing on projects, including search ads, maps, YouTube, cloud, technical infrastructure, sales and research.  So, yeah, a lot of it.  What else did they announce?  So they had they announced the Google News Initiative.

Speaker: SPEAKER_00
Transcript:  So that's why that's of interest to nerdy geeky me.  But it was an impressive set of announcements among them.  They've talked about a little bit of this before, but an effort to help with subscription based publications.  I always worry about the non-subscription based.  But for those who should be publications, they launched the 17 pubs, including McClatchy, New York Times, Washington Post, I think, LeMond, Ray D'Alessandro, and so on.  I think LeMond reform in Mexico City and others, Telegraph and FT.  And what it means is that you if you have a Google sign in a Google payment, you come across you hit a challenge to subscribe at a publication and you can be given models of subscription in two clicks.  You're done.  You don't have to create a new account.  You don't create a new passport, password or passport.  You're using your Google sign in.  That's for us.  Now, Google knows you're a subscriber, but there's a benefit to that.  Google will also promote content to which you subscribe.  So you see the carousel of news that occurs now under that there's going to be a my subscriptions carousel.  So if you subscribe to the Guardian and the teller, the Guardian of the Telegraph and the FT and the New York Times, then when you search on a topic news from those publications, that topic will not be promoted to you.  So that's good for the publications and one hopes for the user.  They'll also third thing on this is they're going to predict using data.  They're going to predict who is a likely mark for your subscription pitch and say, Hey, go after Stacey.  She loves you.  They won't be telling you why they made that decision, how they made that decision.  They won't be sharing the PII with that, but they share the conclusion and one hopes they will do that in other areas.  So that's a that's a big thing that they did.  They're also putting three million dollars into a news literacy campaign.  That's another story.  They're also doing a lot about trying to bring AI to newsrooms by making it a lot easier.  And those same AI stuff that anybody can use.  They put money behind First Draft, which is this wonderful organization that did the election land that was at CUNY during the last election.  And that also did a cross check in France during the French election to really bolster their efforts to for fact checking and quality news.  They're saying that finally, I think this is finally that they're working in breaking new situations.  They realize the problem that somebody is going to throw some crap, you know, the young people at Parkland were paid actors and that kind of stuff will come up.  So when they see a breaking new situation happening, which is vulnerable to this kind of manipulation, they're going to then bias known authoritative sources in trending and such.  So they don't get taken by Bozo.  That's less a problem on Google News and certainly search.  It's much more of a problem and remains a problem on YouTube.  And they've got to worry about that.  So it was a whole raft of things.  They say that they're spending three hundred million dollars over the next three years on this.  There was a little confusion where people from the stage called that a fund.  It is not a fund.  Europe has one hundred fifty million euro fund.  U.S. and the world do not.  But Google says they're making substantial investment.  And in my report, they took out a hypospace in New York.  They brought people in from all around the world, flying people in from India and Australia, publishers to see this.  It was a very interesting and marked contrast from the view toward Facebook and the view toward Google right now this week.  But Google did a good job.  And Richard Jinkerson company deserve a lot of credit, I think, for showing that they listen to publishers.  They collaborated with publishers.  They're trying to help publishers.  There's more we want to do.  There's more that they ought to do.  That's fine.  They're going to try to improve advertising.  They're going to try to do other things, too.  I want them to work with us on user data and how to do it right under GDPR.  But it's a very good big step to helping news and a report.

Speaker: SPEAKER_01
Transcript:  And now Peter Kafka's contrasting point.

Speaker: SPEAKER_00
Transcript:  He said, you take me down from my high of a happy world.  You got to bring it matter how hard Google and Facebook try to help publishers.

Speaker: SPEAKER_01
Transcript:  They'll do more to hurt them because that's the way they're supposed to work.  They're built to eviscerate publishers.  They're aggregators.  And that's all there is to say about it.  Three hundred million dollars over three years.  It's about one percent.

Speaker: SPEAKER_00
Transcript:  Google will tell you, as they did, that they succeed when their partners succeed.  They need the publishers.  They need them for the content.  They need them for the distribution of the ads.  And they share huge amounts, billions of dollars of revenue with their distribution partners of their ads.  And yeah, guess what, Mr. Kafka?  The world has changed utterly.  And maybe publishing and journalism are obsolete institutions that ought to be rethought.  But we will go into that right now.

Speaker: SPEAKER_01
Transcript:  Well, since Peter's not here to defend himself and I don't really have the stomach to do it, I'll just.  No, you don't. Neither do I.

Speaker: SPEAKER_00
Transcript:  And certainly not as Stacey.  Stacey's saying, when does this end?  This is a different kind of geese.

Speaker: SPEAKER_01
Transcript:  Actually, I have the huge story of the week coming up in just a second.  But real quickly, in the related vein, Subscribe with Google has been launched.  That's what I just talked about.  OK.  So if you want to do.  Yeah, that's part of this.  So if you want to do it, what do we do?

Speaker: SPEAKER_00
Transcript:  Do we do we just get the when you when you get hit when you get hit by the subscription go through wall?  Yeah, you'll go through Google.  By the way, the FT presented some really interesting data.  They did a lot of experimentation with Google.  Sorry, Stacey, in two minutes, where they saw it because they have a big paywall.

Speaker: SPEAKER_01
Transcript:  FT does financial times for a paywall.

Speaker: SPEAKER_00
Transcript:  So Google could look at the funnel and who was interested in the FT, who was coming in and they could monitor that for the FT.  The FT looked at the bottom of the funnel and saw that if they so they tried to give people three free pages a month and what happened.  And then they went down to one free page a month.  It pushed up the conversion, but they lost the people coming into the funnel.  They lost the prospects.  They tried three pages.  That was a week or a day.  But then they went to a month, three a month.  And they played with it and they realized they've got to segregate people into three categories.  People who are just coming at the beginning.  You get lots of free stuff because you want you to get a habit.  Then you start getting a little too habitual.  We're going to cut you back and try to push you.  Then you come back all the time.  Boom, no, you get in the wall.  And so Google is working with the publishers on this kind of data to figure out how to make that operate.  I think it's a good and healthy partnership.

Speaker: SPEAKER_01
Transcript:  I'm going to move to a shack in the forest and disconnect from everything.  But I still think I'll have a working toilet.  But other than that, just moving away from it all.

Speaker: SPEAKER_03
Transcript:  No electricity?

Speaker: SPEAKER_01
Transcript:  No, no, no.  The electric company is spying on me.  This week with Thoreau.

Speaker: SPEAKER_03
Transcript:  They actually can't tell quite a bit of information.

Speaker: SPEAKER_01
Transcript:  I know they can.  That son of a gun.  So here's some bad news.  I actually maligned Congress.  I said they can't get anything done.  In fact, they have gotten something done.  They've passed SESTA FOSTA by 97 to 2.  Who are the two?

Speaker: SPEAKER_00
Transcript:  Who's going to vote against child porn?

Speaker: SPEAKER_01
Transcript:  And that's where we are.  But it doesn't.  In fact, it overturns Section 230, which is awful, which protects online platforms from liability.  Again, you know, for some types of speech by their users, putting publishers, we might add.  Yeah. So if somebody posts something horrible and a blog comment, you're the blogger is liable.  No longer protected.

Speaker: SPEAKER_03
Transcript:  So what's going to happen?  Let's let's all take a take a relaxing chill pill for a moment.  What will happen is we'll get a use case that is not sex work related.  They'll sue.  The courts will either carve out an exception or they'll send this back to Congress and say, yo, you screwed up.  Fix it. Yeah.  And then we'll be back because, yes, the first time someone comes in and I mean, we'll see a good test case.  And it is painful.  It is painful.  I'm not saying this is ideal, but this is kind of where we are as a as a people.

Speaker: SPEAKER_01
Transcript:  By the way, the law is retroactive.  What?  So even more opportunity for you to sue.  So, yeah, it's aimed at sex trafficking.  But of course, when you overturn this safe harbor protection, it means all sorts of stuff.  Sad to say, IBM and Oracle companies whose business models don't rely on 230 were quick to jump on board the bill and support it.  So was the Internet Association because big Internet companies aren't going to have too much trouble.  It's the little the little guys who are going to have to put up with this.  Yeah, we'll watch. This is EFF's article.  If you want to read more about it and understand, it is it is passed through both the House and Senate.  So it just remains to be signed before it becomes a law.  Elliott Harmon, how Congress censored the Internet published today on EFF.org.

Speaker: SPEAKER_00
Transcript:  And if we're because we're paying attention to all we're paying attention to the wrong things sometimes.  But we have a way to do.

Speaker: SPEAKER_01
Transcript:  Pay no attention to this sesta fosta.  Look at it. Look at Facebook.

Speaker: SPEAKER_03
Transcript:  OK, I was thinking about look who's been fired lately from, you know, the cabinet slash White House.

Speaker: SPEAKER_01
Transcript:  Well, that too. Right. That's a nice. That's a great circus.  Thank you, Twitter.  Google actually this is an interesting story.  Google has created an open source VPN anyone can install.  But there's one guy who's not too happy about it.  The guy who invented it in the first place or something an awful lot like it.  Dan Guido launched a project.  That gave you called that gave you a VPN in easy to set up VPN in late 2016.  That is very similar, but I don't know. Maybe who knows the more the merrier.  Right. So it's Google's is called Outline.  You could run it on your own server.  But if you don't have a server, Digital Ocean makes it like a one button install rack space.  Google Cloud Engine EC2.  And it'd be easy enough to set up and then have a VPN that you could trust.  And as we've learned, a lot of these big VPNs are not so trustworthy as we thought.

Speaker: SPEAKER_03
Transcript:  You know what I would like? And I don't know if this works.  But, you know, my my hate the reason I don't have a VPN, I don't hate it.  That's way too much. But it breaks a lot of my geographically based functionality.  So my television, my Amazon Echo.  And so I wish there was like a VPN plus a masking service that said, hey, so I could say, hey, I want to be indicated that I'm in Austin or I'm in Texas at this level.  Actually, you know, a tunnel bearer.

Speaker: SPEAKER_01
Transcript:  OK, you could do it with you.  You can say where you want to be. You could do it with this.  If you ran this VPN on a home server, it would identify itself as coming from the house.  And then you log into it from where the outside world, the idea is it protects you when you're out and about,  but still gives you, you know, local.

Speaker: SPEAKER_03
Transcript:  See, I want something on my house that protects me against ISPs looking at all of my data for my connected devices.

Speaker: SPEAKER_01
Transcript:  Right. Yeah, tunnel bearer might do that.  Although, yeah, I don't know.  I just don't want to.

Speaker: SPEAKER_03
Transcript:  OK, yeah, this is so this is because I tried VPNs in the past.  I'm like, oh, I always try many aspects of my life.

Speaker: SPEAKER_01
Transcript:  It slows everything down.

Speaker: SPEAKER_00
Transcript:  Yeah, Leo, I think they were pushing this VPN thing as a way for journalists to be able to.  Well, that's one of the uses for sure. Right.  Yes, yes. Absolutely.

Speaker: SPEAKER_01
Transcript:  It's all encrypted. It's a privacy tool.  Maybe I shouldn't assume that everybody knows what a VPN is, but the idea is it creates an encrypted tunnel between you and the server  so that where you are is hidden and the data you're exchanging with the outside world is hidden until it gets to the server.  At that point, it has to enter the outside world.  That's why if you run it out of your house, it'll look like it's coming from your house.  And many people use commercial VPNs.  But the problem is, it's hard to know what their privacy policies really are.  We just yesterday on Security Now had a report on I think three different well-known VPNs that were leaking information, IP address information, stuff like that.

Speaker: SPEAKER_03
Transcript:  I think this is a good solution.  Hey, talk about news again, because I have to go do something really fast.

Speaker: SPEAKER_01
Transcript:  OK, go do it and I will talk about some news.  Let's talk about what Stacey really disagrees with.

Speaker: SPEAKER_00
Transcript:  All right.

Speaker: SPEAKER_01
Transcript:  How about I like this is South by Southwest YouTube announcing that they're going to use Wikipedia to verify claims on conspiracy theory and other YouTube videos.  They're going to link.  I don't want to fit about that.  They mentioned Wikipedia about it.

Speaker: SPEAKER_00
Transcript:  They're going to link to them.  When did you when did anybody ever say, hello, Wikipedia, I'm going to link to you.

Speaker: SPEAKER_01
Transcript:  Right.  OK.

Speaker: SPEAKER_00
Transcript:  I don't understand what the fuss is.  I think this is an example of people trying to find any angle.  There's problems on YouTube.  There's problems galore.  Wikipedia is probably not the solution.  But linking to Wikipedia is not an evil.

Speaker: SPEAKER_01
Transcript:  No, I think maybe it was the way Susan Wojcicki said as if it was a partnership as opposed to just maybe linking to it.

Speaker: SPEAKER_00
Transcript:  They don't even do partnerships.  You linked to them.  Yeah.  But Google's been doing that for ages.

Speaker: SPEAKER_01
Transcript:  She said, well, I wish I could find the quote.  The New York Times says she she would enlist Wikipedia's help.  It's not exactly if you link to it.  It's not really enlisting their help.  No, it's not.  No.  But maybe that's the New York Times.  I don't have the I don't have the quote in front of me.  So she says YouTube will soon begin experimenting with what it calls, quote, information cues, end quote, sourced from Wikipedia.  The cues would appear as captions and article links beneath videos that deal with topics related to popular conspiracy theories like the moon landing and chem trails.  People who were interested in chem trails wanted to know more about the moon landing.  I don't know.

Speaker: SPEAKER_00
Transcript:  Did you see Zaynab Tversky's piece a week ago?  I don't think we talked about it last week.  I brought it up, but maybe it was on Twitter.

Speaker: SPEAKER_01
Transcript:  Yeah, I thought it was really good.

Speaker: SPEAKER_00
Transcript:  No, actually, we did talk about it.  Never mind.  We did.  We did talk about it here.  We did talk about it.

Speaker: SPEAKER_01
Transcript:  Yeah.  Remind me again what she was talking about.

Speaker: SPEAKER_00
Transcript:  She was saying that it intensifies interest.  And so if you started vegetarian, I think you'd be.  YouTube escalates your escalates, which in one way can make sense.  Oh, you're interested in that.  I'll give you more and deeper.  The other way, it's obviously bad.

Speaker: SPEAKER_01
Transcript:  But but that's the thing about all of these algorithmic things.  They all make sense.  Absolutely.  To the algorithm.  It's all about increasing engagement.  If the algorithm is to get more clicks, it works.  They're not to deliver more value.

Speaker: SPEAKER_03
Transcript:  Well, yeah, it delivers value.  The problem is computers are terrible at context.  So I can walk into a library.  We actually just a few days ago walked into the library, my daughter and I.  My daughter asked for a book for young adults.  She's like, I'm really into mermaids.  Do you have any books on mermaids?  And so the lady brought us to some graphic novels about mermaids and some other stuff.  And then that's why librarians are great.

Speaker: SPEAKER_01
Transcript:  They are.

Speaker: SPEAKER_03
Transcript:  But if if my you know, so but she asked YouTube for the weird sex things about mermaids because she knows.  YouTube's like, oh, you like mermaids?  A lot of people really like this one.

Speaker: SPEAKER_01
Transcript:  That's exactly right.  And that's the problem is that the algorithm is doing everything right.  It's doing what it's supposed to do.  It's just that the consequences or the side effects are not well understood or thought about.

Speaker: SPEAKER_03
Transcript:  And they just don't have human context.  I mean, computers have no common sense.  They know their computers.  I don't this.

Speaker: SPEAKER_01
Transcript:  I'm sure this is out of context.  But another South by revelation, this one from layer layer Cohen, the YouTube's global head of music,  the people who treat music, YouTube like a music service, passively listening for long periods of time are going to encounter more ads.  You're not going to be happy after you're jamming to stairway to heaven and you get an ad right after that.  We're doing that, he says.  So you'll subscribe.  So you'll pay the headline.  YouTube will frustrate some users with ads so they pay for music.  I see nothing wrong with this.  OK, good.  Right.  Nothing.  I don't.  I mean, no.  Hey, you've been listening for four hours.  We're going to give you some more ads to you pay.

Speaker: SPEAKER_03
Transcript:  That's business.  I.  Yeah.  Sorry, I can't see why someone would be frustrated by this.  I can see why someone might be like, dude, that sucks.  But I.

Speaker: SPEAKER_01
Transcript:  Yeah, I pay for it.  I pay for it.  I'm the first to give you something free.  It's not free.  The new service will frustrate and seduce users of YouTube's free service says, frustrate and seduce.

Speaker: SPEAKER_03
Transcript:  Wow.  That's a loaded language right there.

Speaker: SPEAKER_01
Transcript:  That's the that's the whole thing.  I guess a stick and a carrot.  It's just like life.

Speaker: SPEAKER_00
Transcript:  Life frustrates and seduces.  Yeah.

Speaker: SPEAKER_01
Transcript:  So they will.  In time, Cohen says, appreciate the advertising.  Everyone is drunk on the growth of subscription.  I don't know what lately or was smoking before he went up on stage.  OK.  YouTube.  So yes, go ahead.  Uber.  Uber.  I find this story.  Oh, we can talk about this.  Yeah, this is interesting.  The way you put it.  I mean, I think it's a good thing.

Speaker: SPEAKER_00
Transcript:  I think it's a good thing.  I think it's a good thing.  I think it's a good thing.  Oh, we can talk about this.  Yeah, this is interesting.  Yeah.  The way it's being treated.

Speaker: SPEAKER_01
Transcript:  Exactly.  A self-driving Uber, like before last, sadly killed an Arizona pedestrian.  It was about 10 p.m. in the dark.  The Uber car traveling at 40 miles an hour in a 35 mile an hour zone had a safety driver.  Neither the safety driver nor the car saw the woman or stopped.  They didn't try.  There's no evidence that it braked at all.  The woman was struck and killed.  She was walking her bike across a four lane street, not in the crosswalk.  After reviewing the video, Tempe police told the San Francisco Chronicle, no one could  have avoided this driver or a self-driving car.  She came out of the shadows so suddenly there was nothing anybody could do about it.  Of course, because it's a self-driving vehicle that killed a human, Uber immediately suspended  all of its self-driving vehicle research indefinitely.  And I imagine, I haven't seen it, but people are calling for the end of self-driving vehicles.  I don't know.  Do you blame the self-driving vehicle?  Well, okay, don't say that.

Speaker: SPEAKER_03
Transcript:  Don't say that.  That was massive speculation.  That was bad behavior right there.

Speaker: SPEAKER_01
Transcript:  I'm sure somebody is.  I just don't have anybody in front of me doing that.

Speaker: SPEAKER_03
Transcript:  Well, let's not give them the time of day.

Speaker: SPEAKER_01
Transcript:  You don't think anybody's saying, oh, see, this is why we shouldn't have self-driving  vehicles?  That's kind of the tenor of all the coverage was.

Speaker: SPEAKER_03
Transcript:  Well, that's what we're setting expectations for.  That's on the media.  Like, oh, someone died.  I think people would be legitimately freaked out.  I think it's fine to say, hey, this happened and we don't love it.  Let's talk about what happened.  Just like if someone hits a child, that's terrible.

Speaker: SPEAKER_01
Transcript:  Well, and you know that's the next thing that's going to happen.  An autonomous vehicle will, at some point, some kid will dart out in front of an autonomous  vehicle. It'll hit a kid.  And you think that people will be smart enough to say, well, nothing you can do about that.

Speaker: SPEAKER_00
Transcript:  Will we also do this to the statistical view is that in five years when there's half the  cars or autonomous vehicles and we've gone from six thousand pedestrian deaths to three  thousand, will anyone notice?

Speaker: SPEAKER_01
Transcript:  The first thing I thought when I heard the story and I didn't hear anything more than  the self-driving car hit somebody was there's only two possibilities.  The self-driving car malfunctioned or she jumped.  You know, she was not visible.  She came out from between two cars or something and there was no way it could stop.  And I because I trust the cameras and I trust the braking and I trust all of that much more  than I trust a human to notice it and respond appropriately.  The cars can respond immediately as soon as it notices it.  So I actually do trust the self-driving car more than the human.

Speaker: SPEAKER_00
Transcript:  Did you have a sense of Uber car versus Google car?  Who would have done a better job?

Speaker: SPEAKER_03
Transcript:  Well, I thought that Uber.  I didn't think about its technical prowess, but I was like, oh, it'll be interesting to  see how Uber handles yet another blow to its.

Speaker: SPEAKER_01
Transcript:  Well, that's the point. Yeah. So maybe because of it's because it's Uber.  But it wouldn't be ahead. I mean, the same day, I'm sure cars hit a hundred other

Speaker: SPEAKER_00
Transcript:  pedestrians. Six thousand pedestrian deaths a year.  OK, I'm sure it is a legitimate headline.  Twenty a day. Oh, it is.

Speaker: SPEAKER_01
Transcript:  Oh, absolutely. Well, it is.  But it happens 20 times a day.  You don't see the headline 20 times a day.

Speaker: SPEAKER_03
Transcript:  Well, OK, it's a legitimate headline because this is a new thing.  Right. What you're trying to do is shade it in this tone that I don't think a lot of  people. I'm wrong.  You're implying a tonality that I'm sure the occasional person might have, but I don't  think that's the mass. Nobody blames the autonomous vehicle for this.

Speaker: SPEAKER_01
Transcript:  This is just normal.

Speaker: SPEAKER_03
Transcript:  Well, I think people are like, hey, this happened.  What happened? Let's get information.  That's the first thing I was like, whoa, this is a big deal because this is the first  thing that happened. How did it happen?  Once I found out the car was driving at 40 miles an hour, I had a friend who actually  hit somebody who was crossing a major thoroughfare at 10 o'clock at night outside of any  crosswalk. And you know what?  My friend hit her and it was horrible for her.  Right.  You know, this was the kind of thing that could not be.  Well, on the bright side, the car isn't going to feel bad in an autonomous vehicle.

Speaker: SPEAKER_01
Transcript:  Oh, no.  I'm sure the safety driver feels bad.  Oh, no, sir.

Speaker: SPEAKER_03
Transcript:  I'm sure the safety driver feels pretty bad.  The safety driver probably does feel bad.

Speaker: SPEAKER_01
Transcript:  Yeah. He says the first thing I knew, first thing I noticed.  There's cameras everywhere.  So the first thing I know, I didn't know anything was wrong until I heard the us hit  her. So it's very sad.  I mean, it's very sad. I'm not I'm not in any word.

Speaker: SPEAKER_00
Transcript:  Well, it's worse because she was apparently homeless and she had a bike loaded up with  stuff and wasn't paying attention.  Obviously, she went right in front of a car and it makes it.  But but but also the other problem is, oh, it was a homeless person.  And I actually didn't know she was homeless.  I actually. OK.

Speaker: SPEAKER_03
Transcript:  Yeah. Well, and so here's some interesting things to think about.  Right. When you have this sort of thing happen in in an autonomous car, we're going to  have instant data about what happens.

Speaker: SPEAKER_01
Transcript:  True. We have video from front back.  Good point. Yeah.

Speaker: SPEAKER_03
Transcript:  And so so there's interesting things about liability, but also like right now, you know,  the NT National Transportation Safety Board, NTSB is out there.  Those things take like 18 months to resolve.  So when can we start expecting the speed of investigations on these things to, you know,  have that information come out in an authoritative way that hasn't happened before?  And I think that'll start shading coverage a little bit, too.

Speaker: SPEAKER_01
Transcript:  I actually here's a great I really actually like this take from Curbed.  Really, the problem is that we've built our cities in favor of cars more than humans.  Yes. The cities are not pedestrian friendly at all.  And if you've ever been in these Arizona streets, they're big.  Why? There's a four lane street through a city, through a town.  I mean, that's there are all these big streets.  This is I mean, look at this intersection.  This is this is an intersection that favors.  This is a browser that doesn't favor me.  I hate Chrome. Or is this edge?  Oh, Jesus, you're hating everything now.  I'm hating everything. Well, look what it's doing.  I can never show anything anymore.  It just it just freaks. It's only on Twitter.  It freaks out. It doesn't like Twitter for some reason.  I'll zoom in here.  Well, you don't either. Maybe maybe it's taking on your biases.  But I mean, this is clearly an intersection that favors cars more humans.

Speaker: SPEAKER_00
Transcript:  Right. Well, let's count that one, two, three, four, five lanes just in that one part.  Yeah. Yeah. Yeah.

Speaker: SPEAKER_01
Transcript:  So I think that actually is a if I would not mind that story,  if I was worried that there would be all this.  And you're right, there hasn't been.  So I was worried for no reason that there'd be backlash against autonomous vehicles because of  this. But there doesn't seem to be.  Moving right along.  What else do you want to talk about?

Speaker: SPEAKER_00
Transcript:  I saw some other things that were interesting, I think.  What was it?  Amazon. How I thought the numbers is not a number, but the numbers on how what video has done for  Amazon, a leaked memo from 2016 says that man in the high castle, which cost 72 million to produce,  Wow, drew in one point one five million prime subscribers.  Right. So that was a subscriber acquisition cost of 63 dollars for 99 dollars cash from the ice.  And God knows what it paid off right away.  Yeah. Yeah. So you see why why  profit content is has value again if it sells subscriptions.

Speaker: SPEAKER_01
Transcript:  This is actually also an interesting graph from that internal document.  Amazon evaluates TV shows by their cost per first stream.  The price to hook a customer on prime.  They divide the show's production and marketing expenses by the number of people who stream that  program first after signing up. So if you sign up for prime, the first show you stream.  So so the grand lower is better, obviously.  Grand tour number one, then the man in the high castle number two,  Bosch. These are presumably shows people signed up to prime for, although I don't think that's  always the case. For instance, Bosch shows up with a big logo whenever I go to Amazon Prime.  So that may be why it got a lot of hits. Hand of God.  It's also a book.  It's a book. People might who like the book might have might have gone there.

Speaker: SPEAKER_03
Transcript:  It's I'm curious how much of these shows are based on kind of books or reader type,  you know, like people.

Speaker: SPEAKER_01
Transcript:  The Grand True brought people in because the BBC canceled it and it moved there and that had a  built in audience. Man in the High Castle was a book, got a lot of attention. It's not very good,  nor is Bosch. In my opinion, my humble opinion, Hand of God, Goliath, Mozart in the Jungle,  Bosch season two, Man in the High Castle season two. That's interesting. Season one  did much better for both of these sneaky Pete and good girls revolt.

Speaker: SPEAKER_03
Transcript:  If it's the first thing you stream, I imagine that you would do season one first. You sign up.  Oh, you're so smart.

Speaker: SPEAKER_01
Transcript:  Of course you don't start in the middle.  I don't know. Maybe you don't start in the middle. You start at the beginning.  Course. See, this is why computers will never be better than Stacey.

Speaker: SPEAKER_00
Transcript:  I'm sorry. There's your there's your title of the show.

Speaker: SPEAKER_01
Transcript:  Yeah, it's not. It's it's obvious.  I you know, you asked me for some mermaid videos. I'll show you some mermaid videos.

Speaker: SPEAKER_03
Transcript:  I'm not going to try.

Speaker: SPEAKER_01
Transcript:  Amazon's adding a fingerprint reader to its key app.  That's not for the person who's got access to your home. That's for you.

Speaker: SPEAKER_03
Transcript:  So is it for the app on? So is it just to only on Android? Yeah. Yeah. Yeah. Yeah.

Speaker: SPEAKER_01
Transcript:  So somebody can't get your phone and let people into the house, I guess.

Speaker: SPEAKER_03
Transcript:  Yay. Yay. I still would not let an Amazon into my house.

Speaker: SPEAKER_01
Transcript:  No way. Google's now letting you download a handful of programs without downloading them.  Or maybe I should say it's only if it's fuchsia, right?

Speaker: SPEAKER_00
Transcript:  Dan, no, no, no, no, no. This is this is down.

Speaker: SPEAKER_01
Transcript:  PWA's. Yeah, it's basically PWA's. It's actually something Google announced long before that.  Remember, they announced it at Google I.O. a couple of years back. The idea that Google  Play Instance, Instance, instant instant. Now it's games. They've had some other instant apps.  So essentially it's like try before you download Final Fantasy 15, A New Empire, Panda Pop,  Clash Royale. It's a big deal because in the past, these these, you know, instant apps were  limited to two megabytes games. Two megabytes isn't enough. So they've increased the maximum  file size to 10 megabytes. You know, it's funny because they announced this so long ago and it's  finally kind of materializing. But PWA, I think in the long run is going to replace this.  Wait, what is PWA again? Progressive Web Apps. Oh, OK, thank you.

Speaker: SPEAKER_00
Transcript:  A web page that acts like an app. Because the two, you know, so the right thing,  the one makes the web page act more like an app. The other makes the app act more like a web page,  but they basically come together. But isn't that the essence of fuchsia?  This whole notion of a drawer and downloading is now just gone and you just do what you want to do

Speaker: SPEAKER_01
Transcript:  when you want to do it. A system built for instant apps on steroids, according to 9to5 Google.  Fuchsia, which we're still not kind of really clear what the plan is for fuchsia. Does it  replace Android? Does it replace Chrome OS? What is it? What does it do? Mostly because the fuchsia  today is not very capable. But you do run it on a Chromebook, right? So maybe it's to replace

Speaker: SPEAKER_03
Transcript:  Chrome OS. I don't know. Oh, I was hoping for explanation. I saw the story and I was like,  oh, I can't wait for you guys to explain fuchsia to me.

Speaker: SPEAKER_01
Transcript:  It's just another operating system from Google. It's early days yet.  So the most important thing though is these processes will be completely transparent.  It looks like Google is building fuchsia. This is 9to5 Google writing so that when users know what  they want to do next, fuchsia will be happy to accommodate. You don't have to have the app.  This is all kind of modern. I think fuchsia really is designed primarily to be a modern OS.  And I think this is actually smart of Google to be doing this.  I don't know if other companies are doing the same thing.

Speaker: SPEAKER_03
Transcript:  I'm sure every company has to have some sort of effort like this.

Speaker: SPEAKER_01
Transcript:  Because it takes forever to write an operating system, a build from scratch operating system,  which fuchsia is. It's already three or four years old and it's not even close to being a real OS.

Speaker: SPEAKER_03
Transcript:  I mean, think about chip architectures. ARM has been working for five years on their brand new  architecture and we won't even see it in products forever. So an OS is,  I mean, the closer you get to the bone, the bare metal, the closer, the harder it is.

Speaker: SPEAKER_00
Transcript:  Yeah. How long has it been since we've seen a new OS in the world, right?  We used to have more time. Forever.

Speaker: SPEAKER_01
Transcript:  Yeah, forever. What was the last new OS you saw? BOS 20 years ago. I can't think of Mac OS has  been around longer. Windows has been around longer. Linux has been around since 1992. I mean,  they all evolve and change dramatically. Yeah. Yeah. Yeah. Yeah. But the red hat may be sold.

Speaker: SPEAKER_00
Transcript:  Sold to whom? Maybe Google. What was that? I saw that. Oh, that's interesting.  That's really interesting. It is really interesting. Not red hats, Google.  Hats for sale. Only red hats for sale. I wouldn't be looking for red hats.  Red hat could be a Google target. I mean, it's purely speculation.

Speaker: SPEAKER_03
Transcript:  Interesting. Who wrote that article?

Speaker: SPEAKER_00
Transcript:  This one is from WRAL Techwire. Oh, where red hat is.

Speaker: SPEAKER_03
Transcript:  Hold on. No, the author. In Raleigh.  Let's see if they're credible. Rick Smith.

Speaker: SPEAKER_01
Transcript:  Yeah, I don't know who this person is. That's the local TV station where red hat is.  Oh. The Raleigh-Durham Triangle.

Speaker: SPEAKER_00
Transcript:  Bloomberg News is where this is a rewrite of a Bloomberg.

Speaker: SPEAKER_03
Transcript:  Oh, it's a Bloomberg story. Okay. So here we go.  I trust Bloomberg. Well, I can tell you who wrote it.  At Bloomberg, I'll tell you who. I'll tell you how smart they are.

Speaker: SPEAKER_01
Transcript:  In the dancing on their grave department, Amazon is considering buying some Toys R Us stores.

Speaker: SPEAKER_03
Transcript:  I don't think this is a dancing on the... I mean, yes, it's crap to go to Toys R Us,  but part of that is just it was crap to go to Toys R Us. It's always easier to buy from Amazon.

Speaker: SPEAKER_01
Transcript:  There have been a lot of articles recently saying don't blame Amazon for Toys R Us bankruptcy.

Speaker: SPEAKER_03
Transcript:  I do think there's a real interesting issue though with private equity firms buying into  retail chains with physical stores, inventory, all of this, and then putting a lot of debt.

Speaker: SPEAKER_00
Transcript:  Racking up massive debt to do it, yeah. Same thing happened to media.  This is exactly what happened to the newspaper business.

Speaker: SPEAKER_01
Transcript:  It's why Cumulus and very nearly my own company, iHeart Media, iHeart Radio, both  Cumulus did do Chapter 11. iHeart was this close to Chapter 11. They were able to negotiate,  renegotiate their deals with lenders, but yeah, these companies were hugely leveraged.

Speaker: SPEAKER_03
Transcript:  Right, and that used to make sense in a world where there were not options, but it's like...  So I think there's actually a cool story here, and I know some people have reported a bit on it,  is what is the... When your distribution costs come closer to zero, you become more flexible  and less reliant on physical infrastructure from a store or printing press, for example, way.  What kind of model works for a private equity firm going forward? I don't know.

Speaker: SPEAKER_01
Transcript:  I think my money's on CryptoKitties. 12 million bucks!

Speaker: SPEAKER_00
Transcript:  I put this one up there. I was amazed by it. Look at who invested it.  And recent Horowitz, Union Square Ventures.

Speaker: SPEAKER_01
Transcript:  Wow, so CryptoKitties is a blockchain game where it's like a virtual creature game where you create  a virtual creature... Actually, you buy virtual creatures and then deck them out and then sell  them. And CryptoKitties have sold for as much as $200,000. They're virtual, they're not real.  It's really all speculation, right? In fact, at one point they were so popular,  they accounted for 30% of the volume of trading in Ethereum. Yeah, former founder of Coinbase,  CEO and founder of Angelist, Mark Pinkus from Zynga. Some big name investors putting  $12 million into CryptoKitties. Here's the story from VentureBeat. CryptoKitties isn't much when  it comes to gameplay. You can buy a kitten with Ethereum and hold on to it. You can breed kittens,  which have different visual features dubbed traits. Some of the traits are rarer than others.  There are thousands of kittens in the game. Some of them breed fast, others are slower.  CryptoKittieDex keeps track of trains... Trains? Trains. Traits, I think he means,  and how common or rare they are. So it's just a silly little game worth $12 million.  A tamper-proof cyber currency wallet, hardware wallet, ledger. Just got backdoored by a 15-year-old.  Well, maybe they'd be able to find your password that way.  See, now I'm in trouble with Section 230 without doing anything. Yeah, maybe my wallet's in there.  A 15-year-old from the UK, Saleem Rashid, published on his personal blog a proof of concept code that  allowed him to backdoor the Ledger Nano S $100 hardware wallet that the company's marketers say  has sold by the millions. Oh boy, it's a 300-byte long proof of concept.  Wow, good going. See, now I have hope for the future.  Go kid. Young people like this, like Rashid.  AMD says, we've been talking about this weird bunch of flaws on AMD chips and the speculation  that the Israeli security firm, CTS Labs, which seems to have been created entirely to publicize  this issue, they don't have any track record, and only gave AMD 24-hour notice and apparently  tried to make money on the revelation by shorting AMD stock. So there was some concern.  I know. There was some concern.

Speaker: SPEAKER_03
Transcript:  That's like they did this in the medical device community last year. There was a company against  St. Jude's. They published a vulnerability in their, I think it was an insulin pump,  and then they shorted the stock and people were like, yeah, that's not legit.

Speaker: SPEAKER_01
Transcript:  So what's going on apparently is some of these security firms, if you don't have a bug bounty,  which apparently AMD does not, they want to get paid for their research. They've spent years  apparently researching these flaws. They wanted to get paid for the research. So since AMD ain't  going to pay them, they short the stock and then announce it. Profit. But there really is a problem  because AMD has now said, oh yeah, we do have this problem and we're going to patch this.  Nope, we don't expect any performance impact. It's not like the Spectre and Meltdown flaws.  It is interesting though because AMD, among other flaws, AMD was using a chip from a company called  ASM Media and these chips had hardware backdoors built into them by the Taiwanese company that  made the chips. AMD didn't know that or didn't care or whatever, but they put them in their  systems, their systems on a chip, and that means they had hardware backdoors.  It's not a serious flaw in the sense that you have to have root access. You have to  have administrator access to do it. So somebody presumably would have some pretty good access  to your system already. Nevertheless, they're real and AMD is going to fix them. I agree.  First of all, there's a standard in the security community that you give a company 90 days at  least to fix the flaw before you publicize it and it is kind of, come on, shorten the stock really?

Speaker: SPEAKER_03
Transcript:  Yeah, this is kind of a self-regulatory option for these guys, right? So having a community that says,  dude, that's not cool. That might work. Who knows? Who knows?

Speaker: SPEAKER_01
Transcript:  I wonder, I mean, it doesn't really hurt AMD to short the stock as much as it hurts the investors.

Speaker: SPEAKER_03
Transcript:  Or the people with the chips with the security flaw that is now open to everyone for a while.

Speaker: SPEAKER_01
Transcript:  Those guys too. This is a really weird story. And again, this might go back to FOSTA SESTA.  Child abuse imagery has been found within the blockchain, which means everybody who has  a wallet, a Bitcoin wallet on their computer has this imagery in the blockchain.  A wallet or the entire chain? Well, in order to have a Bitcoin wallet,  not one stored on Coinbase or somewhere out there, but on your computer, you have to download  the entire blockchain. The entire?

Speaker: SPEAKER_03
Transcript:  The entire blockchain. That's how it works. So now you're a criminal, dude.

Speaker: SPEAKER_01
Transcript:  Apparently, 1600 files were stored in Bitcoin's blockchain. One of the files, at least eight,  were sexual, including one thought to be an image of child abuse and two that contained links to  child abuse content, 142 of which linked to dark web services. Now, I don't know even how  you get into that imagery. I mean, it's hidden in plain sight, I guess. But yeah, that's messed up.  So don't, I don't know what you should do. It creates a moral and legal quandary.

Speaker: SPEAKER_00
Transcript:  Yeah. Do you think it was subversion?

Speaker: SPEAKER_01
Transcript:  No, I think that somebody, you know, people said, oh, look,

Speaker: SPEAKER_03
Transcript:  You know the people who invest in blockchain? Come on, man.

Speaker: SPEAKER_01
Transcript:  You can stick it in here.  Yeah, well, yeah.

Speaker: SPEAKER_03
Transcript:  Well, that's why IBM created their hyperledger efforts. It's not IBM. IBM worked with other  companies to create hyperledger because companies were afraid of putting some of their data in the  blockchain because they were like, well, people can access this. This is also why whenever I  talked to anybody in the SEC financial crimes or the DEA, they were ecstatic about the blockchain  because they were like, oh, we can, people are creating digital records of all of their  illicit transactions. This is wonderful. I couldn't design a better system.  So there's a lot of misconceptions around what this is and what you're sharing.

Speaker: SPEAKER_01
Transcript:  So I have the blockchain on a couple of my computers because I have a Bitcoin wallet.  I've been trying, remember, this is the one I can't get into.  I guess I'll delete it. I can't get into it anyway.

Speaker: SPEAKER_03
Transcript:  What if, have you gone to a hypnotist yet? I want you to go to a hypnotist to get your.  That's a good idea. I like that.  Okay.  You're going back, Leo. You're going back.

Speaker: SPEAKER_00
Transcript:  I remember.  You're younger now.  It would be a good idea to have a, to take Bitcoin.

Speaker: SPEAKER_01
Transcript:  And then I thought.  Because I do everything.  Yeah, I do. I try.  I throw money away. That's what I do for a living.  I don't have money.  I don't want any.

Speaker: SPEAKER_00
Transcript:  Karsten. Well done, Karsten. Well done.

Speaker: SPEAKER_01
Transcript:  What did he do? I didn't even see it. My eyes are closed.  What are you doing to me, Karsten?  Oh, nice.  Plattsburgh, New York. Is that near you?  Plattsburgh, New York.  Is it upstate?  Boonies.  Well, they have hydroelectric, which means their electricity is only four and a half cents.  This is fascinating.  Per kilowatt hour. Cheap enough to make it actually, I guess, a good place to mine Bitcoin.

Speaker: SPEAKER_03
Transcript:  Oh, I loved this story. The people in Washington and the people in Nowheresville, New York. Yes.

Speaker: SPEAKER_01
Transcript:  So they, the city council of Plattsburgh is worried because a bunch of people are moving in.  City slickers. They're moving in to mine Bitcoin on our cheap electricity.

Speaker: SPEAKER_00
Transcript:  And then what happens to the price of electricity?

Speaker: SPEAKER_01
Transcript:  Goes up. I've been hearing a lot of complaints that electric bills have gone up by 100 or $200.  You can understand why people are upset.  You should see my electric bill if you think that's a bad thing.  Oh, man.  My electric bill.  Hoi. Yeah, look at Stacy's electric bill.  But we live in a hot and expensive electricity towns.

Speaker: SPEAKER_03
Transcript:  16.5 cents per kilowatt hour.

Speaker: SPEAKER_01
Transcript:  I rest my case. You couldn't Bitcoin mine in Austin?

Speaker: SPEAKER_03
Transcript:  No, no. So the city.  Unless it was winter and I wanted to generate my own heat.

Speaker: SPEAKER_01
Transcript:  There is. We saw this on Security Now. There's a Norwegian or a Scandinavian company selling a  heater, a space heater that mines Bitcoin.

Speaker: SPEAKER_03
Transcript:  Oh, yeah. I ran into this at CES. I thought this was awesome.

Speaker: SPEAKER_01
Transcript:  Or maybe it's the other way around. Maybe it's a Bitcoin miner that you can use to heat your house.  That's exactly what it is.  Yeah.

Speaker: SPEAKER_00
Transcript:  But what do you do in the summer?  You sweat.  Open the windows.

Speaker: SPEAKER_03
Transcript:  Yeah.  Evaporative cooling. It's so hot right now.

Speaker: SPEAKER_00
Transcript:  And you turn down the heat, turn down the heat. No, I can't afford it.

Speaker: SPEAKER_01
Transcript:  I can't afford it. They have turned off Bitcoin mining in the city.  The city council voted to ban it for 18 months.  You know, so I don't know how many Bitcoin miners moved into Plattsburgh.

Speaker: SPEAKER_00
Transcript:  Is that I guess there's no legal problem with that, right?  It's not like a free speech or anything.  It's business activity they choose not to have.

Speaker: SPEAKER_01
Transcript:  That's what it is. It's a business activity.

Speaker: SPEAKER_03
Transcript:  Yeah. I guess you could argue you could zone it.  So like some of these, I'm trying to think how you would  It's a reasonable problem.

Speaker: SPEAKER_01
Transcript:  A type of  Plattsburgh only gets, they have an allotment from the  St. Lawrence hydroelectric dam of 104 megawatts per month.  The biggest Bitcoin mining operation that moved in,  a Puerto Rican company called Coinment used 10% of that in January and February.  They used 10% of their power allotment.  In January, Plattsburgh went over the power allotment  and had to buy electricity on the open market.  That's why they had to pay more.

Speaker: SPEAKER_03
Transcript:  So that's an interesting  It's reasonable, I think.  Yeah.  It is reasonable, but that doesn't mean it's against the law,  which is a very different thing.

Speaker: SPEAKER_01
Transcript:  I, you know, if I were this company that moved all the way from Puerto Rico to Plattsburgh.

Speaker: SPEAKER_03
Transcript:  Well, but if you knew how the power was structured,  then that might not be a great move.

Speaker: SPEAKER_01
Transcript:  You did it for four and a half, because of the four and a half cents a kilowatt.  Yeah, if you didn't understand.  Yeah.  Yeah.

Speaker: UNKNOWN
Transcript:  But.

Speaker: SPEAKER_01
Transcript:  I don't know what happened.  This is another one from WRAL.  They're really breaking the stories this week.  Yeah.  Why are they suddenly up?  I know.  This is another one.

Speaker: SPEAKER_03
Transcript:  Maybe they did something to get on your Google list?  No.  Maybe they just optimize Google News.

Speaker: SPEAKER_01
Transcript:  This is a real story from Raleigh.

Speaker: SPEAKER_03
Transcript:  Oh, okay.  Okay.

Speaker: SPEAKER_01
Transcript:  Um, to find suspects, police quietly turned to Google.  I don't know if this is, I don't know what the upshot of this is.

Speaker: SPEAKER_00
Transcript:  I didn't know what the story was trying to be.

Speaker: SPEAKER_01
Transcript:  So, well, so they're.  Scary.  So.  Well, what is it these days?  There were gunshots, shots fired.  And, um, I don't know, something happened.  There were shootings.  And, uh, Google, the police in Raleigh said, wait a minute.  If we could only get the cell phone records of everybody who was in the vicinity,  we'd have some idea who to talk to.  So they drew shapes around the crime scenes on a satellite image,  marking the coordinates on the map.  They convinced a Wake County judge they had enough probable cause to order Google  to hand over account identifiers on every single cell phone  that entered that area during certain times.  They've done it at least four times.  It last year.  Search warrants demanding Google accounts of, so it's not the phone number.  It's not from the cell carrier, but Google's, you know, if you have an Android phone,  or actually, even if you don't, if you have a Apple phone, right?  The maps might be telling Google where you are.  You'll just look at your location tracker on Google and it knows where you are.  What I don't know is if Google hit them over or not.

Speaker: SPEAKER_03
Transcript:  Uh, so what is it?  Is it a dragnet?  What is it?  It's a phishing expedition.  It shouldn't be.  That's the words.  It's not legal.

Speaker: SPEAKER_01
Transcript:  But the judge approved it.  Now, if Google wanted to fight it, I think they could probably make a pretty good case for it.  For that ACLU says-

Speaker: SPEAKER_00
Transcript:  What's the difference between asking Google for this and asking a phone company for this?

Speaker: SPEAKER_01
Transcript:  In both cases, a phishing expedition.  The theory is that you are allowed to go out and ask for information  about people who didn't commit a crime in order to kind of net the guy who did.

Speaker: SPEAKER_03
Transcript:  Right?  Now, but did Google- how could you write this story and not know what Google did here?

Speaker: SPEAKER_01
Transcript:  Well, you should ask that question of WRAL public records reporter Tyler Dukes.

Speaker: SPEAKER_03
Transcript:  Well, no, I mean, it's interesting that police are trying to get to this data.  But then, so that's newsworthy.  I just don't understand what happened.

Speaker: SPEAKER_00
Transcript:  But it's very cool to be RAL has a public records reporter.  That's nice.  That is nice.

Speaker: SPEAKER_03
Transcript:  So, sorry.  Because, no, I'm just-  I don't know.

Speaker: SPEAKER_01
Transcript:  The only thing missing is what's Google's response to this.  He's got a lot of detail about these warrants.

Speaker: SPEAKER_03
Transcript:  Because you can say- I mean, this isn't like a FISA court thing.  So you can actually say that you've got the warrants.  Right.  That someone's-  True, right.  So I don't understand why.  Because it says in the article that people- Google had no way to tell people that they  were being swept up in this.  But I actually think you would.  But again, I'm not an expert here.  This is- I'm like- I want to talk to this man's editor.

Speaker: SPEAKER_01
Transcript:  Well, here's the only thing he says.  Google declined to say whether it released data in any of the RALI cases.  And it's unclear from the search warrants exactly what information was seized  or whether it has been effective in moving the investigations forward.  Of the four cases, only one has resulted in an arrest.  So Google's not saying, which sounds to me like-  Google's not saying.

Speaker: SPEAKER_03
Transcript:  So then we have to wait for the court.

Speaker: SPEAKER_01
Transcript:  Yeah, the trial.

Speaker: SPEAKER_03
Transcript:  They may not put it in the trial.  No.

Speaker: SPEAKER_01
Transcript:  That's weird.  Because they didn't- that's part of- in a way, that would be a litmus test of whether this is  legit or not.  If it's not used in evidence at trial, then you were asking for a lot of information you  wasn't relevant to the trial.

Speaker: SPEAKER_03
Transcript:  Well, but then you wouldn't have a reason to sue either.  So if this guy gets caught and he doesn't understand that this was how he possibly was  gotten caught, then he'd have to sue- like he'd have to appeal, basically.  Sorry, not sue.  Appeal his- on like bad evidence collection.  And there is a word for that that is an actual legal word and I don't know what it is.  But if they don't use it in the trial, then he wouldn't actually have grounds to appeal  based on that, which means it could continue until someone does.  I mean, this was what happened with the stingray type stuff.  Right.  They were doing it, but they just didn't ever use the evidence.  Right.

Speaker: SPEAKER_01
Transcript:  They just use it in their investigation to help them get leads and then continue on.  It really comes down to the judge doing the warrant and then Google fighting it.  And I'm a little disappointed that Google hasn't said whether they are  honoring the warrant or fighting it.  I think they should fight it.

Speaker: SPEAKER_03
Transcript:  Well, maybe they will after people pay attention to it.  Okay, we're paying attention.

Speaker: SPEAKER_01
Transcript:  We're paying attention.  I think we've talked long and hard about all sorts of things.  I'm, you know, I'm just, I'm just, I don't know what to say about Facebook or privacy and-

Speaker: SPEAKER_00
Transcript:  I just-  You've said it, as Howard Stern says, once again, rid of the guest, you've said it all.  I've said it all, my friend.

Speaker: SPEAKER_01
Transcript:  That just means get out of here.  It does, it does.  That's what he says.  Said it all, my friend.  Get out of here.  Let's wrap things up with-  How about you, Jeff?  Let's start with you this time.  A number this week.

Speaker: SPEAKER_00
Transcript:  All right, so minor note, but I think noteworthy that Amazon has surpassed  Alphabet for the first time to become the second most valuable company in the world.  Wow.  After Apple.

Speaker: SPEAKER_01
Transcript:  Yep.  And Jeff Bezos is the richest man in the world.  Yep.

Speaker: UNKNOWN
Transcript:  Yep.

Speaker: SPEAKER_00
Transcript:  Market capitalization of 7.7, 768 billion dollars.

Speaker: SPEAKER_01
Transcript:  So it's a race to see who's the first trillion dollar company.  Yes.  Who do you think?  Will it be Apple?  Will it be Amazon?  Will it be Google?  Whose ego will be satisfied first?  Hey, that's how many commas is trillion?  That's a lot of commas.

Speaker: SPEAKER_00
Transcript:  I don't think it's going to be Facebook real quick.

Speaker: SPEAKER_01
Transcript:  No, I think it's-  Facebook might be going nowhere.  Don't think it's going to be Twitter.  Hey, did I talk about my S9 last week?  I think I did.  Yes.  Yes.  I'm liking it.  What do you think?  Do you like it?  Yeah.  Now I have a longitudinal study of a whole week.  You're putting your finger in the right spot by now?  Yeah.  And it has the face-  I showed you the face recognition is pretty good.  It's a beautiful screen.  Yeah, I quite like it.  And you know what I like?  It's kind of the anti-iPhone.  It has a SD card.  It has a headphone jack.  It has a fingerprint reader.  It's like they decided anything Apple does, we're going to do the opposite.  There's no not.  That's actually smart.  Yeah.  It's an alternative, right?  Stacey, do you have a pick of the week?

Speaker: SPEAKER_03
Transcript:  I read about this on Lifehacker and so I tried it.  Not because I have two children, but because I and my husband have juvenile silly fights  all the time.  So I thought this was fun.  Do you have pillow fights?  What kind of fights do you have?  Oh, I got one.  We just snippet each other over silly things.  How to load the dishwasher.  But those kind of things.  We do.  Every couple does.  We do complain about that.  Right.  I mean, this is just part of living with people.

Speaker: SPEAKER_01
Transcript:  You could ask, hey, Echo, what's the right way to load the dishwasher?

Speaker: SPEAKER_00
Transcript:  Whatever she says is the answer that comes back.

Speaker: SPEAKER_03
Transcript:  Hold on.  Let's see what happens.  What's the right way to load the dishwasher?

Speaker: UNKNOWN
Transcript:  Sorry.

Speaker: SPEAKER_02
Transcript:  Yeah.  Ah, silly.  Sorry.

Speaker: SPEAKER_00
Transcript:  What's the right way to load the dishwasher?

Speaker: SPEAKER_01
Transcript:  Nobody.  Nobody has anything to say.

Speaker: SPEAKER_03
Transcript:  No one's waiting into that fight.  No, not going to get involved in that.

Speaker: SPEAKER_00
Transcript:  That's right.  Yes.

Speaker: SPEAKER_01
Transcript:  What kinds of things do you debate?

Speaker: SPEAKER_03
Transcript:  Called kids court.  Yes.  This is for when your children are, that's not fair.  He took that.  That and the other thing.  You can have your kids, you put information about your kids in here.  They do not get it.  The company does not track it or keep it because of COPPA,  the Children's Online Privacy and Protection Act.  But it basically has the kids make their case and it makes a ruling.  And then it provides a silly punishment for your kids.  Like, you know, hold an ice cube in your hand for X number of minutes or  hop on one leg while singing the alphabet backwards.

Speaker: SPEAKER_01
Transcript:  It's harsh.

Speaker: SPEAKER_03
Transcript:  These are basically silly, fun, and I just, I thought it was fun.  Nice.  And I don't have anything other than that to say this did not solve any burning  issues in our house.  But if a fight gets too tense, you can be like, I know we'll take it to kids court.

Speaker: SPEAKER_01
Transcript:  I think you should.  I'm going to nominate you for the Echo Skill of the Week.

Speaker: SPEAKER_03
Transcript:  Okay.  To provide that every week.  Good.  But next week, you know, I am going to have a Nest doorbell and a Yale Google Lock.  So you may want to not, I just pull from, oh, and here's, well,  we talk about it on our show.  So if you listen to our show, you're going to be bored out of your mind.  But I am, I'm proposing for people to do an IOT spring cleaning, which is what I did for 10 hours.  10 hours?  I have a lot of gadgets.  Yours may not take this long, but what I did is I ended up going through all of my devices  and I said, you know, is this working?  Is this not working?  I went through my Eero to see what was on my network.  I found some things that I don't use at all that I was like, oh, let's take this off.  And then after you take things off your network, and I deleted the accounts and I reset some of  the devices, then I went through things that I want to use, but still don't work.  And I deleted all of those.  And then I put everything back on the network.  And then I deleted everything from Amazon and everything from Google.  And I rebuilt all of my automations and routines.  Is this a pain in the butt?  Yes.  But everything except one device works now, which is a far cry from how my house usually is,  which is like, my husband's like, I used to be able to turn on this light.  What happened?  I'm like, I don't know.

Speaker: SPEAKER_01
Transcript:  I think you better go to kid's court on that one.  Your honor.  I'm sorry.  You're going to be standing on one leg for some time, Ms. Stacey Higginbotham.

Speaker: SPEAKER_03
Transcript:  You need to make that light roar.

Speaker: SPEAKER_01
Transcript:  I don't know what happened.  It just stopped working.  Did I show you my blood pressure reading thing on the S9 last week?  No.  I don't remember.  I didn't.  I showed it on the new screensavers.  So there's a hidden ability in the Galaxy S9.  I think.  I'm not sure.  So they've always, the S9 has always had this feature, which nobody ever talks about.  Underneath the flash on the camera, there's a heart rate sensor.  I think it's actually a very accurate heart rate reading from it.  It flashes.

Speaker: SPEAKER_00
Transcript:  There are apps that you can do that with any phone's light.

Speaker: SPEAKER_01
Transcript:  Yeah, with the camera.  But this is much more accurate.  Samsung Health uses it to do the reading.  I think it actually is very accurate because I've compared it to other sensors and so forth.  You wouldn't want to do it while exercising.  You have to put your finger on here and all of that.  But what I didn't know is apparently that in the S9 is also capable, and I wonder if this is true,  of doing blood pressure readings.

Speaker: SPEAKER_03
Transcript:  Have you tried it?  Or what do you wonder if it's true?

Speaker: SPEAKER_01
Transcript:  No, no.  The reason I, here's why I wonder it.  There is a University of California at San Francisco health project called My BP Lab.  And you sign up for it.  And of course you're giving them all your information.  But the idea is you take three, and they call them blood pressure readings.  But here's the funny thing.  They don't show you what your blood pressure is.  So it might be you have to calibrate it or maybe it's not that calibrated.  But the idea is three times a day you take your blood pressure.  Again, they don't tell you what it is.  They tell you what they do tell you is it's 8% below your baseline or 4% above your baseline.  But they don't say the actual number.  And my suspicion is that's because it's not accurate.  Although they do ask you when you first sign up if you have a blood pressure cuff,  and then they ask you to calibrate.  So maybe it is.  Again, it's a study with the University of California San Francisco, which is fairly reliable.  They say it allows blood pressure to be directly measured by the smartphone  without any external hardware.  UCSF is going to get information about stress and blood pressure levels throughout the day.  One aim of the study is to optimize My BP Lab to provide contextualized and scientifically  informed feedback so users will be able to get better understanding of their stress and  blood pressure levels and manage their health more effectively.

Speaker: SPEAKER_03
Transcript:  Okay, you want to learn about I actually did a blood pressure cuff review for the wire cutter.  And as part of that, I learned a lot about wire like how to how blood pressure cuffs work.  So do you think it's accurate?  It depends.  So the most accurate is going to be the arm cuff that  puffs up because they're actually measuring pressure in your arm.  And the sphygmometer.  That's figure.  Yes. Thank you for saying those words.  What these do is there it's kind of like the wrist.  I'm assuming I am making some assumptions.  This is probably closer to what the wrist monitors are measuring.  And in that case, what they're measuring, they're measuring.  What they're measuring is then run through an algorithm that they have  correlated to a good blood pressure.  So it all depends on how good your algorithm is.  How much data?  In general and how it reacts to your particular body.  Right.  So because it's a generic algorithm.  So that's one.  And then two, you have to take your blood pressure for accurate readings.  You actually have to there's a very specific way you should take it.  And you have to be seated.  You can't be talking.

Speaker: SPEAKER_01
Transcript:  Both feet on the ground, no movement, no talking, and your hand has to be at heart level.  They say that in the app.

Speaker: SPEAKER_03
Transcript:  So that's good that they're telling you that.  Because even like the doctor's office, I can't tell you how many times the nurse is talking to me while  she's taking my blood pressure.  All the time.  So yes, so there you go.  It may work for you.

Speaker: SPEAKER_01
Transcript:  Well, I have one of these cardio QA-RDI.  Oh, that was one of the worst ones.  Great to know.  All right.  Because that's the one I have.  So you think an Omron, which they've been around forever.  I have a more traditional Omron.

Speaker: SPEAKER_03
Transcript:  The Omron.  Google the wire cutter blood pressure.

Speaker: SPEAKER_01
Transcript:  You said the Omron Series 10 with Bluetooth is the best.

Speaker: SPEAKER_03
Transcript:  There you go.  That's the one.

Speaker: SPEAKER_01
Transcript:  I don't mind you have Bluetooth, but it's just like that.  It's just older.  Yeah.  I mean, we just, I don't know.  At the doctor's office, they calibrate it all the time.  I mean, they're really always making sure it's accurate.  So the other thing is your blood pressure varies wildly throughout the day.  Well, yes.  So that's why, although if this were a reliable measurement on this S9,  that would be hugely valuable, I think, to people in general.

Speaker: SPEAKER_03
Transcript:  Well, and there is value if it's a relative value.  It's a relative value.  A relative value over time.  Thank you.  And that's what they're giving you.

Speaker: SPEAKER_01
Transcript:  That's what they're saying.  They're saying based on your baseline, which you get provided to us,  you're 8% above that, you're 4% below that.  Your blood pressure is a little high.  It's a little low, that kind of thing.  I think that's actually as useful as it gets.  Because I go to the doctor and he measures it.  And that's the official measurement.  Right?  Of course, it's always higher at the doctor's office because I'm scared.  I have white coat hypertension.

Speaker: SPEAKER_03
Transcript:  Yeah.  So get your armor on.  Measure your thing first thing in the morning and for a couple of days.

Speaker: SPEAKER_01
Transcript:  So the cardio is the least accurate.

Speaker: SPEAKER_03
Transcript:  Was it the least?  I was like, it was not a good one.

Speaker: SPEAKER_01
Transcript:  Karsten, did she hate the cardio?  It's so convenient because it's so small and it works with an iPhone and  it's wireless and all that.  But I have some other ones as well.  Because I have slightly high blood pressure, so I've always monitored it a little bit.  It's lower now that I work with you and Jeff.  Until we talk about certain topics.  This show is the number one source of stress in my life.  All right, Jeff, high five.  You're killing me, Stacey.  You're killing me.

Speaker: SPEAKER_03
Transcript:  I would hate that.  I don't think we ended up...  I didn't run the cardio through the biggest...  I'm trying to remember now.  As you can see, there were 10 of them.  We have the article.  I'm sure I can find out.  Scroll down because I feel like it was awkward to use and it...  Hey, that's my arm.  Look at that, you guys.

Speaker: SPEAKER_01
Transcript:  You know what's cute?  This is one of those things where you'd be insane to take this article on.  You've got to do so much testing that normally they would do this with a dozen people in a lab.  It's crazy.  You said I was going to just do this all myself?

Speaker: SPEAKER_03
Transcript:  I did individually test each one and then I think I ran it through some friends,  but I brought it to the University of Texas nursing school.  Wow, you are such a great...  Wow.  So, and they had...  They...  For two hours, it was a little shorter because they had just had the murder on campus.  So this was whenever that was.

Speaker: SPEAKER_01
Transcript:  I know you're busy right now, but yeah.

Speaker: SPEAKER_03
Transcript:  Yeah, but they got extra credit.  So it was 20 nursing students and three actual professors.  And we actually ran through...  The hardest part was actually making sure everyone tried everything and wrote down their thoughts.

Speaker: SPEAKER_01
Transcript:  Oh yeah, here they are, the student nurses.

Speaker: SPEAKER_00
Transcript:  So when you said the wire cutter thing, how many hours you spent doing it,  how many hours did it take?

Speaker: SPEAKER_03
Transcript:  I would have to add...

Speaker: SPEAKER_00
Transcript:  You don't want to know, I'm telling you.  Don't they always say at the beginning of the stories, we spent 80 hours looking at...  Oh, they do, don't they?

Speaker: SPEAKER_03
Transcript:  That's because they used to pay by the hour.

Speaker: SPEAKER_01
Transcript:  So it says after spending 20 hours researching more than 50 blood pressure monitors,  interviewing medical professionals and testing 10 finalists with a group of nursing professors  and students, we can say the best.  This is why I love the wire cutter.  They're great.  And I admire you so much for doing this.  That's a lot of work.

Speaker: SPEAKER_03
Transcript:  I thought it'd be kind of fun, but it did suck.  I love the wire cutter, but doing the wire cutters standards in stories, they are...

Speaker: SPEAKER_00
Transcript:  It's like Consumer Reports cannot be a fun place to work.  Right.  Your blood pressure...  I used to work there.

Speaker: SPEAKER_03
Transcript:  What?  You did?  Sorry, I used to work for Consumers Union.  It was my one PR thing.

Speaker: SPEAKER_01
Transcript:  Yeah, clearly the publishers of the Consumers Report.  I love Consumers Union.

Speaker: SPEAKER_03
Transcript:  So I worked on the campaign to get your prescription for eyeglasses from your optometrist so you could  go on the web or anywhere.  It was before the web.  You won.  So we won on that one.

Speaker: SPEAKER_01
Transcript:  You won.  Every time I go to the eye doctor now, they print out a prescription,  then they give it to me, and then grit their teeth.

Speaker: SPEAKER_03
Transcript:  So this was when I discovered that the quotes and press releases...  Oh my gosh, you guys, they are made up.  Yes, they are.  As a fledgling journalism student, I walked in and I'm like,  hey, can I interview you because I've got to write this press release up?  He's like, what?  I was like, I need a quote about why we're doing this.

Speaker: SPEAKER_01
Transcript:  You write it and then run it by me.  I was like, oh my God.  I just want to say I write my own quotes.  I stopped that, nipped that in the butt.  I said nobody's writing my quotes.  It has to be in my language.

Speaker: SPEAKER_00
Transcript:  Well, I've had to write other people's quotes.  I hate writing other people's quotes.  I hate it.

Speaker: SPEAKER_03
Transcript:  It feels so dishonest.  I was just like...

Speaker: SPEAKER_01
Transcript:  Oh, it's standard practice.  It's not that you write something that they wouldn't say.  You write it and then you say, is this okay?  And they say yes or no.  But it's still...

Speaker: SPEAKER_00
Transcript:  But then it's quoted in the media as if the person actually said this.

Speaker: SPEAKER_03
Transcript:  The fact that I, as a journalism student, could barely stomach my PR advocacy at...  For shame.  ...Consumers Union.  You went over...

Speaker: SPEAKER_01
Transcript:  Yeah, you really are a journalist.  Shame.  It's in the dark side.  Yeah, but Consumers Union, come on.  Good question.  You say that cardio has a poor accuracy rating.

Speaker: SPEAKER_03
Transcript:  It has a poor accuracy rating.  And I think it was difficult to turn off, thinking about everyone else.  Yeah, it's kind of weird.  The UI is kind of weird.  The UI was not conducive to everyone.  It was harder than it needed to be, I thought.

Speaker: SPEAKER_01
Transcript:  You say it seemed accurate in my initial test because it won't turn off until it's properly closed.  It tends to run out of batteries very easily.

Speaker: SPEAKER_03
Transcript:  There you go.

Speaker: UNKNOWN
Transcript:  Yeah.

Speaker: SPEAKER_03
Transcript:  And the other thing is some of these were better with people who were  skinny versus people who were overweight.  So that was something that was interesting.

Speaker: SPEAKER_01
Transcript:  Us fatsoes, we need extra large cuffs.

Speaker: SPEAKER_03
Transcript:  Well, I was not going to...

Speaker: SPEAKER_01
Transcript:  I'm not fat, I'm buff.  Ladies and gentlemen, on that note, I will bid you a fond farewell.  Stacey Higginbotham, you catch her work at stacey on iot.com.  She also does the Stacey on iot podcast with Kevin Toffle every week.  She's at gigastacey on the Twitter.  Jeff Jarvis is at buzzmachine.com.  And he's of course...  Not to go out with a snowblower.  Are you going to do that?

Speaker: SPEAKER_00
Transcript:  No, you don't use a snowblower.  Well, the guy who does our driveway is gone for two weeks.  So I'm going to have to do it.

Speaker: SPEAKER_01
Transcript:  Dude, you have AFib.  You cannot shovel snow.  I'm not shoveling, I'm using the blower.

Speaker: SPEAKER_03
Transcript:  Don't you have a family member who can do this for you?

Speaker: SPEAKER_01
Transcript:  No, it's him and the cats and your wife.  Get your wife out there.  Your daughter's still...  Yeah, get your wife out there.  Yeah.

Speaker: SPEAKER_03
Transcript:  I mean, seriously, if my husband was going to keel over from a heart attack, I'd shovel the snow.

Speaker: SPEAKER_01
Transcript:  Shoveling snow, at least when I lived back East, was the number one cause of heart attacks.  I'm not shoveling, I'm pushing the dam.

Speaker: SPEAKER_00
Transcript:  It's a big one.  It's a monster snowblower.  I'm trying to figure out how to get a selfie of myself with the snowblower.  I want to see this.  Nice.  Make your wife do it.

Speaker: SPEAKER_01
Transcript:  Yeah.  Thank you both for being here.  Thank you both for being here.  I do stay safe in the snow, Jeff, I'm sorry.  Nor'easter number four, Toby.  Yep.  Four Nor'easters in what, two weeks, I think.  And Stacey, try to enjoy the grackles and the nut hatches.  Yeah, and your damn sunlight, Stacey.  And the swifts.

Speaker: SPEAKER_03
Transcript:  It is nice.  I'm going to be up on the roof tonight with my margarita.  And your queso?  Will there be queso?  Just shush.  Jeff, if you're coming for ice...  Is it ice-hodge?

Speaker: SPEAKER_00
Transcript:  No, I'm not.  I'm going to be in Perugia.

Speaker: SPEAKER_01
Transcript:  Oh, golly.  Now I feel terrible.  He has to go all the way to Perugia, Italy.  That's news.  What is that?  Newsgeist?  What is that you do there?

Speaker: SPEAKER_00
Transcript:  That's the Perugia Journalism Festival.  Okay.  Newsgeist is going to be in Lisbon.  Oh, nice.  And Buenas Ares.

Speaker: SPEAKER_01
Transcript:  I'm glad I don't live in Austin.  I would die for a heart attack because the food there is so good.  I would just not be able to stop eating.  Welcome to the club.  Queso.  Oh my God.

Speaker: SPEAKER_03
Transcript:  You just have to exercise more.

Speaker: SPEAKER_01
Transcript:  Yeah.  Thank you for joining us.  We do this show every Wednesday.  It's kind of the last show of my week.  Every Wednesday about 1.30 Pacific, 4.30 Eastern, 20.30 UTC.  If you want to watch live, we'd love it.  Just tune in, twit.tv slash live.  Join us in the chat room at irc.twit.tv.  If you'd like to join us in the studio like Paul and Laurie did,  they're visiting us for the most scenic place in America.  According to Good Morning America?  Wow.  Well, that must be true.  Lake Anne, Michigan.  Sleeping Bear?  Okay.  Whatever.  You know, you Michiganders, you know what we're talking about.  You know.

Speaker: SPEAKER_03
Transcript:  What are people from Michigan called?  Michiganders.

Speaker: SPEAKER_01
Transcript:  Really?  Yeah, I know.  It's kind of embarrassing.  You got Texans there, Michiganders.

Speaker: UNKNOWN
Transcript:  Okay.

Speaker: SPEAKER_01
Transcript:  Oh, he's a Texan.  So it's all right.  Nice to have you Paul and Laurie.  If you want to do that, email tickets at twit.tv.  So no charge.  We just want to know ahead of time and make sure we can have a seat put out for you.  You can also get these-  He'll wear his pants.  Yeah.  I get the memo every morning.  Yeah, there'll be people in studio.  You better wear your pants today.  Oh man, I was going to wear slippers.  You can also get in any version of any show we do on demand.  We've got audio, we've got video, we've got high def, low def, medium def.  All at the website, twit.tv slash twig for this show.  twit.tv slash twig or wherever you get podcasts.  We need a new picture for this week in Google.

Speaker: SPEAKER_03
Transcript:  I was going to say, Karstil, I thought you changed that picture.

Speaker: SPEAKER_01
Transcript:  Yeah, because I see it's Elgin and Jarvis and it's in the old studio and we can do better.  We'll get up.  Did we take any pictures when Stacey was here?  We'd like to have live people in the studio, but we'll figure it out.

Speaker: SPEAKER_03
Transcript:  Yeah, well, sometime I'll come up again.

Speaker: SPEAKER_01
Transcript:  It's your show now.  And now that she's met me twice, she's not so afraid.  All right.  And we're going to get some of your friends from Austin on the show, I believe.  Oh, yes.  Like Wendy Nather from a duo.  We're trying to get her on.  Wesley, we're trying to get on.  Is his name Wesley?  Yeah, Wesley.

Speaker: SPEAKER_03
Transcript:  Wesley Faulkner?  Yeah.

Speaker: SPEAKER_01
Transcript:  What's he going to talk about?  And it doesn't matter.  He's a nice guy.  He's a nice guy.  He's a nice guy.  He used to work at AMD.  He knows tech.

Speaker: SPEAKER_03
Transcript:  That's how I know Wesley.

Speaker: SPEAKER_01
Transcript:  Right.  And Bo Woods, we're going to try to get him on.  We're going to get all the people we met down there in Austin.  That was a lot of fun.  Thanks for joining us, everybody.  We'll see you next time on Twig.

