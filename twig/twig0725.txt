;FFMETADATA1
album=This Week in Google
genre=Podcast
encoded_by=Uniblab 5.3
title=But Senator Trellis...
language=English
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
track=725
date=2023
TRDA=2023-07-20
comment=<p>IoT cybersecurity label, defending Lina Khan, LLaMA 2, AI Synthetic data</p>\

encoder=Lavf60.3.100

[00:00:00.000 --> 00:00:02.960]   It's time for Twig this week in Google.
[00:00:02.960 --> 00:00:05.640]   Jeff's here, Ants here, Stacey's here.
[00:00:05.640 --> 00:00:08.440]   We're going to talk about Stacey, especially the new
[00:00:08.440 --> 00:00:12.400]   Smart Home Cybersecurity label and what it will mean.
[00:00:12.400 --> 00:00:17.280]   We'll defend Lena Kahn and her FTC despite a couple of notable losses.
[00:00:17.280 --> 00:00:20.360]   And then we'll talk a little bit about AI.
[00:00:20.360 --> 00:00:22.240]   What is synthetic data?
[00:00:22.240 --> 00:00:24.640]   And is it a good thing for AI?
[00:00:24.640 --> 00:00:27.840]   All that and more coming up next on Twig.
[00:00:29.760 --> 00:00:32.920]   The show is brought to you by Cisco Maraki.
[00:00:32.920 --> 00:00:37.120]   Without a cloud managed network, businesses inevitably fall behind.
[00:00:37.120 --> 00:00:42.320]   Experience the ease and efficiency of Maraki's single platform to elevate the place
[00:00:42.320 --> 00:00:44.720]   where your employees and customers come together.
[00:00:44.720 --> 00:00:51.240]   Cisco Maraki maximizes uptime and minimizes loss to digitally transform your organization.
[00:00:51.240 --> 00:00:55.720]   Maraki's intuitive interface, increased connectivity and multi-site management keep
[00:00:55.720 --> 00:01:00.360]   your organization operating seamlessly and securely wherever your team is.
[00:01:00.360 --> 00:01:05.720]   Let's Cisco Maraki's 24/7 available support help your organization's remote, on-site
[00:01:05.720 --> 00:01:08.720]   and hybrid teams always do their best work.
[00:01:08.720 --> 00:01:12.360]   Visit maraki.sisco.com/twit.
[00:01:12.360 --> 00:01:18.760]   Podcasts you love from people you trust.
[00:01:18.760 --> 00:01:21.480]   This is Twig.
[00:01:21.480 --> 00:01:35.480]   This week in Google episode 725 recorded Wednesday, July 19th, 2023.
[00:01:35.480 --> 00:01:42.040]   But Senator Trellis, this week in Google is brought to you by ACI Learning.
[00:01:42.040 --> 00:01:45.280]   IT skills are outdated in about 18 months.
[00:01:45.280 --> 00:01:50.280]   You could launch or advance your career today with quality, affordable and entertaining training.
[00:01:50.280 --> 00:01:56.520]   Individuals use the code TWIT30 for 30% off a standard premium individual IT Pro membership
[00:01:56.520 --> 00:02:01.080]   at go.acilearning.com/twit.
[00:02:01.080 --> 00:02:04.680]   And by Brooke Lennon, summer's in full swing.
[00:02:04.680 --> 00:02:10.040]   Brooke Lennon's here to help you swap out that old, warm winter stuff for easy, breezy comfort
[00:02:10.040 --> 00:02:12.840]   with their award-winning sheets and home essentials.
[00:02:12.840 --> 00:02:19.160]   Visit brooklenden.com today and get $20 off plus free shipping on orders of $100 plus
[00:02:19.160 --> 00:02:20.440]   with the code TWIG.
[00:02:20.440 --> 00:02:24.200]   It's time for Twig.
[00:02:24.200 --> 00:02:26.200]   Oh yeah, this week in Google.
[00:02:26.200 --> 00:02:29.560]   The show where we cover all the latest news from the internet.
[00:02:29.560 --> 00:02:31.720]   Jeff Jarvis is here.
[00:02:31.720 --> 00:02:35.880]   He is our townite professor of journalistic innovation.
[00:02:35.880 --> 00:02:39.720]   He's the Leonard Taure professor at the Craig Newmark.
[00:02:39.720 --> 00:02:45.400]   Graduate School of Journalism at the University of New York.
[00:02:45.400 --> 00:02:47.000]   Hello Jeff.
[00:02:47.000 --> 00:02:48.280]   Hello there.
[00:02:48.280 --> 00:02:50.840]   You're coming out in a week, right?
[00:02:50.840 --> 00:02:51.480]   Coming out.
[00:02:51.480 --> 00:02:54.040]   Yes, I'll be out the next Tuesday.
[00:02:54.040 --> 00:02:55.720]   I'm speaking at the Commonwealth Club.
[00:02:55.720 --> 00:02:58.040]   I'll be honest and humble about it.
[00:02:58.040 --> 00:02:59.960]   There aren't enough people buying tickets yet.
[00:02:59.960 --> 00:03:04.680]   So I want TWIT army has come out nicely to my talks in London and in Boston.
[00:03:04.680 --> 00:03:10.920]   Can TWIG army come out to San Francisco on the night of the 25th of the next Tuesday,
[00:03:10.920 --> 00:03:11.800]   6 p.m.?
[00:03:11.800 --> 00:03:14.200]   I can't because I have to do the show.
[00:03:14.200 --> 00:03:17.640]   But Lisa and others from TWIT will be there.
[00:03:18.280 --> 00:03:19.240]   So I love her.
[00:03:19.240 --> 00:03:20.280]   Yeah, she bought tickets.
[00:03:20.280 --> 00:03:24.920]   And if you go to a bit.ly/here, Jeff.
[00:03:24.920 --> 00:03:30.280]   And if you use the code, Jarvis CWC, you get 10 bucks off.
[00:03:30.280 --> 00:03:32.840]   Now there are online only tickets.
[00:03:32.840 --> 00:03:34.680]   So you don't have to be in the Bay Area.
[00:03:34.680 --> 00:03:38.440]   That could also use the Jarvis CWC code and get 10 bucks off.
[00:03:38.440 --> 00:03:39.160]   That's so it's free.
[00:03:39.160 --> 00:03:40.600]   Oh, that's nice.
[00:03:40.600 --> 00:03:43.160]   So do it online if you're not in the Bay Area.
[00:03:43.160 --> 00:03:44.680]   But if you are in the Bay Area,
[00:03:45.240 --> 00:03:47.240]   Tuesday, July 25th, go see.
[00:03:47.240 --> 00:03:47.720]   Please come.
[00:03:47.720 --> 00:03:51.080]   You can see Jeff and Lisa go say hi.
[00:03:51.080 --> 00:03:51.560]   Yes.
[00:03:51.560 --> 00:03:51.880]   Yes.
[00:03:51.880 --> 00:03:52.520]   Lots of people.
[00:03:52.520 --> 00:03:53.880]   A lot of my old friends be there.
[00:03:53.880 --> 00:03:54.680]   Very nice of them.
[00:03:54.680 --> 00:03:56.120]   Oh, that's great.
[00:03:56.120 --> 00:03:58.600]   My old friend Susan Mernitt is bringing eight people.
[00:03:58.600 --> 00:03:59.880]   Oh, I follow her.
[00:03:59.880 --> 00:04:01.480]   Well, I know Susan Mernitt.
[00:04:01.480 --> 00:04:02.120]   Yeah.
[00:04:02.120 --> 00:04:03.480]   You should have her on Twitter.
[00:04:03.480 --> 00:04:04.520]   She used to follow her on Twitter.
[00:04:04.520 --> 00:04:06.040]   News.
[00:04:06.040 --> 00:04:07.320]   Pioneer.
[00:04:07.320 --> 00:04:13.800]   Yeah, I think she invited me to speak to the online news association.
[00:04:13.800 --> 00:04:14.440]   I think.
[00:04:14.440 --> 00:04:15.320]   I think it was.
[00:04:15.320 --> 00:04:15.960]   Oh, she might have.
[00:04:15.960 --> 00:04:16.200]   Yeah.
[00:04:16.200 --> 00:04:21.400]   Also with us, Mr. Ant Pruitt, manager, community guy,
[00:04:21.400 --> 00:04:25.160]   and the beautiful club twit, environs.
[00:04:25.160 --> 00:04:29.960]   He's going to be doing a photo critique in club twit.
[00:04:29.960 --> 00:04:30.760]   That's exciting.
[00:04:30.760 --> 00:04:32.920]   I hope you do this on a more on a regular basis.
[00:04:32.920 --> 00:04:34.360]   I'd like to do that as well.
[00:04:34.360 --> 00:04:38.280]   So yeah, join us coming up here soon at the beginning of August.
[00:04:38.280 --> 00:04:40.680]   And hopefully my description doesn't confuse anybody.
[00:04:40.680 --> 00:04:43.160]   I'm not just wanting to have coffee with people.
[00:04:43.160 --> 00:04:44.520]   You don't have to have coffee to do it.
[00:04:44.520 --> 00:04:49.560]   It's coffee time, but it's based on coffee with your photography.
[00:04:49.560 --> 00:04:50.600]   So nice.
[00:04:50.600 --> 00:04:52.200]   The theme is coffee time.
[00:04:52.200 --> 00:04:55.160]   Get your camera regardless of what camera you have.
[00:04:55.160 --> 00:04:58.600]   Go take some dad gum pictures and let's do a critique show.
[00:04:58.600 --> 00:04:59.400]   Friday August.
[00:04:59.400 --> 00:05:02.680]   Wait, are we supposed to take pictures of our coffee?
[00:05:02.680 --> 00:05:03.560]   Are the coffee themed?
[00:05:03.560 --> 00:05:05.160]   Coffee, that's coffee time.
[00:05:05.160 --> 00:05:06.760]   That's so confused.
[00:05:06.760 --> 00:05:08.920]   What does coffee timing to you?
[00:05:08.920 --> 00:05:10.040]   Put that in a photograph.
[00:05:10.040 --> 00:05:10.440]   That's all.
[00:05:10.440 --> 00:05:12.920]   I didn't think it'd be that difficult.
[00:05:12.920 --> 00:05:13.880]   That's your assignment.
[00:05:13.880 --> 00:05:14.920]   The assignment is...
[00:05:14.920 --> 00:05:19.080]   Well, it sounds like maybe that it's coffee time with Ant.
[00:05:19.080 --> 00:05:20.760]   But it isn't coffee time with Ant.
[00:05:20.760 --> 00:05:22.760]   It is, assignment is Ant.
[00:05:22.760 --> 00:05:24.600]   Let me go in here and rewrite this deck.
[00:05:24.600 --> 00:05:29.880]   And you just showed a picture of Ant's coffee time.
[00:05:29.880 --> 00:05:30.920]   Coffee time, life, life.
[00:05:30.920 --> 00:05:31.800]   A minute ago.
[00:05:31.800 --> 00:05:33.640]   It is comfortable chair outside.
[00:05:33.640 --> 00:05:33.960]   Yes.
[00:05:33.960 --> 00:05:36.040]   I don't know if he had coffee.
[00:05:36.040 --> 00:05:38.120]   Stacy's book club is also coming up.
[00:05:38.120 --> 00:05:41.000]   August 31st, you have decided on a book.
[00:05:41.000 --> 00:05:41.480]   Yep.
[00:05:41.480 --> 00:05:42.600]   Translation Staked.
[00:05:42.600 --> 00:05:44.280]   What's that all about?
[00:05:44.280 --> 00:05:45.880]   We're reading and lucky.
[00:05:45.880 --> 00:05:50.520]   Finally, it's been suggested probably four times.
[00:05:50.520 --> 00:05:51.640]   It's been a year and a half.
[00:05:51.640 --> 00:05:53.800]   It's probably finally got this book in.
[00:05:53.800 --> 00:05:55.400]   It didn't get the vote.
[00:05:55.400 --> 00:05:59.880]   Stacy Higginbothamhost@stacyoniot.com
[00:05:59.880 --> 00:06:01.480]   @gigastacy on the Twitter.
[00:06:01.480 --> 00:06:06.760]   And our IoT expert, and actually the first story is right up your alley.
[00:06:06.760 --> 00:06:09.560]   And I know you're ready to talk about this.
[00:06:09.560 --> 00:06:16.040]   The Biden administration has announced its new smart home cyber security label.
[00:06:16.040 --> 00:06:19.960]   It's the US cyber trust mark.
[00:06:19.960 --> 00:06:21.240]   Really?
[00:06:21.240 --> 00:06:24.280]   You're going to use your stories?
[00:06:24.280 --> 00:06:25.240]   You could show Stacy's.
[00:06:25.240 --> 00:06:26.920]   I have an eye put in the rundown.
[00:06:26.920 --> 00:06:27.320]   OK.
[00:06:27.320 --> 00:06:28.680]   I don't know what I'm going to show you.
[00:06:28.680 --> 00:06:29.160]   How dare you?
[00:06:29.160 --> 00:06:30.120]   I literally got it.
[00:06:30.120 --> 00:06:33.480]   Their headline was Biden administration.
[00:06:33.480 --> 00:06:35.960]   Does the cyber security label for smart TVs?
[00:06:35.960 --> 00:06:37.400]   Which I was like, I mean.
[00:06:37.400 --> 00:06:38.440]   How about this?
[00:06:38.440 --> 00:06:41.320]   According to Stacy on IoT, the White House is.
[00:06:41.320 --> 00:06:41.960]   There you go.
[00:06:41.960 --> 00:06:44.360]   It's IoT security label plan.
[00:06:44.360 --> 00:06:48.360]   So what is it and how do it work?
[00:06:48.360 --> 00:06:50.920]   This, yeah.
[00:06:50.920 --> 00:06:53.080]   This is the US cyber trust mark.
[00:06:53.080 --> 00:06:56.680]   They introduced the concept of doing this back in October.
[00:06:56.680 --> 00:07:00.440]   And the Biden administration is basically creating a voluntary program,
[00:07:00.440 --> 00:07:02.680]   kind of like the EPA's energy star thing.
[00:07:02.680 --> 00:07:06.680]   And if you see this on a product, and it won't be out until like 2024,
[00:07:06.680 --> 00:07:10.120]   but if you see this mark on a product, you can trust it.
[00:07:10.120 --> 00:07:14.520]   This product has met cyber security standards.
[00:07:14.520 --> 00:07:17.160]   And you might be like, what standards, Stacy?
[00:07:17.160 --> 00:07:21.560]   And the answer is we don't exactly know that.
[00:07:21.560 --> 00:07:21.800]   But.
[00:07:21.800 --> 00:07:28.120]   So it's your article says they're leaning on a NIST document,
[00:07:28.120 --> 00:07:30.760]   the National Institutes of Standards in Tom.
[00:07:30.760 --> 00:07:31.000]   Yes.
[00:07:31.000 --> 00:07:32.600]   So what is it?
[00:07:32.600 --> 00:07:37.800]   The NIST 8524 document, it's actually really good document.
[00:07:37.800 --> 00:07:40.200]   So this came out in 2022.
[00:07:40.200 --> 00:07:43.560]   Y'all are like, Stacy, that's crazy.
[00:07:43.560 --> 00:07:44.280]   It's too much.
[00:07:44.280 --> 00:07:45.240]   It's 30 pages.
[00:07:45.240 --> 00:07:45.800]   It's not.
[00:07:45.800 --> 00:07:46.760]   Don't worry.
[00:07:46.760 --> 00:07:50.360]   What NIST has done basically is they took a deep dive into IoT and they were like,
[00:07:50.360 --> 00:07:53.320]   well, crap, IoT is really hard to secure for a couple of reasons.
[00:07:53.320 --> 00:07:56.920]   One, you got hardware, software, cloud, all this stuff.
[00:07:56.920 --> 00:07:59.720]   You got an ecosystem of devices talking to each other.
[00:07:59.720 --> 00:08:03.800]   And then you've got devices that range from really important things,
[00:08:03.800 --> 00:08:09.320]   medical devices all the way down to, I used to have an egg counter,
[00:08:09.320 --> 00:08:10.600]   that counted my eggs.
[00:08:10.600 --> 00:08:12.680]   And all of those different levels of time.
[00:08:12.680 --> 00:08:14.280]   Your eggs are counted them.
[00:08:14.280 --> 00:08:18.520]   It was called the quirky egg counter.
[00:08:18.520 --> 00:08:21.080]   Did it count how many eggs you have?
[00:08:21.080 --> 00:08:25.880]   It literally only existed to let me know how many eggs were in my fridge.
[00:08:25.880 --> 00:08:28.040]   It was a fun thing that I got.
[00:08:28.040 --> 00:08:30.360]   Wait, I've heard of an egg timer.
[00:08:30.360 --> 00:08:35.560]   But an egg counter that seems not confusing is coffee time.
[00:08:35.560 --> 00:08:36.600]   So useful.
[00:08:36.600 --> 00:08:37.800]   That's all that.
[00:08:37.800 --> 00:08:39.320]   Anyway, good.
[00:08:39.320 --> 00:08:40.440]   I hope it's secure.
[00:08:40.440 --> 00:08:41.160]   God bless it.
[00:08:41.160 --> 00:08:41.960]   Better be secure.
[00:08:41.960 --> 00:08:44.520]   Don't want to try these.
[00:08:44.520 --> 00:08:45.880]   Know how many eggs you have.
[00:08:45.880 --> 00:08:48.280]   The server went down, but the point is like,
[00:08:48.280 --> 00:08:50.360]   that thing did not need like.
[00:08:50.360 --> 00:08:51.480]   They went out of business.
[00:08:51.480 --> 00:08:54.520]   We can try out that.
[00:08:54.520 --> 00:08:55.480]   How do I count my eggs?
[00:08:57.560 --> 00:08:59.560]   Not until they're hatched, young man.
[00:08:59.560 --> 00:08:59.960]   Wow.
[00:08:59.960 --> 00:09:00.920]   Exactly.
[00:09:00.920 --> 00:09:02.520]   Is that a dozen?
[00:09:02.520 --> 00:09:03.560]   Is it five?
[00:09:03.560 --> 00:09:04.280]   I don't know.
[00:09:04.280 --> 00:09:06.280]   Unbelievable.
[00:09:06.280 --> 00:09:08.840]   So that is an example of a device that you're like,
[00:09:08.840 --> 00:09:11.640]   if someone can see how many eggs I have, I don't care.
[00:09:11.640 --> 00:09:12.200]   Yeah, it's right.
[00:09:12.200 --> 00:09:17.080]   But if someone can see like my blood glucose or maybe can
[00:09:17.080 --> 00:09:18.840]   hack my case maker, that's a very different thing.
[00:09:18.840 --> 00:09:23.000]   Even more than that, it could be a gateway into your personal, into your network.
[00:09:23.000 --> 00:09:27.480]   So even if, you know, it's not merely that they could hack your egg counter.
[00:09:27.480 --> 00:09:30.440]   It's that they could then get into your network and do other bad things.
[00:09:30.440 --> 00:09:31.080]   That is true.
[00:09:31.080 --> 00:09:33.400]   And that is why they're doing this.
[00:09:33.400 --> 00:09:38.760]   That is one of the big reasons why the Biden administration has come out with this labeling plan
[00:09:38.760 --> 00:09:40.040]   and everybody's on board.
[00:09:40.040 --> 00:09:42.040]   And by everybody, I do mean everybody.
[00:09:42.040 --> 00:09:46.600]   Like the CTA is on board, the CSA, which is doing the matter, standard is on board,
[00:09:46.600 --> 00:09:51.000]   Amazon, Google, Samsung, Best Buy, Blodgetack, like lots of companies are on board.
[00:09:51.000 --> 00:09:54.040]   But the standards so far are really good.
[00:09:54.040 --> 00:09:56.520]   They're things like no default passwords.
[00:09:56.520 --> 00:10:00.920]   They're in any multifactor authentication, encryption at rest on the device,
[00:10:00.920 --> 00:10:05.000]   encryption when it's going anyplace else, and then encryption where it's stored in the cloud.
[00:10:05.000 --> 00:10:05.480]   Nice.
[00:10:05.480 --> 00:10:11.800]   There's a ton of, there's actually a really interesting thing that we don't see on a lot of IoT devices,
[00:10:11.800 --> 00:10:13.320]   which is data logging.
[00:10:13.320 --> 00:10:21.320]   Well, NIST calls, let me re-process, NIST calls for things like data logging and monitoring.
[00:10:21.320 --> 00:10:27.160]   So manufacturers and consumers should be able to see what's happening, who are their devices
[00:10:27.160 --> 00:10:31.720]   are communicating with, how often they're communicating, logging changes to the systems.
[00:10:31.720 --> 00:10:38.440]   So you can read the document, you can read my blog post where I sum up fairly quickly
[00:10:38.440 --> 00:10:42.360]   what the six items that NIST is kind of looking for are.
[00:10:42.360 --> 00:10:48.120]   But here's the, here's the devil in the details that always happens with any legislative effort.
[00:10:49.720 --> 00:10:53.640]   What the White House announced is like, we've got this label and they showed off the label.
[00:10:53.640 --> 00:10:53.880]   Yay.
[00:10:53.880 --> 00:10:59.080]   What they still need to do is build the program and actually set the security criteria.
[00:10:59.080 --> 00:11:05.320]   So they're going to do that with the FCC and the FCC has, they're going to start what they call,
[00:11:05.320 --> 00:11:09.640]   it's called a Notice of Propose Rulemaking, but it's basically how the FCC sets rules.
[00:11:09.640 --> 00:11:13.240]   And you'll get a chance to comment at all these rules.
[00:11:13.240 --> 00:11:14.760]   Everybody else will.
[00:11:14.760 --> 00:11:19.720]   And then the FCC is going to be like, here are the official criteria from on high.
[00:11:19.720 --> 00:11:20.680]   Is this somewhere else?
[00:11:20.680 --> 00:11:21.160]   And then we will.
[00:11:21.160 --> 00:11:26.200]   We see lobbyists coming in to play and saying, hey, well, we really want this.
[00:11:26.200 --> 00:11:27.480]   Because this stuff costs money.
[00:11:27.480 --> 00:11:34.760]   The only reason they wouldn't build it is because it costs money to do it and to maintain it.
[00:11:34.760 --> 00:11:35.800]   Can I ask one question?
[00:11:35.800 --> 00:11:39.800]   Does it, does the NIST document talk about update, auto updating?
[00:11:39.800 --> 00:11:43.960]   Because that's, I think we agree, the most important feature of all.
[00:11:44.920 --> 00:11:47.000]   It does talk about auto updates.
[00:11:47.000 --> 00:11:50.840]   There's, I think it's like the fourth section, maybe you were scrolling through.
[00:11:50.840 --> 00:11:51.800]   Yeah, good.
[00:11:51.800 --> 00:11:57.000]   So it does suggest that these devices should be updated, firmware updateable, in case of.
[00:11:57.000 --> 00:11:58.840]   They have to be firmware updateable.
[00:11:58.840 --> 00:11:59.720]   Very important.
[00:11:59.720 --> 00:12:00.680]   Good.
[00:12:00.680 --> 00:12:05.000]   And they, I mean, yeah, the, I will say the NIST rules are very comprehensive.
[00:12:05.000 --> 00:12:09.480]   The one thing this label doesn't have that I'm really bummed about, but maybe it'll get it
[00:12:09.480 --> 00:12:11.720]   in the comments, I don't know, is privacy.
[00:12:11.720 --> 00:12:16.920]   So there's nothing in here talking about like how someone's data, like what device,
[00:12:16.920 --> 00:12:21.320]   what sensors are on a device, how your data from that device is actually used.
[00:12:21.320 --> 00:12:22.840]   It's just all about.
[00:12:22.840 --> 00:12:25.640]   So this is all security only, which is good.
[00:12:25.640 --> 00:12:28.040]   But privacy is still a big part.
[00:12:28.040 --> 00:12:29.160]   They most consumers.
[00:12:29.160 --> 00:12:32.200]   They do talk a little bit about it should, you know, the data should be.
[00:12:32.200 --> 00:12:34.200]   They don't talk about.
[00:12:34.200 --> 00:12:35.400]   So a 2.2.2.
[00:12:35.400 --> 00:12:40.440]   It's in documentation, but that is not actually one of the things you're going to need.
[00:12:40.440 --> 00:12:41.000]   Okay.
[00:12:41.000 --> 00:12:47.480]   Like to get it's not one of their, I don't remember the words they use to describe it,
[00:12:47.480 --> 00:12:49.560]   but the product capabilities.
[00:12:49.560 --> 00:12:51.720]   So there's six product capabilities that need to be there.
[00:12:51.720 --> 00:12:57.640]   And if you go into developer activities, 2.2.2 is documentation.
[00:12:57.640 --> 00:13:01.320]   And that talks about like things you should have in there, but.
[00:13:01.320 --> 00:13:08.040]   Well, and also the, the Biden administration did not commit to fully supporting this document.
[00:13:08.040 --> 00:13:10.120]   They said leaning, leaning.
[00:13:10.120 --> 00:13:13.320]   On this document, which is going to see.
[00:13:13.320 --> 00:13:17.080]   National Institute of Standards and Technology, not time.
[00:13:17.080 --> 00:13:20.760]   I said time and they are responsible for time, but it's technology.
[00:13:20.760 --> 00:13:22.600]   So more than just time.
[00:13:22.600 --> 00:13:25.400]   This looks pretty good though.
[00:13:25.400 --> 00:13:27.080]   I mean, it's impressive.
[00:13:27.080 --> 00:13:27.960]   What's the next step?
[00:13:27.960 --> 00:13:31.160]   Do they have to go to Congress or can this be done by fiat?
[00:13:31.160 --> 00:13:38.520]   No, they, the FCC is going to sometime soon, ish, within the next month or so, I hope.
[00:13:38.520 --> 00:13:41.160]   We'll issue what they call a notice of proposed rulemaking.
[00:13:41.160 --> 00:13:41.160]   Okay.
[00:13:41.160 --> 00:13:44.360]   And then they'll be like, it's usually like a 60 day comment period.
[00:13:44.360 --> 00:13:47.480]   It can be anywhere from like 30 to 90 day comment period.
[00:13:47.480 --> 00:13:50.280]   And that's when they're going to be like, hey, here's the NIST document.
[00:13:50.280 --> 00:13:51.080]   Here are the rules worth it.
[00:13:51.080 --> 00:13:51.400]   Get out.
[00:13:51.400 --> 00:13:52.360]   What do you think?
[00:13:52.360 --> 00:13:55.400]   And then people like right, right in, tell them what they think.
[00:13:55.400 --> 00:13:59.320]   And then the FCC will vote at a meeting to approve the program.
[00:13:59.320 --> 00:13:59.640]   Good.
[00:13:59.640 --> 00:14:02.520]   Now, open questions.
[00:14:02.520 --> 00:14:03.560]   I mentioned privacy.
[00:14:03.560 --> 00:14:04.760]   Second, open question.
[00:14:04.760 --> 00:14:06.200]   Hey, what happens if.
[00:14:07.080 --> 00:14:11.720]   You don't like, if you say you're secure and then you're not.
[00:14:11.720 --> 00:14:12.600]   Yeah, what's the fine?
[00:14:12.600 --> 00:14:13.000]   We don't.
[00:14:13.000 --> 00:14:16.200]   That's, well, that's more FTC than FCC in that case.
[00:14:16.200 --> 00:14:21.000]   Well, no, because the FCC is responsible for this program.
[00:14:21.000 --> 00:14:24.840]   And if you talk to people in Washington, the reason why is the FCC is like, we'll do it.
[00:14:24.840 --> 00:14:26.920]   And the FTC is like, God, give it to them.
[00:14:26.920 --> 00:14:28.360]   We are so busy.
[00:14:28.360 --> 00:14:32.440]   FCC has more enforcement arms than the FTC.
[00:14:32.440 --> 00:14:34.440]   FTC is very limited enforcement capability.
[00:14:34.440 --> 00:14:36.760]   FTC is busy losing cases these days too.
[00:14:36.760 --> 00:14:37.640]   So I'll say that.
[00:14:37.640 --> 00:14:39.400]   Corey Doctor would spank you.
[00:14:39.400 --> 00:14:42.600]   And I'm going to get to that in a little bit.
[00:14:42.600 --> 00:14:46.040]   So that's, so that's this.
[00:14:46.040 --> 00:14:49.240]   Oh, there's also a, so the trust mark will be on the box.
[00:14:49.240 --> 00:14:52.120]   Presumably sometime around 2024 is the hope.
[00:14:52.120 --> 00:14:54.120]   And then there's also going to be a QR component,
[00:14:54.120 --> 00:14:57.320]   which is where you can look to see more fine tune details,
[00:14:57.320 --> 00:14:58.680]   fine grade details about things.
[00:14:58.680 --> 00:15:01.080]   And because security doesn't stay still,
[00:15:01.080 --> 00:15:04.040]   it's also where you'll be able to check and make sure it's still secure.
[00:15:04.040 --> 00:15:08.280]   So if you're buying this box, you're buying it off of a,
[00:15:08.280 --> 00:15:09.880]   from a best buy in the middle of nowhere,
[00:15:09.880 --> 00:15:14.040]   and it's been on the shelf for two years, you can be like, hey, are you still legit?
[00:15:14.040 --> 00:15:17.000]   With this announcement, have you seen any opposition?
[00:15:17.000 --> 00:15:19.000]   No one's going to.
[00:15:19.000 --> 00:15:19.800]   No one's.
[00:15:19.800 --> 00:15:20.360]   No one's.
[00:15:20.360 --> 00:15:20.840]   No one's.
[00:15:20.840 --> 00:15:21.160]   Yeah.
[00:15:21.160 --> 00:15:21.720]   No one's.
[00:15:21.720 --> 00:15:22.600]   Not yet anyway.
[00:15:22.600 --> 00:15:25.720]   No, they'd all be under the table and say, hey, a Senator Manchin, you know,
[00:15:25.720 --> 00:15:27.240]   this is a terrible idea.
[00:15:27.240 --> 00:15:28.680]   Would you mind helping us?
[00:15:28.680 --> 00:15:30.840]   That's like, we're live somewhere.
[00:15:30.840 --> 00:15:31.560]   All of this stuff.
[00:15:31.560 --> 00:15:34.440]   Because it just seems like, so they'll come into weekend.
[00:15:34.440 --> 00:15:36.360]   Some of the things you'll see their comments.
[00:15:36.360 --> 00:15:39.800]   Yeah, you'll see them in the comments saying things like, all right,
[00:15:39.800 --> 00:15:46.200]   no default passwords is fine, but requiring multi-factor authentication for some of the stuff is just too much.
[00:15:46.200 --> 00:15:47.000]   Too much.
[00:15:47.000 --> 00:15:47.480]   Do they?
[00:15:47.480 --> 00:15:50.280]   Do they require MFA?
[00:15:50.280 --> 00:15:51.640]   You would be surprised.
[00:15:51.640 --> 00:15:55.480]   So MFA is mentioned in there as one of the better ways to handle this.
[00:15:55.480 --> 00:15:56.120]   But do I think?
[00:15:56.120 --> 00:15:56.440]   I think.
[00:15:56.440 --> 00:16:01.240]   Because that's a little, that most consumers are not going to know how to do that.
[00:16:02.120 --> 00:16:05.000]   Every device is actually, so I talked to Ann.
[00:16:05.000 --> 00:16:06.520]   Oh gosh, Nurburger.
[00:16:06.520 --> 00:16:08.200]   Oh, what's her title?
[00:16:08.200 --> 00:16:10.680]   Ah, this is audio.
[00:16:10.680 --> 00:16:13.560]   Ah, anyway, I talked to.
[00:16:13.560 --> 00:16:16.280]   I'm not going to call it a security.
[00:16:16.280 --> 00:16:20.440]   Ann Neuweberger, National Security Official Services, the Deputy National Security Advisor
[00:16:20.440 --> 00:16:23.800]   for Cyber and Mercing Technologies for the Biden administration.
[00:16:23.800 --> 00:16:24.760]   That Ann Nurburger.
[00:16:24.760 --> 00:16:25.480]   Yes, that is you guys.
[00:16:25.480 --> 00:16:25.880]   That Ann Nurburger.
[00:16:25.880 --> 00:16:26.440]   Yes, that.
[00:16:26.440 --> 00:16:27.320]   Okay.
[00:16:27.320 --> 00:16:27.800]   New Burger.
[00:16:27.800 --> 00:16:28.520]   That's Ann.
[00:16:29.320 --> 00:16:36.120]   So she like it, like with Energy Star, you know, every appliance has different criteria.
[00:16:36.120 --> 00:16:39.320]   They have to meet different levels based on what they are because like a wash, to make
[00:16:39.320 --> 00:16:43.000]   an energy efficient washing machine is different than making an energy efficient refrigerator.
[00:16:43.000 --> 00:16:48.600]   So there will be standards for different sets of devices, different IoT devices.
[00:16:48.600 --> 00:16:55.320]   So you may not need MFA on your egg counter, but you might need it on your baby monitor.
[00:16:55.320 --> 00:16:55.800]   Okay.
[00:16:55.800 --> 00:16:56.360]   Okay.
[00:16:56.360 --> 00:16:57.720]   That's going to be part.
[00:16:57.720 --> 00:16:58.520]   Yeah, yeah, yeah.
[00:16:58.520 --> 00:16:58.840]   Yeah.
[00:16:58.840 --> 00:17:03.480]   So they're going to hide a lot of complexity behind this, which is good because those consumers,
[00:17:03.480 --> 00:17:06.840]   they don't want to understand all that and they shouldn't have to.
[00:17:06.840 --> 00:17:13.000]   There's an old joke about Silicon Valley companies, new startups, releasing the t-shirt before they
[00:17:13.000 --> 00:17:14.920]   release the product.
[00:17:14.920 --> 00:17:19.160]   And this is the Biden administration releasing the t-shirt before they have anything.
[00:17:19.160 --> 00:17:19.640]   This is indeed.
[00:17:19.640 --> 00:17:22.680]   They got a label anyway.
[00:17:22.680 --> 00:17:23.720]   Hey, we got a label.
[00:17:23.720 --> 00:17:24.200]   All right.
[00:17:25.720 --> 00:17:28.600]   There'll be a lot of states you would always say discussion is good.
[00:17:28.600 --> 00:17:29.880]   We need the discussion.
[00:17:29.880 --> 00:17:33.720]   But really seriously, they're announcing a label before they have a standard.
[00:17:33.720 --> 00:17:35.400]   Good point.
[00:17:35.400 --> 00:17:35.880]   Yeah.
[00:17:35.880 --> 00:17:36.440]   I mean, come on.
[00:17:36.440 --> 00:17:39.240]   Well, in October, I mean, so they announced this.
[00:17:39.240 --> 00:17:42.600]   They actually announced that they wanted to create this label back in October.
[00:17:42.600 --> 00:17:43.240]   Okay.
[00:17:43.240 --> 00:17:44.760]   So we were all like, okay.
[00:17:44.760 --> 00:17:49.640]   And then Nisk got, they were like, okay, here's what we want this to do.
[00:17:49.640 --> 00:17:53.400]   And this was supposed to be announced in April and it was not.
[00:17:53.400 --> 00:17:56.360]   So, you know, these things take more time than it is.
[00:17:56.360 --> 00:17:57.320]   Hey, we got a t-shirt.
[00:17:57.320 --> 00:18:01.160]   I'm not complaining, but let's now let's finish the job.
[00:18:01.160 --> 00:18:03.160]   And we're going to get an NPRM.
[00:18:03.160 --> 00:18:04.280]   It's going to be amazing.
[00:18:04.280 --> 00:18:04.760]   Good.
[00:18:04.760 --> 00:18:11.320]   So what the record also show I got CC'd on an email this morning from Craig Newmark
[00:18:11.320 --> 00:18:16.760]   to Stacy responding to her newsletter saying what good news this is, how excited he is.
[00:18:16.760 --> 00:18:19.960]   Oh, I think it's huge news about cybersecurity.
[00:18:20.600 --> 00:18:23.320]   But remember, there's a presidential election in a year.
[00:18:23.320 --> 00:18:28.680]   And who knows, you know, I knew Congress in two years.
[00:18:28.680 --> 00:18:29.320]   Mm hmm.
[00:18:29.320 --> 00:18:31.000]   Well, but okay.
[00:18:31.000 --> 00:18:34.040]   So and new members of the SEC have made their rules.
[00:18:34.040 --> 00:18:34.840]   Yeah.
[00:18:34.840 --> 00:18:35.880]   Well, no, but do members.
[00:18:35.880 --> 00:18:36.360]   Let's get going.
[00:18:36.360 --> 00:18:36.920]   Let's hurry up.
[00:18:36.920 --> 00:18:41.960]   We'll have the criteria by the end of this year, presumably.
[00:18:41.960 --> 00:18:42.360]   Good.
[00:18:42.360 --> 00:18:45.240]   And then so you would be able to see it before the election.
[00:18:45.240 --> 00:18:47.160]   2024 is when we're doing the election.
[00:18:47.160 --> 00:18:53.160]   The other thing I really like is there's a kind of a nutrition label for IoT security,
[00:18:53.160 --> 00:18:56.520]   security and privacy facts that will be on the box.
[00:18:56.520 --> 00:19:03.480]   So I think that's a really that's for I think for a consumer even more than the badge.
[00:19:03.480 --> 00:19:05.480]   That's going to be a value.
[00:19:05.480 --> 00:19:09.480]   So that's not necessarily what it's going to look like.
[00:19:09.480 --> 00:19:10.760]   That's theoretical.
[00:19:10.760 --> 00:19:13.880]   You mean there's no bravo temp or eco house.
[00:19:14.600 --> 00:19:19.960]   I'm talking about I'm talking about the data that's going to be on that label.
[00:19:19.960 --> 00:19:24.280]   But at least they want to do something like this.
[00:19:24.280 --> 00:19:24.680]   Everybody does.
[00:19:24.680 --> 00:19:26.200]   Well, of course.
[00:19:26.200 --> 00:19:27.880]   Yeah, that's the last thing.
[00:19:27.880 --> 00:19:30.360]   So that's no, that's a proposed label.
[00:19:30.360 --> 00:19:32.440]   I think that's the side lab is proposed label.
[00:19:32.440 --> 00:19:33.400]   That's not actually.
[00:19:33.400 --> 00:19:36.520]   Well, this is from the FCC.
[00:19:36.520 --> 00:19:39.000]   I mean, yeah, okay.
[00:19:39.000 --> 00:19:41.480]   You know, I don't know.
[00:19:41.480 --> 00:19:43.320]   I mean, obviously there will be opposition.
[00:19:44.040 --> 00:19:49.160]   Although the good news is a lot of the IoT manufacturers that don't want this
[00:19:49.160 --> 00:19:54.760]   are Chinese no name brands that aren't probably going to have much lobbying clout in the halls of
[00:19:54.760 --> 00:19:56.680]   Washington DC.
[00:19:56.680 --> 00:20:02.840]   And in fact, the big companies like Amazon and Wise who are trying to compete against these
[00:20:02.840 --> 00:20:04.600]   no name companies will.
[00:20:04.600 --> 00:20:10.120]   So let's hope that Amazon and Wise and Apple and all these other companies say,
[00:20:10.120 --> 00:20:15.000]   here's an opportunity for us to distinguish our high quality products from these inexpensive
[00:20:15.000 --> 00:20:16.040]   Chinese knockoffs.
[00:20:16.040 --> 00:20:18.920]   I hope that is.
[00:20:18.920 --> 00:20:19.880]   Yeah, that is the hope.
[00:20:19.880 --> 00:20:22.600]   Oh, that's a different label.
[00:20:22.600 --> 00:20:22.920]   Okay.
[00:20:22.920 --> 00:20:27.480]   So that might be the FCC's label because that's a take from the Cylabs label,
[00:20:27.480 --> 00:20:29.720]   which has way too much information on it.
[00:20:29.720 --> 00:20:31.480]   So yeah, can I ask questions, Stacy?
[00:20:31.480 --> 00:20:36.120]   Yeah, it strikes me that when we talk about cybersecurity or there's two ends of this.
[00:20:36.120 --> 00:20:39.560]   You've talked about the consumer end of this, you want your baby monitor to be safe.
[00:20:39.560 --> 00:20:42.360]   So somebody can't nuke in on it or whatever.
[00:20:42.360 --> 00:20:47.400]   But then there's the larger national cybersecurity question about
[00:20:47.400 --> 00:20:50.680]   you're shutting down systems and wreaking havoc.
[00:20:50.680 --> 00:20:53.320]   What is this really more about?
[00:20:53.320 --> 00:21:00.280]   So remember Mariah all the way back in 2015, which is December 2015.
[00:21:00.280 --> 00:21:01.320]   Yeah, yeah.
[00:21:01.320 --> 00:21:01.320]   Yeah.
[00:21:01.320 --> 00:21:01.800]   Yeah.
[00:21:01.800 --> 00:21:07.960]   So that actually was taking over networked video cameras and they weren't actually
[00:21:07.960 --> 00:21:09.160]   true IoT devices.
[00:21:09.160 --> 00:21:09.880]   They weren't modern.
[00:21:09.880 --> 00:21:17.960]   But what that did is that allowed hackers to take over and gather an enormous amount of bandwidth
[00:21:17.960 --> 00:21:23.480]   that was then used against US companies or just companies.
[00:21:23.480 --> 00:21:26.440]   So this is more so your there's two things.
[00:21:26.440 --> 00:21:32.840]   The main thing is we don't want to give hackers access to bandwidth from consumer devices that
[00:21:32.840 --> 00:21:38.680]   they can then use to attack websites of whatever infrastructure or whatever.
[00:21:38.680 --> 00:21:41.800]   So to do those kind of DDoS attacks.
[00:21:41.800 --> 00:21:46.280]   The other element is because consumers are dumb.
[00:21:46.280 --> 00:21:48.840]   We're not cybersecurity experts.
[00:21:48.840 --> 00:21:49.800]   We shouldn't have to be.
[00:21:49.800 --> 00:21:52.680]   So they want to lock that vector down.
[00:21:52.680 --> 00:21:58.680]   And then the other part is they it's good for consumers.
[00:21:58.680 --> 00:22:01.880]   I mean, if you can also benefit consumers by making it.
[00:22:01.880 --> 00:22:07.320]   So I mean, we haven't seen anything like this yet, but it's not implausible to think of hackers
[00:22:07.320 --> 00:22:12.680]   getting a hold of popular like thermostats and doing a ransomware attack that
[00:22:12.680 --> 00:22:17.080]   locks everybody's thermostat locks everyone out in.
[00:22:17.080 --> 00:22:18.200]   Right.
[00:22:18.200 --> 00:22:20.120]   Well, I'm trying to get to is this.
[00:22:20.120 --> 00:22:26.040]   I wonder whether there's going to be much consumer interest or demand for labels on things
[00:22:26.040 --> 00:22:28.360]   beyond your standard moral panic.
[00:22:28.360 --> 00:22:34.040]   The Chinese are watching, whereas the real import here is to not let their devices be taken over.
[00:22:34.040 --> 00:22:39.320]   And bad things happen at a national level, at an infrastructural level.
[00:22:39.320 --> 00:22:44.360]   So it's really the government that cares should care more about this than the consumer,
[00:22:44.360 --> 00:22:50.840]   whether the consumer cares or not may not matter so much as whether or not there's an agreement on
[00:22:50.840 --> 00:22:53.800]   the things that matter at the higher infrastructural level.
[00:22:55.000 --> 00:22:59.800]   There's two points there. One is that we're bringing things into our home that are going to be
[00:22:59.800 --> 00:23:03.720]   ultimately connected back to our public infrastructure. If you look at like,
[00:23:03.720 --> 00:23:08.280]   demand response programs and this goal towards smarter energy management,
[00:23:08.280 --> 00:23:14.200]   and they mentioned in here they're going to work with the EPA for inverters and car chargers,
[00:23:14.200 --> 00:23:18.920]   we're bringing things into our home that we're buying that are going to be connected back to
[00:23:18.920 --> 00:23:22.600]   the public grid in that case. And the government does have a vested interest in us.
[00:23:22.600 --> 00:23:25.640]   I agree with that. I think that's what I'm saying. That's the important part as the government
[00:23:25.640 --> 00:23:31.240]   citrus more than the numbers. Yeah, in stopping DDoS attacks and those things are very important.
[00:23:31.240 --> 00:23:39.080]   But I do think there is also interest in making sure that a large swath of consumers doesn't suddenly
[00:23:39.080 --> 00:23:44.760]   see all of their door locks open at once because we will. Okay, okay, that's good. Right. All right.
[00:23:44.760 --> 00:23:49.880]   You know, like there's a variety of things. So yeah, I'm just trying to run on the agenda.
[00:23:49.880 --> 00:23:55.880]   Super demand for this. But it is voluntary. And so I could always buy an insecure device,
[00:23:55.880 --> 00:24:01.320]   but then at least the government will say, Oh, well, I'm sorry that you are one. You're going to hear
[00:24:01.320 --> 00:24:06.840]   people like us and other, you know, the verge and everybody else say, make sure you get this.
[00:24:06.840 --> 00:24:11.000]   You want this. You don't want to buy a device that doesn't have this. I think that that's,
[00:24:11.000 --> 00:24:17.000]   it just gives us a way to in a simple way. I mean, we've been saying for ages, you've been saying
[00:24:17.880 --> 00:24:23.560]   for ages, Stacy, for instance, don't buy any IoT devices, not firmware updateable over OTA.
[00:24:23.560 --> 00:24:28.120]   And we could say that to her blue in the face, but most people don't even understand what we're
[00:24:28.120 --> 00:24:33.160]   saying. I was going to say you would say that badge and say it and that's bad. You still probably
[00:24:33.160 --> 00:24:38.280]   want to add an additional note to that and say, Hey, make sure it's this particular badge and not
[00:24:38.280 --> 00:24:45.160]   just a company XYZ throwing their own version of the badge on there. Because you'll want the.
[00:24:45.160 --> 00:24:49.880]   You have to look for the US cyber trust. Yeah, there's a really new
[00:24:49.880 --> 00:24:54.520]   you know, cyber trust mark. No, I think that that people aren't that dumb.
[00:24:54.520 --> 00:25:02.840]   No, they're not. In fact, it's one of the reasons Apple with home kit has an advantage because
[00:25:02.840 --> 00:25:07.400]   people go Apple, they're going to do this more securely. So I'm going to trust home kit devices
[00:25:07.400 --> 00:25:13.880]   over some no name, Chinese smart plug apples that trust. Yeah. And I think that this is, you know,
[00:25:14.680 --> 00:25:19.400]   I look, it's our job to make this work. It's their job to make sure that this happens.
[00:25:19.400 --> 00:25:24.120]   And there are reasonable laws and the lobbyists don't take all this stuffing out of the ottoman.
[00:25:24.120 --> 00:25:30.840]   But I think it's our job also to tell consumers, make sure you know, look for the cyber trust mark.
[00:25:30.840 --> 00:25:36.600]   Right. Right. That's important. I don't know if it's done right looks for the
[00:25:36.600 --> 00:25:41.080]   energy star. I mean, they're they're not doing this out of the energy. We all do.
[00:25:41.080 --> 00:25:45.800]   And you're a creature. You buy. No, no, no, you can't miss it. Go to an appliance store.
[00:25:45.800 --> 00:25:48.760]   That's true. There's a big number on that. It's the only thing.
[00:25:48.760 --> 00:25:53.160]   Oh, thank God. And people are smart enough to say that's going to cost me more in electricity than
[00:25:53.160 --> 00:25:56.840]   that one is. Yeah. Well, okay, well, that's cost. That's money. That's a pocketbook thing.
[00:25:56.840 --> 00:26:02.680]   This isn't money so much as I in fact, people are still concerned about security. Like,
[00:26:02.680 --> 00:26:07.880]   I will tell you in granted my audience, they're great big giant nerds. Yes, they are. You know,
[00:26:07.880 --> 00:26:14.600]   they're like us, but they're like probably 50% of the questions I get about connected devices are
[00:26:14.600 --> 00:26:20.760]   hey, I'm looking at this thing. It's awesome. It's only $5. Is it secure? Oh, oh, I can't believe
[00:26:20.760 --> 00:26:26.200]   Jeff and Ant are arguing that in the favor of people being too stupid to care about this. They
[00:26:26.200 --> 00:26:33.640]   care about this. They really do. They do. They do and they will. These are the same people that are
[00:26:33.640 --> 00:26:38.600]   just all over TikTok and worried about China and those threats to you. Well, that's because other
[00:26:38.600 --> 00:26:44.440]   media forces have made them care about that. People listen and we just have to make sure they
[00:26:44.440 --> 00:26:50.040]   understand this is important. I think this is not going to really, I hope they care. I just hope
[00:26:50.040 --> 00:26:57.160]   that there is an extra bit of diligence on our part in administration to say, hey, this is what
[00:26:57.160 --> 00:27:01.560]   we really need to care about. Well, the only thing you could do a step beyond this was something
[00:27:01.560 --> 00:27:06.760]   like the FDA, which is if it's not NIST approved, it's not available for sale in the US. And I
[00:27:06.760 --> 00:27:12.920]   imagine if there's security becomes a bigger issue, maybe that will happen. People are aware.
[00:27:12.920 --> 00:27:15.160]   People don't want to get hacked. How many?
[00:27:15.160 --> 00:27:22.120]   People don't want to get hacked. But how many people you know what their password is password?
[00:27:22.120 --> 00:27:27.480]   Well, that's just because the word hasn't come out. I mean, that's a failure of technology.
[00:27:27.480 --> 00:27:32.280]   It's gotten much better. It has. I actually got data on this like a couple
[00:27:32.280 --> 00:27:37.400]   months ago that showed and it was actually generationally broken out by generations.
[00:27:37.400 --> 00:27:40.040]   And except for boomers, everybody's gotten them.
[00:27:40.040 --> 00:27:46.600]   It's funny looking here in the discord and my password
[00:27:46.600 --> 00:27:51.640]   point. There you go. Oh, you put a bang on yours. Okay. Yeah. Yeah.
[00:27:51.640 --> 00:27:56.520]   People in the discord are applauding you and your faith in people's intelligence where
[00:27:57.160 --> 00:28:02.920]   I not so much. No, Quippy says I'm with Jeff and people are dumber than you think.
[00:28:02.920 --> 00:28:07.640]   And even the smart ones pretend to be dumb for people fear for fear. People will think
[00:28:07.640 --> 00:28:11.800]   them intelligent. They'll be kicked out of the engraving group. That's right. It's not that
[00:28:11.800 --> 00:28:16.760]   they're dumb. It's the do that what's their what they don't care. They don't care. There's plenty
[00:28:16.760 --> 00:28:23.000]   of motivation to care. There's been enough news about bad guys spying on you now that they are aware
[00:28:23.000 --> 00:28:26.360]   of it. And I'm not sure that is moral panic and ridiculous. That's fine.
[00:28:26.360 --> 00:28:32.440]   Casey's argument is the best one is I'm never going to put it on these door locks anyway.
[00:28:32.440 --> 00:28:41.400]   But you don't want it to open without your advice by some bad. Honestly, that's how people
[00:28:41.400 --> 00:28:46.680]   show they care. They say I'm staying away from IOT, which is why IOT companies ought to go
[00:28:46.680 --> 00:28:51.880]   along with this. They ought to go along with this. Well, in IOT companies have wanted the
[00:28:51.880 --> 00:28:56.040]   government to do something like the good ones have like when I talk to people who are building
[00:28:56.040 --> 00:29:02.040]   connected devices, they're like, look, Stacy, I would love to like really go whole hog on this.
[00:29:02.040 --> 00:29:07.320]   But I can't because I have to make my device as cheap as this no name thing from China because
[00:29:07.320 --> 00:29:13.880]   people don't know the difference with this. They've got something like everybody who's who cares
[00:29:13.880 --> 00:29:19.400]   about this will be working towards this standard. And then everybody who doesn't. Yeah, you could
[00:29:19.400 --> 00:29:26.280]   still buy a cheap no name sensor, but you won't know if it's as secure. And there will be I mean,
[00:29:26.280 --> 00:29:30.200]   and people will still buy that. Sure. Don't get me wrong. There will be plenty of 100%
[00:29:30.200 --> 00:29:35.720]   do that. But you know, that's it's like public health. You don't have to have everybody get
[00:29:35.720 --> 00:29:42.840]   vaccinated to have a real impact on the spread of a disease. And so it's just a public health
[00:29:42.840 --> 00:29:48.360]   measure. And if you know more people buy this than don't, then that's a good thing. And it'll make
[00:29:48.360 --> 00:29:54.040]   a big difference, I think. And I think that there's some incentive for the for business not to
[00:29:54.040 --> 00:30:00.280]   undermine this, but to make it a real. A lot of companies legitimately want this. And the fact
[00:30:00.280 --> 00:30:06.120]   that best buy in Amazon were there and talking about how good could be used in retail settings.
[00:30:06.120 --> 00:30:10.680]   So like it's not just the manufacturers. It's also the people who sell these devices are like,
[00:30:10.680 --> 00:30:17.240]   we really want to make this work. That's in their interest. They get DDoSed. They get
[00:30:17.240 --> 00:30:23.960]   breached. It's in their interest to do this. It's good for everybody. And okay, economically,
[00:30:23.960 --> 00:30:29.240]   it gives them a foot up against Chinese knockoffs, which are eating their lunch. So I think this
[00:30:29.240 --> 00:30:33.960]   might actually have a shot crossing my fingers because this doesn't need to happen. I hope so.
[00:30:33.960 --> 00:30:40.040]   Let's take a little break. And then I'm going to tell you why Jeff is a is a
[00:30:44.600 --> 00:30:51.720]   kid who's saying I'm trying to find a phrase from Corey Dr. Rose. Great piece. He is somebody bad,
[00:30:51.720 --> 00:30:57.480]   something like that. I will never be the words myth that Corey Dr. O is, but I will read to you.
[00:30:57.480 --> 00:31:01.000]   We shall all show Corey's great work in just a little bit. First to word,
[00:31:01.000 --> 00:31:06.920]   from ACI learning, our great sponsors, our listeners know, I know you know the name IT Pro.
[00:31:06.920 --> 00:31:12.520]   They've been one of our trusted, beloved sponsors for more than a decade since they started in
[00:31:12.520 --> 00:31:17.240]   Gainesville, Florida. Now they're part of ACI learning. That's why you see ACI learning all over
[00:31:17.240 --> 00:31:25.480]   our studios. They are our studio name sponsor and a great company. Together ACI learning and IT
[00:31:25.480 --> 00:31:32.120]   Pro have elevated their highly entertaining, highly informative, bingeable video content with
[00:31:32.120 --> 00:31:40.840]   over 7,200 hours of on demand, IT training to choose from. Not just IT, but audit and cyber security
[00:31:40.840 --> 00:31:46.200]   too. And it's all freshen up today because they've got those studios running out of Gainesville,
[00:31:46.200 --> 00:31:54.440]   Florida, Monday through Friday, 9 to 5, creating more great, bingeable, high quality IT training.
[00:31:54.440 --> 00:31:59.800]   ACI learning provides world class service from assisting you in choosing which learning path
[00:31:59.800 --> 00:32:03.560]   suits you best all the way through helping you find the right career opportunity. If you're
[00:32:03.560 --> 00:32:10.120]   beginning a new career in IT, this is the place to go. You could fortify existing expertise if
[00:32:10.120 --> 00:32:16.280]   you're already in IT with self-paced IT training videos, interactive practice labs,
[00:32:16.280 --> 00:32:19.800]   practice tests so you could take the test before you take the final exam.
[00:32:19.800 --> 00:32:28.120]   And it really works. Felipe B was in public relations. He just on the website shared his
[00:32:28.120 --> 00:32:34.440]   testimonial about and how much he loves ACI learning. He's great edutators, they're instructors.
[00:32:34.440 --> 00:32:39.720]   He talked about Wes who's been with IT Pro forever. Wes is an awesome instructor, says Felipe.
[00:32:40.120 --> 00:32:48.760]   I passed my 220-1101 on July 4th on the first try. He says, I'm coming from zero. I was a PR
[00:32:48.760 --> 00:32:54.680]   guy, but I love it. He manages to explain Wes. He's talking about it. It manages to explain
[00:32:54.680 --> 00:32:59.800]   concepts in a way. You understand it very well. He says Felipe says, I can't wait to start
[00:32:59.800 --> 00:33:06.840]   220-11-02 now. That's a great story, isn't it? No, you got to try the ACI learning's amazing
[00:33:06.840 --> 00:33:12.520]   instructors. And then when you got the skills, practice them in the practice labs, you can test,
[00:33:12.520 --> 00:33:18.760]   experiment, setup, Windows Server, Windows clients, all in an HTML5 browser, which means you can do
[00:33:18.760 --> 00:33:25.400]   it on anything, even a Chromebook. And MSPs love it because they can actually test new apps and
[00:33:25.400 --> 00:33:32.040]   updates without compromising their live systems. The practice exams, fantastic. ACI learning gives
[00:33:32.040 --> 00:33:38.040]   you practice exam questions from all the big certs, Microsoft, CompTIA, EC, Council, PMI, many, many
[00:33:38.040 --> 00:33:44.360]   more. I've always thought this, if you can take a practice exam over and over till you really got
[00:33:44.360 --> 00:33:49.000]   it down, when you sit for the real exam, you're going to be so ready. You're going to,
[00:33:49.000 --> 00:33:55.320]   ACI, just like Felipe did, access every vendor, every skill you need to advance your IT career
[00:33:55.320 --> 00:34:01.960]   in one place. ACI learning is the only official video training for CompTIA. They've got my
[00:34:01.960 --> 00:34:05.640]   Microsoft IT training. They've got Cisco training, Linux training, Apple training,
[00:34:05.640 --> 00:34:10.600]   security, cloud, and I go on and on. And you'll love it because you'll have a personal account
[00:34:10.600 --> 00:34:15.720]   manager who's there as your kind of ombudsman to guide you every step of the way, make sure you're
[00:34:15.720 --> 00:34:20.600]   getting exactly what you need so that you could be successful in your field. Your win is their win,
[00:34:20.600 --> 00:34:28.040]   and they really believe that. Learn IT. Pass your certs. Get your dream job with ACI learning.
[00:34:28.040 --> 00:34:33.960]   Learn more about ACI learning's premium training options across audit, IT and cybersecurity readiness
[00:34:33.960 --> 00:34:43.080]   at go.acilearning.com/twit. That's the special address. Go.acilearning.com/twit
[00:34:43.080 --> 00:34:48.760]   and individuals. Twit30, you see that code right there. Twit30 gets you 30% off a standard of premium
[00:34:48.760 --> 00:34:59.160]   individual IT pro membership. Go.acilearning.com/twit. We love ACI learning. We thank them. We love
[00:34:59.160 --> 00:35:03.960]   IT pro. Some of the best people in the business. You're really going to get great. There's no better
[00:35:03.960 --> 00:35:10.920]   place to get that training. Go.acilearning.com/twit. Please use that address so they know you saw it here.
[00:35:10.920 --> 00:35:17.800]   Thank you. Here's a new badge. We kind of get stickers. We really got to get these stickers.
[00:35:18.600 --> 00:35:26.200]   Well, we do have some. Leo LeBorz. Blue Sky Hype Influencer. Thank you, Joe Esposito.
[00:35:26.200 --> 00:35:32.840]   What? We have stickers. We do have a... Oh my God. Where did we get these? Oh my Lord.
[00:35:32.840 --> 00:35:38.440]   Oh, Miss Bizzito setting us up and Mr. Victor and Mr. Nielsen.
[00:35:38.440 --> 00:35:43.400]   They impreenty him. Let's him. Set down and got a square away with some stickers.
[00:35:43.400 --> 00:35:51.960]   I love this. I love yours, Jeff. Oh, F me. Go bore a hole. It's brilliant. Oh my gosh. These are
[00:35:51.960 --> 00:35:58.760]   great. Jeff Jarvis declares this is moral panic. Stacey, I'm sorry about your bougie seal of approval,
[00:35:58.760 --> 00:36:03.240]   but it really is. No, the bougie seal of approval is awesome. It is really awesome.
[00:36:03.240 --> 00:36:10.280]   And Pruitt's seal of approval. Now that's legit. Yes. Or no. Thank you, sir, for this. Disapprove.
[00:36:10.280 --> 00:36:13.560]   That horrible whiskey. Oh, those are wonderful.
[00:36:13.560 --> 00:36:20.600]   Oh, okay. You got to get Lisa to add this as a member premium. We're going to find it.
[00:36:20.600 --> 00:36:23.800]   I'm taking these though to put them on my laptop. We're working on it. Yeah, I'm actually going
[00:36:23.800 --> 00:36:31.160]   to put some on my laptop too. I was just going to mention before we get to Lena Kahn and Corey
[00:36:31.160 --> 00:36:38.680]   Doctorow's spirited defense of her that the folks at framework have just a couple of days ago opened
[00:36:38.680 --> 00:36:45.320]   up pre-orders for their next generation framework 16. These are PCs. They're windows or Linux.
[00:36:45.320 --> 00:36:49.160]   Yeah, I remember. I thought you were all about Linux for these. Yeah, I run Linux on my front.
[00:36:49.160 --> 00:36:54.680]   I have the 13 inch, but this new one. Oh, you said 16. This is this. Yeah, this is what you
[00:36:54.680 --> 00:37:00.680]   should be thinking about, Mr. Pruitt. The whole idea of these, which I, you know, I'm not a
[00:37:00.680 --> 00:37:07.640]   shareholder in these like Linus is. I'm not, they're not an advertiser. I just really think it's,
[00:37:07.640 --> 00:37:15.080]   it's the answer to the right of for repair and for reusability and sustainability.
[00:37:15.080 --> 00:37:19.000]   These are upgradable laptops. You can, you can buy them assembled or assemble them yourself,
[00:37:19.000 --> 00:37:24.360]   but more importantly, if a new processor comes out, you can get a new motherboard and put it in
[00:37:24.360 --> 00:37:29.560]   yourself. They have keyboard modules. Look, you can have a number pad or if you don't want a number
[00:37:29.560 --> 00:37:34.760]   pad, you can take out the number pad and slide the keyboard over and make it central. You can,
[00:37:34.760 --> 00:37:39.000]   you can change where the trackpad, you just, this thing is completely customizable.
[00:37:39.000 --> 00:37:43.400]   Can you hack and toss it? Kidding. You could probably. Yeah. You,
[00:37:43.400 --> 00:37:47.720]   you have, I don't know. They don't, that's not the promise of it. I would put Linux on it.
[00:37:47.720 --> 00:37:52.520]   Anyway, I just ordered it. They, I won't be getting it to the first of the year. So I will,
[00:37:52.520 --> 00:37:57.720]   I will do a unboxing and I ordered the DIY edition, which means I have to put it.
[00:37:57.720 --> 00:38:01.480]   So did all the, oh man. Well, that's right. I did the last time I have the show.
[00:38:02.360 --> 00:38:08.200]   So, in fact, I, at first I ordered an upgrade for the 13 inch. You can upgrade it with an AMD
[00:38:08.200 --> 00:38:14.120]   processor and so forth. And then I thought, well, or I can spend just a few thousand dollars,
[00:38:14.120 --> 00:38:20.520]   more and get the 16 inch. So guess what? That means that Leo's next Leo's garage sale will have
[00:38:20.520 --> 00:38:25.400]   the 13 inch. Yeah. I wonder if they're going to update the Chromebook version.
[00:38:25.400 --> 00:38:31.560]   That's a good question. I know they did a Chromebook. 16. They've lived up to their promise
[00:38:31.560 --> 00:38:38.440]   because they shipped the 13 inch in 2021. I bought it. You, I could upgrade it now to a newer
[00:38:38.440 --> 00:38:44.600]   processor, a new AMD or Intel processor. I can upgrade a lot of features. So I betcha,
[00:38:44.600 --> 00:38:51.480]   I betcha. I don't know. I don't see that here. Their new 16 inches are all AMDs, which is interesting.
[00:38:51.480 --> 00:38:59.080]   Radeon, Ryzen 7s. I don't have a problem with the chip set. Most of my problems with windows.
[00:38:59.720 --> 00:39:03.320]   Well, that's why I'm running Lyth. Yeah. And the other thing is you don't have to pay the
[00:39:03.320 --> 00:39:08.040]   windows premium on this. Yeah. You can order it without windows and it's 139 bucks less.
[00:39:08.040 --> 00:39:11.400]   What's amazing is Dell show me this is what this is how Dell started.
[00:39:11.400 --> 00:39:15.960]   Right. Well, but Michael Dell was building it. Yeah. Right. And his dorm.
[00:39:15.960 --> 00:39:23.080]   Oh, let's even see. Even so. Yeah. That was the dorm I was an RA and no kidding. You were in the
[00:39:23.080 --> 00:39:28.920]   same dorm? Yeah. Oh, that's so cool. Not at the same time as Michael Dell. He was there in 1984.
[00:39:28.920 --> 00:39:34.760]   Yeah. That was pretty cool. No. Well, the school. The college didn't know that he was running his
[00:39:34.760 --> 00:39:41.000]   business. It wasn't a college dorm. It was a private dorm. Oh, okay. Yeah. So it was legal for
[00:39:41.000 --> 00:39:45.880]   him to do that. Imagine Stacy is a good RA. Can't you? Oh, she'd be so great. You can't?
[00:39:45.880 --> 00:39:53.480]   I can't. I can't. Oh, yeah. My boyfriend broke up with me. I'm a mother. I don't know what to do.
[00:39:54.520 --> 00:40:03.640]   Mother plucker. I just imagine Stacy is a movie. Come in too late. What? What? What? What?
[00:40:03.640 --> 00:40:10.040]   What? So Ari stands for resident assistant. It's a upper class resident. What? Advisor. What?
[00:40:10.040 --> 00:40:16.440]   It's a upper class person who is living in the dorm to advise the underclassmen, right?
[00:40:16.440 --> 00:40:22.920]   Kind of a little bit of that. I've never heard it upper class of like, that was Stacy's RA. Boogie.
[00:40:24.120 --> 00:40:31.640]   Yeah. It's an older. Yeah. I was a sophomore when I was at RA. Oh, you were sophomore. Oh,
[00:40:31.640 --> 00:40:35.480]   gosh. I thought it would be higher, higher than that. But okay. Yeah. I was like,
[00:40:35.480 --> 00:40:40.120]   Junior's. They're talking about the blind leading the blind. Yeah. My boyfriend. That's me too.
[00:40:40.120 --> 00:40:47.160]   So what was what? When did people come to you as an RA? What did people come to you?
[00:40:47.160 --> 00:40:53.880]   Your job was just to be like this person, like the adult on the floor, but not actually an adult.
[00:40:53.880 --> 00:40:58.520]   And you would, I mean, yes, you were there to help your residents. You plan social activities.
[00:40:58.520 --> 00:41:02.520]   I had a bulletin board that I would, I used to physically print out onion articles.
[00:41:02.520 --> 00:41:09.240]   Wow. Did you cut out little animals when they arrived? You know, your spirit.
[00:41:09.240 --> 00:41:14.200]   Oh, so my first, my first door, like you, you do stick their door knockers with their names on
[00:41:14.200 --> 00:41:18.600]   their door before they show up. My first one was actually three and a half and no, no, five and
[00:41:18.600 --> 00:41:25.080]   a quarter floppies. So I had a big cause we clean outside. Oh, you. Perfect. I put them on there.
[00:41:25.080 --> 00:41:29.160]   Yeah. So to make them feel welcome, because we're coming there for living away from home for
[00:41:29.160 --> 00:41:37.560]   the first time. And you're supposed to like stop them from smoking and playing like one social
[00:41:37.560 --> 00:41:42.200]   outing. I did actually have to, I mean, there was a lot of drama. These kids. Yeah. My daughters
[00:41:42.200 --> 00:41:45.960]   are a kept taking her magic mushrooms away from her. Things like that. Yeah.
[00:41:48.040 --> 00:41:52.040]   Yeah. And that's a, that's actually where I met Andrew. He was an RA too. Oh, you're kidding.
[00:41:52.040 --> 00:41:57.400]   Oh, when you're, when you're together at two in the morning and you can call someone and be like,
[00:41:57.400 --> 00:42:02.200]   Hey, I'm on call and someone just dumped out a keg a beer down the stairwell.
[00:42:02.200 --> 00:42:08.040]   Oh, geez. You might help me mop that up. Oh, that's. And they show up. True romance. Wow. Yeah.
[00:42:08.040 --> 00:42:12.440]   That's a guy you should marry. It's a wedding of Narks. It's just great.
[00:42:14.280 --> 00:42:19.080]   And Nark. I'm proud to think of them as prefix if you don't. Yes.
[00:42:19.080 --> 00:42:24.920]   That's a call. I have to call an ambulance on two of my residents. Okay.
[00:42:24.920 --> 00:42:29.960]   Like that happens. Sure. Did you have a summer camp where you all could bond and
[00:42:29.960 --> 00:42:35.800]   want to get to know each other before the semester? We drank a lot. Yes. Yes. You do that.
[00:42:36.840 --> 00:42:46.040]   Texas after all. All right. Well, on to Corey Doctorow. From his fabulous blog,
[00:42:46.040 --> 00:42:50.760]   Pluralistic.net. And as you know, Corey, and he's been on the shows many times. Good friend,
[00:42:50.760 --> 00:42:56.520]   love Corey is a bit of a muck raker. Some might think of him as a leftist, a progressive.
[00:42:56.520 --> 00:43:04.760]   He is a little hop and mad at all of the aprobrium. Lena Khan is receiving from people like the
[00:43:04.760 --> 00:43:10.920]   Wall Street Journal because she's a loser. You said it even yourself, Jeff. It's kind of getting
[00:43:10.920 --> 00:43:15.240]   into the aprobrium so much as it would think that her fans are disappointed in her. No.
[00:43:15.240 --> 00:43:23.160]   Let me read to you. My God. Right, Corey Doctorow. They sure hate Lena Khan. This once in a
[00:43:23.160 --> 00:43:28.680]   generation groundbreaking, brilliant legal scholar and fighter for the public interest.
[00:43:29.320 --> 00:43:34.440]   This sounds like I could be the opening of Hamilton. Yes. The Slayer of Reaganomics has
[00:43:34.440 --> 00:43:39.880]   attracted more vitriol, mockery and dismissal than any of her predecessors in living memory.
[00:43:39.880 --> 00:43:45.640]   She sure must be doing something right, huh? A quick refresher in 2017, Khan, then a law student
[00:43:45.640 --> 00:43:52.200]   published Amazon's antitrust paradox in the Yale Law Journal. It was brilliant blistering analysis
[00:43:52.200 --> 00:43:58.360]   showing how the Reagan era theory of anti-trust with celebrates monopolies as efficient had failed
[00:43:58.360 --> 00:44:03.400]   in its own terms using as Amazon as an exhibit A of the ways in which post Reagan antitrust had
[00:44:03.400 --> 00:44:11.160]   left Americans vulnerable to corporate use or abuse. The paper sent seismic shocks through both
[00:44:11.160 --> 00:44:18.600]   legal and economic circles and goose the neo-bransian movement, brand I see in movement, I should say
[00:44:18.600 --> 00:44:26.040]   sneeringly dismissed as hipster anti-trust. This movement is I can skip the history lesson.
[00:44:26.040 --> 00:44:32.600]   You just want the zings. Yeah, I like the zings. History is good though. You should you should
[00:44:32.600 --> 00:44:38.760]   read the whole thing. When Biden won the election, history is good. History is very well. There's
[00:44:38.760 --> 00:44:47.000]   history calling right now. I told you not to bother me.
[00:44:47.800 --> 00:44:53.640]   Mr. Miedel. Mr. Miedel. When Biden won the election, he surprised everyone by appointing Khan to the
[00:44:53.640 --> 00:44:58.440]   FTC. She's the chairperson. It wasn't just that she had such a radical vision, it was also that
[00:44:58.440 --> 00:45:03.640]   she lacked the usual corporate law experience that such an appoint he would normally require
[00:45:03.640 --> 00:45:08.120]   experience that would ensure that the FTC was helmed by people whose default view of the world
[00:45:08.120 --> 00:45:13.240]   is that should be structured and regulated by powerful wealthy people in corporate boardrooms.
[00:45:14.360 --> 00:45:23.080]   Even more surprising was that Khan was made chair of the FTC. Of course, she got in because
[00:45:23.080 --> 00:45:28.200]   it just happened to be a moment in time where there were some Republicans who wanted to screw big tech,
[00:45:28.200 --> 00:45:35.320]   not because it was too big and powerful, but because tech leaders failed to wield that power
[00:45:35.320 --> 00:45:41.640]   in the ways these Republicans preferred. They gave him some votes and he got her in.
[00:45:43.960 --> 00:45:50.440]   But while tech leaders are 100% committed to the project, a permanent oligarchic takeover
[00:45:50.440 --> 00:45:58.280]   of every sphere of American life, there are less full-throated in their support for hateful
[00:45:58.280 --> 00:46:01.000]   cruel discrimination against disfavored minorities.
[00:46:01.000 --> 00:46:07.880]   Biden's seating of antitrust policy to the left wing of the party combined with
[00:46:07.880 --> 00:46:12.920]   disaffected GOP senators viewing Khan as their enemy led to Khan's historic appointment as
[00:46:12.920 --> 00:46:19.960]   FTC chair joined by Jonathan Cantor at DOJ in the antitrust division. Tim Wu in the White House,
[00:46:19.960 --> 00:46:28.760]   Varo Bedoy at FTC Rebecca Slaughter, Rohit Chopra, really some big antitrust brains.
[00:46:28.760 --> 00:46:33.960]   Crucially, these new employees weren't just principled. They were good at their jobs.
[00:46:35.560 --> 00:46:43.560]   In 2021, Tim Wu, who brilliant professor wrote an executive, you don't like him so much.
[00:46:43.560 --> 00:46:49.240]   Okay. We're at an executive order for Biden that laved out 72 concrete ways in which the
[00:46:49.240 --> 00:46:54.760]   administration could act with no further congressional authorization to blunt corporate power
[00:46:54.760 --> 00:47:02.120]   and insulate Americans from oligarchs, abusive and extractive practices since then the antitrust
[00:47:02.120 --> 00:47:07.960]   arm of the Biden administration have been effing ninjas in getting expletive, getting
[00:47:07.960 --> 00:47:12.440]   tough, done in ways both large and small working for the first time since Reagan
[00:47:12.440 --> 00:47:16.920]   to protect Americans from predatory businesses. You can gather from this that Cory is a fan.
[00:47:16.920 --> 00:47:22.920]   You're right. You think? Yeah. This in market contrast, this is interesting to the corporate
[00:47:22.920 --> 00:47:28.120]   Dems champions in the administration, the corporate Dems champions like Pete Buttigieg,
[00:47:28.680 --> 00:47:35.880]   heralded his competent technocrats, realists who are too principled to peddle Hopium to the base
[00:47:35.880 --> 00:47:42.040]   writing checks they can't cash. Hopium? Hopium. Yeah, I like that. That's a that's a Coryism.
[00:47:42.040 --> 00:47:44.760]   That's a Coryism. I don't think that I've ever seen that word before.
[00:47:44.760 --> 00:47:49.240]   It's a good one, Obama. I'm surprised that wasn't put on Obama. Hopium. Yeah.
[00:47:49.240 --> 00:47:55.720]   Anyway, naturally, this is really pissed off all the right people. America's billionaires
[00:47:56.520 --> 00:48:01.880]   and their cheerleaders in the press government and the hive of scum and villainy that is the big
[00:48:01.880 --> 00:48:08.280]   law think tank industrial complex. Hive of scum and villainy is a Star Trek, a Star Wars
[00:48:08.280 --> 00:48:12.520]   reference right? Yeah. I think that was the I was in the first one. Yeah, it's the
[00:48:12.520 --> 00:48:19.720]   it's the Moss-Eisley spaceport. Yeah. A hive as Opie one says of scum and villainy.
[00:48:21.480 --> 00:48:27.560]   That's beautiful. Take the Wall Street Journal. Since Khan took office, they've published 67
[00:48:27.560 --> 00:48:33.960]   vicious editorials attacking her in their policies. Khan is living rent-free in Rupert Murdoch's head.
[00:48:33.960 --> 00:48:42.680]   One major subgenre of the attacks is that Khan shouldn't bring bringing in a action against Amazon
[00:48:42.680 --> 00:48:46.840]   because her groundbreaking scholarship about the company means she has a conflict of interest.
[00:48:47.640 --> 00:48:54.760]   Holy moly says Corey Doctorow is this a stupid thing to say. The idea that the chair of an expert
[00:48:54.760 --> 00:49:02.840]   agency should recuse herself because she is an expert is what the physicists call not even wrong.
[00:49:02.840 --> 00:49:15.240]   He is a very good writer in any event. He he defends Connie says the fact that people are saying
[00:49:15.240 --> 00:49:22.040]   she's losing. It means that she's actually winning. Well, we lost some cases. Stop there. Stop there.
[00:49:22.040 --> 00:49:28.200]   Okay. Right. I mean, that's he wants her to be winning. But the truth is she's been losing.
[00:49:28.200 --> 00:49:35.240]   Which is she cases she lost. I'm not you. You couldn't remember the word trellis.
[00:49:35.240 --> 00:49:42.520]   Well, I'll give you two recent ones the Activision case and then she sued meta over their acquisition
[00:49:42.520 --> 00:49:48.680]   of within which makes a thing called supernatural. She didn't want them to buy a company that does
[00:49:48.680 --> 00:49:54.520]   a VR game. Yes, she's lost in those two cases and I expected to lose more. That is not a reason not
[00:49:54.520 --> 00:50:01.080]   to file those cases. I would submit a the very fact that the FTC is finally doing something
[00:50:01.080 --> 00:50:06.040]   is enough to get corporate boardroom. Well, I'm not looking for any of these. Sorry.
[00:50:07.320 --> 00:50:13.800]   The only thing so the FTC does a lot more than just cases. So they've actually revoked some
[00:50:13.800 --> 00:50:19.960]   of their merger guidelines for older merger guidelines that are no longer applicable. They've
[00:50:19.960 --> 00:50:24.120]   also so they're trying to modernize the industry. They're also issuing rulemaking
[00:50:24.120 --> 00:50:29.640]   new draft merger guidelines that are actually really good. They're actually talking about what
[00:50:29.640 --> 00:50:35.720]   we talk about Jeff changing the fact that we only look at low prices for consumers as the only reason
[00:50:35.720 --> 00:50:41.880]   to prevent a merger. So they actually this month issued they're asking for comments on new merger
[00:50:41.880 --> 00:50:46.600]   guidelines. So in addition to the cases that we're hearing a lot about, they're also doing a lot
[00:50:46.600 --> 00:50:51.560]   of stuff behind the scenes. There are very this is and they're doing a lot of smaller enforcement
[00:50:51.560 --> 00:50:58.920]   actions. Here's more Corey. Before con the FTC was a conflict of interest assembly line
[00:50:58.920 --> 00:51:03.320]   moving through corporate lawyers and industry hangers on without resistance for decades.
[00:51:03.960 --> 00:51:10.440]   Con is the first FTC head with no conflicts. This leaves her opponents in the sweaty,
[00:51:10.440 --> 00:51:15.880]   desperate position of inventing conflicts out of thin air for these corporate lickspittles.
[00:51:15.880 --> 00:51:23.480]   Con's conflict is that she has a point of view. Specifically, she thinks the FTC should do its job.
[00:51:23.480 --> 00:51:27.880]   And yeah, you're going to lose a few. You're going to win a few maybe we're going to lose a few.
[00:51:27.880 --> 00:51:33.080]   Okay. So here's here's the here's the question though. If they had someone who was pardon me,
[00:51:33.080 --> 00:51:38.200]   older and more experienced, would they have done a better job of picking the battles right
[00:51:38.200 --> 00:51:42.920]   and winning more and then making more actual progress rather than giving the right wing
[00:51:42.920 --> 00:51:50.520]   cases or not the opportunity to say, yeah, yeah, we can do anything we want now because because
[00:51:50.520 --> 00:51:56.680]   you keep losing. So cases are brought for a couple reasons. And yes, it does suck that she's
[00:51:56.680 --> 00:52:07.160]   taken on two big cases and lost at well, at that moment. Now, she's taking on a lot of other cases
[00:52:07.160 --> 00:52:14.680]   that they're either driving to settlement or actually winning on or being told, hey, you have a point
[00:52:14.680 --> 00:52:23.560]   but you got to come back with this. And what I think is worth celebrating is the fact that A,
[00:52:23.560 --> 00:52:29.080]   there is a point of view and it is a very pro consumer and pro small business point of view.
[00:52:29.080 --> 00:52:36.360]   And B, they're firing on not just the major cases, but all cylinders. And a lot of times you see
[00:52:36.360 --> 00:52:44.680]   like activists kind of or people say they're activists or democratic people come in behind the
[00:52:44.680 --> 00:52:51.240]   scenes and try to like do a lot of fluffy stuff that's pretty low value and low risk.
[00:52:52.120 --> 00:52:58.440]   And they can get like Jessica Rosen Whorsell, bless her heart, is a good case of somebody
[00:52:58.440 --> 00:53:05.160]   who's going for low risk, low value kind of wins when it comes to policy and actman. She's very liberal.
[00:53:05.160 --> 00:53:11.400]   She's making changes that are pretty small fry and not really consequential.
[00:53:11.400 --> 00:53:16.360]   Lena's doing well and now actually going after big stuff.
[00:53:16.360 --> 00:53:20.760]   Why does it seem it's really important that she's there because we have this AI thing
[00:53:20.760 --> 00:53:27.640]   looming over our head. She's announced the FTC is investigating, not suing, but investigating
[00:53:27.640 --> 00:53:34.040]   open AI for non-consensually harvesting a bunch of personal information. That needs to be investigated.
[00:53:34.040 --> 00:53:38.920]   Right. That's an only in most of the. But at least she's keeping everybody honest.
[00:53:38.920 --> 00:53:44.440]   She's gone after Amazon Ring as a privacy dumpster fire. That's this is Corey's words.
[00:53:45.160 --> 00:53:51.800]   Khan has talked to our agency is protecting mom and pop grocers from giant price gouging
[00:53:51.800 --> 00:53:58.040]   greed, flation, drunk national chains. That may be costing consumers more money as a result.
[00:53:58.040 --> 00:54:04.280]   Yeah, just as the old fight, just as the old fight against A&P back in the day,
[00:54:04.280 --> 00:54:08.360]   where consumers were. Yeah, I know. But we can't just look at like lowering prices
[00:54:08.360 --> 00:54:13.720]   as the only metric by which we judge if something is a good thing.
[00:54:13.720 --> 00:54:18.760]   Amazon brought prices down initially, but eventually what happens, they drive everybody out of business
[00:54:18.760 --> 00:54:21.160]   and then gets the same fight over A&P.
[00:54:21.160 --> 00:54:26.920]   Well, that's all right. I think. Well, in so. And the FTC is also look, I mean,
[00:54:26.920 --> 00:54:32.520]   you could lower prices for consumers, but then hurt the environment. You can lower prices for
[00:54:32.520 --> 00:54:36.520]   consumers and drive all of your competition out of business. And those are some of the,
[00:54:36.520 --> 00:54:40.200]   like if you look at her paper, that's something. Wait, I mean, if you did,
[00:54:40.200 --> 00:54:46.040]   did you read her paper? No, I've got to now. I want to read it. Yeah. Yeah. I'm sure that's kind of
[00:54:46.040 --> 00:54:52.280]   was the inspiration for Scory's and Shittification article, which is similar, similar premise,
[00:54:52.280 --> 00:54:58.520]   I think. Cons of Poland did manage to repeat a lot of smears against her, but not the bogus
[00:54:58.520 --> 00:55:03.640]   conflict princess interest story. This is where it's germane to Jeff's accusation.
[00:55:03.640 --> 00:55:09.080]   They also accused her of being 0 and 4 in her actions to block mergers. Now, here's the response,
[00:55:09.080 --> 00:55:14.840]   Jeff, ignoring the huge number of mergers that have been called off or not initiated because M&A
[00:55:14.840 --> 00:55:20.680]   professionals now understand they can no longer expect these mergers to be waived through less
[00:55:20.680 --> 00:55:28.360]   having seen for losses. Maybe they're now going to be emboldened. They might try, but she's still
[00:55:28.360 --> 00:55:33.080]   going to go after them. Cory uses an example just last night. I spoke with a friend who owns a
[00:55:33.080 --> 00:55:38.280]   medium sized tech company that Meta tried to buy out only to withdraw from the deal because Meta's
[00:55:38.280 --> 00:55:43.560]   lawyers told him it would get challenged at the FTC with an uncertain outcome.
[00:55:43.560 --> 00:55:49.240]   That's what I'm digging right there. It's keeping folks honest and not trying to just slide
[00:55:49.240 --> 00:55:53.720]   stuff under the table. You can't, you don't, but you don't want, I understand what you're saying
[00:55:53.720 --> 00:55:57.560]   is there might be a middle ground. What you don't want is an FTC that says, "Come on, everybody,
[00:55:57.560 --> 00:56:02.440]   just merge. We don't care." No, you don't want somebody to pick fights they can't win. And so,
[00:56:02.440 --> 00:56:07.640]   the only argument would be is she picking fights that she can't win. That's my point. That's my point.
[00:56:07.640 --> 00:56:10.760]   Okay, and that's fair. That's fair. And it's not just winning. I mean,
[00:56:10.760 --> 00:56:14.760]   it also, I think one can judge whether all the fights are the right fights. I disagree with some
[00:56:14.760 --> 00:56:18.360]   of the fights. You're going to agree with some. We're going to disagree with some, and that's okay.
[00:56:18.360 --> 00:56:27.480]   But I think she came in on Nell Pissiac. She came in as an idyllog. And it's hard for an idyllog in
[00:56:27.480 --> 00:56:33.880]   Washington to make great victories. And so, at some point, she's going to disappoint
[00:56:34.680 --> 00:56:39.640]   those. At some point, I'm surprised I don't see Corey at some point saying,
[00:56:39.640 --> 00:56:45.880]   "But I wish we win more cases." That's the sort of centrist BS that people say, and that's what ends
[00:56:45.880 --> 00:56:51.080]   up. That's how we have been ending up pandering to the farther and farther right by calling these
[00:56:51.080 --> 00:56:58.440]   people idyllogs. I agree 100%. And I will also say, you mentioned Amazon ring privacy. She did a
[00:56:58.440 --> 00:57:04.680]   privacy settlement against Microsoft around children's rules. She also is helping dismantle
[00:57:04.680 --> 00:57:10.120]   non-competes, which I think, Jeff, you would be a big fan of. She's also...
[00:57:10.120 --> 00:57:15.240]   I'm not saying she's fighting a lot of battles. That's the point. The fact that she's lost four
[00:57:15.240 --> 00:57:21.400]   is not the only story. Remember, click to cancel. We had this story last week.
[00:57:21.400 --> 00:57:26.760]   They're fighting companies that make it so hard. And they're actually changing rule-making
[00:57:26.760 --> 00:57:30.920]   to cancel. I haven't heard a thing about doing that with newspapers, by the way.
[00:57:30.920 --> 00:57:37.640]   Well, but they want to make it as easy to cancel as it was to sign up. And who is against that,
[00:57:37.640 --> 00:57:39.960]   except the police? I know I absolutely agree. I absolutely agree.
[00:57:39.960 --> 00:57:46.200]   They're doing their junk fee stuff. I mean, I get you, Jeff, but I think you're doing the sort
[00:57:46.200 --> 00:57:53.240]   of thing that you actually hate when journalists do headlines that totally occlude the rest of the
[00:57:53.240 --> 00:57:57.240]   value of the story or the situation. I feel like you're looking at the headlines of
[00:57:57.240 --> 00:58:01.320]   Leon Pronze lost four big cases. Well, I think those headlines matter.
[00:58:01.320 --> 00:58:04.680]   I'm not doing what Corey is accused of the right wing of the Wall Street Journal doing.
[00:58:04.680 --> 00:58:07.960]   I'm not accusing her of conflict of interest or any of that stuff.
[00:58:07.960 --> 00:58:14.280]   I do say... I don't know why that was a laugh line. I do say...
[00:58:14.280 --> 00:58:19.480]   Well, she... The right wing is going after all kinds of whole laundry list of things.
[00:58:19.480 --> 00:58:25.000]   I'm not doing any of that. I'm saying A, she's lost important cases. B,
[00:58:25.000 --> 00:58:33.880]   she is treating all of technology, moral panic time, as a bad, a presumed bad.
[00:58:33.880 --> 00:58:39.880]   She is not treating all of technology as a presumed bad. She is actually active. Thank you.
[00:58:39.880 --> 00:58:41.640]   Thank you. That is why I was laughing, Jeff.
[00:58:41.640 --> 00:58:49.320]   Oh, oh, okay. Let's see. Stacy Higginbotham's last nerve has been hit. Yes, let's add another
[00:58:49.320 --> 00:58:54.120]   sticker to the collection. Okay, I didn't have that on my screen.
[00:58:54.120 --> 00:58:57.080]   Usually that's me that gets that button.
[00:58:57.080 --> 00:59:03.400]   Finally, I'm going to finish the Corey Doctor of Article with his concluding paragraph,
[00:59:03.400 --> 00:59:05.480]   and then we can all burn Jeff at the stake.
[00:59:05.480 --> 00:59:09.160]   That's what normally happens.
[00:59:09.160 --> 00:59:16.040]   If way back Kahn and her team, Corey writes, "They will protect us from these scams."
[00:59:16.040 --> 00:59:24.360]   Don't let them, him, convince you to give up hope. This is the start of the fight, not the end.
[00:59:24.360 --> 00:59:29.560]   We're trying to reverse 40 years of Reaganomics here. It won't happen overnight.
[00:59:29.560 --> 00:59:35.560]   There will be setbacks, but keep your eyes on the prize. This is the most exciting moment for
[00:59:35.560 --> 00:59:42.040]   countering corporate power and giving it back to the people in my lifetime. We owe it to ourselves,
[00:59:42.040 --> 00:59:46.440]   our kids, and our planet, to fight one. Union, hell!
[00:59:46.440 --> 00:59:53.240]   I wonder if he's projecting this to be another 40-year battle since we're trying to clean up for it.
[00:59:53.240 --> 00:59:57.320]   Corey is great, and he's also one of the best writers you have.
[00:59:57.320 --> 01:00:02.760]   He's a polemicist and he's a brilliant polemicist, and this is polemic at the highest order.
[01:00:02.760 --> 01:00:07.880]   I don't disagree with him. I understand your point, absolutely, Jeff.
[01:00:07.880 --> 01:00:14.120]   And you're a little bit of hand-ringing here, but I think in the final analysis, if you look at the
[01:00:14.120 --> 01:00:20.520]   entire scope of what this very active FTC has done, it is absolutely positive.
[01:00:20.520 --> 01:00:25.240]   I just want to caution people to be a little bit careful
[01:00:25.240 --> 01:00:31.800]   focusing on, well, she's lost four cases now, because just as you say, Stacey, there's so much
[01:00:31.800 --> 01:00:38.920]   more going on that they've done that we agree with. Things like this can't be useful.
[01:00:38.920 --> 01:00:47.400]   An activist FTC is the right way to look at this. The FTC under Lena Kahn is issuing more
[01:00:47.400 --> 01:00:57.240]   statements in settling and pushing more lawsuits and pushing back more than any other FTC in recent
[01:00:57.240 --> 01:01:02.600]   memory. You could categorically look at that just by looking at the press releases issued.
[01:01:02.600 --> 01:01:08.280]   FTC announces nationwide enforcement sweep to stem the tide of illegal telemarketing calls to
[01:01:08.280 --> 01:01:16.280]   US consumers. That was just yesterday. Look at all that. These are all FTC actions.
[01:01:16.280 --> 01:01:20.760]   FTC and federal and state partners nationwide, Robocall and telemarketing enforcement sweep in
[01:01:20.760 --> 01:01:27.080]   Chicago. FTC sues to block IQV as acquisition of Propel Media to prevent increased concentration
[01:01:27.080 --> 01:01:30.840]   of healthcare programmatic advertising. There's the headlines.
[01:01:30.840 --> 01:01:37.640]   Then if you go to, this is just the last week, by the way, if you go to just the FTC.gov
[01:01:37.640 --> 01:01:44.200]   website and look at their actions, they have been very active in doing a lot of good things, I think.
[01:01:44.200 --> 01:01:53.400]   And a lot of the, I mean, honestly, I want my government agencies to actually take action
[01:01:53.400 --> 01:01:59.160]   against things. And a lot of times there's so much regulatory capture in these things.
[01:01:59.160 --> 01:02:05.560]   And it makes us so cynical about government. And I think we're of an era that is super cynical.
[01:02:05.560 --> 01:02:13.240]   And this is a, Lena Kahn's actually helping me not be a cynical, I guess, by actually taking
[01:02:13.240 --> 01:02:20.120]   these actions. So, thank you, Corey Doctorow to for clearing my mind and allowing me to steal your
[01:02:20.120 --> 01:02:28.280]   pros. And I will get up on this table and hammer it home. We need a, why don't we have a speaker's
[01:02:28.280 --> 01:02:33.400]   corner in the US like they have in Hyde Park in London? We're in Canada and Toronto.
[01:02:33.400 --> 01:02:38.360]   Did they speak? Did they really? I did TV. Oh, yeah. Yeah. Yeah.
[01:02:38.360 --> 01:02:41.720]   They had those machines. You'd go on. Public access. Isn't it Portland?
[01:02:41.720 --> 01:02:48.680]   I actually did one of those machines in Toronto. That was actually really cool. Who is that? Was
[01:02:48.680 --> 01:02:57.720]   Mo Moses Zimmer, Moses Zimmer, who was a great visionary way back when with City TV. And he put
[01:02:57.720 --> 01:03:03.080]   all over the city in the early days, little kiosks with video. I think it was one just,
[01:03:03.080 --> 01:03:08.200]   oh, there was just on Queen Street. There was one on Queen Street. There was one on the City TV
[01:03:08.200 --> 01:03:15.000]   offices on Queen Street. And you had to put it a loon, which is a single dollar coin.
[01:03:15.000 --> 01:03:19.640]   Yeah, with a loon on it. Is that a loony? Oh, yeah, that's a loony.
[01:03:19.640 --> 01:03:24.520]   A loony is the loon. It's a single. Yeah, there's a loony and there's a tuning, just two loons, two
[01:03:24.520 --> 01:03:30.200]   Oh, did not know. Okay. Two dollars. There are dollar coins and two dollar coins in the
[01:03:30.200 --> 01:03:33.720]   game. He made whole shows out of it. I mean, it was the original TikTok. It was the original
[01:03:33.720 --> 01:03:39.480]   YouTube. Yeah. 50. He started his fall players would come in after a bad game and apologize for
[01:03:39.480 --> 01:03:44.360]   play. Yeah. People would propose to each other. They'd make up songs and come into each other.
[01:03:44.360 --> 01:03:50.360]   And they made shows out of this. It was brilliant. When we started what became News 12 New Jersey,
[01:03:50.360 --> 01:03:55.080]   I insisted on doing the same thing here in New Jersey and they build a kiosk and they put it in
[01:03:55.080 --> 01:04:00.680]   the mall. They didn't know how to do it in this. Yeah. It's funny you should say that because I
[01:04:00.680 --> 01:04:08.920]   went, made a field trip to City TV. There it is on the young street to a field trip on to City TV.
[01:04:08.920 --> 01:04:15.400]   And did a tour on the inside with Amber MacArthur as we were setting up a twit. Because I wanted
[01:04:15.400 --> 01:04:20.760]   to have an open studio. He was the one who invented the idea of opening studios. Yes. And I wanted to
[01:04:20.760 --> 01:04:28.520]   duplicate what Moses did there. And he was the whole building was a studio. Yeah. They could
[01:04:28.520 --> 01:04:34.040]   plug in anywhere in the building. It was brilliantly done. And it was also the anchors, you know,
[01:04:34.040 --> 01:04:40.920]   not sitting at a desk walking around. It was exactly what I wanted to do with in the early days of
[01:04:40.920 --> 01:04:45.160]   twit with the brick house. And then you could go to any desk and you'd plug in the camera and
[01:04:45.160 --> 01:04:50.200]   plug in the microphone. And now it's live, you know, if you look up speakers corner and the City TV,
[01:04:50.200 --> 01:04:55.240]   there's some video you can probably show. That's what I'm looking for. Yeah. There's actually a TV
[01:04:55.240 --> 01:05:05.880]   series. Yeah, there's the kiosk. It does say notice out of order. So the guy's just, wow,
[01:05:05.880 --> 01:05:12.280]   it was out of order when I was there as well. Insert Looney. Is that where the kids of the
[01:05:12.280 --> 01:05:18.520]   hall came from? Start now. It might be actually come to the theater. I don't know. City TV was,
[01:05:18.520 --> 01:05:25.240]   you know, acquired and lost its over time. Oh, you lost mojo. But yeah, this is the
[01:05:25.240 --> 01:05:32.280]   discord Leo. Here's another one. This is at the A channel Victoria building. So this would be in
[01:05:32.280 --> 01:05:38.520]   BC. All right, I'm gonna let me let me click this link here. Oh, I still have Stacy Higginbotham.
[01:05:38.520 --> 01:05:42.520]   You put it in discord. That's the discord. I accidentally closed the
[01:05:42.520 --> 01:05:45.720]   discourse. I don't see it in discord. I don't know where you put it.
[01:05:46.840 --> 01:05:51.240]   Mr. Jarvis, say you lost right there. It says Jeff Jarvis today. Oh, it's a brick house. That's
[01:05:51.240 --> 01:05:58.520]   the problem. Look at you. All right. All right. That's why I didn't see it. I got it. See, I get
[01:05:58.520 --> 01:06:03.720]   it before everybody else. Here we go. Speakers, corner ladies and gentlemen. You will fast forward
[01:06:03.720 --> 01:06:03.720]   something.
[01:06:03.720 --> 01:06:13.960]   Give them money what they need because we got problems with food and and that was in the
[01:06:13.960 --> 01:06:19.960]   store that was there at the gym between six and seven because my mother has the biggest crush on you.
[01:06:19.960 --> 01:06:26.520]   Okay. And all those other women are trying to pick up the guys.
[01:06:26.520 --> 01:06:31.960]   Second time he's spoken of it. I think someone's a little obsessed. Don't you say I think somebody
[01:06:31.960 --> 01:06:39.240]   needs him. Oh boy, this is the type of nurses in the NIC. We just like to say that without them,
[01:06:39.240 --> 01:06:43.880]   they too would have never made it here. This is why it works, right? This is why it works.
[01:06:43.880 --> 01:06:51.000]   It's real people talking about stuff, sometimes dumb, that they care about. This TikTok. It is a
[01:06:51.000 --> 01:06:56.360]   TikTok in the early days. It is not. We got all photos to meet. Come on guys. Help us out. We need
[01:06:56.360 --> 01:07:01.240]   money. We need to make a living. We're two teachers. Come on. Guys, you're teenagers. Chief
[01:07:01.880 --> 01:07:10.120]   and boss. And how's it going? Slugger and nice to see a big guy. I hate that.
[01:07:10.120 --> 01:07:15.880]   That's by the way, that's me. I don't know. I recorded that some years ago.
[01:07:15.880 --> 01:07:24.280]   You don't say big. I hate that. Actually, they probably call you big guy all the time.
[01:07:24.280 --> 01:07:29.880]   I have a buddy that calls me tiny actually. Tiny is even one of the unknown things about it was
[01:07:29.880 --> 01:07:33.480]   you thought you had to put it in the loony to make it go. The truth is the camera was just on all
[01:07:33.480 --> 01:07:37.480]   time. It was always on. They were that sophisticated. They hadn't figured that part out.
[01:07:37.480 --> 01:07:41.800]   That's hysterical. That's pretty cool though. That's so, isn't that, that's so.
[01:07:41.800 --> 01:07:46.040]   Zimer was such a pioneer in television. It was. I don't care to be a part of it,
[01:07:46.040 --> 01:07:50.200]   but it's still a pretty cool idea what they did. Yeah. Well, you know.
[01:07:50.200 --> 01:07:54.040]   You could go on there and make a petition for the hardheads to get into college.
[01:07:54.040 --> 01:07:58.280]   Yeah, I could. Good night. You basically, you have to be here. It's called Twitter. Really.
[01:07:58.280 --> 01:08:02.120]   Yeah. Something. This was Twitter before Twitter. That's exactly the thing.
[01:08:02.120 --> 01:08:05.160]   What did you call that? Speaker table? Speaker's corner. Speaker's corner.
[01:08:05.160 --> 01:08:09.160]   And it's because in for, I don't know, for a hundred years, they've had
[01:08:09.160 --> 01:08:13.880]   Speaker's Corner at Hyde Park in London where you people, crazy people would go
[01:08:13.880 --> 01:08:18.280]   put a Apple box. I thought that was Portland. No, London.
[01:08:18.280 --> 01:08:20.280]   No, it's not. That's too easy. That was just New York City.
[01:08:24.280 --> 01:08:28.280]   I love D.D. York. The street preachers. That's basically it. Oh, yeah. Absolutely.
[01:08:28.280 --> 01:08:31.240]   And they have megaphones, right? I don't know if you're allowed to have a megaphone
[01:08:31.240 --> 01:08:36.840]   in Hyde Park. Maybe you are. I don't know. Speaker's corner. Here it is. The Royal Park.
[01:08:36.840 --> 01:08:41.560]   It's a traditional site for public speeches and debates since the mid 1800s.
[01:08:41.560 --> 01:08:47.800]   They even have a little plaque there. That's, see, we didn't invent free speech.
[01:08:51.560 --> 01:08:56.520]   No. No such thing. No, we did not. We did not. All right. Let's take a break.
[01:08:56.520 --> 01:09:02.040]   Well, I come back. I just read a wonderful book. I listened to it on Audible.
[01:09:02.040 --> 01:09:08.280]   Real quick plug here of it called, oh hell, where is it? Life Liberty in the Pursuit of
[01:09:08.280 --> 01:09:16.680]   Happiness by Peter Moore, which is the British roots. No, shush. So we did roots of our sense of
[01:09:16.680 --> 01:09:24.280]   freedom. John Wilkes was one of my heroes, Benjamin Franklin and other characters in it.
[01:09:24.280 --> 01:09:32.760]   It's really well done. Nice. I must read. I will add it to my wish list, which is ever grown.
[01:09:32.760 --> 01:09:36.600]   Oh, you can do a commercial. Ever growing. Well, we just did an Audible commercial and they didn't
[01:09:36.600 --> 01:09:44.760]   even pay for it. Damn it. To get it dang it. Life Liberty in the Pursuit of Happiness
[01:09:44.760 --> 01:09:48.680]   Britain in the American Dream. Well, it makes sense. It is that accent too.
[01:09:48.680 --> 01:09:53.480]   It's very, it's, oh, John Lee is wonderful. One of my favorite readers.
[01:09:53.480 --> 01:09:57.720]   Adams had done as much as anyone to bring the issue of independence to a head.
[01:09:57.720 --> 01:10:04.600]   As he rose in reply to D. Who's here, Hall's here? I love John Lee. I love John Lee. He does
[01:10:04.600 --> 01:10:10.040]   Peter F. Hamilton's novels. You've probably heard his John Lee's narration, John. No,
[01:10:10.040 --> 01:10:17.320]   oh, you read him in book four, you old-fashioned, funny, nutty, you are shot today brought to you by
[01:10:17.320 --> 01:10:23.880]   Brooke Lennon. I must buy more so that I can have, because I change the sheets every week,
[01:10:23.880 --> 01:10:32.840]   because I'm civilized human. Brooke Lennon is the best linen ever from Brooklyn. B-R-O-O-K-L-I-N-E-N
[01:10:32.840 --> 01:10:37.880]   sleeping during the hot summer months and we are getting the hot summer months to say the least.
[01:10:37.880 --> 01:10:42.120]   Can be a little bit sweaty, a little bit difficult. That's why you need
[01:10:42.120 --> 01:10:49.480]   Brooke Lennon's hotel quality luxury bedding delivered directly to you at your door for a fair price,
[01:10:49.480 --> 01:11:00.120]   because they have sheets for the cool sleeper, cool off with their crisp, classic percale weave.
[01:11:00.120 --> 01:11:06.680]   Now, Lisa and I opted for the best-selling buttery smooth looks, satine sheets. Oh, those
[01:11:06.680 --> 01:11:11.720]   feels so good. Brooke Lennon's mission is to provide you with hotel quality luxury bed,
[01:11:11.720 --> 01:11:16.840]   elit bedding, at a very fair price. And I have to say, having bought very expensive
[01:11:16.840 --> 01:11:21.560]   linens, I was so pleased to see the prices of Brooke Lennon. Fatted by husband and wife duo
[01:11:21.560 --> 01:11:28.200]   Richard Vicki in 2014, about nine years ago, Brooke Lennon has everything in need to live
[01:11:28.200 --> 01:11:34.280]   your most comfortable life. Upgrade your home with quality products, curated designs that will
[01:11:34.280 --> 01:11:40.120]   leave your guests swooning. Brooke Lennon has been making dream spaces a reality for almost a decade.
[01:11:40.120 --> 01:11:43.960]   I could see Jeff saying, "Hey, you know those Brooke Lennon sheets, I'm pretty nice. Mind if I
[01:11:43.960 --> 01:11:50.280]   come over and try them out?" I do mine. These are mine. These are mine. They, by the way,
[01:11:50.280 --> 01:11:55.800]   they also have an organic collection now, which is really cool. But the key with Brooke Lennon is
[01:11:55.800 --> 01:12:02.680]   they use the highest quality, long staple cotton that makes such a difference. And it's not just
[01:12:02.680 --> 01:12:09.160]   sheets, by the way. They, in fact, you'll save money if you bundle bed, bath, or both together.
[01:12:09.160 --> 01:12:14.120]   We got the sheets. We got the pillowcases. We got the duvet. We got the towels. I love my
[01:12:14.120 --> 01:12:19.480]   Brooke Lennon towels. Save up to 25% when you're bundling. Wirecutter and good housekeeping. Both
[01:12:19.480 --> 01:12:26.680]   said Brooke Lennon is outstanding. And it's not just those two. They have over 100,000 five-star
[01:12:27.240 --> 01:12:35.560]   customer reviews. 100,000. Brooke Lennon truly is the internet's favorite sheets. One reviewer said,
[01:12:35.560 --> 01:12:40.200]   "I seem to get that wonderful sleeping temperature very quickly and stay there throughout the night
[01:12:40.200 --> 01:12:46.680]   versus with my older cotton sheet sets." Another said, "I look forward to going to bed every night,
[01:12:46.680 --> 01:12:53.560]   slipping in between those beautiful buttery, smooth, look-sattined sheets. Ah, it feels so good. That
[01:12:53.560 --> 01:13:01.000]   reviewers name Leo Laporte." So who's that guy? Who's that guy? Best sheets in the world like
[01:13:01.000 --> 01:13:07.720]   butter? Like butter, my friends. That wasn't me. Brooklyn uses only the highest quality materials
[01:13:07.720 --> 01:13:13.000]   for all of their products. Everything will last and last and last. It's made that way.
[01:13:13.000 --> 01:13:17.960]   Shop in store. Yes, there are Brooklyn in stores. Check for one near you. But I think most of us
[01:13:17.960 --> 01:13:24.680]   are going to do it online at brooklinen.com. Give yourself the cooling sleep you deserve this hot
[01:13:24.680 --> 01:13:32.200]   summer. Use Twig as the offer code $20 off your online purchase of $100 or more plus free shipping.
[01:13:32.200 --> 01:13:42.120]   brooklinen.com. B-R-O-O-K-L-I-N-E-N.com. And don't forget that promo code Twig. Very important.
[01:13:42.120 --> 01:13:46.840]   So they know you saw it here and you get the $20 off and free shipping. Brooke Lennon,
[01:13:47.400 --> 01:13:54.760]   thank you Brooklyn. I slept on them last night but you know Wednesday sheet changed day so
[01:13:54.760 --> 01:14:01.320]   we're gonna have to use some subpar sheet. I'm gonna have washed them the same day. Oh I could do
[01:14:01.320 --> 01:14:06.120]   that couldn't I? I didn't think of that. That's what we do. That's what I do. Take them off
[01:14:06.120 --> 01:14:12.840]   why then you only need one set huh? I mean I have multiple sets of sheets for you know when that
[01:14:12.840 --> 01:14:17.000]   doesn't work but yeah. Well it does solve the problem of how do you fold a fitted sheet.
[01:14:17.000 --> 01:14:20.600]   If you don't ever have to. Yeah that's true. Put it right back on.
[01:14:20.600 --> 01:14:29.000]   I need to let you know. Why is it that you know why is it there at a Silicon Valley startup
[01:14:29.000 --> 01:14:34.280]   to solve that problem the fitted sheet? I'm sure somebody has. Because it's it's not a solvable
[01:14:34.280 --> 01:14:45.240]   problem. It's not solvable. It's like fission. We fission we solve. You mean fusion? Fusion.
[01:14:45.240 --> 01:14:50.040]   Sorry. Fusion. Yeah that one. The sun does. That one. Can't figure that one out for the life of us.
[01:14:50.040 --> 01:14:56.040]   So let's do a Google story. This comes from Paul Thoreau. I love it. Paul does not like Google.
[01:14:56.040 --> 01:15:02.120]   No. Mark Lukowski leaves Google calls company unstable. He didn't like it either.
[01:15:03.320 --> 01:15:11.160]   Former member of the original Microsoft NT team. He left Google 20 years after joining them.
[01:15:11.160 --> 01:15:16.600]   He's been there 20 years. Writing this. Get ready. He actually tweeted it. Does that count?
[01:15:16.600 --> 01:15:23.240]   Is that writing? Today it does. Yeah. I have decided to I'm going to do him in a cranky old man
[01:15:23.240 --> 01:15:30.520]   voice. I have decided to step away from my role at Google where I was a senior director of engineering
[01:15:30.520 --> 01:15:37.800]   responsible for OS and software platform for AR and XR devices. Now this is where it gets mean.
[01:15:37.800 --> 01:15:45.160]   The recent changes in AR leadership and Google's unstable commitment and vision have weighed heavily
[01:15:45.160 --> 01:15:52.680]   on my decision. So let's stop there. He's bitter that he can't make his toys that nobody really wants
[01:15:52.680 --> 01:15:58.920]   anyway. So we will make a smart decision to get out of AR. Moving forward I'm eager to explore
[01:15:58.920 --> 01:16:03.720]   opportunities that allow me to further advance augmenting. Somebody hired this man.
[01:16:03.720 --> 01:16:08.840]   And it's intersection with generative AI because that's the hot thing right now.
[01:16:08.840 --> 01:16:16.040]   Yeah. I'm going to have some NFTs and. Somebody will give you a job out there. He has worked.
[01:16:16.040 --> 01:16:21.800]   Okay. This guy may be less. He's worked at Google Facebook, Mambo, VMware, Google, Microsoft,
[01:16:21.800 --> 01:16:29.480]   X, by test color, data general and Victor. Wow. Okay. He was at Google left, Google came back to
[01:16:29.480 --> 01:16:35.400]   Google. So add that in too. Right. Yeah. He's a grumpy cuss. Okay. But maybe they're okay. I'm
[01:16:35.400 --> 01:16:43.240]   just looking for anything. Well, so Google has been kind of waffly on there. Yeah. They're
[01:16:43.240 --> 01:16:48.520]   watching. Yeah. To do it. Maybe I'm just like if we're going to.
[01:16:49.480 --> 01:16:54.040]   Maybe he should go to meta because meta is finally actually there are two stories this week. One
[01:16:54.040 --> 01:17:00.760]   is meta. The other is Apple, both of whom are working hard. They have never released publicly.
[01:17:00.760 --> 01:17:09.880]   Chat GPT style products as Microsoft and Google have. Microsoft is working with meta on the next
[01:17:09.880 --> 01:17:18.680]   generation of llama. Where's it? Yama? Llama. It's LLM. So it's just they've added some letters
[01:17:18.680 --> 01:17:24.920]   to writing. Oh, I'm making llama. It's that llama to llama one leaked out. I don't think it was ever
[01:17:24.920 --> 01:17:31.400]   officially released by meta. But yeah, that someone kind of stole it. They got stolen.
[01:17:31.400 --> 01:17:36.120]   Today, we're going to talk about them playing fast and loose with open source licensing. Yes.
[01:17:36.120 --> 01:17:42.840]   Okay. Go ahead. No, go on. You're on. I don't know anything about that.
[01:17:44.280 --> 01:17:48.760]   Yes, we will. Today, we're introducing the availability of llama two rights. Mr. Meta.
[01:17:48.760 --> 01:17:54.600]   He's no no byline. The next generation of our open source large language model free
[01:17:54.600 --> 01:18:02.760]   for research and commercial use. And Microsoft making sure they have a hand in every AI basket
[01:18:02.760 --> 01:18:09.640]   is working with meta. They are the preferred partner for llama to. But if it's free, what is
[01:18:09.640 --> 01:18:13.800]   it going to be preferred? Well, the real I think the real issue is and we were talking about this
[01:18:13.800 --> 01:18:19.160]   on Windows Weekly, Richard Campbell's been beating this drum. It's surprisingly expensive
[01:18:19.160 --> 01:18:25.400]   to generate these models and then to support them. And they need somebody like Microsoft with
[01:18:25.400 --> 01:18:33.880]   very big cloud infrastructure and Azure and lots of Nvidia processors to do it. So partnering
[01:18:33.880 --> 01:18:38.440]   with Microsoft makes, I think, a lot of sense. And here's my question. The free part is this
[01:18:38.440 --> 01:18:43.480]   an and this is a stupid question. But most of mine are. Is this an Android like strategy? Like
[01:18:43.480 --> 01:18:49.560]   we're going to undercut everybody else. And that's how llama will become a dominant model.
[01:18:49.560 --> 01:18:58.840]   I don't know. It has to perform, right? Because even with Android, there's people that are so
[01:18:58.840 --> 01:19:03.320]   against Android because it doesn't necessarily perform the way I they're introducing an open
[01:19:03.320 --> 01:19:09.400]   ecosystem for interchangeable AI frameworks. So that sounds interesting. Say you. Wait, what?
[01:19:09.400 --> 01:19:18.200]   I don't know if I can. They are introducing an open ecosystem for interchangeable AI frameworks.
[01:19:18.200 --> 01:19:23.880]   Okay. I think that means they'll be a standard, which would be kind of cool. And having Microsoft
[01:19:23.880 --> 01:19:30.760]   involved is interesting. Is this the llama news? This is the llama. This is all. We're still
[01:19:30.760 --> 01:19:34.840]   talking about you. Wake up. No, wake up. Stay. Wake up. Stay. Hey. Hey.
[01:19:34.840 --> 01:19:43.640]   They are also one with a WS and hugging face. But Azure, it'll be part of the Azure AI model
[01:19:43.640 --> 01:19:48.920]   catalog. So that, you know, this is this Microsoft money because you'll be paying
[01:19:48.920 --> 01:19:56.600]   Azure hours for this Azure minutes. It's framework sounds less like a standard and more like an
[01:19:56.600 --> 01:20:02.040]   app store or something. Maybe you'll have interchangeable models that will work. Yeah.
[01:20:02.040 --> 01:20:08.520]   Although Google was really promoting that idea, but weren't they of kind of plugins and so forth?
[01:20:08.520 --> 01:20:18.040]   Apple has announced or maybe it's leaked Apple GPT. The headline from the Verge Apple is testing
[01:20:18.040 --> 01:20:26.440]   an AI chat bot, but has no idea what to do with it. This is this is a Bloomberg or
[01:20:26.440 --> 01:20:33.240]   for care part of the genius bar. Apple test Apple GPT develops generative AI tools to catch
[01:20:33.240 --> 01:20:40.840]   open AI. Got to catch them all. But you know, I don't know what how this will be released.
[01:20:40.840 --> 01:20:46.680]   According to people with knowledge of the efforts, so this is not an announcement. This is Mark
[01:20:46.680 --> 01:20:53.480]   German, the Apple rumor guy who's very good. Tied on this stuff with that foundation known as
[01:20:53.480 --> 01:20:57.960]   Ajax. Apple has also created a chat bot service. Some engineers call Apple GPT.
[01:20:57.960 --> 01:21:04.680]   What is GPT stand for? Stay. I did. I did look that up just last week. She knows off the top
[01:21:04.680 --> 01:21:10.280]   of it. No, I do not know. It's a generative generative pre-training transformer.
[01:21:14.920 --> 01:21:19.480]   The recent months, AI pushes become a major effort for Apple, says German, with several teams
[01:21:19.480 --> 01:21:27.640]   collaborating on the project. I mean, I guess if you're a meta or Apple or Bob's shop and drop,
[01:21:27.640 --> 01:21:30.440]   you're going to be working on AI because that's the hot thing right now.
[01:21:30.440 --> 01:21:32.760]   Why is it pre-training?
[01:21:32.760 --> 01:21:41.240]   Pre-training? Yeah, generative pre-training transformer. I get generative. I get transformer.
[01:21:41.240 --> 01:21:48.120]   I don't understand the pre-training. Maybe they train it halfway and then they stick it in the
[01:21:48.120 --> 01:21:52.200]   metal. It's not pre-training. This is very important. Pre-trained.
[01:21:52.200 --> 01:22:00.760]   The training does is done first. That's the key. I'm so good. It's pre-trained. It's like
[01:22:00.760 --> 01:22:05.880]   pre-trained. You go there, they chew the meat, then they spit it into your mouth. It's pre-trained.
[01:22:05.880 --> 01:22:12.280]   They've chewed up all of the stuff to make a large language level. As I wrote recently,
[01:22:12.280 --> 01:22:17.320]   it's all the cultural cud. It's the cultural cud, see? You two are a ruminant.
[01:22:17.320 --> 01:22:25.400]   We need a new sticker. Stacey is disgusted.
[01:22:25.400 --> 01:22:34.680]   So here's a picture. Perhaps this will help if I show you this picture of something.
[01:22:34.680 --> 01:22:40.200]   What is this? Oh, well, now it is all clear. Thank you very much.
[01:22:40.200 --> 01:22:43.240]   You people are audio. You're missing.
[01:22:43.240 --> 01:22:51.240]   The light shining on you. I'm telling you. Come on, the video at all will become clear.
[01:22:51.240 --> 01:22:56.440]   But you see, I didn't explain it. You see, you have here on the left is scatter plot
[01:22:56.440 --> 01:23:02.200]   with a linear support vector machine's decision boundary. That's the size, the stash it lines
[01:23:02.200 --> 01:23:08.600]   here. And when we move it over, as we move it over, all the white dots go left, all the black dots
[01:23:08.600 --> 01:23:15.800]   go right, we have the segregated vector machine boundary. It's very simple. I don't understand why
[01:23:15.800 --> 01:23:23.080]   you don't understand. Generative pre-trained transformers are a large language model type
[01:23:23.080 --> 01:23:28.920]   and a prominent framework for generative artificial intelligence. First introduced in 2018 by OpenAI,
[01:23:28.920 --> 01:23:34.120]   but I guess they don't own the name. Okay, you pre-trained a model so you don't have to put a
[01:23:34.120 --> 01:23:38.920]   bunch of data into it. Right. Right. It just sits there. And then you have the model on your
[01:23:38.920 --> 01:23:43.720]   phone. It doesn't have to have an internet connection anymore. It could just do the answers to your
[01:23:43.720 --> 01:23:49.080]   questions. No, no, no, no, no, no. That's inference. That's different than training. So,
[01:23:49.080 --> 01:23:54.440]   pre-training, you can do a model that is pre-trained. So you feed it a whole bunch of stuff, you get
[01:23:54.440 --> 01:23:59.240]   it pretty good. And then you're going to tweak it some more so it's even better.
[01:23:59.240 --> 01:24:03.960]   Oh, so, oh, then I've actually wondered this for a long time. So if I type in a chat GPT,
[01:24:03.960 --> 01:24:08.440]   a question, it's it is working on a static model though, right?
[01:24:08.440 --> 01:24:15.560]   Yeah, so but you could pre-trained a model with like basic rules. So let's say for chat GPT,
[01:24:15.560 --> 01:24:20.440]   you pre-trained it to kind of get a sense of what comes next in English or how the English
[01:24:20.440 --> 01:24:25.560]   language works. Right. So then it can actually generate things. But there's a feedback loop.
[01:24:25.560 --> 01:24:30.520]   But it doesn't need to go out on the internet again, does it? I mean, it's it's got what it
[01:24:30.520 --> 01:24:36.680]   needs, doesn't it? Hold on. Hold on. So you start there, then you take that pre-trained model,
[01:24:36.680 --> 01:24:42.520]   and then you're going to fine tune it with even more stuff. And I don't know what they're doing
[01:24:42.520 --> 01:24:46.920]   for a chat GPT to make it even better. This is a very theoretical thing. So now you're going to do
[01:24:46.920 --> 01:24:51.400]   it so you could do like like Shakespeare maybe. So it's pre-trained on English,
[01:24:51.400 --> 01:24:54.680]   the English language, and then you're going to train it specifically for Shakespeare.
[01:24:54.680 --> 01:25:00.120]   So then when someone gives you the prompt, so now that's a model that can do that. And then
[01:25:00.120 --> 01:25:05.080]   you've got the prompt. The prompt is what you can put in. And eventually the inference is how
[01:25:05.080 --> 01:25:09.720]   it's going to generate the response based on the the prompt that you've asked.
[01:25:09.720 --> 01:25:12.520]   Did that make sense? Yeah. And in fact, I asked chat GPT, it says,
[01:25:13.160 --> 01:25:19.400]   it works by leveraging its pre-trained language understanding and generation capabilities to
[01:25:19.400 --> 01:25:24.440]   process and respond to natural language inputs. So it actually has this long eight step
[01:25:24.440 --> 01:25:30.840]   explanation as starts with training, which is. No, sorry, I asked. I had a question. I didn't
[01:25:30.840 --> 01:25:35.640]   forget what the heck my question was. I gave you like a really easy answer. Yeah, I'm just
[01:25:35.640 --> 01:25:47.640]   there were no scatter plots. I'm just slopes. I stick my face in front of my hand on my face
[01:25:47.640 --> 01:25:52.600]   a lot in this show. I'm just well, we make you do that. And it's understandable to provoke
[01:25:52.600 --> 01:25:59.000]   your profile to put you in that position. With this this partnership here we go.
[01:25:59.000 --> 01:26:05.320]   While chat GPT is pre-trained in a vast data set, it can also be fine tuned on more specific
[01:26:05.320 --> 01:26:12.120]   data sets for particular applications. So that tailors it like Shakespeare. But
[01:26:12.120 --> 01:26:19.960]   it is essential to know chat GPT does have a knowledge cut off. In my case, says chat GPT was
[01:26:19.960 --> 01:26:25.320]   September 2021. So he doesn't have access to information beyond that day. But you could.
[01:26:25.320 --> 01:26:30.280]   You could add in fact, this you can feed in information. This is a new and I think kind of
[01:26:30.280 --> 01:26:36.520]   interesting financial times at a article today on how they've run out of stuff.
[01:26:36.520 --> 01:26:44.920]   They've run out of CUD to chew. And they're looking for new ways to get new data into these new models.
[01:26:44.920 --> 01:26:51.000]   And I'm a little nervous about what they've what they've come up with. They call it
[01:26:54.040 --> 01:27:01.480]   Well, here's the FT Oh darn you. And Bender and company would say this is like a boys
[01:27:01.480 --> 01:27:06.200]   BSD problem is they're trying for ever bigger models. And that only makes it worse. You run out.
[01:27:06.200 --> 01:27:11.160]   Yeah, the internet is not known. So I thought you run out. It's that they're too big. You don't
[01:27:11.160 --> 01:27:16.120]   know what's in them. It's a mess. And there's no need for them to be this big. Here's the FT article.
[01:27:16.120 --> 01:27:22.040]   Why computer made data is being used to train data models. They call it synthetic.
[01:27:22.040 --> 01:27:26.760]   Synthetic data. Yeah, that's just synthetic data. So they do synthetic data not because they run
[01:27:26.760 --> 01:27:31.160]   out of stuff, but because it's cheaper and easier to get. Well, also because they're running out.
[01:27:31.160 --> 01:27:38.760]   Data's been well. In fact, this is no synthetic data. They don't use it because they're running
[01:27:38.760 --> 01:27:44.760]   out of it because like synthetic data is fake. So you can't use it to train something to be
[01:27:44.760 --> 01:27:49.560]   understanding of like how humans are or whatever. You use synthetic data because it's cheaper to get.
[01:27:50.360 --> 01:27:55.400]   Here's what Aiden Gomez, who's the CEO of a startup cohere that's using this a lot. If you
[01:27:55.400 --> 01:28:00.760]   could get all the data you needed off the web, that would be fantastic. In reality, the web is so
[01:28:00.760 --> 01:28:05.880]   noisy and messy that it's not really representative of the data you want. The web just doesn't do
[01:28:05.880 --> 01:28:14.840]   everything we need. So, and Sam Altman talked about this, the founder of OpenAI. He said he's
[01:28:14.840 --> 01:28:22.840]   pretty confident that soon all data will be synthetic data. So, we'll get this. Listen to how they do
[01:28:22.840 --> 01:28:30.280]   this. Gomez says synthetic data is already huge even if it's not broadcast widely. For example,
[01:28:30.280 --> 01:28:36.920]   to train a model on advanced mathematics cohere, which is one of the startup that's doing this,
[01:28:36.920 --> 01:28:44.120]   might use two AI models talking to each other. One is the tutor. One is the student. Gomez says
[01:28:44.120 --> 01:28:50.280]   they're having a conversation about trigonometry. It's all synthetic. It's all just imagined by the
[01:28:50.280 --> 01:28:57.080]   model. And then the human looks at this conversation, goes in and corrects it if the model said something
[01:28:57.080 --> 01:29:03.720]   wrong. That's the status quo today. Two studies from Microsoft Research showed that synthetic data
[01:29:03.720 --> 01:29:09.640]   could be used to train models that were smaller and simpler than the current chat GPT-4, a Palm-2.
[01:29:10.760 --> 01:29:15.720]   One paper described a synthetic data set of short stories generated by GPT-4,
[01:29:15.720 --> 01:29:19.720]   which only contained words that a typical four-year-old might understand. This data
[01:29:19.720 --> 01:29:26.360]   set is known as tiny stories, was then used to train a simple LLM that was able to produce
[01:29:26.360 --> 01:29:33.080]   fluent and grammatically correct stories. So, my concern is it's like mad cow disease, right?
[01:29:33.080 --> 01:29:39.480]   Well, okay. So, synthetic data isn't just for, I get it. I get what you're saying. It's not just for
[01:29:39.480 --> 01:29:43.960]   training language, large language models. They use synthetic data for all kinds of things.
[01:29:43.960 --> 01:29:49.960]   And in other cases, it works really well. So, like, they use it to train drones on how to spot
[01:29:49.960 --> 01:29:55.880]   faults and pipelines. Instead of sending over drones to take photos of millions of lines of
[01:29:55.880 --> 01:30:00.840]   millions of miles of pipelines, they just generate what the flaws look like. They show that to it.
[01:30:00.840 --> 01:30:07.320]   They use that to train it instead of. So, synthetic data is actually highly useful for a lot of
[01:30:07.320 --> 01:30:16.520]   situations. It could even be useful for teaching a computer about how a four-year-old speaks,
[01:30:16.520 --> 01:30:21.160]   because we don't have a lot of examples of four-year-olds and their language abilities on the computer.
[01:30:21.160 --> 01:30:25.560]   But you do have it. You'd have to type that out and figure it out.
[01:30:25.560 --> 01:30:28.120]   You could generate. That chat should be different. You can generate small words,
[01:30:28.120 --> 01:30:33.000]   tiny stories, and then you'll learn from that. They give us an example of hedge funds,
[01:30:33.000 --> 01:30:37.160]   which are looking for black swan events. There aren't a lot of black swan events.
[01:30:37.160 --> 01:30:42.600]   So, what they do is they create a hundred black swan, fake black swan events, and then test against
[01:30:42.600 --> 01:30:48.760]   that. He says, "For banks where fraud constitutes less than a hundredth of a percent of total data,
[01:30:48.760 --> 01:30:54.200]   the software generates thousands of edge case scenarios on fraud and then trains AI models with
[01:30:54.200 --> 01:31:00.600]   this." Just like your drones searching for pipeline breaks. So, okay, so that's not so bad.
[01:31:00.600 --> 01:31:05.640]   It's learning from generated data to simulate the problem you're looking for.
[01:31:05.640 --> 01:31:13.240]   So, it's if you wanted to create a radiology analysis AI, you may not have enough scans to show it,
[01:31:13.240 --> 01:31:17.160]   to train it in from the real world. So, you can create a bunch of...
[01:31:17.160 --> 01:31:19.960]   All the weird stuff that people put up their butts.
[01:31:19.960 --> 01:31:21.080]   You may not have enough of that.
[01:31:21.080 --> 01:31:22.200]   You may not have enough of that.
[01:31:22.200 --> 01:31:27.240]   So, you create many, many more. What's interesting is often with an AI. That's why
[01:31:27.960 --> 01:31:33.800]   I probably inappropriately described it like mad cow disease, which
[01:31:33.800 --> 01:31:43.560]   cows get from eating dead cows. The bovine encephalitis is communicated by eating their brains.
[01:31:43.560 --> 01:31:48.440]   So, this is exactly where Matthew Kirschenbaum goes with his wonderful, I put it in the discord
[01:31:48.440 --> 01:31:55.960]   right place this time. As a prepare for the text pocalypse in the Atlantic, where he says that
[01:31:56.600 --> 01:32:02.200]   we'll have plain, under-door, and text, but in quantity so immense as to seem unimaginable,
[01:32:02.200 --> 01:32:07.000]   a tsunami of text swept into a self-perpetuating, cataract of content that makes it functionally
[01:32:07.000 --> 01:32:11.160]   impossible to reliably communicate in any digital setting. Because there's so much
[01:32:11.160 --> 01:32:13.560]   real with bogus content. So, this one's made up of great glue.
[01:32:13.560 --> 01:32:20.120]   Yeah. And then there's a paper I also put in there where researchers trained a model on
[01:32:21.240 --> 01:32:26.200]   content created by AI models. And they call it the cursive or the recursion.
[01:32:26.200 --> 01:32:29.080]   It leads to what they say is model collapse.
[01:32:29.080 --> 01:32:33.400]   Yeah, you would be almost like, "Junk here and just here and just here." Yeah.
[01:32:33.400 --> 01:32:37.960]   Yeah. We talked about that. We did? I'm just trying to be relevant this
[01:32:37.960 --> 01:32:41.880]   one as well as the last time I talked. My mind is going.
[01:32:41.880 --> 01:32:47.320]   You have been talking about this about 12 minutes and that just now made sense to it.
[01:32:48.760 --> 01:32:53.960]   You're welcome, Ant. You're welcome. By the way, by the way, by the way, I want to make
[01:32:53.960 --> 01:32:59.240]   your your your beard's distinguished. It's getting a nice grade. I like it. Look at that.
[01:32:59.240 --> 01:33:04.280]   That's good. That's always been there. I'm just always saw the same. It's kind of,
[01:33:04.280 --> 01:33:08.280]   it's the right length. It's very good as a white bearded. So, I'm just rather
[01:33:08.280 --> 01:33:13.240]   walking me to the club. I think eventually all the hosts of this show will grow beards.
[01:33:13.240 --> 01:33:18.040]   You think so? Yeah. You think so? It's spreading from Jeff to you.
[01:33:18.040 --> 01:33:22.360]   You think she fears at the end of our show. She's been here so long.
[01:33:22.360 --> 01:33:27.000]   Y'all, I've been tweezing out my beard forever. I mean, my God.
[01:33:27.000 --> 01:33:33.240]   If my beard would look like that, I'm in trying to convince Lisa, "Let me grow a beard." She said,
[01:33:33.240 --> 01:33:37.160]   "Don't grow a beard. Why would you want to grow a beard?" But I just think as you get older,
[01:33:37.160 --> 01:33:42.920]   it kind of hides a multitude of defects. Oh, yeah. I want to look like the most interesting
[01:33:42.920 --> 01:33:50.120]   hide-sjowl. Thank you. I can't say that's why I have a beard, but sure, I get where you come from.
[01:33:50.120 --> 01:33:55.800]   No, you don't have that problem, but some of us. And a little waddle?
[01:33:55.800 --> 01:33:59.560]   I want to look like the waddle waddle. I want to look like the most interesting
[01:33:59.560 --> 01:34:03.560]   man in the world. You know, he's like yours like kind of as close to the ground.
[01:34:03.560 --> 01:34:06.360]   This groomed beard. Yeah. You look good.
[01:34:06.360 --> 01:34:09.720]   Well, it's good. It doesn't hide your face. It looks good. Thank you.
[01:34:09.720 --> 01:34:15.080]   It's smart. Yeah. Speaking of AI, oh, I think we're in our AI segment. I forgot to mention that.
[01:34:15.080 --> 01:34:20.280]   Yeah, where's Mr. How? Where's the trumpets for those? There's no trumpets. It just happens.
[01:34:20.280 --> 01:34:25.960]   You're going to... This is Fox. Waiting, writing about their competitor.
[01:34:25.960 --> 01:34:32.040]   Are you in trouble? Geomedia. What? You say the name. Lisa's coming.
[01:34:32.040 --> 01:34:35.800]   Come on. Lisa came in. Lisa, can you have a beard, please? Can I have a beard?
[01:34:35.800 --> 01:34:41.160]   Can't run in. We want every... So, see? Look at that. See how good that looks?
[01:34:41.160 --> 01:34:46.120]   So, we think the beard is spreading from the left to the right here. So, maybe... Lisa,
[01:34:46.120 --> 01:34:52.360]   just punch him in the arm. Is it too scratchy? Too scratchy. No.
[01:34:52.360 --> 01:34:56.200]   Oh, Lisa, that's her beard. That's her beard. Scratchy. Yeah.
[01:34:56.200 --> 01:35:01.240]   She has very, very delicate skin. Yes. And she doesn't want a big... But see, I'm scratchy
[01:35:01.240 --> 01:35:03.720]   even when I shave. I have to shave like three or four times a day.
[01:35:03.720 --> 01:35:07.640]   Can do you mind? No. It's worth it. What about beard oil?
[01:35:07.640 --> 01:35:11.720]   Beard oil. With that help? I have oil on. This still doesn't help.
[01:35:11.720 --> 01:35:16.520]   Does it get softer? Is it gets longer, Jeff? It does, doesn't it? Not for me.
[01:35:16.520 --> 01:35:22.360]   Yeah. Not for me. But see, he's got a different... I got a different grain of hair, black folks here.
[01:35:22.360 --> 01:35:27.960]   He's got... There is that difference. There is black folks whiskers. That's what it is.
[01:35:27.960 --> 01:35:31.320]   Is it... Yeah, it's tighter. But see, I want it to look like that.
[01:35:32.360 --> 01:35:35.960]   So I guess it's hopeless. How many of you can get that much brown in your beard if you try it?
[01:35:35.960 --> 01:35:43.320]   So Vox, which is, I would imagine, a competitor to Geo Media, right?
[01:35:43.320 --> 01:35:48.120]   Yeah. Not so. Geo Media is like... Yeah, I think that's what I would like to think that they're
[01:35:48.120 --> 01:35:53.800]   above Geo. Yes. Yeah. Vox writes, "You're going to see more AI written articles, whether you like it
[01:35:53.800 --> 01:36:00.600]   or not. They're talking about Geo." And their CEO, Jim Spanfeller, who apparently said,
[01:36:02.040 --> 01:36:05.880]   "Too bad. It's absolutely a thing we want to do more of."
[01:36:05.880 --> 01:36:10.440]   Harold Brown, who's the editorial director, there's an old friend of mine.
[01:36:10.440 --> 01:36:11.960]   Oh, actually it was Meryl who said this.
[01:36:11.960 --> 01:36:20.920]   Yeah. Gizmodo, the onion. AI can't write the onion. No. No. No. No. Jezebel. Not well.
[01:36:20.920 --> 01:36:28.680]   Geo Media. You can write those little comments from people being absolutely ridiculous.
[01:36:28.680 --> 01:36:33.240]   Right. I might even get a little bit more surreal. I wonder if they could. The onion is almost
[01:36:33.240 --> 01:36:38.840]   formulaic, right? Area man. You start with the area man. Well, maybe they could.
[01:36:38.840 --> 01:36:45.640]   Maybe they could. Anyway, they've already published four stories entirely generated by AI.
[01:36:45.640 --> 01:36:51.960]   The stories, and this again is Vox writing, which included multiple errors and ran without
[01:36:51.960 --> 01:36:57.160]   input from Geo's editors or writers, infuriated Geo staff and generated scorn.
[01:36:57.160 --> 01:37:00.680]   Who are infuriating bunch. They like to be infuriated.
[01:37:00.680 --> 01:37:06.280]   Well, but also that's their job. The Gawker ethos brought down in time.
[01:37:06.280 --> 01:37:10.520]   Wouldn't that be ironic? I agree with them. I would be pissed. I would be pissed.
[01:37:10.520 --> 01:37:19.400]   If I worked like when I worked at Fortune or Gigaome, if my company started publishing AI
[01:37:19.400 --> 01:37:24.040]   generated stuff without telling people and it was wrong, you're trying to-
[01:37:24.040 --> 01:37:29.320]   It's a thing. If you're going to use this AI stuff, make sure that you're fact checking and
[01:37:29.320 --> 01:37:31.480]   have some sort of editor in there that's going to make-
[01:37:31.480 --> 01:37:34.920]   I've edited a Star Wars route up that mistakes it. I mean, Jesus.
[01:37:34.920 --> 01:37:40.600]   In a note set, two top editors at his company last Friday, Meryl Brown said, "Editors of
[01:37:40.600 --> 01:37:46.520]   Jalopnik and AAV Club are planning to create content. Summaries are lists that will be produced
[01:37:46.520 --> 01:37:48.600]   by AI, but that's reason-
[01:37:48.600 --> 01:37:50.280]   Summaries like a live with better.
[01:37:50.280 --> 01:37:55.640]   Brown's memo also noted that the Associated Press recently announced a partnership with OpenAI.
[01:37:55.640 --> 01:38:01.240]   I also think doing it like Jalopnik or-
[01:38:01.240 --> 01:38:04.360]   I don't know the other side. AAV Club.
[01:38:04.360 --> 01:38:10.680]   AAV Club. Okay. The people who read those sites are not casual. These are people who know
[01:38:10.680 --> 01:38:15.320]   their stuff. I think it's a real risk. I mean, just generating a list, sure.
[01:38:17.880 --> 01:38:22.840]   It'd be like if someone tried to generate AI content about IoT stuff-
[01:38:22.840 --> 01:38:28.040]   You know what it would be like? If a bot, for instance, tried to generate a chronological
[01:38:28.040 --> 01:38:35.320]   list of Star Wars movies and TV shows, talk about an audience with an investment, right?
[01:38:35.320 --> 01:38:40.600]   And got it wrong and left out, like left out and/or entirely?
[01:38:40.600 --> 01:38:46.040]   Well, they've added and/or since, by the way, they've fixed the errors.
[01:38:46.040 --> 01:38:49.000]   Did they put a disclosure at the end? Yeah.
[01:38:49.000 --> 01:38:53.080]   Correction was made to this story. The episodes' rankings were incorrect.
[01:38:53.080 --> 01:38:58.040]   In particular, the Clone Wars was placed in the correct chronological order in the corrected list.
[01:38:58.040 --> 01:39:00.840]   They also added some- Here's the real problem, Stacey. It's not just that
[01:39:00.840 --> 01:39:07.720]   that they put something next to you that's wrong. It's they've commodified you as the writer, right?
[01:39:07.720 --> 01:39:14.200]   Well, no, they have it. They're trying to commodify me, and they're totally-
[01:39:14.200 --> 01:39:20.200]   He's doing so in making it so wrong, they're actually insulting me by thinking that that's my
[01:39:20.200 --> 01:39:23.880]   sole value. That's where it's actually the years of expertise that I have.
[01:39:23.880 --> 01:39:24.680]   Absolutely. So it's kind of like-
[01:39:24.680 --> 01:39:29.640]   But on the other hand, and most of these articles I would guess-
[01:39:29.640 --> 01:39:33.880]   Oh, we need a sticker for that, bitch, I'm irreplaceable.
[01:39:33.880 --> 01:39:36.600]   I'm just saying.
[01:39:36.600 --> 01:39:40.120]   Yeah, that's a must-sticker.
[01:39:40.120 --> 01:39:41.080]   Yes.
[01:39:41.080 --> 01:39:45.240]   Working on it honestly.
[01:39:45.240 --> 01:39:52.600]   What these are, those, these listicles, these can- because they're really just link bait.
[01:39:52.600 --> 01:39:57.000]   They're just search engine optimized articles that you would type in.
[01:39:57.000 --> 01:39:58.360]   And it's spicy art-curricular.
[01:39:58.360 --> 01:40:01.480]   Chronological list of Star Wars shows.
[01:40:01.480 --> 01:40:03.240]   That's the spicy art-curric you talk about.
[01:40:03.240 --> 01:40:03.800]   Listicles.
[01:40:03.800 --> 01:40:04.680]   Yeah, and it's good at that.
[01:40:04.680 --> 01:40:05.320]   Yeah.
[01:40:05.320 --> 01:40:06.520]   Except it's not.
[01:40:06.520 --> 01:40:08.120]   But if it were, it would be good at that.
[01:40:09.160 --> 01:40:12.440]   If it only were good at that, that would be great.
[01:40:12.440 --> 01:40:16.840]   It's- I mean, and this is an interesting- I think Ed Zitron actually did something with this,
[01:40:16.840 --> 01:40:22.120]   but it is replacing the stuff I used to have- you know, we would have interns do, right?
[01:40:22.120 --> 01:40:28.360]   Go back and check out all of our Wi-Fi 5 coverage and write an explainer for someone.
[01:40:28.360 --> 01:40:33.560]   Or like, basically it's taking the low value stuff that we use to teach people how to do
[01:40:33.560 --> 01:40:34.680]   the jobs that we have today.
[01:40:34.680 --> 01:40:36.200]   Isn't that what CNET said?
[01:40:36.200 --> 01:40:36.760]   Wonder.
[01:40:36.760 --> 01:40:38.600]   Or Red Ventures, whomever that was.
[01:40:38.600 --> 01:40:39.880]   Isn't that what they said as well?
[01:40:39.880 --> 01:40:44.120]   Is it trying to take those entry-level pieces of content?
[01:40:44.120 --> 01:40:47.880]   But maybe they shouldn't be doing that entry-level crap content in the first place.
[01:40:47.880 --> 01:40:49.720]   It's making content for content's sake.
[01:40:49.720 --> 01:40:51.880]   Well, they're all over the whole stuff.
[01:40:51.880 --> 01:40:52.360]   Oh, no.
[01:40:52.360 --> 01:40:52.360]   Somewhere.
[01:40:52.360 --> 01:40:55.720]   I mean, place for the canonical one is what time is the Super Bowl?
[01:40:55.720 --> 01:40:57.160]   Well, no.
[01:40:57.160 --> 01:40:57.400]   Okay.
[01:40:57.400 --> 01:41:00.120]   There's what time is the Super Bowl, but there's also things like,
[01:41:00.120 --> 01:41:02.280]   how do I get a driver's license?
[01:41:02.280 --> 01:41:04.680]   Or think about the sidebars in a comp- like,
[01:41:04.680 --> 01:41:06.520]   in the cyber security label story.
[01:41:06.520 --> 01:41:13.400]   Like any sidebar that I might have, like, the last five big IoT, consumer device IoT hacks,
[01:41:13.400 --> 01:41:15.560]   that's something I would have an internal hold up.
[01:41:15.560 --> 01:41:18.440]   Would you use a thing I'd do this instead of an intern?
[01:41:18.440 --> 01:41:20.840]   No, that's, I mean, would I, oh my.
[01:41:20.840 --> 01:41:25.080]   Would I, no, because I wouldn't trust it.
[01:41:25.080 --> 01:41:30.840]   I should say the OMI has nothing to do with what we're talking about.
[01:41:30.840 --> 01:41:33.240]   It has everything to do with this.
[01:41:33.240 --> 01:41:35.240]   The O just shops, "Generdive AI" tools.
[01:41:35.240 --> 01:41:39.640]   I thought that I could not do, I cannot look at the discord while that being on the show.
[01:41:39.640 --> 01:41:44.600]   No, this is a picture somebody has done that is really not good of me and a beard.
[01:41:44.600 --> 01:41:48.920]   And I bet he used a selection tool and Photoshop, maybe.
[01:41:48.920 --> 01:41:51.000]   I think it's probably exactly what I would look like with a beard,
[01:41:51.000 --> 01:41:53.880]   and that's exactly why I'm not going to grow a beard.
[01:41:53.880 --> 01:41:55.000]   No, that's better.
[01:41:55.000 --> 01:41:56.040]   Oh, that's a better one.
[01:41:56.040 --> 01:41:56.840]   That's a Jeff beard.
[01:41:56.840 --> 01:41:59.160]   Yeah, give me a nice white beard.
[01:41:59.160 --> 01:42:00.520]   Yeah.
[01:42:00.520 --> 01:42:02.760]   Yeah, that looked like Cat and Lee on Below Deck.
[01:42:03.400 --> 01:42:03.800]   Yeah.
[01:42:03.800 --> 01:42:08.360]   I'm mad at it and I'm pissed off chicken.
[01:42:08.360 --> 01:42:11.080]   All right, whatever.
[01:42:11.080 --> 01:42:12.360]   Uh, where were we?
[01:42:12.360 --> 01:42:15.160]   We were, oh, here, I'll give you another one.
[01:42:15.160 --> 01:42:15.960]   Get ready.
[01:42:15.960 --> 01:42:16.920]   Open AI.
[01:42:16.920 --> 01:42:19.400]   And I'm sure Jeff, you might have something to say about this,
[01:42:19.400 --> 01:42:20.520]   has done a deal with the American--
[01:42:20.520 --> 01:42:21.800]   I'm not supposed to say about everything we owe.
[01:42:21.800 --> 01:42:26.360]   But it has done a deal with the American Journalism Project.
[01:42:26.360 --> 01:42:32.440]   For $5 million to help fund efforts by local outlets to
[01:42:32.440 --> 01:42:35.800]   experiment with writing AI articles.
[01:42:35.800 --> 01:42:40.520]   So they also did a deal with Associated Press, as we mentioned earlier,
[01:42:40.520 --> 01:42:45.000]   and this is out of the playbook of Facebook and Google trying to make friends with the
[01:42:45.000 --> 01:42:48.120]   industry before the media industry turns on them and gets regulation.
[01:42:48.120 --> 01:42:51.880]   And then that's all, I'll predict is what's going to happen.
[01:42:51.880 --> 01:42:54.680]   So open AI is committing for a while.
[01:42:54.680 --> 01:43:00.760]   $5 million in funding for local news initiatives through AGP.
[01:43:01.320 --> 01:43:03.400]   Now, it's not all cash.
[01:43:03.400 --> 01:43:07.320]   Some of it is chat GPT API tokens.
[01:43:07.320 --> 01:43:12.120]   American Journalism Project is $50 million.
[01:43:12.120 --> 01:43:18.520]   I forget fund started by John Thornton, who was a profounder of the Texas Tribune.
[01:43:18.520 --> 01:43:20.680]   Try to get--
[01:43:20.680 --> 01:43:24.360]   They're going crazy because of the Discord.
[01:43:24.360 --> 01:43:27.080]   We can't allow this whole thing to show.
[01:43:27.080 --> 01:43:27.880]   No, I don't like John Thornton.
[01:43:27.880 --> 01:43:29.560]   She does not like John Thornton.
[01:43:29.560 --> 01:43:31.160]   I know John Thornton.
[01:43:31.160 --> 01:43:31.880]   Wow.
[01:43:31.880 --> 01:43:33.320]   She was--
[01:43:33.320 --> 01:43:34.280]   I never touched it.
[01:43:34.280 --> 01:43:35.480]   She has face new reactions.
[01:43:35.480 --> 01:43:39.000]   Are you reacting to your bitch?
[01:43:39.000 --> 01:43:41.240]   I'm your replaceable sticker?
[01:43:41.240 --> 01:43:43.400]   Are you reacting to a name I just said?
[01:43:43.400 --> 01:43:46.920]   Are you reacting to the hunger for the waffle?
[01:43:46.920 --> 01:43:48.760]   I can't read your face.
[01:43:48.760 --> 01:43:49.800]   When I can't read you.
[01:43:49.800 --> 01:43:54.200]   I thought that was somebody behind me that was so strong, powerful.
[01:43:54.200 --> 01:43:55.800]   What does that sound come from?
[01:43:57.800 --> 01:43:59.240]   Okay, so we don't like this guy.
[01:43:59.240 --> 01:44:03.160]   I personally-- but I get it.
[01:44:03.160 --> 01:44:04.440]   I can understand it.
[01:44:04.440 --> 01:44:05.080]   One of the things--
[01:44:05.080 --> 01:44:06.280]   He started Texas Tribune.
[01:44:06.280 --> 01:44:09.960]   And he also started American Journalism Project.
[01:44:09.960 --> 01:44:12.600]   And it's an effort to create basically Texas Tribune's across the country,
[01:44:12.600 --> 01:44:13.560]   which would be a good thing.
[01:44:13.560 --> 01:44:17.560]   As a journalist who had to cover John Thornton,
[01:44:17.560 --> 01:44:22.600]   he did not treat journalists well at all.
[01:44:22.600 --> 01:44:23.480]   Oh, that's funny.
[01:44:23.480 --> 01:44:24.920]   There's a revelation, right?
[01:44:24.920 --> 01:44:27.640]   Here's the guy who wants AI to start writing these articles.
[01:44:27.640 --> 01:44:30.520]   Now, this has been a problem in local journalism.
[01:44:30.520 --> 01:44:31.000]   It's not really right.
[01:44:31.000 --> 01:44:33.640]   We're going to get to cover the city council meetings,
[01:44:33.640 --> 01:44:35.000]   the school board meetings.
[01:44:35.000 --> 01:44:39.400]   Forget AI, you go to a startup called City Bureau
[01:44:39.400 --> 01:44:41.960]   that is doing brilliant work training citizens
[01:44:41.960 --> 01:44:44.440]   to cover these kinds of things.
[01:44:44.440 --> 01:44:47.160]   Know the limitations of that and pay them for it.
[01:44:47.160 --> 01:44:49.480]   Daryl Holiday, the co-founder, just announced
[01:44:49.480 --> 01:44:51.320]   and he's going to move on to something else, which is interesting.
[01:44:51.320 --> 01:44:52.920]   City Bureau is a great example of that.
[01:44:52.920 --> 01:44:57.080]   At A.A.I., it is empowered citizens.
[01:44:57.080 --> 01:44:58.280]   That's how you're going to cover that stuff.
[01:44:58.280 --> 01:45:03.720]   Well, but what if, I mean, remember we talked last week about how a conversation
[01:45:03.720 --> 01:45:08.440]   was recorded, fed to OA Wisprei to transcribe,
[01:45:08.440 --> 01:45:10.200]   as you might do with a city council meeting,
[01:45:10.200 --> 01:45:14.040]   and then fed the chat GPT to summarize, and it did a really good job.
[01:45:14.040 --> 01:45:16.680]   Now, I guess you'd still need it or want a human to check it.
[01:45:16.680 --> 01:45:19.160]   Well, no, and here's the thing.
[01:45:19.160 --> 01:45:23.480]   If it's about the sewer plait on Elm Street, okay,
[01:45:23.480 --> 01:45:25.480]   but like the school board meeting that I went to,
[01:45:25.480 --> 01:45:28.840]   the last one I went to, was about the right wing trying to eliminate
[01:45:28.840 --> 01:45:32.280]   a sociology textbook because it was a mean to white people.
[01:45:32.280 --> 01:45:35.640]   No A.I. is going to get the nuance of that, babe.
[01:45:35.640 --> 01:45:36.840]   You might be surprised.
[01:45:36.840 --> 01:45:40.920]   There's a lot of content about that running around right now.
[01:45:40.920 --> 01:45:41.640]   That's the thing.
[01:45:41.640 --> 01:45:45.160]   If you're going to use these A.I.s, go ahead and use them,
[01:45:45.160 --> 01:45:49.960]   but still have some sort of human involved to clean things up
[01:45:49.960 --> 01:45:51.720]   and help with corrections and whatnot.
[01:45:51.720 --> 01:45:53.000]   An editor.
[01:45:53.000 --> 01:45:55.320]   But it's also about going up and asking the right questions,
[01:45:55.320 --> 01:45:56.440]   and it's about more.
[01:45:56.440 --> 01:45:57.800]   I mean, I have to sound like an old fart,
[01:45:57.800 --> 01:45:59.240]   defending the human journalist.
[01:45:59.240 --> 01:46:03.160]   What I'm really saying is that we in journalism
[01:46:03.160 --> 01:46:05.800]   thought our value was in this thing called content,
[01:46:05.800 --> 01:46:08.280]   and so a machine comes along that can make a little bit of a bounce
[01:46:08.280 --> 01:46:08.840]   of content.
[01:46:08.840 --> 01:46:10.680]   That's not where our value has ever been.
[01:46:10.680 --> 01:46:13.160]   Our value has been in asking the right questions,
[01:46:13.160 --> 01:46:15.560]   in challenging power, in representing communities,
[01:46:15.560 --> 01:46:18.440]   and understanding their needs, giving them service.
[01:46:18.440 --> 01:46:19.800]   That's where the value of journalism is,
[01:46:19.800 --> 01:46:21.480]   but we see it at a place like Go Media.
[01:46:21.480 --> 01:46:25.240]   Now you see it as let's make more content to get more pages with more ads.
[01:46:25.240 --> 01:46:29.240]   Yeah, but that's the value of the people who own papers.
[01:46:29.240 --> 01:46:30.760]   Yeah, there's still a balance though,
[01:46:30.760 --> 01:46:33.480]   because like we missed Higgin-Motham was pointing out earlier,
[01:46:33.480 --> 01:46:37.800]   I agree there's still a place for some of those low-hanging fruit pieces of,
[01:46:37.800 --> 01:46:42.520]   you know, how do I connect to a Wi-Fi securely, you know, stuff like that.
[01:46:42.520 --> 01:46:44.440]   Some people still don't know.
[01:46:44.440 --> 01:46:48.200]   We've already talked about how much people don't know previously in the show.
[01:46:48.200 --> 01:46:52.280]   One of my students in our executive program was talking about,
[01:46:52.280 --> 01:46:56.040]   Swarw from Sweden was talking about evergreen content.
[01:46:56.040 --> 01:46:59.000]   Once you've written that once, you don't need to write it 100 times.
[01:46:59.000 --> 01:47:00.040]   You just link to it.
[01:47:00.040 --> 01:47:02.040]   People don't want to link to each other because they want their own page,
[01:47:02.040 --> 01:47:04.360]   with their own content, with their own ads, with their own traffic,
[01:47:04.360 --> 01:47:05.240]   with their own SEO.
[01:47:05.240 --> 01:47:05.880]   Yeah.
[01:47:05.880 --> 01:47:08.920]   And that's the ruin of journalism.
[01:47:08.920 --> 01:47:10.120]   Did you see that?
[01:47:10.120 --> 01:47:12.200]   But it's also how journalists learn.
[01:47:12.200 --> 01:47:17.320]   I mean, the fastest way to get, when you, like, I mean,
[01:47:17.320 --> 01:47:19.480]   I'm sure you tell you, well, you don't teach 101,
[01:47:19.480 --> 01:47:21.480]   but in journalism 101, the fastest way,
[01:47:21.480 --> 01:47:27.720]   okay, is to get on a beat is start doing profiles
[01:47:27.720 --> 01:47:28.200]   of the employee people.
[01:47:28.200 --> 01:47:29.960]   To go to the city council meeting, right?
[01:47:29.960 --> 01:47:32.440]   And start doing it from the ground up.
[01:47:32.440 --> 01:47:34.920]   It's evergreen content.
[01:47:34.920 --> 01:47:36.360]   It's profiles.
[01:47:36.360 --> 01:47:37.480]   It's how tos.
[01:47:37.480 --> 01:47:38.680]   It's those kind of things.
[01:47:38.680 --> 01:47:44.760]   And you do that so you can get, so you can learn and build the expertise.
[01:47:44.760 --> 01:47:46.920]   And I would argue that the one thing you left off,
[01:47:46.920 --> 01:47:50.360]   isn't it isn't knowing what your audience needs or wants,
[01:47:50.360 --> 01:47:52.840]   or isn't just that.
[01:47:52.840 --> 01:47:59.240]   It's the expertise that you build up over years of covering a place or a topic.
[01:47:59.240 --> 01:48:03.000]   Well, and the nose for news, the nose for Amata City Council meeting.
[01:48:03.000 --> 01:48:04.200]   And that's interesting.
[01:48:04.200 --> 01:48:06.760]   The commissioner doesn't want that sewer built.
[01:48:06.760 --> 01:48:09.480]   Doesn't he own a store right next door?
[01:48:09.480 --> 01:48:10.200]   Right.
[01:48:10.200 --> 01:48:13.960]   And then doing an investigative piece on it.
[01:48:13.960 --> 01:48:15.400]   That's right.
[01:48:15.400 --> 01:48:19.000]   You don't get that unless you actually go out and start doing reporting and go out in the field.
[01:48:19.000 --> 01:48:22.520]   And maybe that's doing reporting on something that's ostensibly pretty dull.
[01:48:22.520 --> 01:48:27.880]   I wish we had local reporters who were going to those things and looking for those stories.
[01:48:27.880 --> 01:48:34.280]   But if you're sensible and smart, you're probably not going into journalism anymore, right?
[01:48:34.280 --> 01:48:35.800]   Gone so journaling.
[01:48:35.800 --> 01:48:36.600]   Oh, yes you are.
[01:48:36.600 --> 01:48:39.720]   Hey, yeah, I'm sensible and smart.
[01:48:39.720 --> 01:48:41.880]   No, but yeah, but you went into it years ago.
[01:48:41.880 --> 01:48:47.160]   Oh, but Andrew does tell me that whenever someone like when teenagers ask me about journalism,
[01:48:47.160 --> 01:48:48.280]   I'm like, don't do that.
[01:48:48.280 --> 01:48:52.440]   Well, I say that about podcasting to and radio and everything I've ever done.
[01:48:52.440 --> 01:48:53.480]   Oh, well, the local.
[01:48:53.480 --> 01:48:55.240]   But fortunately, my kids didn't listen to me.
[01:48:55.240 --> 01:48:56.280]   So it's OK.
[01:48:56.280 --> 01:48:57.320]   The local.
[01:48:57.320 --> 01:48:59.560]   What do you call them?
[01:48:59.560 --> 01:49:03.880]   They do strikes reporters reporters and.
[01:49:03.880 --> 01:49:05.240]   And a little trellis.
[01:49:05.240 --> 01:49:07.160]   No, why cannot they come forward?
[01:49:07.160 --> 01:49:09.720]   Does it have to be worse organizations for over the papers?
[01:49:11.080 --> 01:49:11.880]   Union.
[01:49:11.880 --> 01:49:12.200]   That's it.
[01:49:12.200 --> 01:49:12.600]   Law.
[01:49:12.600 --> 01:49:15.560]   Union with press Democrat and.
[01:49:15.560 --> 01:49:16.680]   Are they unionized?
[01:49:16.680 --> 01:49:18.200]   There's there's something going on.
[01:49:18.200 --> 01:49:19.960]   Oh, they are planning unionized.
[01:49:19.960 --> 01:49:23.080]   And see, you know, in some ways, that's good.
[01:49:23.080 --> 01:49:24.040]   You're in contribution.
[01:49:24.040 --> 01:49:24.280]   Yeah.
[01:49:24.280 --> 01:49:29.640]   I mean, I see work to do this work that you're talking about, but there hasn't been any pay in it.
[01:49:29.640 --> 01:49:29.880]   Right.
[01:49:29.880 --> 01:49:31.480]   You know, no, it's five bucks a meeting.
[01:49:31.480 --> 01:49:38.360]   There's been a there was some stat that was listed like two or three days ago that talked about how
[01:49:38.360 --> 01:49:41.640]   those folks on those staffs are not even really making a living.
[01:49:41.640 --> 01:49:46.280]   Actually, I should point out in that alumna, the five bucks a meeting is what the city council
[01:49:46.280 --> 01:49:49.640]   members get paid literally five bucks a meeting.
[01:49:49.640 --> 01:49:55.240]   So they do that, I guess, because, you know, they want to be famous or they want to support the community.
[01:49:55.240 --> 01:49:58.120]   You can't expect people to try and make a living doing that.
[01:49:58.120 --> 01:50:00.680]   But you know, I feel sorry for anybody getting started these days.
[01:50:00.680 --> 01:50:05.560]   I don't with the price of housing and the low wages.
[01:50:05.560 --> 01:50:06.360]   I don't know how anybody.
[01:50:06.360 --> 01:50:09.960]   So guys in this economy, the guys that cover high school.
[01:50:09.960 --> 01:50:14.360]   Yeah, I love them and in their younger guys and I know they're busting their
[01:50:14.360 --> 01:50:16.760]   hump, but I feel bad for them from a financial state.
[01:50:16.760 --> 01:50:21.640]   Yeah, 20 year olds working in a grocery store making 20 bucks an hour, but he can't that wouldn't pay.
[01:50:21.640 --> 01:50:25.400]   He'd have to have three roommates and I mean, that's the norm.
[01:50:25.400 --> 01:50:28.120]   Yeah, I guess I did when I was a kid.
[01:50:28.120 --> 01:50:30.680]   Did you see Devinder Harderwar?
[01:50:30.680 --> 01:50:31.960]   Who's what did he do?
[01:50:31.960 --> 01:50:35.000]   He was putting into a South Park episode, not a real one.
[01:50:35.000 --> 01:50:36.280]   Half a fake one.
[01:50:36.280 --> 01:50:39.160]   Now I have to say South Park is probably something an AI could do.
[01:50:39.160 --> 01:50:40.840]   Good call.
[01:50:40.840 --> 01:50:48.840]   This is a I was thrust into an episode entirely produced by the showrunner AI model from the
[01:50:48.840 --> 01:50:53.160]   simulation, which is the next iteration of the VR studio fable.
[01:50:53.160 --> 01:50:55.400]   You want to see a little bit of Devindra?
[01:50:55.400 --> 01:51:02.360]   He said, all it took was some audio of my voice, literally recorded during a call with the simulation
[01:51:02.360 --> 01:51:07.480]   CEO, a picture and a two sentence prompt to produce an entire South Park episode.
[01:51:07.480 --> 01:51:11.560]   And while it wasn't the best I've ever seen, I was shocked by how watchable it was.
[01:51:11.560 --> 01:51:12.520]   He writes.
[01:51:12.520 --> 01:51:13.480]   That's all it takes.
[01:51:13.480 --> 01:51:15.960]   Yeah, you want to see a lot of I think I can play this.
[01:51:15.960 --> 01:51:16.360]   I don't know.
[01:51:16.360 --> 01:51:19.320]   Actually, that's a really interesting question.
[01:51:19.320 --> 01:51:21.960]   Does this violate Trey Parker's?
[01:51:21.960 --> 01:51:24.680]   So here we are.
[01:51:24.680 --> 01:51:25.720]   And this is fair use.
[01:51:25.720 --> 01:51:27.240]   We're good.
[01:51:27.240 --> 01:51:29.880]   Oh, this is them actually doing it with the showrunner system.
[01:51:29.880 --> 01:51:32.200]   So they're choosing the South Park characters.
[01:51:32.200 --> 01:51:33.320]   The hero is Devindra.
[01:51:33.320 --> 01:51:35.320]   They're going to have Randy and Sharon in it.
[01:51:35.320 --> 01:51:36.680]   It starts in the living room.
[01:51:36.680 --> 01:51:37.720]   And here's the prompt.
[01:51:37.720 --> 01:51:47.240]   Devindra, a tech journalist, is going from door to door to warn people of the coming AI apocalypse.
[01:51:47.240 --> 01:51:54.120]   Sharon is annoyed while Randy wants to learn more.
[01:51:54.120 --> 01:51:57.800]   And now we take you to South Park.
[01:51:59.080 --> 01:52:01.560]   A cop apocalypse now in Zen.
[01:52:01.560 --> 01:52:03.240]   They call him OK.
[01:52:03.240 --> 01:52:07.160]   Hello, anyone home?
[01:52:07.160 --> 01:52:08.120]   I've got some urgent news.
[01:52:08.120 --> 01:52:09.640]   Oh boy, it does sound good.
[01:52:09.640 --> 01:52:11.000]   This better be good.
[01:52:11.000 --> 01:52:11.640]   It's not just good.
[01:52:11.640 --> 01:52:12.600]   It's crucial.
[01:52:12.600 --> 01:52:15.160]   Did you know that artificial intelligence is taking over the world?
[01:52:15.160 --> 01:52:16.600]   AI, huh?
[01:52:16.600 --> 01:52:17.880]   Like in those sci-fi movies?
[01:52:17.880 --> 01:52:19.160]   Exactly.
[01:52:19.160 --> 01:52:25.000]   Part of this succeeds because South Park's animation and voices are intentionally kind of bad.
[01:52:25.000 --> 01:52:25.480]   Mm-hmm.
[01:52:25.480 --> 01:52:25.800]   Right.
[01:52:25.800 --> 01:52:27.880]   I mean, that's part of the aesthetic of it.
[01:52:28.680 --> 01:52:31.960]   It's actually pretty much like South Park, right?
[01:52:31.960 --> 01:52:33.000]   Roll everything.
[01:52:33.000 --> 01:52:35.000]   I wish you could see the video for this, and you'll listen.
[01:52:35.000 --> 01:52:35.480]   Here we go.
[01:52:35.480 --> 01:52:35.880]   Here we go.
[01:52:35.880 --> 01:52:39.160]   So Devindra, how exactly do these robots plan to enslave us?
[01:52:39.160 --> 01:52:41.800]   Well, they're infiltrating every aspect of our lives.
[01:52:41.800 --> 01:52:44.440]   They're in our phones, our cars, even our toasters.
[01:52:44.440 --> 01:52:46.360]   Our toasters, really?
[01:52:46.360 --> 01:52:47.640]   I always knew that little toaster.
[01:52:47.640 --> 01:52:48.120]   Oh, geez.
[01:52:48.120 --> 01:52:51.960]   It's just a step above Eliza.
[01:52:51.960 --> 01:52:52.360]   All right.
[01:52:52.360 --> 01:52:52.840]   All right.
[01:52:52.840 --> 01:52:53.480]   Never mind.
[01:52:53.480 --> 01:52:54.040]   Just out of it.
[01:52:54.040 --> 01:52:55.800]   This is bad as the sign film.
[01:52:55.800 --> 01:52:56.040]   Yeah.
[01:52:56.040 --> 01:52:56.760]   Well.
[01:52:57.240 --> 01:52:58.120]   But that's the thing.
[01:52:58.120 --> 01:52:59.240]   This goes back to your point, Leo.
[01:52:59.240 --> 01:52:59.960]   Still watching.
[01:52:59.960 --> 01:53:01.240]   This is still a parlor trick.
[01:53:01.240 --> 01:53:02.040]   Yeah.
[01:53:02.040 --> 01:53:03.880]   It's, but I have to say,
[01:53:03.880 --> 01:53:07.480]   with the actors and the writers that on strike,
[01:53:07.480 --> 01:53:09.320]   this may be the parlor trick we get.
[01:53:09.320 --> 01:53:09.880]   Mm-hmm.
[01:53:09.880 --> 01:53:11.480]   I mean, this may be the future.
[01:53:11.480 --> 01:53:12.280]   [Laughing]
[01:53:12.280 --> 01:53:13.720]   Nice.
[01:53:13.720 --> 01:53:16.440]   I think we've said before that there's still time.
[01:53:16.440 --> 01:53:17.720]   This stuff can get better.
[01:53:17.720 --> 01:53:19.960]   That's my question.
[01:53:19.960 --> 01:53:21.880]   My full question is, can it get better?
[01:53:21.880 --> 01:53:22.760]   You can't.
[01:53:22.760 --> 01:53:24.360]   That's why this, you think so?
[01:53:24.360 --> 01:53:25.320]   You all can go off again.
[01:53:25.320 --> 01:53:26.520]   It's a lot.
[01:53:26.520 --> 01:53:28.440]   You can't. You sound like, who was it who was like,
[01:53:28.440 --> 01:53:32.120]   no one will ever need more than 50 megabytes of storage or whatever?
[01:53:32.120 --> 01:53:33.000]   There you go.
[01:53:33.000 --> 01:53:33.320]   Yeah.
[01:53:33.320 --> 01:53:35.080]   That's who you sound like.
[01:53:35.080 --> 01:53:36.760]   Well, he wasn't far wrong.
[01:53:36.760 --> 01:53:37.320]   No, he was.
[01:53:37.320 --> 01:53:37.640]   He wasn't.
[01:53:37.640 --> 01:53:41.480]   I feel like we've been here before with AI.
[01:53:41.480 --> 01:53:47.000]   And it's, I think the most important thing we can do at this point is to see what it's,
[01:53:47.000 --> 01:53:49.960]   to use it for the things it's good at, summarizing documents.
[01:53:49.960 --> 01:53:54.280]   We're using surprisingly large amount of AI now in our show production.
[01:53:54.280 --> 01:53:54.520]   Yes.
[01:53:54.520 --> 01:53:56.360]   For instance, this surprises me.
[01:53:56.360 --> 01:54:01.960]   There's an AI we use that goes through each show and picks out clips for highlights.
[01:54:01.960 --> 01:54:06.920]   And it, I mean, you're not going to use them all, but it narrows it down, does the edit,
[01:54:06.920 --> 01:54:08.200]   and then shows it to the editor.
[01:54:08.200 --> 01:54:10.840]   And I'm not sure how it knows that this is a highlight.
[01:54:10.840 --> 01:54:13.080]   Maybe people are jumping up and down or something.
[01:54:13.080 --> 01:54:14.200]   More some flexing.
[01:54:14.200 --> 01:54:15.000]   It could be a lot of different.
[01:54:15.000 --> 01:54:15.960]   It does a good job.
[01:54:15.960 --> 01:54:15.960]   Yeah.
[01:54:15.960 --> 01:54:18.360]   Anthony's telling me it does a good job.
[01:54:18.360 --> 01:54:22.120]   And then we pick a handful of them and that's what we use for our promotion.
[01:54:22.120 --> 01:54:24.280]   That's a huge amount of time saving.
[01:54:24.280 --> 01:54:24.920]   Yep.
[01:54:24.920 --> 01:54:27.160]   And if an AI can do that, that's great.
[01:54:27.160 --> 01:54:30.840]   So I do think there are things it can do and it's important to know what it can do.
[01:54:30.840 --> 01:54:34.840]   But it's also just as important to know some things it's not going to do well
[01:54:34.840 --> 01:54:37.080]   and may never do well, like drive a car.
[01:54:37.080 --> 01:54:42.360]   And you really have to know where to, I mean, maybe that's a bad example,
[01:54:42.360 --> 01:54:45.560]   but you really have to know where to draw the line.
[01:54:45.560 --> 01:54:50.680]   Do you still feel like that AI will fill the world with paper clips?
[01:54:50.680 --> 01:54:51.880]   If we like.
[01:54:51.880 --> 01:54:52.200]   No.
[01:54:52.200 --> 01:54:57.640]   But it will fill the world with disinformation because of bad humans who are going to set AI
[01:54:57.640 --> 01:55:03.160]   to that task and get ready because between now and November 2024, we're going to see an
[01:55:03.160 --> 01:55:04.920]   disinformation apocalypse.
[01:55:04.920 --> 01:55:05.480]   It's coming.
[01:55:05.480 --> 01:55:07.400]   And I think everybody knows that.
[01:55:07.400 --> 01:55:10.760]   Even Bill Gates who just wrote a, he does the Gates notes.
[01:55:10.760 --> 01:55:15.160]   He's not actually disagreeing very much with what I've said.
[01:55:15.160 --> 01:55:22.360]   He's, here's his latest Gates note, the risks of AI are real but manageable.
[01:55:22.360 --> 01:55:26.760]   He compares AI to other breakthroughs.
[01:55:26.760 --> 01:55:30.760]   It wasn't long ago he was starting to decree apocalypse though.
[01:55:30.760 --> 01:55:31.720]   Really?
[01:55:31.720 --> 01:55:32.600]   Well, he's back down.
[01:55:32.600 --> 01:55:34.840]   I forget I found another one there, but yeah, fine.
[01:55:34.840 --> 01:55:38.840]   He says it's not the first time a major innovation has introduced new threats to be controlled.
[01:55:38.840 --> 01:55:43.000]   We've done it before, whether it was the introduction of cars, the rise of personal
[01:55:43.000 --> 01:55:48.360]   computers and the internet, people have managed through other transformative moments and have,
[01:55:48.360 --> 01:55:52.280]   have despite a lot of turbulence come out better off in the end.
[01:55:52.280 --> 01:55:58.440]   He says deep fakes and he's, he says, I've been thinking about the longer term risks.
[01:55:58.440 --> 01:56:02.600]   And he says they should not come at the expense of the more immediate ones.
[01:56:02.600 --> 01:56:04.600]   And these are the more immediate risks.
[01:56:04.600 --> 01:56:10.360]   Deep fakes and misinformation generated by AI could undermine elections in democracy.
[01:56:10.360 --> 01:56:11.800]   Yes, right.
[01:56:11.800 --> 01:56:12.280]   Yeah.
[01:56:12.280 --> 01:56:12.920]   Yeah.
[01:56:12.920 --> 01:56:18.360]   He says to some degree he thinks AI could solve that.
[01:56:18.360 --> 01:56:21.160]   Two things make me guardedly optimistic.
[01:56:21.160 --> 01:56:24.600]   One is that people are capable of learning not to take everything at face value.
[01:56:24.600 --> 01:56:25.800]   We've had to do that, right?
[01:56:25.800 --> 01:56:27.640]   And that's a good skill.
[01:56:27.640 --> 01:56:29.000]   We've got to keep raising awareness.
[01:56:29.000 --> 01:56:30.600]   Just say, hey, that's clearly BS.
[01:56:30.600 --> 01:56:36.040]   He says people, you know, for years people fell for scams and somebody posing as a Nigerian
[01:56:36.040 --> 01:56:41.720]   prince, but we've learned we need to build the same muscles for deep fakes.
[01:56:42.440 --> 01:56:45.320]   The other thing that makes me hopeful is that, and I think he may be wrong on this,
[01:56:45.320 --> 01:56:48.040]   AI can help identify deep fakes as well as create them.
[01:56:48.040 --> 01:56:51.240]   We're starting to learn that AI is not great at finding their AI.
[01:56:51.240 --> 01:56:51.880]   No, of course not.
[01:56:51.880 --> 01:56:52.120]   Yeah.
[01:56:52.120 --> 01:56:57.400]   The Declaration of Independence is often found as fake because one of the stories in the
[01:56:57.400 --> 01:56:58.120]   in the rundown, right?
[01:56:58.120 --> 01:57:04.680]   AI, he also says, and these are the intermediate short term issues, makes it easier to launch a
[01:57:04.680 --> 01:57:06.520]   tax on people and governments.
[01:57:06.520 --> 01:57:06.840]   Yeah.
[01:57:07.960 --> 01:57:11.640]   He's, you know, it's the same kind of, well, maybe AI can solve that one.
[01:57:11.640 --> 01:57:14.200]   AI will take away people's jobs.
[01:57:14.200 --> 01:57:16.280]   And you've talked about this, Jeff, and I think you're right.
[01:57:16.280 --> 01:57:20.840]   All new technologies do, but they don't take them away without adding new jobs.
[01:57:20.840 --> 01:57:24.520]   AI transition for people.
[01:57:24.520 --> 01:57:24.840]   Yes.
[01:57:24.840 --> 01:57:25.560]   Yeah.
[01:57:25.560 --> 01:57:29.640]   Yeah, which is not to downplay the fact that coal miners are, you know, going to have a hard
[01:57:29.640 --> 01:57:30.440]   time going forward.
[01:57:30.440 --> 01:57:33.080]   That's absolutely not many of them actually left.
[01:57:33.080 --> 01:57:34.280]   Well, well, that's it.
[01:57:34.280 --> 01:57:37.720]   As a right, AI, this is a good one.
[01:57:37.720 --> 01:57:39.880]   This is the stochastic parrots issue.
[01:57:39.880 --> 01:57:42.360]   AI inherits our biases and makes things up.
[01:57:42.360 --> 01:57:45.000]   Short term issue.
[01:57:45.000 --> 01:57:49.960]   He has, there's nothing really genius in any of this.
[01:57:49.960 --> 01:57:52.680]   Here's one you might care about.
[01:57:52.680 --> 01:57:55.960]   Students won't learn to write because AI will do the work for them.
[01:57:55.960 --> 01:58:02.040]   So the aforementioned, just
[01:58:03.960 --> 01:58:06.680]   the Modern Language Association just put out guidelines for this.
[01:58:06.680 --> 01:58:10.440]   And they emphasized Matthew Kirschmann was on the committee that did that.
[01:58:10.440 --> 01:58:13.800]   I was almost going to put it in the rundown, but I thought I would get hooted down for such a thing.
[01:58:13.800 --> 01:58:20.520]   But it says that writing is a process, not just a product,
[01:58:20.520 --> 01:58:24.680]   and that a lot of teaching about it is to get to do that.
[01:58:24.680 --> 01:58:25.960]   So there are useful things to do.
[01:58:25.960 --> 01:58:28.920]   Faculty needs support and learning this.
[01:58:28.920 --> 01:58:32.840]   They're not against using it, but we don't use it in the obvious ways.
[01:58:32.840 --> 01:58:37.960]   Well, and as Dr. Du, the EW is saying in our Discord,
[01:58:37.960 --> 01:58:41.720]   people said students wouldn't learn math because it calculators.
[01:58:41.720 --> 01:58:48.360]   And I guess to some degree they don't learn the basics arithmetic, but they certainly still.
[01:58:48.360 --> 01:58:52.920]   Well, my argument has been that what AI can do, one of the things I'm enthusiastic about,
[01:58:52.920 --> 01:58:56.520]   is to extend literacy so that if you want help telling your story,
[01:58:56.520 --> 01:58:59.160]   and that it's your story, so you're going to care to get it right,
[01:58:59.160 --> 01:59:04.360]   you're really going to edit it, but you need to get over a hump of communication.
[01:59:04.360 --> 01:59:05.960]   People are intimidated by writing.
[01:59:05.960 --> 01:59:09.960]   So what does Bill Gates say we have to do?
[01:59:09.960 --> 01:59:14.040]   He says, "I believe there are more reasons than not to be optimistic,
[01:59:14.040 --> 01:59:18.840]   that we can manage the risks of AI while maximizing their benefits,
[01:59:18.840 --> 01:59:20.520]   but we need to move fast.
[01:59:20.520 --> 01:59:25.240]   Governments need to build up expertise in AI so they can make informed laws and regulations.
[01:59:25.240 --> 01:59:27.000]   Political leaders need to be equipped to that."
[01:59:27.000 --> 01:59:28.520]   Let me stay there for a second if I could, Leo.
[01:59:28.520 --> 01:59:29.080]   Yeah.
[01:59:29.080 --> 01:59:29.640]   Really interesting.
[01:59:29.640 --> 01:59:35.080]   One thing I put in the rundown is, is it my senator, the senator from New York,
[01:59:35.080 --> 01:59:37.320]   the Senator Trellis.
[01:59:37.320 --> 01:59:41.160]   Chuck Schumer?
[01:59:41.160 --> 01:59:42.280]   Chuck Schumer, thank you very much.
[01:59:42.280 --> 01:59:49.960]   Before the show began, I was mentioning that I was before the show.
[01:59:49.960 --> 01:59:51.320]   It was before the show this morning.
[01:59:51.320 --> 01:59:55.000]   No, it's a good running joke, but let me explain how it started.
[01:59:55.000 --> 01:59:58.280]   I was mentioning that I played Jeopardy with the Amazon Echo every morning,
[01:59:58.280 --> 02:00:02.280]   and the final Jeopardy was name a criss-cross
[02:00:02.280 --> 02:00:05.880]   it was in the category gardens or something.
[02:00:05.880 --> 02:00:08.760]   Name a criss—oh no, the category was words with two wells.
[02:00:08.760 --> 02:00:10.120]   No one likes well, right.
[02:00:10.120 --> 02:00:15.560]   Name a criss-cross lattice work that is used to prop up plants in the garden.
[02:00:15.560 --> 02:00:21.080]   And I know I knew what that was, but I could not come up with the word trellis.
[02:00:21.080 --> 02:00:23.480]   You can't remember—
[02:00:23.480 --> 02:00:25.720]   No, trellis is the fill-in word when you can't come up with it.
[02:00:25.720 --> 02:00:28.280]   Whenever we can't think of something, you'll now know that.
[02:00:28.280 --> 02:00:30.760]   So Senator Trellis is going to
[02:00:30.760 --> 02:00:35.880]   basically hold courses on AI, which is really smart.
[02:00:35.880 --> 02:00:39.640]   Rather than Australia with hearings, we got to control this stuff.
[02:00:39.640 --> 02:00:42.520]   He's acknowledging that he had to learn about it, which I think is wonderful.
[02:00:42.520 --> 02:00:42.920]   Good.
[02:00:42.920 --> 02:00:45.400]   I mean, he's holding courses.
[02:00:45.400 --> 02:00:46.360]   Who's at—
[02:00:46.360 --> 02:00:46.920]   For Senator?
[02:00:46.920 --> 02:00:47.480]   For Senator—
[02:00:47.480 --> 02:00:47.960]   For other senators.
[02:00:47.960 --> 02:00:50.040]   Who's running courses, stuff like that.
[02:00:50.040 --> 02:00:52.280]   That's a good question, and—
[02:00:52.840 --> 02:00:55.720]   My name is Mark Zuckerberg, but you can call me—
[02:00:55.720 --> 02:00:56.680]   Long criticism.
[02:00:56.680 --> 02:00:58.520]   I'd like to tell you about AI.
[02:00:58.520 --> 02:01:00.040]   And smoking—
[02:01:00.040 --> 02:01:01.560]   Hi, I'm Sam Altman.
[02:01:01.560 --> 02:01:03.720]   I'm going to destroy the world if you don't stop me.
[02:01:03.720 --> 02:01:04.280]   Hi.
[02:01:04.280 --> 02:01:04.760]   Hi.
[02:01:04.760 --> 02:01:05.240]   Yeah.
[02:01:05.240 --> 02:01:07.400]   Well, we agree.
[02:01:07.400 --> 02:01:11.320]   Governments lean to build expertise, and I think Chuck Trellis is right.
[02:01:11.320 --> 02:01:16.040]   Political leaders will need to be equipped to have informed thought for dialogue with their constituents.
[02:01:16.040 --> 02:01:18.280]   Oh, isn't that an thought?
[02:01:18.280 --> 02:01:19.080]   Yeah.
[02:01:20.200 --> 02:01:24.120]   They'll also have to decide how much to collaborate with other countries on these issues versus going
[02:01:24.120 --> 02:01:24.600]   out—
[02:01:24.600 --> 02:01:25.000]   Oh, it alone.
[02:01:25.000 --> 02:01:27.000]   It builds right on that.
[02:01:27.000 --> 02:01:31.160]   In the private sector, AI companies need to pursue their work safely and responsibility,
[02:01:31.160 --> 02:01:36.360]   protect privacy, make sure the models reflect basic human values, minimize bias,
[02:01:36.360 --> 02:01:39.800]   spread the benefits to as many people as possible,
[02:01:39.800 --> 02:01:43.560]   prevent technology from being used by criminals or terrorists.
[02:01:43.560 --> 02:01:46.040]   That's fair.
[02:01:46.040 --> 02:01:51.960]   Finally, I encourage everyone, you all, to follow developments in AI as much as possible.
[02:01:51.960 --> 02:01:55.960]   It's the most transformative innovation any of us will see in our lifetimes.
[02:01:55.960 --> 02:01:58.280]   So that answers the question of whether it's a parlor trick.
[02:01:58.280 --> 02:01:58.920]   He says no.
[02:01:58.920 --> 02:02:02.120]   And a healthy public debate will depend on everyone—
[02:02:02.120 --> 02:02:02.840]   Of course he says no.
[02:02:02.840 --> 02:02:07.480]   He just has a company that's investing heavily in this AI stuff.
[02:02:07.480 --> 02:02:08.120]   Oh.
[02:02:08.120 --> 02:02:09.400]   Yeah, but okay.
[02:02:09.400 --> 02:02:12.120]   And a healthy public debate will depend on everyone being knowledgeable,
[02:02:12.120 --> 02:02:15.560]   but if technology has benefits and its risks, the benefits will be massive.
[02:02:16.200 --> 02:02:19.240]   And the best reason to believe we can manage the risks is that we have done it before.
[02:02:19.240 --> 02:02:22.760]   He liked Clara and the Sun, if that's any help.
[02:02:22.760 --> 02:02:25.400]   Good book.
[02:02:25.400 --> 02:02:25.880]   Good book.
[02:02:25.880 --> 02:02:26.680]   Good book.
[02:02:26.680 --> 02:02:27.480]   That was a good book.
[02:02:27.480 --> 02:02:35.640]   By the way, we are Jeff, you and Jason Howell are working on a new AI show, where this will be
[02:02:35.640 --> 02:02:37.080]   one of the things that we'll talk about.
[02:02:37.080 --> 02:02:38.280]   I love that photo right away.
[02:02:38.280 --> 02:02:39.240]   I like a nice picture.
[02:02:39.240 --> 02:02:39.960]   Did you take that?
[02:02:39.960 --> 02:02:40.520]   It is, doesn't it?
[02:02:40.520 --> 02:02:40.840]   Thank you.
[02:02:40.840 --> 02:02:41.000]   Probably Jason.
[02:02:41.000 --> 02:02:41.960]   I did.
[02:02:41.960 --> 02:02:42.760]   I think he did.
[02:02:42.760 --> 02:02:43.320]   Oh, oh.
[02:02:43.320 --> 02:02:45.160]   Jason's AI playground.
[02:02:45.160 --> 02:02:48.840]   I think we weren't able to get you scheduled in for that because of your travel.
[02:02:48.840 --> 02:02:51.720]   No, because I've just, the way it worked out when it was scheduled,
[02:02:51.720 --> 02:02:53.320]   I already had stuff jammed in.
[02:02:53.320 --> 02:02:53.800]   Oh, okay.
[02:02:53.800 --> 02:02:56.440]   But we do want to get you in that conversation because I want you to,
[02:02:56.440 --> 02:02:57.480]   you want to host that show.
[02:02:57.480 --> 02:02:59.960]   Well, after the show next, after the show next week, when I'm there,
[02:02:59.960 --> 02:03:01.240]   I think Jason and I are going to do something.
[02:03:01.240 --> 02:03:01.880]   Good.
[02:03:01.880 --> 02:03:02.120]   Okay.
[02:03:02.120 --> 02:03:07.400]   So if you are in the club and you want to input tomorrow at 1 p.m.,
[02:03:07.400 --> 02:03:11.400]   Jason's, they call it, he's calling it the AI playground because it's not the show yet.
[02:03:11.400 --> 02:03:14.760]   It's just a conversation about what that show can and will be,
[02:03:14.760 --> 02:03:16.680]   which I think is really important.
[02:03:16.680 --> 02:03:23.400]   And you know what, I think what Bill Gates says is probably as good a charter as anything
[02:03:23.400 --> 02:03:25.240]   for people.
[02:03:25.240 --> 02:03:26.200]   No beef with that.
[02:03:26.200 --> 02:03:28.680]   Yeah, to prepare for the future.
[02:03:28.680 --> 02:03:33.160]   Are there any other AI stories in our this week in AI?
[02:03:33.160 --> 02:03:34.200]   Oh, really?
[02:03:34.200 --> 02:03:40.680]   Hollywood, you know, the real, the fight in the writers' guild and the actors is about a lot of
[02:03:40.680 --> 02:03:42.120]   things, the mini rooms and stuff.
[02:03:42.120 --> 02:03:46.520]   But there is this AI component writers are worried about it, actors are worried about it.
[02:03:46.520 --> 02:03:48.760]   Studios who have proven themselves to be
[02:03:48.760 --> 02:03:55.880]   repacious capitalists are, you know, clearly going to look to whatever they can do to make
[02:03:55.880 --> 02:03:56.760]   this stuff cheaper.
[02:03:56.760 --> 02:03:59.080]   And that means.
[02:03:59.080 --> 02:04:04.200]   I thought about that because I thought about Shonda Rhine sent just how big she she's been
[02:04:04.200 --> 02:04:06.760]   in the black community and the level of.
[02:04:06.760 --> 02:04:08.840]   She's a showrunner for Abbott Element.
[02:04:08.840 --> 02:04:09.400]   What are the show?
[02:04:09.400 --> 02:04:12.280]   She she she she scandal scandal.
[02:04:12.280 --> 02:04:12.680]   That's right.
[02:04:12.680 --> 02:04:14.360]   And Grey's Anatomy.
[02:04:14.360 --> 02:04:14.920]   I mean, you know,
[02:04:14.920 --> 02:04:16.360]   murder something murder.
[02:04:16.360 --> 02:04:18.520]   Mainstream mainstream.
[02:04:18.520 --> 02:04:19.240]   Then she's great.
[02:04:19.240 --> 02:04:20.040]   To Netflix.
[02:04:20.040 --> 02:04:27.320]   And I believe the deal was damn near a hundred million dollars for Budgerton and Queen Charlotte.
[02:04:27.320 --> 02:04:30.280]   And I mean, she's really good at what she does.
[02:04:30.280 --> 02:04:31.160]   She's really good at that.
[02:04:31.160 --> 02:04:35.080]   I'm fortunate that now she's like, hey, wait a minute.
[02:04:35.080 --> 02:04:36.440]   Yeah, I can't come in here and do this.
[02:04:36.440 --> 02:04:37.880]   She's worth a quarter of a billion.
[02:04:37.880 --> 02:04:38.680]   So great.
[02:04:38.680 --> 02:04:43.240]   She steps up and says, I want to protect the low paid writers and actors.
[02:04:43.240 --> 02:04:47.400]   Even though I've made, you know, she was starting a writer, I'm sure.
[02:04:47.400 --> 02:04:53.080]   So yeah, she was an unemployed scriptwriter in Hollywood who had to work as an office administrator,
[02:04:53.080 --> 02:04:55.640]   a counselor at a job center.
[02:04:55.640 --> 02:05:00.760]   But she she worked her way up in Hollywood.
[02:05:00.760 --> 02:05:01.800]   Yeah, she did work.
[02:05:01.800 --> 02:05:02.120]   Yeah.
[02:05:02.120 --> 02:05:04.120]   Princess Diaries.
[02:05:04.120 --> 02:05:05.560]   Did you work done?
[02:05:05.560 --> 02:05:08.280]   She has made people who bet on her bank.
[02:05:08.280 --> 02:05:08.840]   Yeah.
[02:05:08.840 --> 02:05:11.640]   How to get away with murder.
[02:05:11.640 --> 02:05:12.280]   That's the one.
[02:05:12.280 --> 02:05:14.520]   That's the one you were talking about.
[02:05:14.520 --> 02:05:18.120]   I just saw something she did that I thought was really good,
[02:05:18.120 --> 02:05:19.800]   but I can't remember what the name of it was.
[02:05:19.800 --> 02:05:25.240]   Yeah, I can only think of the Bridget and Charles and Adam and Adam.
[02:05:25.240 --> 02:05:26.520]   Yeah, all the netflix stuff she's been doing is great.
[02:05:26.520 --> 02:05:26.760]   Yeah.
[02:05:26.760 --> 02:05:28.360]   And scandal.
[02:05:28.360 --> 02:05:29.400]   Oh, inventing Anna.
[02:05:29.400 --> 02:05:29.880]   I loved it.
[02:05:29.880 --> 02:05:30.360]   Oh, yeah.
[02:05:30.360 --> 02:05:31.160]   That was her too.
[02:05:31.160 --> 02:05:31.400]   Yeah.
[02:05:31.400 --> 02:05:31.960]   That's right.
[02:05:31.960 --> 02:05:32.760]   So good.
[02:05:32.760 --> 02:05:33.640]   Really good.
[02:05:33.640 --> 02:05:34.760]   That was the antidote.
[02:05:34.760 --> 02:05:35.960]   She did Grey's Anatomy.
[02:05:35.960 --> 02:05:36.360]   Yeah.
[02:05:36.360 --> 02:05:36.840]   Yeah.
[02:05:36.840 --> 02:05:37.320]   I said that.
[02:05:37.320 --> 02:05:38.040]   Or she was this issue.
[02:05:38.040 --> 02:05:39.320]   I'm sorry.
[02:05:39.320 --> 02:05:40.360]   So she's supporting.
[02:05:40.360 --> 02:05:44.360]   I'm glad to hear that she because she's, you know, a producer now.
[02:05:44.360 --> 02:05:45.320]   She didn't make her a nut.
[02:05:45.320 --> 02:05:45.880]   You know, but.
[02:05:45.880 --> 02:05:47.720]   She's supporting the writers and the actors.
[02:05:47.720 --> 02:05:48.280]   Good for her.
[02:05:48.280 --> 02:05:48.680]   Mm-hmm.
[02:05:48.680 --> 02:05:49.240]   Good for her.
[02:05:49.240 --> 02:05:53.160]   As am I, because even though I've made my nut.
[02:05:53.160 --> 02:05:54.120]   Yeah.
[02:05:54.120 --> 02:05:54.600]   You have.
[02:05:54.600 --> 02:05:55.160]   Mm-hmm.
[02:05:55.160 --> 02:05:55.720]   I saw.
[02:05:55.720 --> 02:05:59.000]   In my dreams.
[02:05:59.000 --> 02:06:03.000]   Uh, oh, this.
[02:06:03.000 --> 02:06:05.560]   Well, this is another AI story that scares the hell out of me.
[02:06:05.560 --> 02:06:09.160]   This is from Forbes.
[02:06:09.160 --> 02:06:11.240]   That was, that was a waffle side.
[02:06:11.240 --> 02:06:11.880]   Deep side.
[02:06:11.880 --> 02:06:12.200]   Okay.
[02:06:12.200 --> 02:06:12.440]   Okay.
[02:06:12.440 --> 02:06:14.040]   Real quick.
[02:06:14.040 --> 02:06:17.880]   Turns out they're putting in cameras everywhere.
[02:06:17.880 --> 02:06:18.200]   Right?
[02:06:18.200 --> 02:06:19.480]   The police.
[02:06:19.480 --> 02:06:23.160]   Well, for instance, uh, in West Chester County,
[02:06:23.160 --> 02:06:25.800]   up by where you live near the fancy people.
[02:06:25.800 --> 02:06:29.000]   The police have put in cameras on the roads
[02:06:29.000 --> 02:06:30.680]   and cameras in the police cars.
[02:06:31.720 --> 02:06:34.840]   Uh, 434 stationary cameras,
[02:06:34.840 --> 02:06:40.920]   46 mobile cameras in police cars scanning license plates
[02:06:40.920 --> 02:06:44.520]   and building a database of travel,
[02:06:44.520 --> 02:06:46.440]   of people moving around.
[02:06:46.440 --> 02:06:51.560]   They used this recently to bust a drug traveler.
[02:06:51.560 --> 02:06:53.000]   It worked.
[02:06:53.000 --> 02:06:56.040]   Searching through a database of 1.6 billion
[02:06:56.040 --> 02:06:59.000]   license place records collected over the last two years
[02:06:59.640 --> 02:07:01.320]   from locations across New York State.
[02:07:01.320 --> 02:07:04.920]   The AI determined this guy's car was on a,
[02:07:04.920 --> 02:07:08.360]   was typical drug trafficker driving.
[02:07:08.360 --> 02:07:11.800]   You know, he was going short stops in a bunch of places.
[02:07:11.800 --> 02:07:13.800]   He made nine trips from Massachusetts
[02:07:13.800 --> 02:07:15.480]   to different parts of New York,
[02:07:15.480 --> 02:07:18.120]   following routes to be known to you be used by our
[02:07:18.120 --> 02:07:18.440]   publishers.
[02:07:18.440 --> 02:07:18.920]   That's the key.
[02:07:18.920 --> 02:07:20.680]   It's very specific.
[02:07:20.680 --> 02:07:21.240]   Yeah.
[02:07:21.240 --> 02:07:23.640]   And for conspicuously short stays.
[02:07:23.640 --> 02:07:26.920]   So, the West Chester PD pulled him over.
[02:07:27.800 --> 02:07:28.680]   Searched his car.
[02:07:28.680 --> 02:07:31.000]   He was in fact the drug trafficker.
[02:07:31.000 --> 02:07:31.720]   They found out.
[02:07:31.720 --> 02:07:32.520]   But the question there was there,
[02:07:32.520 --> 02:07:34.920]   was there, was there probable cause to the scene?
[02:07:34.920 --> 02:07:35.560]   Well, yeah, wait.
[02:07:35.560 --> 02:07:38.440]   They found 112 grams of crack,
[02:07:38.440 --> 02:07:41.080]   a semi-automatic pistol, $34,000 in cash.
[02:07:41.080 --> 02:07:42.120]   Yeah. What ever they found,
[02:07:42.120 --> 02:07:43.640]   do they have cause to pull them over?
[02:07:43.640 --> 02:07:46.360]   A year later, he pled guilty to drug trafficking,
[02:07:46.360 --> 02:07:48.920]   but his lawyer has now,
[02:07:48.920 --> 02:07:52.440]   said having found out about this system,
[02:07:52.440 --> 02:07:56.360]   the license, the automatic license plate recognition system,
[02:07:57.400 --> 02:07:58.840]   his lawyers said,
[02:07:58.840 --> 02:08:01.880]   this is the specter of modern surveillance
[02:08:01.880 --> 02:08:03.800]   that the Fourth Amendment must guard against.
[02:08:03.800 --> 02:08:06.520]   This is a systematic development and deployment
[02:08:06.520 --> 02:08:08.760]   of a vast surveillance network that
[02:08:08.760 --> 02:08:12.440]   invades society's reasonable expectation of privacy
[02:08:12.440 --> 02:08:14.200]   with no judicial oversight.
[02:08:14.200 --> 02:08:17.320]   This type of system operates at the caprice of every author
[02:08:17.320 --> 02:08:20.360]   with access to it.
[02:08:20.360 --> 02:08:21.800]   So, we didn't know about this case.
[02:08:21.800 --> 02:08:23.240]   He pled guilty, pled out.
[02:08:23.240 --> 02:08:26.520]   But I think you're exactly right.
[02:08:27.480 --> 02:08:31.480]   Who says this data should be gathered?
[02:08:31.480 --> 02:08:32.040]   I mean, how many--
[02:08:32.040 --> 02:08:35.000]   Colombo can sit there and look and see the pattern
[02:08:35.000 --> 02:08:35.640]   of how people--
[02:08:35.640 --> 02:08:36.680]   Okay.
[02:08:36.680 --> 02:08:37.160]   --in the crimes.
[02:08:37.160 --> 02:08:39.240]   And Colombo can then use that.
[02:08:39.240 --> 02:08:41.560]   This has been an issue for years.
[02:08:41.560 --> 02:08:45.560]   And the issue, there's two big issues.
[02:08:45.560 --> 02:08:49.880]   One, there's how are the police in law enforcement
[02:08:49.880 --> 02:08:52.600]   using this data, right?
[02:08:52.600 --> 02:08:54.520]   So, are they using it?
[02:08:54.520 --> 02:08:56.440]   Is it stored forever, for example?
[02:08:56.440 --> 02:08:57.320]   Because it is
[02:08:57.320 --> 02:09:01.400]   unequivocal that this data can be used
[02:09:01.400 --> 02:09:04.360]   to track people at a personal level in monitor,
[02:09:04.360 --> 02:09:07.080]   where they go the same way your geolocation data
[02:09:07.080 --> 02:09:10.280]   can be used to monitor you from your phone, right?
[02:09:10.280 --> 02:09:11.800]   So, if you're driving, if you're not driving--
[02:09:11.800 --> 02:09:12.920]   Which is another issue.
[02:09:12.920 --> 02:09:16.120]   That information is widely collected and sold on,
[02:09:16.120 --> 02:09:17.800]   not only the law enforcement,
[02:09:17.800 --> 02:09:19.480]   so they don't have to do a warrant,
[02:09:19.480 --> 02:09:21.960]   but to foreign governments like China.
[02:09:23.080 --> 02:09:26.440]   Well, so the other issue, and this is a little bit more--
[02:09:26.440 --> 02:09:30.600]   So, A, what kind of do process things?
[02:09:30.600 --> 02:09:32.840]   Are there around this data and data collection
[02:09:32.840 --> 02:09:33.880]   for police officers?
[02:09:33.880 --> 02:09:36.440]   The second issue is you're seeing things like Hoas,
[02:09:36.440 --> 02:09:38.200]   like Hoa-- what does Hoa stand for?
[02:09:38.200 --> 02:09:38.520]   Home--
[02:09:38.520 --> 02:09:41.000]   Oh, the Homeowners Association.
[02:09:41.000 --> 02:09:41.640]   Homeowners?
[02:09:41.640 --> 02:09:42.200]   They do, right?
[02:09:42.200 --> 02:09:43.800]   Yeah, Homeowners Association.
[02:09:43.800 --> 02:09:46.040]   They're actually doing this, and they're using it.
[02:09:46.040 --> 02:09:48.360]   These are just normal citizens who are deploying this,
[02:09:48.360 --> 02:09:50.360]   and then the president of the Hoa uses this data
[02:09:50.360 --> 02:09:51.960]   to track all kinds of things.
[02:09:51.960 --> 02:09:52.840]   I have a friend--
[02:09:52.840 --> 02:09:53.640]   I didn't know that.
[02:09:53.640 --> 02:09:56.840]   Oh, God, I have a friend who was constantly getting notes
[02:09:56.840 --> 02:09:58.360]   in a door, you're parking, and then around,
[02:09:58.360 --> 02:09:59.320]   "Hey, I'm parking."
[02:09:59.320 --> 02:10:02.440]   His car has been parked here for more than 24 hours.
[02:10:02.440 --> 02:10:05.160]   I mean, that was just some busybody watching,
[02:10:05.160 --> 02:10:06.840]   but imagine if you ought to make that.
[02:10:06.840 --> 02:10:08.280]   It was-- yeah, it was Cravitz.
[02:10:08.280 --> 02:10:11.400]   So, there are a number of companies doing this.
[02:10:11.400 --> 02:10:16.840]   The one that Westchester PD uses is Recor, R-E-K-O-R.
[02:10:16.840 --> 02:10:21.480]   Recor has sold its ALPR tech to at least 23 police departments.
[02:10:21.960 --> 02:10:24.360]   And local governments across America.
[02:10:24.360 --> 02:10:27.080]   And they say the benefit of our system is you can use it.
[02:10:27.080 --> 02:10:29.400]   You're already installed cameras.
[02:10:29.400 --> 02:10:32.680]   So, you don't have to install any special cameras.
[02:10:32.680 --> 02:10:35.080]   And I remember about two years ago,
[02:10:35.080 --> 02:10:38.360]   the city of Petaluma installed cameras in all the poles,
[02:10:38.360 --> 02:10:42.200]   all around town, not speeding cameras.
[02:10:42.200 --> 02:10:44.520]   I'm thinking they're probably recording the license plates.
[02:10:44.520 --> 02:10:46.360]   Now, it's great they caught this crack dealer.
[02:10:46.360 --> 02:10:47.240]   That's what I was going to say.
[02:10:47.240 --> 02:10:51.240]   I'm fine that this offender was caught.
[02:10:51.240 --> 02:10:54.120]   Because they were in the wrong, but my problem is,
[02:10:54.120 --> 02:10:58.120]   when they went to pull them over, what was the reason?
[02:10:58.120 --> 02:10:59.720]   I mean, was it speeding?
[02:10:59.720 --> 02:11:00.680]   No, that's the thing.
[02:11:00.680 --> 02:11:01.160]   Right.
[02:11:01.160 --> 02:11:01.880]   You've seen your--
[02:11:01.880 --> 02:11:03.880]   He's not speeding, so he's not breaking that law.
[02:11:03.880 --> 02:11:05.240]   That's exactly what Jeff was saying.
[02:11:05.240 --> 02:11:06.360]   Is there a probable cause?
[02:11:06.360 --> 02:11:09.480]   Well, that's an interesting question for the courts.
[02:11:09.480 --> 02:11:10.680]   The other issue is--
[02:11:10.680 --> 02:11:12.920]   Because what if it was someone that borrowed my car,
[02:11:12.920 --> 02:11:15.400]   doing all of that stuff, and I happen to be driving?
[02:11:15.400 --> 02:11:18.280]   Well, also, something that's illegal in some states
[02:11:18.280 --> 02:11:19.640]   may not be illegal in this state.
[02:11:19.640 --> 02:11:22.600]   For instance, the Sacramento County Sheriff's Office,
[02:11:22.600 --> 02:11:26.760]   in Sacramento, California, where abortion is not illegal,
[02:11:26.760 --> 02:11:29.000]   was sharing license plate reader data with states
[02:11:29.000 --> 02:11:32.840]   that have banned abortion, presumably trying to track people
[02:11:32.840 --> 02:11:35.080]   visiting abortion clinics in California from states
[02:11:35.080 --> 02:11:36.200]   where it's illegal.
[02:11:36.200 --> 02:11:39.240]   And there are legislators all over the country now
[02:11:39.240 --> 02:11:42.600]   that are trying to use this information to prosecute people
[02:11:42.600 --> 02:11:45.080]   in their state who have gone to other states for abortions.
[02:11:46.120 --> 02:11:48.120]   So this is a big issue.
[02:11:48.120 --> 02:11:52.760]   And customs and border control also use it to track immigrants.
[02:11:52.760 --> 02:11:57.160]   So I stuck a link in there about my state
[02:11:57.160 --> 02:11:59.960]   and talking about what it can and can't do with this,
[02:11:59.960 --> 02:12:00.920]   and the concerns.
[02:12:00.920 --> 02:12:04.520]   But no, this has been a real electronic frontier foundation
[02:12:04.520 --> 02:12:07.640]   has been really active on this front for probably
[02:12:07.640 --> 02:12:08.600]   before the pandemic.
[02:12:08.600 --> 02:12:10.440]   But I'm glad--
[02:12:10.440 --> 02:12:13.000]   Yeah, this isn't really an AI story as much as this.
[02:12:13.000 --> 02:12:14.040]   Well, it is.
[02:12:14.040 --> 02:12:14.600]   Because--
[02:12:14.600 --> 02:12:15.240]   Well--
[02:12:15.240 --> 02:12:16.200]   They've got this data.
[02:12:16.200 --> 02:12:19.400]   And now they're using AI to look at this data
[02:12:19.400 --> 02:12:20.680]   and look for patterns.
[02:12:20.680 --> 02:12:24.440]   And that's where it really becomes even more problematic.
[02:12:24.440 --> 02:12:27.080]   It's one thing if Colombo is looking at a--
[02:12:27.080 --> 02:12:29.320]   I shadish cars been going through.
[02:12:29.320 --> 02:12:30.120]   That's one thing.
[02:12:30.120 --> 02:12:34.440]   It's another thing if a machine with untold biases
[02:12:34.440 --> 02:12:36.600]   is looking at this data and saying, you know--
[02:12:36.600 --> 02:12:39.400]   Well, there's not a lot of bias around license plates.
[02:12:39.400 --> 02:12:39.880]   That is a leaking tunnel.
[02:12:39.880 --> 02:12:42.600]   And I know that people of color tend to drive that car.
[02:12:42.600 --> 02:12:45.480]   So I think that adds three points to the likelihood
[02:12:45.480 --> 02:12:47.000]   that they're selling crack and that kind of thing.
[02:12:47.000 --> 02:12:47.800]   Hmm.
[02:12:47.800 --> 02:12:51.480]   Well, no, they're not using it quite that--
[02:12:51.480 --> 02:12:52.920]   Oh, yes, they are.
[02:12:52.920 --> 02:12:53.720]   Read the article.
[02:12:53.720 --> 02:12:55.720]   They're using it to profile.
[02:12:55.720 --> 02:12:58.680]   Because to profile based on the car,
[02:12:58.680 --> 02:13:00.920]   are they using it because they understand that this license plate?
[02:13:00.920 --> 02:13:01.480]   Yes, they record not only--
[02:13:01.480 --> 02:13:03.880]   No, it records not only the license plate
[02:13:03.880 --> 02:13:05.800]   but the make, model, and color of the car.
[02:13:05.800 --> 02:13:09.480]   You got to presume they're recording that
[02:13:09.480 --> 02:13:10.920]   because the AI wants to use it.
[02:13:11.480 --> 02:13:14.680]   They say it's because all the one of people
[02:13:14.680 --> 02:13:16.120]   change their license plate.
[02:13:16.120 --> 02:13:18.440]   But I don't think that's the only thing they're checking.
[02:13:18.440 --> 02:13:19.480]   No, OK.
[02:13:19.480 --> 02:13:22.920]   Anyway, anyway, that's a good question.
[02:13:22.920 --> 02:13:26.600]   If you were a reporter, you might want to ask them that question.
[02:13:26.600 --> 02:13:31.160]   Go down to the Westchester County Sheriff's office and say,
[02:13:31.160 --> 02:13:34.920]   "Dude, why do you need to know if it's a rambler?"
[02:13:34.920 --> 02:13:36.680]   Say that word, "Dude."
[02:13:39.240 --> 02:13:41.000]   All right, quickly, change log because--
[02:13:41.000 --> 02:13:42.600]   Do we have to?
[02:13:42.600 --> 02:13:43.800]   There.
[02:13:43.800 --> 02:13:46.200]   It was nothing.
[02:13:46.200 --> 02:13:49.160]   Change log, Mr. House, this is one of history.
[02:13:49.160 --> 02:13:52.200]   This is the Google show.
[02:13:52.200 --> 02:13:53.800]   We spend an hour talking about AI.
[02:13:53.800 --> 02:13:55.240]   We've got to talk about Google.
[02:13:55.240 --> 02:13:58.120]   Google launches nearby share for Windows and partnering to pre-install.
[02:13:58.120 --> 02:14:01.800]   Google confirms related search operator is going away.
[02:14:01.800 --> 02:14:02.600]   Oh, that's too bad.
[02:14:02.600 --> 02:14:04.360]   I like the related search operator.
[02:14:04.360 --> 02:14:05.000]   Oh, that's a--
[02:14:05.000 --> 02:14:07.720]   It said that the results were very good.
[02:14:07.720 --> 02:14:08.280]   Yeah, he saw--
[02:14:08.280 --> 02:14:09.640]   Is that Danny Sullivan?
[02:14:09.640 --> 02:14:11.480]   Our good friend, Danny Sullivan, who now--
[02:14:11.480 --> 02:14:13.800]   It used to be search engine land now works at Google.
[02:14:13.800 --> 02:14:15.720]   What is the search?
[02:14:15.720 --> 02:14:16.200]   They call him Danny.
[02:14:16.200 --> 02:14:18.840]   Danny said it hasn't really worked well for some time,
[02:14:18.840 --> 02:14:21.160]   as in some cases, the information was dated.
[02:14:21.160 --> 02:14:26.360]   The related search operator allowed you to type in the Google search box.
[02:14:26.360 --> 02:14:31.960]   And Google returned related websites to that URL.
[02:14:31.960 --> 02:14:34.600]   Oh, so you'd actually type in a URL and that would give you some
[02:14:34.600 --> 02:14:37.080]   other URLs that are related.
[02:14:37.080 --> 02:14:37.480]   Oh, OK.
[02:14:37.480 --> 02:14:40.680]   Anyway, not going to be--
[02:14:40.680 --> 02:14:41.960]   Not going to be around if you use it.
[02:14:41.960 --> 02:14:46.280]   Google is testing AI generated meat video backgrounds.
[02:14:46.280 --> 02:14:48.760]   I'll be using those because we have our Google meets.
[02:14:48.760 --> 02:14:49.400]   I like that.
[02:14:49.400 --> 02:14:52.520]   This is actually a little interesting.
[02:14:52.520 --> 02:14:55.960]   Google's bard is now going to respond to visual prompts.
[02:14:55.960 --> 02:15:02.680]   But companies like Google are very concerned about using this for face recognition.
[02:15:02.680 --> 02:15:06.680]   If you feed it a face, are we going to have a little--
[02:15:06.680 --> 02:15:06.680]   Oh, yeah.
[02:15:06.680 --> 02:15:09.080]   Are we going to have a problem here?
[02:15:09.080 --> 02:15:10.920]   - Save cards. - Yeah, save cards.
[02:15:10.920 --> 02:15:17.160]   And Google is deleting some old Hangouts photos this week, so get your photos.
[02:15:17.160 --> 02:15:18.200]   Google takeout.
[02:15:18.200 --> 02:15:18.440]   Yeah.
[02:15:18.440 --> 02:15:22.200]   They better not delete that photo if you're sitting out in the parking lot.
[02:15:22.200 --> 02:15:24.040]   Join it.
[02:15:24.040 --> 02:15:26.120]   Don't worry, it'll be refreshed next week.
[02:15:26.120 --> 02:15:29.400]   And that's--
[02:15:30.120 --> 02:15:31.480]   Your Google change log.
[02:15:31.480 --> 02:15:40.040]   Coming up next, it's going to be picks and tips and numbers and all that stuff as we wrap up
[02:15:40.040 --> 02:15:42.360]   this edition of this week in Google.
[02:15:42.360 --> 02:15:45.160]   Are you the creative type?
[02:15:45.160 --> 02:15:48.360]   And you already know lots of cool things Photoshop can do,
[02:15:48.360 --> 02:15:50.920]   like create, iPod and images for social,
[02:15:50.920 --> 02:15:53.560]   and gorgeous graphics for T-shirts and posters.
[02:15:53.560 --> 02:15:57.560]   But did you also know it can instantly turn a gray sky into a fiery sunset,
[02:15:57.560 --> 02:15:59.800]   change black and white to color in a click,
[02:15:59.800 --> 02:16:02.760]   or make anything in your photo magically disappear?
[02:16:02.760 --> 02:16:07.080]   Maybe you're wondering, can anyone use Photoshop to take images from ordinary to amazing?
[02:16:07.080 --> 02:16:08.440]   Nope.
[02:16:08.440 --> 02:16:09.560]   Everyone can.
[02:16:09.560 --> 02:16:11.880]   Visit photoshop.com and get started for free.
[02:16:11.880 --> 02:16:17.240]   Stacey Higginbotham, do you have a thing of the week to wrap it up this week?
[02:16:17.240 --> 02:16:21.480]   I don't have a thing, but I have a pick.
[02:16:21.480 --> 02:16:22.200]   Better yet.
[02:16:22.200 --> 02:16:26.200]   Because, yeah, you know, with summer, I've been reading.
[02:16:26.200 --> 02:16:31.160]   So this, I actually pitched this as it's not the ones that we're reading.
[02:16:31.160 --> 02:16:34.760]   Yeah, it's not the ones we chose for the book club,
[02:16:34.760 --> 02:16:37.720]   but I just finally finished the whole series.
[02:16:37.720 --> 02:16:39.960]   And it's really good, y'all.
[02:16:39.960 --> 02:16:41.480]   It's kind of a banger of a series.
[02:16:41.480 --> 02:16:46.360]   So the series is it's by Adrian Chikovsky,
[02:16:46.360 --> 02:16:50.840]   and it is the they call it the architecture series.
[02:16:50.840 --> 02:16:53.000]   And the first book is called Shards of Earth.
[02:16:54.360 --> 02:16:56.120]   And it's sci-fi.
[02:16:56.120 --> 02:16:56.760]   I take it.
[02:16:56.760 --> 02:16:59.560]   It's a sci-fi space opera.
[02:16:59.560 --> 02:17:00.280]   I mean, it's summer.
[02:17:00.280 --> 02:17:01.400]   What are you going to read on the beach?
[02:17:01.400 --> 02:17:01.720]   Right.
[02:17:01.720 --> 02:17:07.560]   You know, I'm not going to read about the British influencing our speech.
[02:17:07.560 --> 02:17:09.400]   You're going to read a space opera.
[02:17:09.400 --> 02:17:12.360]   Shards of the earth, eyes of the void,
[02:17:12.360 --> 02:17:17.400]   lords of uncreation, the final architecture series.
[02:17:17.400 --> 02:17:23.400]   It was a fun kind of, I mean, I guess it's longest if you'd like it,
[02:17:23.400 --> 02:17:24.360]   but I liked it.
[02:17:24.360 --> 02:17:25.320]   Good space opera.
[02:17:25.320 --> 02:17:27.720]   Kind of a concept.
[02:17:27.720 --> 02:17:28.840]   The war is over.
[02:17:28.840 --> 02:17:34.600]   It's heroes forgotten until one chance discovery.
[02:17:34.600 --> 02:17:38.120]   Idris Elba.
[02:17:38.120 --> 02:17:39.800]   Idris Elba has never aged.
[02:17:39.800 --> 02:17:42.920]   No slept since they remained him in the war.
[02:17:42.920 --> 02:17:45.400]   It's not Elba.
[02:17:45.400 --> 02:17:46.360]   Not Elba.
[02:17:46.360 --> 02:17:47.080]   Could be Elba.
[02:17:47.080 --> 02:17:48.200]   You know what?
[02:17:48.200 --> 02:17:50.280]   I have to say, if I were reading this book,
[02:17:50.280 --> 02:17:51.480]   I'd see Idris Elba.
[02:17:51.480 --> 02:17:52.360]   Yeah, I would.
[02:17:52.360 --> 02:17:52.600]   Yeah.
[02:17:53.400 --> 02:17:53.800]   Yeah.
[02:17:53.800 --> 02:17:56.280]   Well, no, because they totally describe them as this like,
[02:17:56.280 --> 02:17:58.440]   totally put upon like jug headed person.
[02:17:58.440 --> 02:17:58.760]   Oh.
[02:17:58.760 --> 02:18:00.600]   And I probably would not classify it.
[02:18:00.600 --> 02:18:01.800]   This looks good, actually.
[02:18:01.800 --> 02:18:04.280]   I just, this excerpt looks really good.
[02:18:04.280 --> 02:18:06.120]   I will confess Miss Stacy.
[02:18:06.120 --> 02:18:08.200]   That was my vote initially.
[02:18:08.200 --> 02:18:09.960]   But then I said no.
[02:18:09.960 --> 02:18:10.920]   Oh, look at you.
[02:18:10.920 --> 02:18:14.680]   And I said, no, she's been mentioning in like several times.
[02:18:14.680 --> 02:18:15.160]   So.
[02:18:15.160 --> 02:18:17.560]   Oh, I feel about, well, do you like space operas?
[02:18:17.560 --> 02:18:19.240]   I didn't care.
[02:18:19.240 --> 02:18:20.360]   I was just going to give it a shot.
[02:18:20.360 --> 02:18:22.680]   I was just going to give it a shot.
[02:18:22.680 --> 02:18:23.080]   I saw.
[02:18:23.080 --> 02:18:27.800]   I love you, Anne, for giving it a shot.
[02:18:27.800 --> 02:18:30.120]   I think the people have more normal names in this one.
[02:18:30.120 --> 02:18:32.600]   Oh, is the names the part of what's,
[02:18:32.600 --> 02:18:34.360]   what you're struggling with?
[02:18:34.360 --> 02:18:38.520]   Anne, most of the sci-fi stuff is the names and all the other
[02:18:38.520 --> 02:18:41.400]   terminology that normal folks like myself are going to use.
[02:18:41.400 --> 02:18:42.920]   You should read, I see sci-fi,
[02:18:42.920 --> 02:18:44.120]   I started reading Hyperion,
[02:18:44.120 --> 02:18:47.080]   which is widely considered to be a brilliant sci-fi novel.
[02:18:47.800 --> 02:18:50.760]   And it's like, this is all made up.
[02:18:50.760 --> 02:18:52.600]   Yeah.
[02:18:52.600 --> 02:18:54.040]   It came to me.
[02:18:54.040 --> 02:18:55.160]   This is all made up.
[02:18:55.160 --> 02:18:59.000]   And I thought, I am not in the mood for made up stuff right now.
[02:18:59.000 --> 02:19:03.000]   So I started reading the longest biography in history.
[02:19:03.000 --> 02:19:06.920]   I think it is 1,246 pages.
[02:19:06.920 --> 02:19:07.480]   Son of a.
[02:19:07.480 --> 02:19:09.960]   I was telling the Jeff, I was telling you about.
[02:19:09.960 --> 02:19:11.160]   Whose life is this?
[02:19:11.160 --> 02:19:13.400]   A guy you never heard of, Robert Moses.
[02:19:13.400 --> 02:19:16.680]   Okay, I've heard of Robert Moses.
[02:19:16.680 --> 02:19:17.480]   I love this book.
[02:19:17.480 --> 02:19:19.160]   I have a.
[02:19:19.160 --> 02:19:19.640]   I have to.
[02:19:19.640 --> 02:19:20.680]   The power broker.
[02:19:20.680 --> 02:19:23.320]   It's called the power broker and the Pulitzer Prize winner.
[02:19:23.320 --> 02:19:27.960]   It's a post Robert Caro and it is considered by many to be the best biography of all time.
[02:19:27.960 --> 02:19:28.440]   Really?
[02:19:28.440 --> 02:19:31.240]   It is great because I grew up in New York.
[02:19:31.240 --> 02:19:35.160]   So this is the guy who built the Cross Bronx Expressway Riverside Park.
[02:19:35.160 --> 02:19:40.440]   He turned Long Island into a park for the poor tenement dwellers of New York City.
[02:19:40.440 --> 02:19:44.200]   But he did it by raising homes,
[02:19:44.200 --> 02:19:46.280]   cutting right through neighborhoods.
[02:19:46.280 --> 02:19:50.200]   I mean, it's a fascinating story.
[02:19:50.200 --> 02:19:54.200]   And I have to say, I really am enjoying it.
[02:19:54.200 --> 02:19:55.080]   It's better than I thought.
[02:19:55.080 --> 02:19:56.840]   Maybe because I know that.
[02:19:56.840 --> 02:19:58.520]   You know the history.
[02:19:58.520 --> 02:19:59.640]   I know that area.
[02:19:59.640 --> 02:20:01.000]   I don't actually know the history.
[02:20:01.000 --> 02:20:01.640]   So that's the thing.
[02:20:01.640 --> 02:20:05.080]   I'm learning the history of an area that I know pretty well.
[02:20:05.080 --> 02:20:09.880]   He built $27 billion worth of public works.
[02:20:11.640 --> 02:20:16.280]   Every senator and lobbyist, everybody in Washington, D.C.
[02:20:16.280 --> 02:20:17.720]   has that book on their shelves.
[02:20:17.720 --> 02:20:19.320]   Yeah, because he had an all Reddit.
[02:20:19.320 --> 02:20:20.520]   He knew how to do it.
[02:20:20.520 --> 02:20:22.360]   Well, it is quite a read.
[02:20:22.360 --> 02:20:24.760]   It's 65 hours on a whole.
[02:20:24.760 --> 02:20:28.600]   I'm only about a third of the way in,
[02:20:28.600 --> 02:20:31.720]   but which is the equivalent of like reading three normal books,
[02:20:31.720 --> 02:20:32.840]   but it's really good.
[02:20:32.840 --> 02:20:33.240]   Wow.
[02:20:33.240 --> 02:20:34.120]   It's so good.
[02:20:34.120 --> 02:20:36.520]   Or one or two sci-fi books.
[02:20:36.520 --> 02:20:36.920]   Two of them.
[02:20:36.920 --> 02:20:40.760]   This is like reading the whole the whole Nutcracker series.
[02:20:40.760 --> 02:20:42.600]   They're architecture series or whatever you call it.
[02:20:42.600 --> 02:20:43.480]   So.
[02:20:43.480 --> 02:20:44.840]   Nutcracker series.
[02:20:44.840 --> 02:20:46.280]   That's cracker series.
[02:20:46.280 --> 02:20:47.480]   Such as God, Dad.
[02:20:47.480 --> 02:20:50.360]   Jeff, actually, that's not my pick.
[02:20:50.360 --> 02:20:52.760]   I did want to mention real quickly my pick,
[02:20:52.760 --> 02:20:59.400]   which is wiby.me/surprise.
[02:20:59.400 --> 02:21:02.920]   The reason I say that is you have to click the link.
[02:21:02.920 --> 02:21:07.800]   And every time you do, you'll get a website that's still around,
[02:21:07.800 --> 02:21:10.760]   that's firmly from the early 1990s.
[02:21:10.760 --> 02:21:11.880]   A web point.
[02:21:11.880 --> 02:21:12.440]   Very fun.
[02:21:12.440 --> 02:21:15.320]   One point one web one point oh website.
[02:21:15.320 --> 02:21:17.320]   So I'm going to click the link right now,
[02:21:17.320 --> 02:21:21.320]   and I will get a random surprise.
[02:21:21.320 --> 02:21:22.440]   Look it.
[02:21:22.440 --> 02:21:24.440]   Everything you'd ever want to know about Arrowheads.
[02:21:24.440 --> 02:21:25.000]   Oh my gosh.
[02:21:25.000 --> 02:21:27.960]   Oh my.
[02:21:27.960 --> 02:21:29.480]   But every time it's different.
[02:21:29.480 --> 02:21:31.160]   So that's a good one.
[02:21:31.160 --> 02:21:32.520]   Let me click it again.
[02:21:32.520 --> 02:21:33.480]   See what else we get.
[02:21:33.480 --> 02:21:35.320]   I clicked it.
[02:21:35.320 --> 02:21:38.440]   And it's overviews of plugs and sockets.
[02:21:38.440 --> 02:21:42.120]   Actually, I've literally been in this.
[02:21:42.120 --> 02:21:45.480]   The digital museum of plugs and sockets website.
[02:21:45.480 --> 02:21:48.920]   I like how it says right as it loads.
[02:21:48.920 --> 02:21:49.720]   You asked for it.
[02:21:49.720 --> 02:21:50.600]   You asked for it.
[02:21:50.600 --> 02:21:54.440]   Wiby.me/surprise.
[02:21:54.440 --> 02:21:55.480]   It's a lot of fun.
[02:21:55.480 --> 02:21:56.360]   It is because.
[02:21:56.360 --> 02:21:59.320]   Okay, this is good.
[02:21:59.320 --> 02:22:02.680]   There's just some really bad websites out there.
[02:22:02.680 --> 02:22:03.480]   This is good.
[02:22:04.360 --> 02:22:07.000]   But all our websites looked like this.
[02:22:07.000 --> 02:22:07.560]   Sure did.
[02:22:07.560 --> 02:22:08.280]   In the early days.
[02:22:08.280 --> 02:22:09.160]   Sure did.
[02:22:09.160 --> 02:22:10.120]   So this is.
[02:22:10.120 --> 02:22:12.600]   I built a website in 1998.
[02:22:12.600 --> 02:22:13.800]   I wish it were still around.
[02:22:13.800 --> 02:22:15.000]   It was amazing.
[02:22:15.000 --> 02:22:17.480]   Did you do it with MySpace?
[02:22:17.480 --> 02:22:18.280]   Tell the truth.
[02:22:18.280 --> 02:22:19.880]   Gee, you did it with AngelFire?
[02:22:19.880 --> 02:22:20.920]   AngelFire.
[02:22:20.920 --> 02:22:22.040]   Gee, you did it.
[02:22:22.040 --> 02:22:22.440]   Gee, you did it.
[02:22:22.440 --> 02:22:23.480]   Gee, you did it.
[02:22:23.480 --> 02:22:24.440]   Thank you.
[02:22:24.440 --> 02:22:25.000]   Oh, actually no.
[02:22:25.000 --> 02:22:27.800]   We coded it in HTML for my journalism class.
[02:22:27.800 --> 02:22:28.360]   Oh, cool.
[02:22:28.360 --> 02:22:29.560]   By hand.
[02:22:29.560 --> 02:22:32.200]   Because we had to do it by hand at first too.
[02:22:32.200 --> 02:22:34.040]   It was a bespoke artisanal.
[02:22:34.040 --> 02:22:35.080]   Well, it will.
[02:22:35.080 --> 02:22:36.120]   But it's expensive.
[02:22:36.120 --> 02:22:36.840]   It will.
[02:22:36.840 --> 02:22:40.440]   It's always fun to see what you can get.
[02:22:40.440 --> 02:22:41.800]   The simplicity boats.
[02:22:41.800 --> 02:22:44.120]   A free boat building.
[02:22:44.120 --> 02:22:44.520]   You know what?
[02:22:44.520 --> 02:22:46.760]   This is what's kind of fun about this.
[02:22:46.760 --> 02:22:48.680]   At first you go, oh, these are so bad.
[02:22:48.680 --> 02:22:50.680]   These are all kind of built by.
[02:22:50.680 --> 02:22:51.480]   More innocent time.
[02:22:51.480 --> 02:22:52.280]   They're innocent.
[02:22:52.280 --> 02:22:53.080]   Normal people.
[02:22:53.080 --> 02:22:54.760]   Here's a variety of,
[02:22:54.760 --> 02:22:56.600]   varieties of goldfish in Japan.
[02:22:56.600 --> 02:22:59.400]   The Japanese goldfish catalog.
[02:22:59.400 --> 02:23:00.120]   Hi, this is Benito.
[02:23:00.120 --> 02:23:02.040]   This is the stuff that Google killed.
[02:23:02.040 --> 02:23:03.800]   Google killed this stuff because they.
[02:23:03.800 --> 02:23:08.760]   This is not authoritative, right?
[02:23:08.760 --> 02:23:11.960]   Be careful what you get because I'm not quite sure
[02:23:11.960 --> 02:23:13.160]   what the heck this is.
[02:23:13.160 --> 02:23:13.640]   Oh my.
[02:23:13.640 --> 02:23:15.160]   I want that site.
[02:23:15.160 --> 02:23:16.120]   I don't know what it is.
[02:23:16.120 --> 02:23:20.040]   There's a guy in a bathtub and there's a woman in a nightgown.
[02:23:20.040 --> 02:23:24.920]   And she's just thinking, why is this guy in my bathtub?
[02:23:24.920 --> 02:23:25.400]   I think.
[02:23:25.400 --> 02:23:27.240]   I don't know what's going on.
[02:23:27.240 --> 02:23:28.920]   What the heck happened there?
[02:23:28.920 --> 02:23:30.280]   That's a great site though.
[02:23:31.400 --> 02:23:33.000]   Don't click whatever you do.
[02:23:33.000 --> 02:23:36.040]   I just, I don't know what that is.
[02:23:36.040 --> 02:23:38.760]   I mean, he's got a rope and what the.
[02:23:38.760 --> 02:23:41.080]   Oh, there's a rope.
[02:23:41.080 --> 02:23:43.560]   That's what the hell's going on here.
[02:23:43.560 --> 02:23:44.440]   Boy, good old.
[02:23:44.440 --> 02:23:46.520]   Meanwhile, I'm at mythandfantasy.com
[02:23:46.520 --> 02:23:49.800]   where unicorn foals prance through the fresh green grasses.
[02:23:49.800 --> 02:23:52.040]   Let's try.
[02:23:52.040 --> 02:23:53.000]   Stacey's hungry.
[02:23:53.000 --> 02:23:53.400]   All right.
[02:23:53.400 --> 02:23:53.800]   All right.
[02:23:53.800 --> 02:23:54.200]   All right.
[02:23:54.200 --> 02:23:55.320]   Jeff Jarvis, number.
[02:23:55.320 --> 02:23:57.880]   You've blended up Stacey.
[02:23:57.880 --> 02:23:58.680]   I'm using you.
[02:23:58.680 --> 02:23:59.880]   I'm using you Stacey.
[02:23:59.880 --> 02:24:01.320]   He's tired of the way I went.
[02:24:01.320 --> 02:24:02.840]   I'll be used in this case.
[02:24:02.840 --> 02:24:03.480]   That's fine.
[02:24:03.480 --> 02:24:03.880]   I thought so.
[02:24:03.880 --> 02:24:05.400]   I thought it would be in your interest.
[02:24:05.400 --> 02:24:09.720]   So I'm going to go up the rundown here to the fact that
[02:24:09.720 --> 02:24:17.240]   Vox mentioned earlier, which made a big deal of building its own CMS.
[02:24:17.240 --> 02:24:18.920]   Oh, this is an interesting story.
[02:24:18.920 --> 02:24:19.240]   Yes.
[02:24:19.240 --> 02:24:22.120]   It's got rid of the CMS and it's going to, what's it going to?
[02:24:22.120 --> 02:24:22.920]   WordPress.
[02:24:22.920 --> 02:24:23.320]   Yes.
[02:24:23.320 --> 02:24:23.880]   The V.I.
[02:24:23.880 --> 02:24:29.080]   And I did a thread where I went through, I installed my first editorial system with the Chicago
[02:24:29.080 --> 02:24:31.720]   Tribune on this old folks in 1974.
[02:24:31.720 --> 02:24:34.360]   And everybody thought they had to have their own special system,
[02:24:34.360 --> 02:24:35.400]   their own special.
[02:24:35.400 --> 02:24:37.480]   Well, and that was their secret sauce.
[02:24:37.480 --> 02:24:38.280]   Exactly.
[02:24:38.280 --> 02:24:40.760]   Because they thought too much of their value was in their content.
[02:24:40.760 --> 02:24:43.560]   And whether we have to manage, we have the system to manage this content.
[02:24:43.560 --> 02:24:46.440]   Rather than they're making paragraphs, that's all they were doing.
[02:24:46.440 --> 02:24:49.800]   And so again, I got into fights with this in San Francisco.
[02:24:49.800 --> 02:24:54.920]   I got, I timed the fights between the SII system and the ATEX system were horrendous.
[02:24:54.920 --> 02:24:59.800]   When I started E.W., I left all of them behind and my wife set up our Macintosh network.
[02:24:59.800 --> 02:25:02.840]   And we left behind all of that computer junk and it was,
[02:25:02.840 --> 02:25:06.520]   Macintosh became the standard until it came along the internet.
[02:25:06.520 --> 02:25:10.440]   And we all needed our special content management systems because we're going to do special things
[02:25:10.440 --> 02:25:11.000]   on the internet.
[02:25:11.000 --> 02:25:16.520]   And, and, you know, Vox and Trebrundt read who ran product at Vox was brilliant.
[02:25:16.520 --> 02:25:18.280]   And then Mr. was a great system.
[02:25:18.280 --> 02:25:20.600]   And then Washington Post has its own special system.
[02:25:20.600 --> 02:25:22.520]   And everybody built their own damn systems.
[02:25:22.520 --> 02:25:25.720]   Well, worse than that, Vox was licensing it.
[02:25:25.720 --> 02:25:28.200]   They were, yes, they were, our system is so close.
[02:25:28.200 --> 02:25:29.080]   This is Washington Post.
[02:25:29.080 --> 02:25:33.400]   You can buy it and use it for your, we use Drupal for our content management system,
[02:25:33.400 --> 02:25:37.240]   which is an old CMS, but we keep it up to date.
[02:25:37.240 --> 02:25:40.600]   But WordPress now is almost half of all the internet.
[02:25:40.600 --> 02:25:42.920]   It's really started to win it all.
[02:25:42.920 --> 02:25:46.680]   So have you done Bless Matt Mullenweig and the open source model.
[02:25:46.680 --> 02:25:47.880]   He's been on the show one in the day.
[02:25:47.880 --> 02:25:48.520]   One in the day.
[02:25:48.520 --> 02:25:50.680]   Yep. And so that's wild.
[02:25:50.680 --> 02:25:56.360]   They must have spent millions on chorus, which was their proprietary content management system.
[02:25:56.360 --> 02:25:59.720]   Maybe they made the millions back with licensing.
[02:25:59.720 --> 02:26:00.520]   I don't know.
[02:26:00.520 --> 02:26:04.440]   They made something, you know, that, and they, they did update the paper a lot.
[02:26:04.440 --> 02:26:07.080]   It was Bezos putting technology in when they updated it a lot.
[02:26:07.080 --> 02:26:10.360]   And they did build some other things like ad systems.
[02:26:10.360 --> 02:26:13.240]   And they're now going to bundle that stuff to big clients with WordPress.
[02:26:13.240 --> 02:26:14.440]   They still have an ad system.
[02:26:14.440 --> 02:26:14.840]   So yeah.
[02:26:14.840 --> 02:26:20.280]   And by the way, kudos, because this is a scoop for Sarah Fisher and Kerry Flynn.
[02:26:20.280 --> 02:26:21.480]   It looks like a quarter.
[02:26:21.480 --> 02:26:21.960]   Yeah.
[02:26:21.960 --> 02:26:24.200]   So good job Fisher and Flynn.
[02:26:24.200 --> 02:26:24.760]   Nice.
[02:26:24.760 --> 02:26:31.400]   Vox media after all this time drops its custom content management system.
[02:26:31.400 --> 02:26:36.200]   At Pruitt, you, it is up to you to redeem our picks of the week.
[02:26:36.200 --> 02:26:41.640]   I think I've already done this before, but I said, let me try just to be sure.
[02:26:42.440 --> 02:26:44.600]   My pick the week is that's pretty.
[02:26:44.600 --> 02:26:45.400]   What is this?
[02:26:45.400 --> 02:26:46.680]   You know, I mean bags.
[02:26:46.680 --> 02:26:48.520]   This is the same way.
[02:26:48.520 --> 02:26:58.360]   I'm using it as a pouch because I just wanted to pack light some days when I come to the studio.
[02:26:58.360 --> 02:26:59.480]   That's your light pack?
[02:26:59.480 --> 02:26:59.800]   Yeah.
[02:26:59.800 --> 02:27:01.080]   Yeah.
[02:27:01.080 --> 02:27:02.120]   That's your house.
[02:27:02.120 --> 02:27:02.840]   I've got five lenses.
[02:27:02.840 --> 02:27:04.200]   There's there's my camera.
[02:27:04.200 --> 02:27:06.040]   Bigger than my face purse.
[02:27:06.040 --> 02:27:07.560]   My camera's in here.
[02:27:07.560 --> 02:27:08.360]   Some tools.
[02:27:08.360 --> 02:27:10.120]   That's vitamins.
[02:27:10.120 --> 02:27:11.000]   My glasses.
[02:27:11.000 --> 02:27:12.360]   That's an ant broken bag.
[02:27:12.360 --> 02:27:13.800]   That's phones.
[02:27:13.800 --> 02:27:15.480]   There's some about big eyes.
[02:27:15.480 --> 02:27:17.960]   They like to carry big bags.
[02:27:17.960 --> 02:27:18.760]   Couple of lenses.
[02:27:18.760 --> 02:27:19.320]   But this is, this is.
[02:27:19.320 --> 02:27:21.720]   Patrick Darn, you're the biggest pack pack.
[02:27:21.720 --> 02:27:22.600]   Right.
[02:27:22.600 --> 02:27:24.440]   This is way smaller than my backpack.
[02:27:24.440 --> 02:27:24.680]   Okay.
[02:27:24.680 --> 02:27:25.080]   You're right.
[02:27:25.080 --> 02:27:26.120]   It's way smaller than my backpack.
[02:27:26.120 --> 02:27:26.920]   It's all relative.
[02:27:26.920 --> 02:27:30.200]   But I really love their little camera cube system.
[02:27:30.200 --> 02:27:33.000]   They make a small, a medium and a large.
[02:27:33.000 --> 02:27:36.200]   So you can, that's a medium that looks like, right?
[02:27:36.200 --> 02:27:36.440]   Yeah.
[02:27:36.440 --> 02:27:37.400]   This one is the media.
[02:27:37.400 --> 02:27:37.560]   Yeah.
[02:27:37.560 --> 02:27:39.880]   And then I just stuck a camera strap on it
[02:27:39.880 --> 02:27:41.880]   because you can use it with the strap.
[02:27:41.880 --> 02:27:43.720]   Oh, so this isn't really a backpack.
[02:27:43.720 --> 02:27:44.600]   It's just a cube.
[02:27:44.600 --> 02:27:45.320]   It's a cube.
[02:27:45.320 --> 02:27:45.960]   Oh.
[02:27:45.960 --> 02:27:47.880]   But you can use it however you want.
[02:27:47.880 --> 02:27:50.520]   And for me, this works as a nice little pouch
[02:27:50.520 --> 02:27:51.800]   to carry some stuff around.
[02:27:51.800 --> 02:27:53.800]   If I want to go a little bit lighter.
[02:27:53.800 --> 02:27:55.240]   And I could take some stuff out.
[02:27:55.240 --> 02:27:57.960]   I just packed it full just as an example.
[02:27:57.960 --> 02:27:59.960]   But because, I mean, I don't really need
[02:27:59.960 --> 02:28:04.120]   this in here right now or my microphone in here.
[02:28:04.120 --> 02:28:05.960]   Or, you know, I don't really need all of that.
[02:28:05.960 --> 02:28:07.480]   But I got plenty of stuff in there.
[02:28:07.480 --> 02:28:08.200]   And it's it's kind of.
[02:28:08.200 --> 02:28:09.480]   I should point out.
[02:28:09.480 --> 02:28:12.680]   You also carry a capture with you all the time.
[02:28:12.680 --> 02:28:16.360]   So it's it's probably relative, right?
[02:28:16.360 --> 02:28:18.200]   That cube isn't all that large.
[02:28:18.200 --> 02:28:21.080]   Who's getting ready for the show out in the front of it.
[02:28:21.080 --> 02:28:22.360]   I hate on my chair.
[02:28:22.360 --> 02:28:24.840]   That's where I sat.
[02:28:24.840 --> 02:28:26.120]   Do you drive around with that?
[02:28:26.120 --> 02:28:26.760]   You always have it.
[02:28:26.760 --> 02:28:28.120]   Yeah, this one was in the trunk of my car.
[02:28:28.120 --> 02:28:31.160]   You don't do the thing where you come out with one hand and go.
[02:28:31.160 --> 02:28:32.200]   I am.
[02:28:32.200 --> 02:28:35.080]   I am.
[02:28:35.080 --> 02:28:36.520]   That was Matthew McConvie.
[02:28:36.520 --> 02:28:37.160]   Have you not?
[02:28:37.160 --> 02:28:37.800]   Famously.
[02:28:38.440 --> 02:28:40.200]   Did that with his camp chair?
[02:28:40.200 --> 02:28:42.040]   If not.
[02:28:42.040 --> 02:28:44.360]   But no, that's just peak design.
[02:28:44.360 --> 02:28:44.920]   I love people.
[02:28:44.920 --> 02:28:46.280]   I love I you know, I love them.
[02:28:46.280 --> 02:28:47.240]   They used to be a sponsor.
[02:28:47.240 --> 02:28:48.360]   I'd love to get them back on.
[02:28:48.360 --> 02:28:48.600]   But.
[02:28:48.600 --> 02:28:49.800]   Hey, you design.
[02:28:49.800 --> 02:28:50.760]   Come on back.
[02:28:50.760 --> 02:28:52.360]   All my gear is peak design.
[02:28:52.360 --> 02:28:53.800]   I just I bought.
[02:28:53.800 --> 02:28:57.720]   I just bought and I'm waiting for their latest Kickstarter,
[02:28:57.720 --> 02:29:01.320]   which is a, you know, a little handle that goes on your camera.
[02:29:01.320 --> 02:29:01.640]   Yeah.
[02:29:01.640 --> 02:29:04.200]   Um, they they work with the gimbals too.
[02:29:04.200 --> 02:29:06.840]   I want to say they didn't they do something with a gimbal?
[02:29:06.840 --> 02:29:09.400]   They might they have this, you know, thing that they,
[02:29:09.400 --> 02:29:12.680]   uh, this set that let me just draw on a blank.
[02:29:12.680 --> 02:29:14.360]   No, I can't remember squat today.
[02:29:14.360 --> 02:29:15.240]   It's been that kind of.
[02:29:15.240 --> 02:29:16.680]   Trellis just calling all the trellis.
[02:29:16.680 --> 02:29:17.960]   Trellis to trellis.
[02:29:17.960 --> 02:29:18.600]   To trellis.
[02:29:18.600 --> 02:29:20.360]   Peak design trellis is quite famous.
[02:29:20.360 --> 02:29:21.640]   Let's see.
[02:29:21.640 --> 02:29:24.360]   This is what the capture clip.
[02:29:24.360 --> 02:29:26.680]   This is what I got the micro clutch.
[02:29:26.680 --> 02:29:26.920]   All right.
[02:29:26.920 --> 02:29:28.280]   That's the last thing I remember seeing.
[02:29:28.280 --> 02:29:28.600]   Yeah.
[02:29:28.600 --> 02:29:30.840]   I have I have their clutch and I love their clutch.
[02:29:30.840 --> 02:29:31.800]   Actually, I'm waiting for it.
[02:29:31.800 --> 02:29:33.480]   That was micro because I always do this.
[02:29:33.480 --> 02:29:36.680]   I pledged and I keep forgetting that I'm going to have to
[02:29:36.680 --> 02:29:38.280]   get an email where I fill in the survey.
[02:29:38.280 --> 02:29:39.160]   I'm not going to get it.
[02:29:39.160 --> 02:29:40.520]   Oh, always forget that.
[02:29:40.520 --> 02:29:40.920]   Come on.
[02:29:40.920 --> 02:29:41.400]   Man.
[02:29:41.400 --> 02:29:41.880]   Come on.
[02:29:41.880 --> 02:29:42.360]   Man.
[02:29:42.360 --> 02:29:43.160]   Gotta do better.
[02:29:43.160 --> 02:29:45.480]   But yeah, that's that's my pick.
[02:29:45.480 --> 02:29:47.480]   And then my last one, I wanted to throw this out here.
[02:29:47.480 --> 02:29:50.600]   Because this has been an interesting day for me for whatever reason.
[02:29:50.600 --> 02:29:52.040]   And I noticed this is first world problem.
[02:29:52.040 --> 02:29:53.480]   So I won't get into all the details,
[02:29:53.480 --> 02:29:56.280]   but I had to have a reboot this afternoon.
[02:29:56.280 --> 02:30:02.040]   And this road titled by Mr. Yellowgold himself,
[02:30:03.720 --> 02:30:05.480]   our very own Jason Howell.
[02:30:05.480 --> 02:30:06.680]   Such a great song.
[02:30:06.680 --> 02:30:07.480]   Go check it out.
[02:30:07.480 --> 02:30:08.440]   Oh, you're kidding me.
[02:30:08.440 --> 02:30:08.680]   Yeah.
[02:30:08.680 --> 02:30:09.800]   That's the name of the album.
[02:30:09.800 --> 02:30:11.560]   Is there a song that you.
[02:30:11.560 --> 02:30:13.160]   Yeah, this road is this road.
[02:30:13.160 --> 02:30:14.200]   Oh, that's the name of the song.
[02:30:14.200 --> 02:30:17.160]   Jason, oh, I have to sign up.
[02:30:17.160 --> 02:30:18.200]   Oh, well.
[02:30:18.200 --> 02:30:19.720]   Oh, wait a minute.
[02:30:19.720 --> 02:30:21.240]   I have a spotify account.
[02:30:21.240 --> 02:30:22.040]   At least we know.
[02:30:22.040 --> 02:30:22.760]   Wait a minute.
[02:30:22.760 --> 02:30:23.960]   He won't take us down on YouTube.
[02:30:23.960 --> 02:30:24.120]   Wait a minute.
[02:30:24.120 --> 02:30:25.400]   I have a spotify account.
[02:30:25.400 --> 02:30:26.040]   Hold on there.
[02:30:26.040 --> 02:30:26.920]   I don't pay for it.
[02:30:26.920 --> 02:30:28.920]   Oh, incorrect username or password.
[02:30:28.920 --> 02:30:29.800]   You struggle it.
[02:30:29.800 --> 02:30:30.440]   Uh oh.
[02:30:30.440 --> 02:30:31.720]   I'm in trouble here.
[02:30:31.720 --> 02:30:32.680]   Here we go.
[02:30:32.680 --> 02:30:33.560]   Let me play a little.
[02:30:33.560 --> 02:30:34.040]   Yeah.
[02:30:34.040 --> 02:30:36.120]   Well, this road, as we wind up,
[02:30:36.120 --> 02:30:39.160]   this edition of our show,
[02:30:39.160 --> 02:30:42.280]   this is Jason on all the instruments too, right?
[02:30:42.280 --> 02:30:43.720]   Yes.
[02:30:43.720 --> 02:30:44.280]   Very nice.
[02:30:44.280 --> 02:30:44.920]   Yellowgold.
[02:30:44.920 --> 02:30:46.280]   It's the name of his band.
[02:30:46.280 --> 02:30:47.240]   It's so talented.
[02:30:47.240 --> 02:30:48.040]   Very talented.
[02:30:48.040 --> 02:30:49.400]   I don't know what he's doing.
[02:30:49.400 --> 02:30:49.480]   We're from Jason.
[02:30:49.480 --> 02:30:50.680]   Doing, working for me.
[02:30:50.680 --> 02:30:51.080]   But anyway.
[02:30:51.080 --> 02:30:54.040]   Just a good song and just sit back and.
[02:30:54.040 --> 02:30:55.880]   That's you singing?
[02:30:55.880 --> 02:30:57.480]   Jason.
[02:30:57.480 --> 02:30:59.240]   I love it.
[02:30:59.240 --> 02:31:00.840]   Have you never heard Jason before?
[02:31:00.840 --> 02:31:02.840]   [LAUGH]
[02:31:02.840 --> 02:31:06.920]   That's good.
[02:31:06.920 --> 02:31:07.480]   I like it.
[02:31:07.480 --> 02:31:08.280]   Well, frickin'
[02:31:08.280 --> 02:31:09.160]   Yellowgold.
[02:31:09.160 --> 02:31:11.320]   This road on the fever dreamer album.
[02:31:11.320 --> 02:31:14.360]   Our big hit this week.
[02:31:14.360 --> 02:31:15.000]   What's that?
[02:31:15.000 --> 02:31:20.680]   Once upon a time, there was a guy named Jason Hell.
[02:31:20.680 --> 02:31:23.960]   Six foot four and a bundle of fun.
[02:31:23.960 --> 02:31:26.280]   See, you feel better already.
[02:31:26.280 --> 02:31:26.760]   I do.
[02:31:26.760 --> 02:31:28.680]   I, I, you just, I, it's summer time.
[02:31:28.680 --> 02:31:29.000]   So.
[02:31:29.720 --> 02:31:31.640]   Aunt Pruitt, catch him, uh,
[02:31:31.640 --> 02:31:33.160]   well here every week, of course,
[02:31:33.160 --> 02:31:35.240]   and is one of the hosts of this week in Google.
[02:31:35.240 --> 02:31:38.920]   And he is also our community manager for the club.
[02:31:38.920 --> 02:31:40.600]   Get on in there and join the club.
[02:31:40.600 --> 02:31:42.360]   If you're not already, remember seven bucks a month.
[02:31:42.360 --> 02:31:45.880]   Gets you ant and add free versions of all our shows.
[02:31:45.880 --> 02:31:46.280]   Exhale.
[02:31:46.280 --> 02:31:47.640]   This is the content.
[02:31:47.640 --> 02:31:49.160]   All the bonus content we do,
[02:31:49.160 --> 02:31:51.480]   including shows we don't put out anywhere else.
[02:31:51.480 --> 02:31:53.880]   Including the AI show when it starts coming out.
[02:31:53.880 --> 02:31:55.400]   That's going to be a club only.
[02:31:55.400 --> 02:31:57.880]   And you'll know it does a very important thing,
[02:31:57.880 --> 02:31:59.080]   which is support.
[02:31:59.080 --> 02:32:01.080]   This wonderful team we've put together here.
[02:32:01.080 --> 02:32:01.640]   That's right.
[02:32:01.640 --> 02:32:03.800]   At, at the Twit Studios.
[02:32:03.800 --> 02:32:04.360]   That's right.
[02:32:04.360 --> 02:32:05.880]   Sign up, join the discord,
[02:32:05.880 --> 02:32:09.560]   and hop into the upcoming photo critique.
[02:32:09.560 --> 02:32:10.600]   Live photo critique.
[02:32:10.600 --> 02:32:10.920]   That's coming out.
[02:32:10.920 --> 02:32:11.720]   I have some fun.
[02:32:11.720 --> 02:32:12.280]   August 4th.
[02:32:12.280 --> 02:32:14.520]   Take pictures of something that says
[02:32:14.520 --> 02:32:16.040]   "Coffee time to you."
[02:32:16.040 --> 02:32:16.520]   Okay.
[02:32:16.520 --> 02:32:16.520]   Okay.
[02:32:16.520 --> 02:32:17.000]   Much of that.
[02:32:17.000 --> 02:32:17.640]   Easy.
[02:32:17.640 --> 02:32:20.920]   And, uh, Twit.tv/ClipTwit is the address.
[02:32:20.920 --> 02:32:21.720]   That's right.
[02:32:21.720 --> 02:32:22.520]   Thank you so much.
[02:32:22.520 --> 02:32:24.600]   Ladies and gentlemen,
[02:32:24.600 --> 02:32:27.000]   Jeff Jarvis is the director of the Townite Center
[02:32:27.000 --> 02:32:28.200]   for Entrepreneurial Journalism.
[02:32:28.200 --> 02:32:28.840]   I got to go.
[02:32:28.840 --> 02:32:28.840]   Great.
[02:32:28.840 --> 02:32:29.400]   Come on out.
[02:32:29.400 --> 02:32:30.120]   Graduate schools.
[02:32:30.120 --> 02:32:30.600]   It's just a plug.
[02:32:30.600 --> 02:32:32.120]   I'm chair hall of the city university.
[02:32:32.120 --> 02:32:32.600]   I got to go.
[02:32:32.600 --> 02:32:34.120]   But more importantly,
[02:32:34.120 --> 02:32:36.200]   he is going to be speaking
[02:32:36.200 --> 02:32:39.000]   at the fabulous Commonwealth Club.
[02:32:39.000 --> 02:32:40.440]   And if you go to
[02:32:40.440 --> 02:32:44.840]   bit.ly/herejef,
[02:32:44.840 --> 02:32:45.960]   H-E-A-R
[02:32:45.960 --> 02:32:48.120]   J-E-F-F.
[02:32:48.120 --> 02:32:50.360]   You can use the code.
[02:32:50.360 --> 02:32:51.240]   Oh, what's the code?
[02:32:51.240 --> 02:32:53.000]   Jarvis CWC for $10 off.
[02:32:53.000 --> 02:32:55.720]   Jeff is going to be talking about his latest book,
[02:32:55.720 --> 02:32:56.920]   The Gutenberg Parenthesis.
[02:32:56.920 --> 02:32:59.320]   The title of the talk is The Age of Print.
[02:32:59.320 --> 02:33:00.040]   And the internet.
[02:33:00.040 --> 02:33:03.080]   Please, if you're in the Bay Area,
[02:33:03.080 --> 02:33:04.040]   come see Jeff,
[02:33:04.040 --> 02:33:08.040]   July 25th, 6 p.m. Pacific.
[02:33:08.040 --> 02:33:09.880]   That's this coming Tuesday.
[02:33:09.880 --> 02:33:11.480]   If you're not in the Bay Area,
[02:33:11.480 --> 02:33:12.760]   you can do it online as well.
[02:33:12.760 --> 02:33:13.480]   And with that code,
[02:33:13.480 --> 02:33:14.520]   it'll cost you nothing.
[02:33:14.520 --> 02:33:16.760]   What's the code again, Jeff?
[02:33:16.760 --> 02:33:19.240]   Jarvis CWC.
[02:33:19.240 --> 02:33:21.400]   Jarvis CWC.
[02:33:21.400 --> 02:33:22.440]   CWC.
[02:33:22.440 --> 02:33:23.400]   Commonwealth.
[02:33:23.400 --> 02:33:24.040]   Oh, I get it.
[02:33:24.040 --> 02:33:24.760]   CWC.
[02:33:25.960 --> 02:33:27.400]   We thank you for the plug.
[02:33:27.400 --> 02:33:29.560]   I can't go because I'll be doing the security now,
[02:33:29.560 --> 02:33:31.000]   but Lisa's going to go.
[02:33:31.000 --> 02:33:32.360]   I'll forgive you if you plug it on Twitch.
[02:33:32.360 --> 02:33:33.560]   And I will plug it on Twitch,
[02:33:33.560 --> 02:33:34.600]   and everybody should go.
[02:33:34.600 --> 02:33:36.920]   Because it's...
[02:33:36.920 --> 02:33:39.800]   I love seeing Twig put fans at these events.
[02:33:39.800 --> 02:33:40.360]   Yeah.
[02:33:40.360 --> 02:33:43.640]   Yeah, and you know how much you love Jeff and his insights.
[02:33:43.640 --> 02:33:47.000]   Or hate me and want to come and make fun of me.
[02:33:47.000 --> 02:33:48.600]   No haters allowed.
[02:33:48.600 --> 02:33:50.520]   All right.
[02:33:50.520 --> 02:33:51.000]   So...
[02:33:51.000 --> 02:33:52.120]   You can make a poster that says
[02:33:52.120 --> 02:33:53.000]   "Moral Panic."
[02:33:53.000 --> 02:33:54.200]   You can just bring it up.
[02:33:54.200 --> 02:33:54.760]   Oh, yes.
[02:33:54.760 --> 02:33:56.280]   Everybody bring your moral panic.
[02:33:56.280 --> 02:33:56.840]   Or...
[02:33:56.840 --> 02:34:00.040]   Bring your stickers and get them autographed.
[02:34:00.040 --> 02:34:03.960]   That's awesome.
[02:34:03.960 --> 02:34:04.520]   I love you.
[02:34:04.520 --> 02:34:05.640]   Only for club members.
[02:34:05.640 --> 02:34:06.280]   Only.
[02:34:06.280 --> 02:34:08.280]   Well, we don't actually know one has these stickers.
[02:34:08.280 --> 02:34:09.160]   No one has them yet.
[02:34:09.160 --> 02:34:11.640]   I want to...
[02:34:11.640 --> 02:34:15.560]   Joe made them and Mr. Victor got them on the vinyl.
[02:34:15.560 --> 02:34:18.280]   A victor has a like a sticker maker?
[02:34:18.280 --> 02:34:19.160]   Yes, he does.
[02:34:19.160 --> 02:34:19.800]   Cricket?
[02:34:19.800 --> 02:34:21.080]   Mr. Victor is awesome.
[02:34:21.080 --> 02:34:22.600]   Yeah.
[02:34:22.600 --> 02:34:23.080]   Wow.
[02:34:23.080 --> 02:34:24.600]   The Twig editor.
[02:34:24.600 --> 02:34:27.960]   Stacey Higginbotham is at StaceyOnIOT.com.
[02:34:27.960 --> 02:34:29.240]   She just woke up.
[02:34:29.240 --> 02:34:30.760]   Great to see you Stacey.
[02:34:30.760 --> 02:34:35.320]   Her podcast, the Internet of Things podcast,
[02:34:35.320 --> 02:34:39.320]   featuring Kevin Tofel, is available at StaceyOnIOT.com.
[02:34:39.320 --> 02:34:41.160]   Oh, y'all.
[02:34:41.160 --> 02:34:42.600]   If you like the cybersecurity level,
[02:34:42.600 --> 02:34:43.720]   you should listen to it.
[02:34:43.720 --> 02:34:46.440]   One, because you'll learn more about it with my guest this week.
[02:34:46.440 --> 02:34:49.000]   And two, he sounds just like Fred Rogers,
[02:34:49.000 --> 02:34:49.720]   and it's a delight.
[02:34:49.720 --> 02:34:52.760]   I can't wait.
[02:34:52.760 --> 02:34:54.600]   And you'll find that in all of the stuff she does,
[02:34:54.600 --> 02:35:00.040]   including her free newsletter at staceyonIOT.com.
[02:35:00.040 --> 02:35:03.640]   We do this week in Google every Wednesday, 2 p.m. Pacific,
[02:35:03.640 --> 02:35:06.440]   5 p.m. Eastern 2100 UTC.
[02:35:06.440 --> 02:35:08.360]   If you want to watch live, you can.
[02:35:08.360 --> 02:35:11.480]   There's a live stream audio and video, actually,
[02:35:11.480 --> 02:35:13.480]   at live.twit.tv.
[02:35:13.480 --> 02:35:16.280]   If you're watching live chat live in our open to all chatroom,
[02:35:16.280 --> 02:35:17.880]   irc.twit.tv.
[02:35:17.880 --> 02:35:20.520]   Just go in there with a browser.
[02:35:20.520 --> 02:35:22.520]   That's all you need, although if you have an IRC client,
[02:35:22.520 --> 02:35:25.320]   if you really want to rock at old school, you can do that.
[02:35:25.320 --> 02:35:29.880]   After the fact-on-demand versions of the show available at twit.tv/twig,
[02:35:29.880 --> 02:35:35.480]   you can also get shows on the dedicated YouTube channel,
[02:35:35.480 --> 02:35:37.800]   youtube.com/thisweekingoogle.
[02:35:37.800 --> 02:35:40.120]   Or, that's the thing to do of all,
[02:35:40.120 --> 02:35:43.240]   subscribe in your favorite podcast player.
[02:35:43.240 --> 02:35:46.520]   And that way you'll get it automatically the minute it is available.
[02:35:46.520 --> 02:35:48.200]   Thanks for joining us.
[02:35:48.200 --> 02:35:49.320]   We really appreciate it.
[02:35:49.320 --> 02:35:51.400]   I hope you'll come back next week.
[02:35:52.040 --> 02:35:56.840]   For Aunt Stacy, Jeff, and some old guy falling asleep in a chair.
[02:35:56.840 --> 02:35:59.800]   And I hope you'll tell some other folks about the show too.
[02:35:59.800 --> 02:36:02.360]   Just remember, it's trellis.
[02:36:02.360 --> 02:36:02.840]   Trellis.
[02:36:02.840 --> 02:36:03.880]   We'll see you next time.
[02:36:03.880 --> 02:36:04.280]   Bye-bye.
[02:36:04.280 --> 02:36:09.880]   Hey, we should talk Linux.
[02:36:09.880 --> 02:36:11.640]   It's the operating system that runs the internet,
[02:36:11.640 --> 02:36:13.560]   but you game consoles, cell phones,
[02:36:13.560 --> 02:36:15.640]   and maybe even the machine on your desk.
[02:36:15.640 --> 02:36:17.080]   You already knew all that.
[02:36:17.080 --> 02:36:20.120]   What you may not know is that Twit now is a show dedicated to it,
[02:36:20.120 --> 02:36:22.040]   the untitled Linux show.
[02:36:22.040 --> 02:36:23.560]   Whether you're a Linux pro,
[02:36:23.560 --> 02:36:24.760]   a burgeoning cishet man,
[02:36:24.760 --> 02:36:26.600]   or just curious what the big deal is,
[02:36:26.600 --> 02:36:28.760]   you should join us on the Club Twit Discord
[02:36:28.760 --> 02:36:32.200]   every Saturday afternoon for news, analysis,
[02:36:32.200 --> 02:36:34.600]   and tips to sharpen your Linux skills.
[02:36:34.600 --> 02:36:36.280]   And then make sure you subscribe
[02:36:36.280 --> 02:36:39.880]   to the Club Twit exclusive untitled Linux show.
[02:36:39.880 --> 02:36:42.200]   Wait, you're not a Club Twit member yet?
[02:36:42.200 --> 02:36:45.640]   We'll go to twit.tv/clubtwit and sign up.
[02:36:45.640 --> 02:36:46.440]   Hope to see you there.
[02:36:46.440 --> 02:36:50.440]   [MUSIC PLAYING]
[02:36:50.440 --> 02:37:00.280]   When you're drinking frozen beverage from McDonald's,
[02:37:00.280 --> 02:37:03.800]   your brain may not like how refreshingly cold it is,
[02:37:03.800 --> 02:37:05.320]   but the rest of your body?
[02:37:05.320 --> 02:37:06.520]   Oh, yes.
[02:37:06.520 --> 02:37:09.320]   It's going to relish every moment of it,
[02:37:09.320 --> 02:37:11.000]   because there are drinks.
[02:37:11.000 --> 02:37:13.480]   Then there are drinks from McDonald's.
[02:37:13.480 --> 02:37:16.680]   Get all the chill you need for just $1.69,
[02:37:16.680 --> 02:37:18.280]   from any size frozen drink,
[02:37:18.280 --> 02:37:20.360]   like a frozen fan to Blue Raspberry,
[02:37:20.360 --> 02:37:22.200]   to a new ice cold lemonade.
[02:37:22.200 --> 02:37:23.720]   Prices and participation may vary,
[02:37:23.720 --> 02:37:25.400]   cannot be combined with any other offer.
[02:37:25.400 --> 02:37:27.640]   [MUSIC PLAYING]
[02:37:27.640 --> 02:37:31.080]   How should you plan for when your home becomes too small?
[02:37:31.080 --> 02:37:33.720]   Or when the next one gets too big?
[02:37:33.720 --> 02:37:35.080]   At Sandy Spring Bank,
[02:37:35.080 --> 02:37:37.400]   we're here to help create personalized solutions
[02:37:37.400 --> 02:37:39.480]   for financing your home loan.
[02:37:39.480 --> 02:37:42.120]   Whether it's a new home or refinance, renovation,
[02:37:42.120 --> 02:37:45.240]   or addition, fixer upper or new build.
[02:37:45.240 --> 02:37:47.160]   Banking is a conversation.
[02:37:47.160 --> 02:37:49.000]   Let's talk about your mortgage.
[02:37:49.000 --> 02:37:52.120]   Visit sandyspringbank.com/mortgage.
[02:37:52.120 --> 02:37:53.880]   Mortgage home equity and other credit products
[02:37:53.880 --> 02:37:57.000]   offered by Sandy Spring Bank, Eagle Housing Lender.

