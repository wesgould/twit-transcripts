
[00:00:00.000 --> 00:00:04.880]   It's time for Mac Break Weekly Andy and Renee and Alex are all here and we're all excited because
[00:00:04.880 --> 00:00:10.240]   there's an Apple event coming up in a week. We'll talk about what to expect, what not to expect.
[00:00:10.240 --> 00:00:16.160]   Also look at Apple's future in displays in self-driving vehicles. It's all coming up next
[00:00:16.160 --> 00:00:36.960]   a Mac Break Weekly.
[00:00:36.960 --> 00:00:50.080]   This is Mac Break Weekly, episode 602 recorded Tuesday, March 20th, 2018. I'm the Cobb Baron.
[00:00:50.080 --> 00:00:58.000]   Mac Break Weekly is brought to you by Wink, the best way to discover new wines you'll love. Go
[00:00:58.000 --> 00:01:06.000]   to trywink.com/macbreak and get $20 off your first shipment. And by wordpress, reach more customers
[00:01:06.000 --> 00:01:12.720]   when you build your business website at wordpress.com. Plan start at just $20 a month. Get 15% off
[00:01:12.720 --> 00:01:21.520]   any new plan at wordpress.com/macbreak and by Lighthouse with 3D and AI. Lighthouse is the only
[00:01:21.520 --> 00:01:27.280]   camera that gets your busy life at home. Get 15% off a Lighthouse camera with promo code TWIT
[00:01:27.280 --> 00:01:34.880]   at light.house. It's time for Mac Break Weekly, the show we talk about the latest Apple News.
[00:01:35.040 --> 00:01:41.600]   >> You're joining me right now from Montreal where the snow falls from the sky. You're
[00:01:41.600 --> 00:01:47.760]   around. It's Renee Richie at imore.com. Hello Renee. It's the first day of spring so it's got to be
[00:01:47.760 --> 00:01:52.080]   covered in snow and sub freezing temperatures. Rightly otherwise it wouldn't be Canadian.
[00:01:52.080 --> 00:01:57.440]   >> Happy. Now in the United States, we say March comes in like a lion and goes out like a lamb.
[00:01:57.440 --> 00:02:01.600]   But is that applied to Canadian? >> I think so. Isn't it like April showers,
[00:02:01.600 --> 00:02:05.440]   bring late flowers? You have to go from month to month. Every month everything is like time
[00:02:05.440 --> 00:02:11.600]   and space just ends. >> Anyway, it's great to see you Renee and everybody's raving about your
[00:02:11.600 --> 00:02:16.080]   vector. I haven't heard yet the vector podcast on Apple Education initiatives. We'll talk about
[00:02:16.080 --> 00:02:21.760]   that in just a second. >> Thank you. >> Looking forward to it also from Chicago in the Chicago
[00:02:21.760 --> 00:02:28.960]   Sun times but he's not in Chicago. He's Andy Anaco and he's somewhere else. >> And we're expecting
[00:02:28.960 --> 00:02:35.280]   another Norista tomorrow. >> There he is. The Norista Andy in the Norista.
[00:02:35.280 --> 00:02:41.520]   In Washington DC where the snow is about to come, Alex Lindsay. It's coming from the nation's capital.
[00:02:41.520 --> 00:02:45.600]   >> Rain this morning. >> Is this a live shot behind you or still?
[00:02:45.600 --> 00:02:50.240]   >> It's a still. >> Oh, so it's not a live shot. >> We won't see the snow start to come down or
[00:02:50.240 --> 00:02:54.720]   anything. >> No, I'm working on it. I'm working on it. >> And added in post.
[00:02:56.320 --> 00:03:02.960]   >> No, no, no. We're in DC. I'm in like, I'm not exactly where this picture is but I'm not very
[00:03:02.960 --> 00:03:11.200]   far away from it. >> Where was it? >> Were you with me? We did something on the roof of the CNN
[00:03:11.200 --> 00:03:18.560]   studios in DC and you're sitting on the roof but you're outside and the live white, it's not even a
[00:03:18.560 --> 00:03:24.800]   blue screen is behind you. >> Yeah, that's gone. >> That was on the top of that.
[00:03:24.800 --> 00:03:29.520]   That was it? No, no, no, it was just the company doing it, the rent got too high.
[00:03:29.520 --> 00:03:37.520]   So that's it 16th and I and that is a, they had, what happened was, is that they had, I think,
[00:03:37.520 --> 00:03:42.080]   three or four different tents there where it oversees the White House.
[00:03:42.080 --> 00:03:45.280]   >> Yeah, you can see the snikers on the roof moving around. It was wild.
[00:03:45.280 --> 00:03:48.000]   >> To make, well, and they're all paying attention to that.
[00:03:48.000 --> 00:03:50.720]   >> They're watching that. >> Like that is. >> Yeah, yeah.
[00:03:50.720 --> 00:03:56.320]   >> Were you with me? Was that with you that I did that? >> No, no, we've just done, we've done
[00:03:56.320 --> 00:03:59.840]   stuff in that building and that, where we can walk up to that. >> I remember why it was there.
[00:03:59.840 --> 00:04:04.160]   I think maybe it was the screen savers, the old screen savers we did a segment up there.
[00:04:04.160 --> 00:04:10.160]   >> Yeah, the interesting thing is that then to, I think, to make a little extra money, they put a
[00:04:10.160 --> 00:04:14.400]   little camera and made it available to a new organization. The new organization didn't come
[00:04:14.400 --> 00:04:20.320]   over anymore. They were like, well, I got what I need. So right down the hallway, there's three
[00:04:20.320 --> 00:04:25.520]   more insert studios right on the other side of this wall. And right down the hallway, they have
[00:04:25.520 --> 00:04:29.360]   three keyers that are pointed at the White House. >> Oh, how fun.
[00:04:29.360 --> 00:04:33.680]   >> So that European news folks can jump on in front of the White House anytime they want.
[00:04:33.680 --> 00:04:37.760]   And it's not cold. And it's not in that little area that's right inside of the Westgate.
[00:04:37.760 --> 00:04:42.400]   >> Nice. It was weird to be outside on that building knowing the snikers were looking at you the whole
[00:04:42.400 --> 00:04:43.680]   time. >> Oh, yeah.
[00:04:43.680 --> 00:04:48.880]   >> And your backs turned to them. And it was a little strange doing that. I was never quite
[00:04:48.880 --> 00:04:50.320]   comfortable. >> Yeah, we joke not to do anything.
[00:04:50.320 --> 00:04:52.800]   >> Yeah, no sudden movements. >> No.
[00:04:52.800 --> 00:04:59.520]   >> So speaking of sudden movements, Apple, with a little Apple food, announced that they're going
[00:04:59.520 --> 00:05:05.040]   to have an event, March 27th. We were thinking last week, oh, they can't possibly have an
[00:05:05.040 --> 00:05:10.480]   event. It's too late now. Well, turns out, nobody was looking at Lane Tech College Prep High School
[00:05:10.480 --> 00:05:15.520]   in Chicago. Nobody was thinking, is that booked up? Well, I mean, they built the Steve Jobs
[00:05:15.520 --> 00:05:20.480]   Theatre Leo. So obviously they're going to go to Chicago. >> But it's an education event. Actually,
[00:05:20.480 --> 00:05:28.320]   I love the calligraphy in the Apple on let's take a field trip, it says. And so there's some
[00:05:28.320 --> 00:05:32.560]   speculation because of that calligraphy that maybe an Apple pencil be one of the announcements.
[00:05:32.560 --> 00:05:35.280]   >> Yeah. Wouldn't that be nice? >> I think so.
[00:05:35.280 --> 00:05:37.440]   >> What would you do to make a better Apple pencil though?
[00:05:37.440 --> 00:05:41.680]   >> Well, it's not just the better Apple pencil, but it's pushing the Apple pencil across the
[00:05:41.680 --> 00:05:46.080]   iPad line. Right now, the Apple pencil is restricted to the iPad's Pro. And one of the
[00:05:46.080 --> 00:05:50.000]   analysts' analysts' forecasts for this year was that Apple can make the Apple pencil available
[00:05:50.000 --> 00:05:54.080]   widely across the line, especially on the education models, which started a much, much lower price
[00:05:54.080 --> 00:05:57.200]   point. >> You don't have to speculate too much on what Apple's going to be talking about because
[00:05:57.200 --> 00:06:04.000]   we know that their new class kit will be revealed, debuted with iOS 11.3. So that's any day now.
[00:06:04.000 --> 00:06:10.640]   I'm sure that's topic number one on this thing, right? Yeah. A lot of the problems with the
[00:06:10.640 --> 00:06:18.080]   iPads in education right now is just simple admin and simple management. So if they simply pitch
[00:06:18.080 --> 00:06:23.360]   to the one or three admins in every school that it'll be so easy to maintain a card of iPad Pros
[00:06:23.360 --> 00:06:27.600]   or iPads for now on, that's going to help them stem the tide against Chromebooks.
[00:06:27.600 --> 00:06:31.600]   >> Do we know what class kit? Go ahead, Alex.
[00:06:31.600 --> 00:06:36.480]   >> They're pretty far behind. I mean, so this is one of the few places where I think Apple is
[00:06:36.480 --> 00:06:42.720]   really trying to catch up. They are way behind in education behind Google. I mean, Google has
[00:06:42.720 --> 00:06:48.560]   really made the inroads that Apple had hoped to make. They came out strong with the iPad, but the
[00:06:48.560 --> 00:06:54.240]   Chromebook really has by far, and it really came down to exactly what Andy was talking about,
[00:06:54.240 --> 00:06:58.960]   which is administration turned out to be one of the big things. Having a computer that
[00:06:58.960 --> 00:07:04.000]   kids can type on is part of it, but a big piece of it was when the iPads came out, they were just a
[00:07:04.000 --> 00:07:10.480]   disaster to administer for thousands of users. It just didn't work at all for the way schools work.
[00:07:10.480 --> 00:07:16.800]   >> Do we say another quick problem is that, excuse me, another quick advantage of the Chromebook
[00:07:16.800 --> 00:07:22.160]   is that most of the schools that are using Chromebooks, the Chromebook is just a wrapper for the Google
[00:07:22.160 --> 00:07:29.840]   Education software, and that software that they can deploy on a number of different legacy hardware
[00:07:29.840 --> 00:07:35.520]   that they have. They can also have some expectation that the kids homes are going to have something
[00:07:35.520 --> 00:07:40.560]   that can run. If that runs a web browser, you can run these apps. So they just provided a much
[00:07:40.560 --> 00:07:47.120]   tidier and much more flexible solution. And I was curious to see if they were going to make a big
[00:07:47.120 --> 00:07:52.160]   push back towards education. I don't think they'd ever abandon it, but it seemed as one of the
[00:07:52.160 --> 00:07:56.720]   things they seemed to be doing to address all of this was to pivot the iPad a little bit more
[00:07:56.720 --> 00:08:01.920]   towards enterprise. They've been really, really closely tied to IBM, for instance,
[00:08:01.920 --> 00:08:07.520]   providing apps and services to make iPads work much better for specific business purposes.
[00:08:07.520 --> 00:08:12.080]   Just I think just today, there was a news release that they're integrating IBM Watson
[00:08:12.080 --> 00:08:19.040]   development, their AI development into iOS as part of their of iOS's own machine learning system.
[00:08:19.040 --> 00:08:24.800]   So that if you've developed a model using IBM Watson, there's a very, very easy way,
[00:08:24.800 --> 00:08:30.160]   or easy issue way to convert that model sort of work with iOS so that you can have that business
[00:08:30.160 --> 00:08:37.200]   business app working very, very well on an iPad as well. So it's very, very good to see that not
[00:08:37.200 --> 00:08:43.280]   only are they finding, it would have been nice if they've simply found 20 minutes during a keynote
[00:08:43.280 --> 00:08:47.040]   to talk about education, it's even better that they're saying, no, we're actually holding an
[00:08:47.040 --> 00:08:51.520]   event in Chicago at a school, and we're making this the entire theme of the presentation.
[00:08:51.520 --> 00:08:57.200]   That's the sort of stuff you like to see. You like to see that they're, that helps to convince
[00:08:57.200 --> 00:09:01.840]   you that they are still very, very excited and interested in education as opposed to simply not
[00:09:01.840 --> 00:09:08.000]   willing to give up that part of the market yet. What, will we see hardware Renee? What do you think?
[00:09:08.000 --> 00:09:13.520]   Yeah, I mean, the rumor is that we'll see a new version of the low cost iPad. Last year's was
[00:09:13.520 --> 00:09:18.240]   a bit of an iPad Air 1.5. It didn't have the laminated screen, but it had some of the other
[00:09:18.240 --> 00:09:22.640]   features that the iPad Air 2 did and the rumors that will get updated and maybe the price will get
[00:09:22.640 --> 00:09:27.120]   pushed down a little bit further. But it's still a total cost of ownership equation for Apple.
[00:09:27.120 --> 00:09:30.800]   For example, when you get the Chromebook, you get the screen, you get the keyboard with the iPad,
[00:09:30.800 --> 00:09:34.720]   they have discounted keyboards. They do in partnership with Logitech, but they're still as to the total
[00:09:34.720 --> 00:09:38.880]   price of the iPad. And if you add an Apple pencil to it, it adds to the total price of the iPad.
[00:09:38.880 --> 00:09:42.800]   And then if you want to do things with HDMI or VGA, you have to buy dongles and that adds
[00:09:42.800 --> 00:09:47.840]   total price of the, I mean, dongles at scale is still expensive. So it just, the price starts
[00:09:47.840 --> 00:09:52.960]   creeping up. And then there's a couple other issues too, like Apple, Google's ID management
[00:09:52.960 --> 00:09:56.800]   isn't perfect. For example, if you get a school ID with Google, then you leave that school,
[00:09:56.800 --> 00:10:00.640]   you lose that ID and a lot of people have lost accounts that were just connected to those IDs.
[00:10:00.640 --> 00:10:04.480]   But at least I make it super simple to spin one up to get all the email and contact management
[00:10:04.480 --> 00:10:08.480]   and calendars with it to get access to docs and things where Apple still has a lot of legacy
[00:10:08.480 --> 00:10:13.760]   systems with disparate logins. And it's very hard. You can add email, you can add, you can add user
[00:10:13.760 --> 00:10:17.120]   accounts, but they don't necessarily have email with them. Then you can increase the storage if you
[00:10:17.120 --> 00:10:22.480]   want to. So throughout the entire chain, I think the hardware in some ways will be the easy part
[00:10:22.480 --> 00:10:26.720]   for Apple and the classroom software and things like Swift will be the easy software, but sorry,
[00:10:26.720 --> 00:10:31.200]   it'll be the easy part. But the glue that holds it all together from creating the accounts to having
[00:10:31.200 --> 00:10:36.800]   iWork really work as something that can be Google Docs-ish or Microsoft Office-ish, because Google
[00:10:36.800 --> 00:10:41.680]   goes out of their way, it seems sometimes not to make docs a great experience on the iPad.
[00:10:41.680 --> 00:10:46.160]   And then just having everything else that the students can count on. I mean, Google is pretty
[00:10:46.160 --> 00:10:49.520]   good at protecting the privacy of kids, but it's also indoctrinating all these kids into using
[00:10:49.520 --> 00:10:54.400]   Google services, which means that once they leave school, they'll be handing over vast amounts of data.
[00:10:54.400 --> 00:10:58.720]   So to me, it's critical to have competition in this area and to have options. But I think
[00:10:58.720 --> 00:11:03.680]   until Apple decides to treat education like an end-to-end product, a completely differentiated and very
[00:11:03.680 --> 00:11:06.240]   good experience, it's going to be a bit of a piecemeal thing.
[00:11:06.240 --> 00:11:11.840]   It is kind of now. They've got a classroom app that's a couple of years old, but really seems
[00:11:11.840 --> 00:11:18.080]   more about managing multiple iPads in a classroom. The last time you, but it's not really connected,
[00:11:18.080 --> 00:11:21.360]   and then they have the new Swift Playgrounds, but it's not really connected to anything.
[00:11:21.360 --> 00:11:25.200]   Swift Playgrounds seems to be the best thing they've got, really, for school.
[00:11:25.200 --> 00:11:30.000]   Well, they've got a telling argument that in the midst of a Google Docs has a huge advantage,
[00:11:30.000 --> 00:11:33.280]   but Google's not really addressing code as a primary language, which is something that's very
[00:11:33.280 --> 00:11:36.800]   near and dear to Tim Cook's heart. And if they could provide a product that essentially worked
[00:11:36.800 --> 00:11:40.960]   with iWork and iLife also, a lot of companies don't have an iLife equivalent that just gave you a
[00:11:40.960 --> 00:11:45.680]   more well-rounded solution. That's the problem. A school is not going to buy a bunch of iPads
[00:11:45.680 --> 00:11:51.120]   just for the coding class. Yes. You need to be a full garage band or anything. Right. Exactly.
[00:11:51.120 --> 00:11:55.680]   Well, that's the unique problem that Apple has in education. They're business, until they pivot
[00:11:55.680 --> 00:12:00.640]   completely to becoming a services company, ha, ha, ha, ha. Their business has always been
[00:12:00.640 --> 00:12:06.400]   make, sell really well-made, almost luxury level, high markup hardware, and make that hardware
[00:12:06.400 --> 00:12:10.480]   compelling through the software that they build for it. The software is sort of the way that they
[00:12:10.480 --> 00:12:15.680]   sell the hardware. And education, that really doesn't work because you can't count on somebody
[00:12:15.680 --> 00:12:21.520]   saying, you can't count on a school district saying, appreciating the lamination of the screen
[00:12:21.520 --> 00:12:27.200]   on a solution that costs $300 more than a much cheaper thing made by HP or Dell that will
[00:12:27.200 --> 00:12:34.800]   do everything that they need to do. So it's where a company like Google or anybody else who's
[00:12:34.800 --> 00:12:40.640]   selling a solution of software that will burn on a Raspberry Pi for God's sake that has the edge
[00:12:40.640 --> 00:12:43.920]   up here. So Apple definitely faces a lot of challenges here.
[00:12:43.920 --> 00:12:50.800]   And I do think that, I mean, there are ways to throw a lot of money at the problem and make it,
[00:12:50.800 --> 00:12:56.080]   you know, and possibly make some inroads. And it'll be interesting to see what they do there. I do
[00:12:56.080 --> 00:13:04.560]   think that obviously being able to draw on a lower cost iPad is it would be really good.
[00:13:04.960 --> 00:13:09.280]   I think that Apple also could, they could theoretically get much more deep into, I think,
[00:13:09.280 --> 00:13:15.120]   into educational products. I think that there are, it's a much heavier lift to do that, but I think
[00:13:15.120 --> 00:13:19.760]   that there's a lot of opportunities for them to create something that would have people like the
[00:13:19.760 --> 00:13:28.240]   SWIFT programming become, you want to use that product line more often. And I don't know,
[00:13:28.240 --> 00:13:32.160]   and I think there's a, it'll be interesting to see, I mean, I think that their target may be magnet
[00:13:32.160 --> 00:13:40.320]   schools slash a variety of voucher schools, those types of things, things that are not necessarily
[00:13:40.320 --> 00:13:45.440]   in the general public system that can experiment a little bit more aggressively than what, I think
[00:13:45.440 --> 00:13:50.480]   that might have more money. The niche. Yeah. And they, but they have more, that's the niche.
[00:13:50.480 --> 00:13:57.360]   I mean, that might be Apple's niche, where they can provide that higher end solution that might be
[00:13:57.360 --> 00:14:01.920]   something that distinguishes that school. And they'd have to show some real things. I think that,
[00:14:01.920 --> 00:14:06.080]   I think that upgrading iBooks dramatically, I don't know if they're going to do that,
[00:14:06.080 --> 00:14:12.800]   but I think that upgrading the, the, the able to develop iBooks more effectively, I think, would,
[00:14:12.800 --> 00:14:18.480]   would make a big difference. I think that it just got, it was a good, it was an incredible start that
[00:14:18.480 --> 00:14:22.800]   needed two or three more revisions. If they were, if they revised it the same way that they revised,
[00:14:22.800 --> 00:14:30.000]   iWatches, you know, or Apple Watch, you know, they, they would, they would be doing very well.
[00:14:30.880 --> 00:14:34.400]   I think that, you know, the way they left it, which was kind of dead on the vine,
[00:14:34.400 --> 00:14:39.680]   I think has been a huge challenge because I think that Apple needs to, I mean, if they really want
[00:14:39.680 --> 00:14:44.160]   to get into the book area, if they really want to get into the school area, they need to change
[00:14:44.160 --> 00:14:48.640]   the paradigm that that books live in, you know, where it's heavily, you know, it's media rich,
[00:14:48.640 --> 00:14:53.360]   it's easy to develop. Those are the kind of things. I think that also, I'd be really
[00:14:53.360 --> 00:14:58.320]   interested to see if they talk about, you know, Swift, you know, I think that what's really missing
[00:14:58.320 --> 00:15:03.440]   for kids, looking at how my kids interact with Swift Playgrounds and, you know, my kids are 8 and 10.
[00:15:03.440 --> 00:15:11.280]   I think that what's really missing is a, and, you know, we'll see if it comes out as something
[00:15:11.280 --> 00:15:19.520]   like an advanced, some halfway between workflow and, and Swift, which is a nodal based, you know,
[00:15:19.520 --> 00:15:23.520]   programming language, you know, that basically lets you, if you look, if you think back on
[00:15:23.520 --> 00:15:28.480]   Quartz Composer, something that's a little like that, but not quite as geeky as that was, you know,
[00:15:28.480 --> 00:15:33.120]   but something that you can, you can draw out, like, I want to go do this, then this, then this,
[00:15:33.120 --> 00:15:37.680]   then this, and then I can compile that into code. You know, and I think that that would be an
[00:15:37.680 --> 00:15:44.000]   incredible way to get kids, to get hobbyists, to get a lot of other folks into programming
[00:15:44.000 --> 00:15:48.720]   without them having to figure out how to, you know, how to type everything, you know, and then
[00:15:48.720 --> 00:15:53.200]   they would get into the, into the actual programming because that will not do everything that you
[00:15:53.200 --> 00:15:58.240]   want to do. But I think that that visual version could be very powerful. It'd be really interesting
[00:15:58.240 --> 00:16:03.600]   to see if they take that on either here or WWDC or somewhere in the future.
[00:16:03.600 --> 00:16:11.280]   Why do people, why does a company like Apple, why does Google, why did Microsoft care so much
[00:16:11.280 --> 00:16:18.400]   about education? Is it because it's a profitable business or is it that old idea that you get
[00:16:18.400 --> 00:16:22.240]   kids when they're young and they're going to stick with the brand forever or is it both?
[00:16:22.240 --> 00:16:27.520]   No, it's both, but don't look, don't overlook the fact that if you get a contract to
[00:16:27.520 --> 00:16:33.040]   server to provide not only the hardware, but also the service contract for that hardware,
[00:16:33.040 --> 00:16:39.360]   that's a lot of money. So you want to also, also this is a big government contracts, they don't tend
[00:16:39.360 --> 00:16:45.120]   to, they tend to change hands when there's a huge screw up, not when things are just going
[00:16:45.120 --> 00:16:50.320]   mediocre or Lee. So that's, that's a really, really good thing to pursue if you want to make money.
[00:16:50.320 --> 00:16:55.360]   Okay. I mean, it's a giant corporate space from their perspective. I mean, when they're buying,
[00:16:55.360 --> 00:17:01.680]   they're buying by millions or tens of millions of dollars at a time, it's something that once
[00:17:01.680 --> 00:17:08.400]   you commit to a platform, and again, I think this is why Apple might behoove Apple to build more
[00:17:08.400 --> 00:17:13.840]   content for it, for the education space. But as you build something that is more and more specific
[00:17:13.840 --> 00:17:19.920]   to your platform, it makes it much less likely that people are going to venture out to other things.
[00:17:20.240 --> 00:17:23.600]   What happened? I think that also highlights one of the issues that Apple has, and that is that
[00:17:23.600 --> 00:17:27.600]   it's not, it's not critical to their business. Like making more money is great, but they have to
[00:17:27.600 --> 00:17:31.520]   ship an iPhone every year. All of their business is predicated on hardware profits. They have to
[00:17:31.520 --> 00:17:36.080]   ship a new iPad. They have to ship a new Mac and Apple has limited attention, despite the amount of
[00:17:36.080 --> 00:17:40.080]   money and resources and engineers. There's only so many things they could put the IAFs
[00:17:40.080 --> 00:17:44.400]   Sauron on at any given time, where it is absolutely fundamental and key to Google's business that
[00:17:44.400 --> 00:17:49.040]   they have as many users as possible. They require massive scale to operate, and they,
[00:17:49.040 --> 00:17:52.640]   they got a lot of pressure when they tried to advertise to kids saying, "Start your
[00:17:52.640 --> 00:17:56.560]   Gmail account for your kidneys, a baby, and just start adding things forever." But going in through
[00:17:56.560 --> 00:18:01.840]   the schools is a good way. Even if they have much stricter privacy policies, it's a good way to
[00:18:01.840 --> 00:18:06.640]   get those kids familiar with Google Docs and using Google Docs and using Gmail. That converts when
[00:18:06.640 --> 00:18:10.240]   they leave school, or when they have a personal account that they want to link to a game, you're
[00:18:10.240 --> 00:18:14.240]   12 years old, or whatever, and you want to play a game, you can log in through Google's, or you
[00:18:14.240 --> 00:18:17.920]   want a YouTube account. You want to start your own channel. It's very easy to create a personal
[00:18:17.920 --> 00:18:22.240]   account that gets linked in, and then all of a sudden that is absolutely core to go to those
[00:18:22.240 --> 00:18:26.640]   business, and they've done a brilliant job in ensuring that that user base is growing and
[00:18:26.640 --> 00:18:33.280]   absolutely getting them young. So the thing, if you're not a student or a teacher, the thing
[00:18:33.280 --> 00:18:38.960]   people will be watching for next week, though, is at least I'll be watching for next week, is
[00:18:38.960 --> 00:18:47.280]   a MacBook Air priced $7.99. That's what Ming Chi Quo predicted. You think that's reasonable?
[00:18:48.080 --> 00:18:53.040]   I heard $8.99, but we'll see. You never know, because Apple doesn't pre-announce, so if that
[00:18:53.040 --> 00:18:58.880]   computer's not ready, it could easily ship it. It would be great to see it there, because there
[00:18:58.880 --> 00:19:03.440]   are still schools that still want to Mac. They have iPads, and that's fine, but they still want to
[00:19:03.440 --> 00:19:07.120]   Mac. There's a lot of students going to school who want to Mac, and the MacBook Air is incredibly
[00:19:07.120 --> 00:19:10.880]   attractive because it has all those ports. Is that the one, not the MacBook, not the... Oh, because
[00:19:10.880 --> 00:19:15.760]   of the ports. Yeah, the MacBook Air, it has regular USB ports. It would be nice if Apple threw a
[00:19:15.760 --> 00:19:18.400]   USB on there, but... It's kind of in the air, but... It's kind of in the air, but... You kind of need the...
[00:19:18.400 --> 00:19:26.880]   ...but it's got a lot of things that are just key to adaptability. Yeah, we're living in the age of
[00:19:26.880 --> 00:19:30.160]   retina now, and I think the older panels get more and more expensive, and the retina panels get
[00:19:30.160 --> 00:19:35.440]   cheaper, and the competition has all that. The original MacBook Air platform, as far as I recall,
[00:19:35.440 --> 00:19:39.920]   didn't support it, which is why they went to the MacBook, but they have had years to engineer
[00:19:39.920 --> 00:19:43.840]   their way around that. Those components have gotten smaller and more power efficient over those years,
[00:19:43.840 --> 00:19:47.920]   too, so I think that's almost a no-brainer machine. They have not been able to scale the MacBook
[00:19:47.920 --> 00:19:53.120]   down to a sub $1,000 price point, so scaling up the MacBook Air to sort of keep that price point
[00:19:53.120 --> 00:19:58.400]   or go $100, maybe $200 if they can, below that. I think that's just a fabulous product for them.
[00:19:58.400 --> 00:20:05.920]   Any chance that it's all... That all will see a Mac Pro. No. Zero. That would be a stunner, Leo.
[00:20:05.920 --> 00:20:11.440]   No, I am very interested in the whole pencil angle of the whole thing, because we see that
[00:20:11.440 --> 00:20:16.880]   calligraphy of just seeing really what kind of... I think that... So you think there'll be a low-cost
[00:20:16.880 --> 00:20:21.360]   iPad Pro in the sense that it works with a pencil part? Well, pencil will go to a good pad and
[00:20:21.360 --> 00:20:25.520]   not just be restricted to Pro. Maybe that's a better way to put it, yeah. Although they always said,
[00:20:25.520 --> 00:20:32.000]   didn't they, that when you have pencil, you can't do force touch? No, they're different things. The
[00:20:32.000 --> 00:20:36.160]   problem with force touch was that it worked on the deformation of the glass screen being measured
[00:20:36.160 --> 00:20:41.520]   by the LED backlight, and that deformation is very hard to measure on large screens. And there was
[00:20:41.520 --> 00:20:47.200]   two problems that 3D touch solved. One was the shortcuts, which would be great on iPad, but the
[00:20:47.200 --> 00:20:52.160]   other one was the iPhone only has one column. iPad has two, so it took a lot more time to go
[00:20:52.160 --> 00:20:55.680]   into and out of views than on the iPad where you can just go side to side, and that's like...
[00:20:55.680 --> 00:20:58.240]   You don't really need it on the iPad. You can pop into views. So you don't need that part of it,
[00:20:58.240 --> 00:21:01.920]   but the shortcuts would be nice. And I think when they move to the new display technology...
[00:21:01.920 --> 00:21:07.040]   Sorry, the new pressure technology, it's in the iPhone 10 that'll solve a lot of those problems.
[00:21:07.040 --> 00:21:13.920]   And I wonder whether there were some rumors that there were extra... They had been thinking about
[00:21:13.920 --> 00:21:20.080]   other nibs for the pencil. It'll be really interesting to see if that is something that comes out
[00:21:20.080 --> 00:21:24.880]   where we have a little bit more granularity. I think that feeling a little bit more like you're
[00:21:24.880 --> 00:21:30.240]   on paper, I think could make a big difference for some people. I'm really feeling like it's organic.
[00:21:30.240 --> 00:21:35.440]   Didn't they do nibs? Didn't they announce nibs? So they swatched those. They ended up changing the
[00:21:35.440 --> 00:21:38.560]   nib. They have replacements, but they changed the material of the nib at the last minute,
[00:21:38.560 --> 00:21:41.840]   and that's what caused a lot of the supply constraints. They didn't have any ability to play
[00:21:41.840 --> 00:21:48.240]   around with variations when they first launched it. Plus, I hope that they give some attention to
[00:21:48.240 --> 00:21:53.520]   making the Apple Pencil a little bit more convenient to use. They could make it more
[00:21:53.520 --> 00:21:59.200]   pocketable. They could just add a clip to it for heaven's sake. That's one of the... I love
[00:21:59.200 --> 00:22:04.160]   everything about the pencil, except for when I'm not using it with the iPad. In which case,
[00:22:04.160 --> 00:22:10.160]   I have a lot of things I don't like about the pencil. Does anyone have... Do you still have the
[00:22:10.160 --> 00:22:15.440]   little eraser end that it comes with? I don't know why. I have more ends than I have Apple Pencils.
[00:22:15.440 --> 00:22:25.280]   I got to the point now where I put them somewhere because I'm just tired of losing them.
[00:22:25.280 --> 00:22:29.280]   I put them in my desk. I have them sitting there, but I'm just like, "I can't be bothered."
[00:22:29.280 --> 00:22:34.000]   Wouldn't that be sad if this whole thing is just about new nibs?
[00:22:34.000 --> 00:22:37.680]   "Serenity, I'll be nib-enouncement at the school."
[00:22:37.680 --> 00:22:42.720]   "Serenity wants some haptic so that you can get different feeling on different canvases or paper
[00:22:42.720 --> 00:22:48.160]   type." She's an artist. She draws. That would be interesting. If it's a little more gritty,
[00:22:48.160 --> 00:22:55.440]   because you're on a course or paper or something like that. What else? iPhone SE2?
[00:22:55.440 --> 00:23:04.960]   It's maybe. I'd love to see a Mac mini. Any chance at all?
[00:23:04.960 --> 00:23:11.360]   I don't see it. This would be a perfect place to announce something like that.
[00:23:11.360 --> 00:23:13.920]   Again, given that they've declared this to be an education event,
[00:23:13.920 --> 00:23:18.160]   you're basically taught. That's a way of saying low-cost hardware on top of service.
[00:23:18.160 --> 00:23:23.840]   That's the lowest cost Mac there is, is a Mac mini. I'd be surprised if we saw a new Mac mini.
[00:23:23.840 --> 00:23:30.240]   I'd also be super delighted to see one. We need at least a sign of life.
[00:23:30.240 --> 00:23:37.920]   I'd like to see a new MacBook Air because they didn't kill it. That tells me that they saw some
[00:23:37.920 --> 00:23:43.760]   future for it. They're torturing it. It is right now. It's stuck between a MacBook and a MacBook
[00:23:43.760 --> 00:23:50.080]   Pro. It doesn't really seem to do anything better. Price is the only advantage a MacBook Air has.
[00:23:50.080 --> 00:23:55.200]   This chipset is outdated at this point. At this point, you'd be buying a three-year-old computer
[00:23:55.200 --> 00:23:59.120]   if you bought a new MacBook Air. Usually, when you see a product like that that's still in the
[00:23:59.120 --> 00:24:04.560]   price list, you at least suspect that there are valuable contracts that aren't obvious that are
[00:24:04.560 --> 00:24:09.760]   just buying these by the boatload. Given that they don't have to do anything to keep making
[00:24:09.760 --> 00:24:14.560]   them, they're just going to keep making them. Three pounds, it's a little heavy compared to the Mac
[00:24:14.560 --> 00:24:21.360]   book. Chromebooks are cheaper by a third. It's not an AgSafe Leo. I hope they don't. I
[00:24:21.360 --> 00:24:24.240]   hope they agree with you. I hope they put the Type C on this. Otherwise,
[00:24:24.240 --> 00:24:29.360]   one of each have one side be the Type C, one side be the Type A. I mean,
[00:24:29.360 --> 00:24:33.040]   well, and I wonder, if it is really just for school, maybe they're going to assume schools
[00:24:33.040 --> 00:24:37.040]   have a lot of MagSafe. I don't know what they're going to do. Well, remember they kept the
[00:24:38.160 --> 00:24:42.400]   optical drive version of the MacBook Pro around for a long time because students were ordering it.
[00:24:42.400 --> 00:24:47.440]   Yeah. The MacBook, the plastic MacBook, was the cheap one.
[00:24:47.440 --> 00:24:52.800]   Then there was a really old MacBook Pro, this little optical drive that you had to roll all the
[00:24:52.800 --> 00:24:58.560]   way to the bottom of the page. They still sell the 15-inch 2015 MacBook, my favorite MacBook Pro.
[00:24:58.560 --> 00:25:03.120]   They still sell it. They'll hit the new one. You can still buy it. This is the only thing I'm
[00:25:03.120 --> 00:25:07.120]   just praying and I know my prayers will never be answered that they get rid of this god-awful
[00:25:07.120 --> 00:25:11.280]   keyboard and put a good keyboard on this MacBook. Then there's a long time ago.
[00:25:11.280 --> 00:25:16.720]   I'm telling you, Tim, I would buy it. Just put a retina screen on it. I don't care about the price,
[00:25:16.720 --> 00:25:21.760]   put a retina screen on it. You must have a lot of those old keyboards lying around.
[00:25:21.760 --> 00:25:28.480]   Keep the good keyboard. Don't put that butterfly key crap on there. Man, I'd buy that in the heartbeat.
[00:25:28.480 --> 00:25:34.880]   And about the only other thing that's leaked is the Gold iPhone X. I was hoping for Red,
[00:25:34.880 --> 00:25:40.720]   but so far old. They didn't release the Gold back in the end of the year because
[00:25:40.720 --> 00:25:46.240]   sorry, reportedly it was too hard of a process to get the stainless steel turned gold. It just
[00:25:46.240 --> 00:25:51.680]   added it to complexity to manufacturing, but it could be ready now. They'd sell some of those,
[00:25:51.680 --> 00:25:56.000]   gold. You mean instead of the stainless that have gold all the way around it?
[00:25:56.000 --> 00:25:59.280]   Yeah, it will be like the rose gold back and then the gold stainless steel.
[00:25:59.840 --> 00:26:07.520]   All right. Well, we'll find out. We're going to do the live stream, 8 AM Pacific noon Eastern.
[00:26:07.520 --> 00:26:14.640]   I'm sorry, 11 AM Eastern. 1500 UTC, Megan, Moroni and I, we're not going to make you guys get up.
[00:26:14.640 --> 00:26:19.600]   Megan, Moroni and I'll get up early for that. Actually, you wouldn't be early for you. It'd be
[00:26:19.600 --> 00:26:24.800]   noon. Andy and I will be in the audience waving at you. Are you you're both flying out?
[00:26:25.360 --> 00:26:32.160]   Yep. Oh, nice. Did you get an invitation, Andy? Probably because of the Chicago Sun times, right?
[00:26:32.160 --> 00:26:37.760]   Well, okay, I'll say that. Yes. But you haven't had an invitation to an Apple event in a while. I
[00:26:37.760 --> 00:26:43.600]   mean, I often often on I just don't I just don't go to the one. It's a different system to go all
[00:26:43.600 --> 00:26:49.520]   the way. It's expensive and it's like it's a lot of trouble and a lot of time in the air and
[00:26:50.080 --> 00:26:55.600]   whereas a show in Chicago, it's not just less expensive. It's less it's less time. Oh, no, I could drive
[00:26:55.600 --> 00:27:02.480]   your pickup. I took the train from Boston, Chicago once when when I was like 23 24 on a book tour
[00:27:02.480 --> 00:27:06.400]   wondering worried that they would stop the service and I want to do that at least once.
[00:27:06.400 --> 00:27:13.040]   And after 23 hours in coach class on a solid train, I realized that I said, I'm glad I did that.
[00:27:13.040 --> 00:27:18.080]   I'm never going to do that again unless there's a bed and a sink. There's a lot of truth to the
[00:27:18.080 --> 00:27:22.000]   fact that many of us like if you have to file immediately after the event, you have to shoot video
[00:27:22.000 --> 00:27:26.640]   and upload video immediately, you just have to go. But where many of us are so jealous at, you know,
[00:27:26.640 --> 00:27:30.080]   people like Andy or people like even like Gruber, who just shows up doesn't even touch a phone or
[00:27:30.080 --> 00:27:34.560]   a computer. It's just there to be there just to be. Yeah, no, because they can take the time they
[00:27:34.560 --> 00:27:38.800]   can cover it later. They're reviewing like that. It is a very different sort of a product you have
[00:27:38.800 --> 00:27:43.760]   to do. And then there's people like me who will be watching on TV. I don't know if they're going to
[00:27:43.760 --> 00:27:47.120]   stream it. I presume they are. They've streamed everything in the last few years. I've heard both
[00:27:47.120 --> 00:27:51.120]   because it's not a typical venue for them. So maybe they're going to figure it out at the last
[00:27:51.120 --> 00:27:54.960]   minute. Well, if they don't stream it, then I'm not getting up at eight in the morning.
[00:27:54.960 --> 00:28:00.880]   The good thing is there's also a chance that the school's production, the pajama game is going
[00:28:00.880 --> 00:28:05.040]   to run long. So you might actually be able to see the finale. Oh, I'd love to. During the life.
[00:28:05.040 --> 00:28:08.640]   Love to. That'd be great. I think it would be wicked this year. Aren't they?
[00:28:08.640 --> 00:28:13.680]   You know, I have to amend them. I announced that if they stream it, I was foolishly assuming
[00:28:13.680 --> 00:28:17.120]   they would if they stream it, we will be here. Megan Moroni and I at eight in the morning.
[00:28:17.120 --> 00:28:25.120]   Pacific time. That's 11 a.m. Eastern. 10 a.m. Chicago time. And we'll do that. And then
[00:28:25.120 --> 00:28:30.240]   right after that, I iOS today and then right after that, Mac break weekly. So the show next week
[00:28:30.240 --> 00:28:35.360]   might be delayed a little bit. But certainly. Well, now, will you guys be around for Mac break weekly?
[00:28:35.360 --> 00:28:38.960]   Maybe you're going to be busy now. I believe I will. I believe I believe I have stuff.
[00:28:38.960 --> 00:28:46.160]   Yeah, I will be. I have a plan to get out of either the PBS station nearby or back to my hotel to
[00:28:46.160 --> 00:28:51.120]   be there to be there for the show. Thank you, Andy. But the show will be like because you'll
[00:28:51.120 --> 00:28:54.720]   have time because we're going to have iOS today in between. So probably we won't begin till noon
[00:28:54.720 --> 00:29:00.400]   next week. We'll see. And Alex, you're going to be around? Yeah, I'm going to be in DC, but I'm
[00:29:00.400 --> 00:29:08.320]   going to have a bigger mic. PR. No more. It's the size of the mic that matters. That's all it
[00:29:08.320 --> 00:29:12.160]   matters. It is. It is. So that's it. Yeah, I'll be available.
[00:29:12.160 --> 00:29:16.880]   So we'll have lots of coverage, whether we'll have a coverage of a live stream depends on whether
[00:29:16.880 --> 00:29:20.000]   there's a live stream. I didn't even think. I figure, you know, I could, I could live
[00:29:20.000 --> 00:29:23.520]   blog the left side of the stage and he could do center and center. He could do right. You know,
[00:29:23.520 --> 00:29:26.960]   to get a key player. Here's what they love. Here's what they love with these events.
[00:29:26.960 --> 00:29:31.760]   You can take your laptop and you jump on Skype and you turn it around and just use your camera
[00:29:31.760 --> 00:29:35.680]   and stream it back to play. And then we can talk about it saves us a lot.
[00:29:35.680 --> 00:29:40.800]   Dallying would chop lock me so fast saves us a lot of time. But no, but
[00:29:40.800 --> 00:29:44.080]   so when I do and rain, I have a plan, we're going to do computational coverage where we're
[00:29:44.080 --> 00:29:49.120]   going to be by adjusting where our coverage is based on the room. You will feel as though
[00:29:49.120 --> 00:29:52.400]   you are getting the entire situation there. No matter where you're listening from.
[00:29:52.400 --> 00:29:56.560]   I am actually very much looking forward to this. I hope there are some new hardware announcements.
[00:29:56.560 --> 00:30:02.240]   And it's not just new nibs. But it would be kind of. I just, I'm just.
[00:30:02.240 --> 00:30:08.160]   No, but it's more education. One of the reasons why I didn't even think about not accepting that
[00:30:08.160 --> 00:30:13.920]   invitation was again, the idea of having an entire themed presentation on one topic,
[00:30:13.920 --> 00:30:20.080]   particularly on a topic where I don't, I was not worried that Apple had given up on education.
[00:30:20.080 --> 00:30:24.960]   I had always, I had known that Apple was figuring out what they need to do in response to
[00:30:24.960 --> 00:30:30.960]   some really, really terrific competition out there. And so I'm really, really keen to see what they
[00:30:30.960 --> 00:30:37.520]   do. Actually, Microsoft last year had an education event. It was weird because it was announcing
[00:30:37.520 --> 00:30:44.000]   the creators update to Windows, but they had kids on stage. They were doing 3D modeling and stuff.
[00:30:44.000 --> 00:30:48.240]   It is a big business for Microsoft. And I know their lunch has also been eaten by Google and
[00:30:48.240 --> 00:30:53.280]   Chromebooks. So Joe Belfiori looks so young. He could easily pass for it.
[00:30:53.280 --> 00:31:00.320]   So we'll look forward to that. That's a week from today, March 27th. And well, I'll let you know
[00:31:00.320 --> 00:31:05.360]   for a, we'll be in, we'll probably be here anyway. As you know, we get here very early preparing for
[00:31:05.360 --> 00:31:14.800]   iOS today. So not, you have hats to choose. I have wine to drink. My wine is here. I am a
[00:31:14.800 --> 00:31:24.800]   wink fan and I encourage you all to try wink W.I.N.C. This is, I'll tell you who this is. Well,
[00:31:24.800 --> 00:31:30.000]   it's for a number of different kinds of people. If you, if you like wine, I think most of us like
[00:31:30.000 --> 00:31:34.640]   wine, but you would like to try other wines. You know, you always get the same bottle because you,
[00:31:34.640 --> 00:31:38.320]   that's what you know and you like. Or maybe you go to have people's houses. It's always nice to
[00:31:38.320 --> 00:31:42.240]   bring a bottle of wine and you'd like to bring something good, something they don't know, something
[00:31:42.240 --> 00:31:49.920]   different. I think you really like wink. The idea of wink is four bottles a month, which is great.
[00:31:49.920 --> 00:31:54.880]   Actually, you don't have to go with the four bottles a month plan, but there are all sorts of
[00:31:54.880 --> 00:31:59.760]   things you can do. I love though, coming home to my box. This is how it comes. Very nicely,
[00:31:59.760 --> 00:32:04.800]   delivered. And, and now what you're going to do, and we'll go through this in a second, is you go to
[00:32:04.800 --> 00:32:15.360]   W.I.N.C. Try wink.com is the website. Try t-r-y-w-i-n-c.com. And you go through the, the palette taste test,
[00:32:15.360 --> 00:32:19.840]   where you tell them what you like. And then you pick the wines and you can have four bottles of
[00:32:19.840 --> 00:32:26.800]   white, four bottles of red or mix and match. It's up to you. Oh boy, I'm excited. Oh boy.
[00:32:27.680 --> 00:32:33.840]   This is good. This is so, the field theory. I really like the field theory. There's a red.
[00:32:33.840 --> 00:32:39.120]   This is the field theory albereneal. I can't wait to try that. 2016 albereneal.
[00:32:39.120 --> 00:32:43.360]   Wink makes all their own wine. This is another thing. They are a wine maker. They are not a wine
[00:32:43.360 --> 00:32:48.240]   reseller. They're not taking old wine that somebody couldn't sell and rebotting it.
[00:32:48.240 --> 00:32:52.720]   They do have beautiful labels. They get artists to do the beautiful labels. This is called Diviner.
[00:32:52.720 --> 00:32:58.960]   It's a cabernet solvignon. We've got all new wines in here. Somebody must have consumed the other
[00:32:58.960 --> 00:33:06.640]   wines. Dime. This is a Santa Barbara County blend. So you ask, answer questions like, how do you like
[00:33:06.640 --> 00:33:13.280]   your coffee? Here's peppers. How adventurous are you when it comes to food and drink? I'll try
[00:33:13.280 --> 00:33:18.960]   anything. It's going to be sunny sometime, guys. I promise. And then you'll want the summer water.
[00:33:18.960 --> 00:33:25.600]   This rosé is chilled perfect for a hot summer day. Wink works directly with the top wine makers
[00:33:25.600 --> 00:33:31.360]   and growers from around the world to make all of their own wine. Each month, there are new wines,
[00:33:31.360 --> 00:33:37.440]   delicious wines. They also partner with local artists to make those labels. Shipping is covered.
[00:33:37.440 --> 00:33:41.360]   And if you don't like a bottle, they send you for whatever reason. You just go, yeah, I thought
[00:33:41.360 --> 00:33:44.560]   it was going to be good. I don't like it. They'll replace it with a bottle. You love no questions.
[00:33:44.560 --> 00:33:50.400]   Ask, no, you don't have to ship the other bottle back. Sit back, relax, celebrate, learn about
[00:33:50.400 --> 00:33:57.840]   wines, learn new wines. They have the best winemaker. And I just love. I have never had a more
[00:33:57.840 --> 00:34:04.720]   consistently delicious experience. W-I-N-C fill out Winx palette profile quiz.
[00:34:04.720 --> 00:34:11.520]   And then choose some wines and they will curate them to your taste. Something you can, you know,
[00:34:11.520 --> 00:34:16.000]   four bottles is perfect. So a couple of Saturday nights, bring a bottle to a party and then you
[00:34:16.000 --> 00:34:21.920]   got one for celebrating. Why settle for the same old bottle of wine when you could try something new
[00:34:21.920 --> 00:34:26.160]   and delicious that you will love? By the way, you can buy into it. We go buy, we actually go and
[00:34:26.160 --> 00:34:30.800]   buy cases of the field theory, the red field theory. I love it so much. Discover great wine today.
[00:34:30.800 --> 00:34:38.080]   Go to try wink, T-R-Y-W-I-N-C, try wink.com/macbreak. And we're going to give you $20 off your first shipment.
[00:34:38.960 --> 00:34:47.360]   $20 off try wink.com/macbreak. Just try it. You might, you might discover something
[00:34:47.360 --> 00:34:53.680]   really delicious. Actually, I really like having a good, I drink, I don't drink, I maybe once a
[00:34:53.680 --> 00:34:57.280]   week, I'll have one glass. I don't drink a lot of wine, but when you drink one glass a week,
[00:34:57.280 --> 00:35:02.320]   you want it to be a really good glass of wine. And it is fabulous.
[00:35:05.280 --> 00:35:14.400]   Facebook in the news. Wow. I finally, I finally, after this last Cambridge Analytica revelations,
[00:35:14.400 --> 00:35:19.360]   and then yesterday we saw video, the CEO of Cambridge Analytica saying, yeah, we throw elections.
[00:35:19.360 --> 00:35:22.560]   Yeah, we do that. We have compromising information.
[00:35:22.560 --> 00:35:29.200]   So anything you want. After that, it's like, and by the way, Alex Stamos, who I have a lot of respect
[00:35:29.200 --> 00:35:33.120]   for, he was the chief security, although I have to say at this point, Alex, you got to pick your
[00:35:33.120 --> 00:35:40.400]   companies better. He was the chief security officer at Yahoo. And you know, had to leave there because
[00:35:40.400 --> 00:35:46.480]   they didn't listen to him, went to Facebook. We had a team of 120 people and he's slowly been
[00:35:46.480 --> 00:35:52.560]   shuffled off to the side. He's now got a team of three in Facebook because they moved the security
[00:35:52.560 --> 00:35:56.880]   team out into operations or something. I don't know. Now he's leaving Facebook, he says,
[00:35:56.880 --> 00:36:01.120]   I've been begging them to do something about Russian influence for months. And they,
[00:36:01.120 --> 00:36:06.000]   they're the executives just to be ignoring me. I find it was the last draw. I just, I realized,
[00:36:06.000 --> 00:36:11.360]   I don't check it that much. I don't need Facebook. This is a great article from you guys. This is
[00:36:11.360 --> 00:36:17.600]   Renee's article, delete your Facebook. The only way to win the social game is not to play. I like
[00:36:17.600 --> 00:36:22.560]   and I like that picture of you looking for, lonely at your screen, your Facebook list, phone,
[00:36:22.560 --> 00:36:26.640]   Martin, race, classic, have you done that? By the way, have you deleted Facebook?
[00:36:26.640 --> 00:36:30.960]   I have done the most of it. My biggest problem with Facebook right now is that because of my
[00:36:30.960 --> 00:36:34.480]   job, I have a bunch of Facebook pages that I'm the admin for like the I'm more page.
[00:36:34.480 --> 00:36:37.760]   Yeah, they'll all go away because they're attached to your account. Yeah.
[00:36:37.760 --> 00:36:41.840]   And some of them are uniquely attached to my account. So what I've done is I've deleted every
[00:36:41.840 --> 00:36:45.680]   bit of personal information from my account. I could. I turned off Facebook platform. I removed
[00:36:45.680 --> 00:36:49.760]   every single app. I turned off, you have to go to a separate area to prevent them from doing app
[00:36:49.760 --> 00:36:53.600]   permissions with your friends. So I went to that area and I turned that also. I basically made it
[00:36:53.600 --> 00:36:58.320]   as empty as shell as I possibly could. And also, and it sounds, it sounds terrible saying this,
[00:36:58.320 --> 00:37:02.640]   but when you're someone in public, your username is also important and some people will turn it
[00:37:02.640 --> 00:37:06.400]   off and someone else will take their name and start spoofing them or pretending to be them or
[00:37:06.400 --> 00:37:11.520]   squatting on it or whatnot. So I've basically just put it down to the bareest possible minimum.
[00:37:11.520 --> 00:37:15.440]   And I'm only logging in if I absolutely have to fix something on an admin page.
[00:37:15.440 --> 00:37:21.520]   If you want to delete it entirely, if you're not in the Renee situation, you know, I kept my
[00:37:21.520 --> 00:37:26.080]   Facebook account for a long time with that, with the rationalization. Well, I have to cover it.
[00:37:26.080 --> 00:37:33.600]   But now I don't even feel like I have to cover it anymore. Facebook.com/help/delete_account.
[00:37:33.600 --> 00:37:41.600]   Ironically, you won't find that page in your settings. I think this page exists probably
[00:37:41.600 --> 00:37:48.880]   because of GDPR more than anything else. It is an instantaneous. It takes a few days.
[00:37:49.760 --> 00:37:56.880]   They also say, if you ever accidentally log back into Facebook, oh, we'll turn it back on.
[00:37:56.880 --> 00:38:00.000]   Yeah, there's a 14 day counter. If you do anything, it resets.
[00:38:00.000 --> 00:38:04.400]   So I have deleted all my Facebook login cookies. I've deleted all my Facebook bookmarks.
[00:38:04.400 --> 00:38:09.520]   You know, there are going to be some pain points because a lot of times we use Facebook
[00:38:09.520 --> 00:38:15.200]   to log into other apps, right? You're going to have to redo that somehow. I guess I'll lose some
[00:38:15.200 --> 00:38:22.400]   accounts that I have through my Facebook account. But I feel like this is a bridge too far.
[00:38:22.400 --> 00:38:30.000]   I guess for me, a long time ago, in fact, I think the thought process for me changed when,
[00:38:30.000 --> 00:38:37.200]   remember, path? Yeah, I loved paths. You could have your 50 friends and we were friends.
[00:38:37.200 --> 00:38:40.400]   We were friends, but we were friends on the path. I wish there was something like paths.
[00:38:41.120 --> 00:38:46.560]   Well, but the problem was is that I actually, so I stopped posting to path pretty quickly. I
[00:38:46.560 --> 00:38:50.880]   started playing with it and it was really cool. And I started, you know, it would be like my
[00:38:50.880 --> 00:38:55.040]   kids at Christmas and all the things I don't usually post on other things because it was like,
[00:38:55.040 --> 00:39:00.640]   oh, it's only 50 people and I know everybody. And then I just really came to the conclusion that I
[00:39:00.640 --> 00:39:04.640]   should not be posting anything on the internet that I do not want to be public. I just shouldn't
[00:39:04.640 --> 00:39:10.080]   be thinking that way. And so as a result, after path came out years ago, I don't know when that
[00:39:10.080 --> 00:39:17.840]   was 2009, 2008. I ceased to post. I don't do anything that's only to friends. I don't do anything
[00:39:17.840 --> 00:39:22.320]   that's private. You know, like, if I'm going to post something, I post it publicly. You know,
[00:39:22.320 --> 00:39:30.320]   my, I think of that as a public place, like everything that I'm posting. You know, and I think
[00:39:30.320 --> 00:39:35.040]   that that's a safe way to approach the internet. You probably said that.
[00:39:35.040 --> 00:39:41.280]   And his path got purchased by a Korean company called Daoam Kukao. And of course, when they, when
[00:39:41.280 --> 00:39:46.720]   somebody purchases all of that, they get all that information and you know, they aren't necessarily
[00:39:46.720 --> 00:39:53.280]   bound by paths privacy agreements. So you're probably, it's not just the purchasing stuff.
[00:39:53.280 --> 00:39:56.480]   Like, sometimes they'll change CEOs, they'll change the people who run the company,
[00:39:56.480 --> 00:39:59.840]   they'll change their policies. We've seen with Facebook, Mark Zuckerberg famously announced that
[00:39:59.840 --> 00:40:04.160]   we're no longer forcing app developers to delete your information after 24 hours and they all
[00:40:04.160 --> 00:40:07.680]   applauded. And then later they had to come back and say, Oh, but we want you to make sure you ask
[00:40:07.680 --> 00:40:11.120]   them before you get permission to use their friend stuff. You know, like all of that stuff
[00:40:11.120 --> 00:40:14.720]   vacillates back and forth. And then you also want to worry about government because they,
[00:40:14.720 --> 00:40:18.720]   government can go in and get a lot of information from these companies too. So even if you trust
[00:40:18.720 --> 00:40:22.720]   a company, you can't possibly trust future leadership. You can't trust every employee who
[00:40:22.720 --> 00:40:26.720]   legitimately or illegitimately accesses your data within that company. We saw like people in
[00:40:26.720 --> 00:40:30.880]   Uber monitoring car travels. We've got concrete examples of all of these things happening. So
[00:40:30.880 --> 00:40:34.320]   I think it's just it's safe to assume exactly what Alex is saying. If anything,
[00:40:34.320 --> 00:40:40.720]   if anything, this really underscores the need for a new addition to the bill of rights,
[00:40:40.720 --> 00:40:47.200]   which is the idea that personal information belongs to the person that personally belongs to.
[00:40:47.200 --> 00:40:53.200]   That if you that the only thing that I can do is that a company can do is they can license it
[00:40:53.200 --> 00:40:58.960]   from me and the user has the ability to revoke that license. This is this is such a huge mess
[00:40:58.960 --> 00:41:02.160]   that that's the only way that we're going to attempt to solve this problem because
[00:41:02.160 --> 00:41:07.920]   just as you say, all that has to happen is that they have to they have you click through a brand
[00:41:07.920 --> 00:41:11.840]   new terms of service, which suddenly tells you, Oh, remember how we said that we will not sell?
[00:41:11.840 --> 00:41:16.000]   We will only sell anonymized personal information. Well, now we're selling we're selling actual
[00:41:16.000 --> 00:41:20.800]   personal information and they bury that on page 23. And what if you do say, you know what,
[00:41:20.800 --> 00:41:24.880]   I'm sick and tired of Facebook. I'm deleting my account and everything. And I'm doing all
[00:41:24.880 --> 00:41:28.160]   my cookies. I've got to have nothing to do with Facebook. Well, guess what? People are taking
[00:41:28.160 --> 00:41:31.680]   pictures of you with parties and they already know what your face looks like. So even though
[00:41:31.680 --> 00:41:36.560]   you're not a member of Facebook, they know that you running Richie or me and you not go work together
[00:41:36.560 --> 00:41:41.360]   at this place at this time. So that's why I'm saying we need something on the level of a
[00:41:41.360 --> 00:41:46.400]   constitutional amendment saying that this is part of the rights of being an American in the EU. They
[00:41:46.400 --> 00:41:54.640]   have certain they in the in in England, for instance, there is a government commission that is that
[00:41:54.640 --> 00:42:00.960]   is tasked to do nothing but to protect the privacy of citizens online. And Google has run a follow
[00:42:00.960 --> 00:42:06.880]   this of this organization. Facebook has run a follow this this organization. We have no advocacy
[00:42:06.880 --> 00:42:14.080]   for personal individual citizens rights online. And we need something as we even a hammer as big as
[00:42:14.080 --> 00:42:18.480]   a constitutional amendment saying that you don't have the you don't get to argue balls and strikes
[00:42:18.480 --> 00:42:22.160]   on this you don't get to do anything that's cute. And it doesn't matter if you suddenly if your
[00:42:22.160 --> 00:42:28.000]   business model changes or if your company changes hands, you do not own this personal information.
[00:42:28.000 --> 00:42:33.200]   And if I decide to rescind that license that I've given to you by agreeing to this user agreement,
[00:42:33.200 --> 00:42:38.640]   then you have no ability to use it legally. Well, and I think I think the part of this has been
[00:42:38.640 --> 00:42:44.400]   kind of a slow boiling of a frog, you know, because I mean, there was, you know, the amount of
[00:42:44.400 --> 00:42:48.640]   information that we can talk about Facebook, you know, having whatever information about where
[00:42:48.640 --> 00:42:52.800]   we went and all those other things that are are there. I can't tell you we're planning an advertising
[00:42:52.800 --> 00:42:59.760]   campaign and it is kind of when you go through the back end of the the Facebook ads, it's kind of
[00:42:59.760 --> 00:43:03.760]   stunning. You don't get to see anybody. I can't find anything that's personal to anyone, but
[00:43:03.760 --> 00:43:08.160]   the amount of information like when they when they when you go, okay, well, I want target.
[00:43:08.160 --> 00:43:13.360]   I want people in this area and I want them to be interested in these things. And I want them to
[00:43:13.360 --> 00:43:19.040]   lean this way politically and I want them to like it's amazing. You know, and but but the thing
[00:43:19.040 --> 00:43:24.400]   the thing is is that that kind of information, it's exposed right here because of Facebook, but I
[00:43:24.400 --> 00:43:30.800]   it should be clear that there's a lot of information being gathered all the time. I mean, you know,
[00:43:30.800 --> 00:43:38.560]   I was talking to someone at Safeway one time about, you know, how I don't I didn't want to use
[00:43:38.560 --> 00:43:41.680]   their little Safeway card because I don't want them tracking me and they were like, oh, they just
[00:43:41.680 --> 00:43:45.520]   tracked you use your credit card. Have you used your credit card ever? You know, like, yeah,
[00:43:45.520 --> 00:43:50.000]   that you're in you're in you're like everything you bought is in, you know, and they're figuring
[00:43:50.000 --> 00:43:54.080]   out all kinds of stuff, you know, related to that. And some of that stuff is stuff that freaks us out
[00:43:54.080 --> 00:43:58.720]   and some of the stuff that is really, you know, convenient, you know, you know, to people and it
[00:43:58.720 --> 00:44:03.120]   makes those things easy. And I think for the most part that stuff has been happening for so long
[00:44:03.120 --> 00:44:09.040]   since the early 90s that I think that we don't really think about it that much, you know, and I
[00:44:09.040 --> 00:44:13.120]   think that the average person we can talk about all this, but I don't think a I don't think the
[00:44:13.120 --> 00:44:19.840]   government is interested in I don't think our government anyway is interested in deep diving into
[00:44:19.840 --> 00:44:24.560]   this. There's a lot of information that they're getting as well. Like, so if you look at the
[00:44:24.560 --> 00:44:29.360]   partnerships with the government organizations with something like Planetir, you know, the, you know,
[00:44:29.360 --> 00:44:35.120]   there's a lot of data that they're gathering that that they would consider, you know, terror,
[00:44:35.120 --> 00:44:39.220]   but terrorism related and so on and so forth, this is all really useful for it.
[00:44:39.220 --> 00:44:41.900]   And so I don't think that there's an interest internally for them to do that.
[00:44:41.900 --> 00:44:46.420]   I don't think there's enough Americans that really understand that or care, in this process
[00:44:46.420 --> 00:44:47.420]   as well.
[00:44:47.420 --> 00:44:53.060]   So I don't know if any of this really is going to happen that dramatically.
[00:44:53.060 --> 00:44:58.600]   And again, I've slowly gathered that I just shouldn't be online doing anything that I'm
[00:44:58.600 --> 00:45:02.020]   worried about because I just feel like it's all being tracked.
[00:45:02.020 --> 00:45:05.660]   I just shouldn't do that, just do anything that if I'm not willing to have someone take
[00:45:05.660 --> 00:45:09.640]   a picture of it and post it to people or keep track of it and use it against me later,
[00:45:09.640 --> 00:45:12.260]   then I should just put my phone away.
[00:45:12.260 --> 00:45:14.820]   And not think that I should be connected.
[00:45:14.820 --> 00:45:17.740]   Because I think that's the future that we all need to get ready for.
[00:45:17.740 --> 00:45:20.940]   We can talk about wanting to be private, but we're not going to be.
[00:45:20.940 --> 00:45:24.300]   I think we need to take a little bit of a reality check here.
[00:45:24.300 --> 00:45:25.500]   It is not going to happen.
[00:45:25.500 --> 00:45:27.780]   We are not going to have a private life.
[00:45:27.780 --> 00:45:30.140]   That is impossible.
[00:45:30.140 --> 00:45:34.500]   We can talk about what we wish would happen, but that is what we're wishing and it's not
[00:45:34.500 --> 00:45:35.500]   going to happen.
[00:45:35.500 --> 00:45:36.500]   It's not going to happen in Europe.
[00:45:36.500 --> 00:45:38.820]   It doesn't happen in China and they spend a lot of money on it.
[00:45:38.820 --> 00:45:45.620]   So the thing is, it's just the idea that it can be controlled or put out there, it's
[00:45:45.620 --> 00:45:47.260]   just a pipe dream.
[00:45:47.260 --> 00:45:53.340]   >> Unfortunately, it's really being online, using anything, anything that the connection
[00:45:53.340 --> 00:45:58.780]   to anything else, it really is a lot like driving in that there are things you can do to make
[00:45:58.780 --> 00:45:59.780]   yourself safer.
[00:45:59.780 --> 00:46:02.420]   There are things you should, everybody should do to understand what the risks are and how
[00:46:02.420 --> 00:46:03.900]   to mitigate them.
[00:46:03.900 --> 00:46:07.940]   Check your blind spots, sing the signal, look before you change lanes, all this sort of
[00:46:07.940 --> 00:46:08.940]   stuff.
[00:46:08.940 --> 00:46:13.300]   But one of the other sadly important lessons we're going to have to learn is that, or excuse
[00:46:13.300 --> 00:46:18.220]   me, accept is that you cannot be out on the highway without running a risk that you're
[00:46:18.220 --> 00:46:21.780]   going to get into an accident or someone's going to kill you with their car.
[00:46:21.780 --> 00:46:26.420]   And unfortunately, stuff that we're running into with Facebook and these other marketing
[00:46:26.420 --> 00:46:27.700]   companies.
[00:46:27.700 --> 00:46:32.740]   Is that, again, do you want to not drive a car or do you want to simply acknowledge that
[00:46:32.740 --> 00:46:38.100]   these risks exist and try to mitigate them as much as you possibly can?
[00:46:38.100 --> 00:46:41.900]   >> And there's also the other issue, which is that for a lot of people, there's no alternatives.
[00:46:41.900 --> 00:46:44.620]   Like in some areas, Facebook is just synonymous with the internet.
[00:46:44.620 --> 00:46:48.300]   And because companies like Google and Facebook, their entire business is subsidized by data
[00:46:48.300 --> 00:46:52.020]   harvesting, they can offer these services for free, which means that it's very hard for
[00:46:52.020 --> 00:46:56.140]   any other company that doesn't have massive amounts of income like Apple or a Samsung
[00:46:56.140 --> 00:47:00.780]   or someone who makes large amounts of consumer goods to offer anything remotely like a competitive
[00:47:00.780 --> 00:47:01.780]   service.
[00:47:01.780 --> 00:47:03.260]   It just sucks all the air out of the room.
[00:47:03.260 --> 00:47:07.660]   We saw that with Google Reader, for instance, where all the other RSS readers basically went
[00:47:07.660 --> 00:47:10.260]   away and then Google canceled it and we have nothing.
[00:47:10.260 --> 00:47:13.780]   You destroy your Facebook account, how do you share your photos, how do you share events,
[00:47:13.780 --> 00:47:15.660]   how do you coordinate, how do you message.
[00:47:15.660 --> 00:47:17.580]   There are alternatives, but none of them are easy.
[00:47:17.580 --> 00:47:19.620]   Some of them are not inexpensive.
[00:47:19.620 --> 00:47:23.780]   Some require a lot of network effect to move your family on to them or to move your friends
[00:47:23.780 --> 00:47:24.780]   on to them.
[00:47:24.780 --> 00:47:28.300]   So it's almost like a self-perpetuating snowballing problem at the same time.
[00:47:28.300 --> 00:47:29.300]   I just want to be clear.
[00:47:29.300 --> 00:47:32.580]   The reason I'm deleting Facebook is not because I want to protect my privacy.
[00:47:32.580 --> 00:47:37.220]   I'm kind of acknowledged that that's futile at this point.
[00:47:37.220 --> 00:47:41.140]   I can't stop using Google, for instance.
[00:47:41.140 --> 00:47:42.340]   It's almost impossible.
[00:47:42.340 --> 00:47:51.820]   However, I want to punish them because they really have done a terrible, terrible thing.
[00:47:51.820 --> 00:47:56.620]   They basically didn't care, they didn't pay attention, they exfiltrated information,
[00:47:56.620 --> 00:48:01.740]   they didn't disclose, they allowed features like the Friends of Friends Quiz feature.
[00:48:01.740 --> 00:48:05.860]   They did so many things wrong and they've done so many things wrong time and time again
[00:48:05.860 --> 00:48:07.820]   and then apologized.
[00:48:07.820 --> 00:48:11.380]   I don't buy it anymore and I just hope they're out of business.
[00:48:11.380 --> 00:48:14.260]   I think everybody should just quit Facebook.
[00:48:14.260 --> 00:48:15.260]   We don't need any Facebook.
[00:48:15.260 --> 00:48:18.380]   Facebook is a lot more vulnerable than you think.
[00:48:18.380 --> 00:48:21.500]   It's already happened with a whole bunch of the demographic has left Facebook for something
[00:48:21.500 --> 00:48:25.180]   more interesting and more Snapchat-ish.
[00:48:25.180 --> 00:48:30.020]   That's why there's the reputation of Facebook as the place where your mom and your grandmother
[00:48:30.020 --> 00:48:31.540]   hang out, not where the kids hang out.
[00:48:31.540 --> 00:48:37.500]   Really, all that has to happen is if 20 to 50 people that I know were to switch from
[00:48:37.500 --> 00:48:42.020]   Facebook to something else, that would allow me to drop Facebook like a hot potato.
[00:48:42.020 --> 00:48:44.220]   By the way, though, what else is there?
[00:48:44.220 --> 00:48:46.220]   There isn't anything else.
[00:48:46.220 --> 00:48:49.820]   There are individual services that you can string together, but it is more of a pain.
[00:48:49.820 --> 00:48:52.420]   I dropped Instagram and WhatsApp too.
[00:48:52.420 --> 00:48:56.300]   Again, not because those were privacy leakers, but because I don't want to have anything
[00:48:56.300 --> 00:48:58.620]   to do with a bad company.
[00:48:58.620 --> 00:49:06.500]   Well, see, it is also difficult because I would hit Facebook maybe once every three
[00:49:06.500 --> 00:49:10.860]   weeks, maybe once a month, maybe not even that often because they just skived me out.
[00:49:10.860 --> 00:49:14.060]   I just did not want to participate in it.
[00:49:14.060 --> 00:49:18.300]   It became a critical mass situation where unfortunately way too many of my friends and
[00:49:18.300 --> 00:49:22.180]   almost all my family members, that was the only way that they want to have these sort
[00:49:22.180 --> 00:49:25.580]   of incidental interactions with each other.
[00:49:25.580 --> 00:49:30.140]   The little quick phone calls that used to be 20 or 30 years ago, the little quick postcards
[00:49:30.140 --> 00:49:33.780]   used to be 20 or 30 years ago.
[00:49:33.780 --> 00:49:37.860]   As much as I've tried to get them onto other things, they're just not having it.
[00:49:37.860 --> 00:49:43.700]   Unfortunately, when I first dipped a toe, I was a quick example, I was dipping a toe
[00:49:43.700 --> 00:49:49.700]   back into Facebook and I was at a tech conference, a MacTech, I think in LA.
[00:49:49.700 --> 00:49:53.100]   I posted a video I shot at the LA County Museum of Art.
[00:49:53.100 --> 00:49:57.900]   And of course, almost the same afternoon, I got an email, a message on Facebook from
[00:49:57.900 --> 00:50:02.300]   a cousin of mine who because he follows me on Facebook saw that I was in LA.
[00:50:02.300 --> 00:50:03.300]   He's in LA.
[00:50:03.300 --> 00:50:07.620]   We haven't seen each other in five or six or seven years and we went on and going to dinner
[00:50:07.620 --> 00:50:08.620]   that night.
[00:50:08.620 --> 00:50:10.140]   It's like, damn it.
[00:50:10.140 --> 00:50:14.340]   This is what gets people using Facebook because unfortunately it's for people.
[00:50:14.340 --> 00:50:19.260]   It's a so slippery, easy way for people to keep connected to each other, even though
[00:50:19.260 --> 00:50:26.580]   it's the most awful way of doing so other than Craigslist hookups at a truck rest up.
[00:50:26.580 --> 00:50:29.900]   By the way, your iPhone won't be that secure going forward.
[00:50:29.900 --> 00:50:30.900]   It looks like either.
[00:50:30.900 --> 00:50:33.220]   There's a new company called Graykey.
[00:50:33.220 --> 00:50:34.740]   Have you guys been paying attention to this?
[00:50:34.740 --> 00:50:37.180]   We've talked a long time about celebrate.
[00:50:37.180 --> 00:50:43.180]   And the celebrate, of course, required law enforcement to send the phone to them.
[00:50:43.180 --> 00:50:47.060]   They would send it back with a hard drive full of the data on the phone.
[00:50:47.060 --> 00:50:52.580]   This new company, Graykey made by a company called Grayshift out of Atlanta.
[00:50:52.580 --> 00:50:59.540]   It's only a few years old, claims that they can unlock any kind of iPhone, even an iPhone
[00:50:59.540 --> 00:51:00.540]   10.
[00:51:00.540 --> 00:51:05.220]   And furthermore, law enforcement can buy the device and keep it at their place.
[00:51:05.220 --> 00:51:06.220]   It's a gray box.
[00:51:06.220 --> 00:51:09.540]   Two models, two models, just right for you.
[00:51:09.540 --> 00:51:13.820]   You can connect up to two iPhones at a time, takes about two minutes.
[00:51:13.820 --> 00:51:18.620]   At that point, they're not yet cracked, but sometimes later the phone will display a black
[00:51:18.620 --> 00:51:19.700]   screen with a passcode.
[00:51:19.700 --> 00:51:24.220]   In other words, it takes longer for the six-day jet and even longer for the password.
[00:51:24.220 --> 00:51:25.460]   It puts something on the phone.
[00:51:25.460 --> 00:51:29.340]   I will say that I'm pretty certain that Apple is really glad that someone posted all
[00:51:29.340 --> 00:51:30.340]   these photos.
[00:51:30.340 --> 00:51:33.180]   Figure out where it's coming in.
[00:51:33.180 --> 00:51:35.500]   And also I wonder, you know, Apple's got some cash.
[00:51:35.500 --> 00:51:38.740]   I don't know why they don't just go to Grayshift and go billion dollars.
[00:51:38.740 --> 00:51:41.100]   Like let's just buy you, take your engineers.
[00:51:41.100 --> 00:51:43.340]   Well, I mean, because there's always another exploit.
[00:51:43.340 --> 00:51:45.860]   So presumably this is an exploit, right?
[00:51:45.860 --> 00:51:48.180]   This is taking advantage of a flaw in iOS.
[00:51:48.180 --> 00:51:50.980]   But you want those guys, you want those guys working for you.
[00:51:50.980 --> 00:51:51.980]   That's true.
[00:51:51.980 --> 00:51:56.180]   And another one is you close this down, you close this down and then you bring those guys
[00:51:56.180 --> 00:51:59.740]   in and you figure out what they know that you don't know.
[00:51:59.740 --> 00:52:01.740]   It'd be worth a billion dollars, I think, to Apple.
[00:52:01.740 --> 00:52:03.500]   I mean, typically it's a chain of exploits.
[00:52:03.500 --> 00:52:07.100]   And this is a box could mean, for example, I'm just a bit balling.
[00:52:07.100 --> 00:52:11.420]   It could be an example that they're using a USB based exploit as a first level of this.
[00:52:11.420 --> 00:52:13.740]   And then they're escalating it through something else.
[00:52:13.740 --> 00:52:18.420]   And Apple has a crack team, a red team of security people who are all over stuff like
[00:52:18.420 --> 00:52:19.420]   this.
[00:52:19.420 --> 00:52:22.220]   But it happens, it's that cat and mouse game where people will find vulnerabilities.
[00:52:22.220 --> 00:52:25.900]   There's a huge market for people to pay a fortune for zero to exploits of things like
[00:52:25.900 --> 00:52:27.180]   iOS and Android.
[00:52:27.180 --> 00:52:30.100]   And then you'll see this come up and then it'll be fixed in another update.
[00:52:30.100 --> 00:52:33.220]   And then, you know, the bad guys or the law enforcement companies will work again on
[00:52:33.220 --> 00:52:34.220]   cracking them.
[00:52:34.220 --> 00:52:35.220]   And that's how it goes.
[00:52:35.220 --> 00:52:37.060]   I think this, I think, at least it's a fair game.
[00:52:37.060 --> 00:52:40.180]   When the government starts to mandate back doors, I think that's incredibly dangerous.
[00:52:40.180 --> 00:52:42.980]   But if they're going out and trying to hack their way in, I mean, that's how they do
[00:52:42.980 --> 00:52:43.980]   their thing.
[00:52:43.980 --> 00:52:44.980]   >> I think it's a fair game.
[00:52:44.980 --> 00:52:45.980]   I think it's totally fair game.
[00:52:45.980 --> 00:52:49.540]   I think that this is something that hopefully we can all hope that Apple is going to close.
[00:52:49.540 --> 00:52:53.260]   But I think that in the same way that the government doesn't have the right to ask Apple
[00:52:53.260 --> 00:52:57.020]   to give them the information, the government has the right to do whatever they can to get
[00:52:57.020 --> 00:52:58.020]   the information out.
[00:52:58.020 --> 00:52:59.700]   So I don't have any problem with it.
[00:52:59.700 --> 00:53:05.340]   This to me is the absolute worst expression of this kind of David Hart, data harvesting.
[00:53:05.340 --> 00:53:09.460]   Because people get pulled over, quote, randomly unquote.
[00:53:09.460 --> 00:53:11.780]   And they don't understand what their rights are.
[00:53:11.780 --> 00:53:16.180]   They don't understand the difference between the subtle language between a cop saying,
[00:53:16.180 --> 00:53:19.780]   I need to see your phone versus demanding your phone.
[00:53:19.780 --> 00:53:21.780]   You can say, I need to see your phone.
[00:53:21.780 --> 00:53:25.660]   And you can say, no, you may need to see it, but I'm not going to give it to you.
[00:53:25.660 --> 00:53:29.940]   I'm not interested in their other devices like this for lesser protected phones where
[00:53:29.940 --> 00:53:33.020]   they don't know that they think that, I just got a lock screen.
[00:53:33.020 --> 00:53:34.180]   There's not really anything on it.
[00:53:34.180 --> 00:53:35.460]   I'm pretty much protected.
[00:53:35.460 --> 00:53:39.140]   They don't know that all they need is to hook this up to a box that's that the county
[00:53:39.140 --> 00:53:43.900]   or the city bought for a disgustingly small amount of money that just simply dumped all
[00:53:43.900 --> 00:53:45.700]   of their messages, all their context.
[00:53:45.700 --> 00:53:50.620]   And now they have all this stuff that they can now actually use because the cops said,
[00:53:50.620 --> 00:53:52.860]   I need to see your phone and you handed it over.
[00:53:52.860 --> 00:53:57.220]   That is willfully relinquishing control of your device, of your personal data.
[00:53:57.220 --> 00:54:02.780]   There are other communities that are trying to make the statement that previous laws that
[00:54:02.780 --> 00:54:07.420]   simply say that give the police the right to search your vehicle under certain circumstances.
[00:54:07.420 --> 00:54:08.980]   Well, your phone is inside your vehicle.
[00:54:08.980 --> 00:54:10.940]   It doesn't matter that it's locked for the password.
[00:54:10.940 --> 00:54:13.740]   It doesn't mean that it doesn't matter this personal data.
[00:54:13.740 --> 00:54:16.700]   If it is inside the car, then that is something that they have the right to search.
[00:54:16.700 --> 00:54:21.500]   So again, they take it back to the car, they plug it in while they, quote, run your license
[00:54:21.500 --> 00:54:23.180]   plate and check for your insurance.
[00:54:23.180 --> 00:54:26.700]   And three minutes later, they get a Bing and they really, and then I was time to unplug
[00:54:26.700 --> 00:54:28.140]   the phone and return it.
[00:54:28.140 --> 00:54:34.620]   This is the sort of stuff that leads to absolute destruction of individual liberties and individual
[00:54:34.620 --> 00:54:35.620]   rights.
[00:54:35.620 --> 00:54:39.660]   This is the sort of stuff that should that we need to lock the hell down on.
[00:54:39.660 --> 00:54:46.020]   I don't, I agree that it's it's something that law enforcement has the ability to do
[00:54:46.020 --> 00:54:53.020]   and the right to do.
[00:54:53.020 --> 00:54:53.820]   And when you talk to even so when you talk to people who work in this industry, the theme
[00:54:53.820 --> 00:54:58.860]   that keeps coming up time and time again is that we have the we have a mandate to protect
[00:54:58.860 --> 00:55:03.500]   the public peace and our lead on how what we should not do is the law.
[00:55:03.500 --> 00:55:06.380]   If you don't want us to do that, pass laws to tell us we can't do that.
[00:55:06.380 --> 00:55:08.540]   Oh, here's the larger issue in it.
[00:55:08.540 --> 00:55:09.940]   They're selling two devices.
[00:55:09.940 --> 00:55:14.540]   One is pretty well locked down to the network that it's set up on.
[00:55:14.540 --> 00:55:19.580]   But the second device, if the law enforcement agency is willing to spend $30,000, this is
[00:55:19.580 --> 00:55:28.820]   from malware bites labs, the device is unlocked with a token and doesn't require phoning home.
[00:55:28.820 --> 00:55:34.980]   And the issue here is that device, if stolen and if the bad guy gets the token, if let's
[00:55:34.980 --> 00:55:40.860]   say maybe detective Joe Friday decides to put it on a post it note on the device because
[00:55:40.860 --> 00:55:44.260]   he can never remember it, then that device will work forever.
[00:55:44.260 --> 00:55:50.300]   And so this is always the problem with these back doors, even if they're in the hands of
[00:55:50.300 --> 00:55:57.780]   legitimate, duly, you know, conscripted law enforcement, is they can they can be exfiltrated
[00:55:57.780 --> 00:55:59.740]   and then bad guys have this.
[00:55:59.740 --> 00:56:04.740]   I mean, the bottom line for me is that I staunchly believe that phones are the closest
[00:56:04.740 --> 00:56:08.540]   we have to cybernetics and they should be protected by privilege that way exceeds spousal
[00:56:08.540 --> 00:56:13.540]   clergy, you know, doctor lawyer, any of those things, it should be name possible.
[00:56:13.540 --> 00:56:14.540]   It's like your Silicon mind.
[00:56:14.540 --> 00:56:16.540]   It should be name possible to get into them.
[00:56:16.540 --> 00:56:19.740]   But in the world we live in, the best thing I can recommend for everybody is to use a
[00:56:19.740 --> 00:56:22.740]   long, strong password because it is not trivial to hack those.
[00:56:22.740 --> 00:56:24.540]   It's trivial to hack a four digit.
[00:56:24.540 --> 00:56:26.540]   It takes a little bit longer to hack a six digit.
[00:56:26.540 --> 00:56:29.540]   But if you have even just for your phone, like you don't have to do it for everything,
[00:56:29.540 --> 00:56:32.540]   but for your phone, if you can put in a password that's easy enough for you to type, but that
[00:56:32.540 --> 00:56:37.540]   consists of letters and numbers and uppercase and lowercase, switch to your phone, and then
[00:56:37.540 --> 00:56:41.620]   uppercase, switch that on in settings and iPhone and just put that in there and it makes it
[00:56:41.620 --> 00:56:43.180]   very difficult to get in.
[00:56:43.180 --> 00:56:44.180]   Yeah.
[00:56:44.180 --> 00:56:48.300]   My Android password is 14 digits, one letters long for that reason.
[00:56:48.300 --> 00:56:49.300]   Do you enter that?
[00:56:49.300 --> 00:56:51.460]   How often do you have to enter it?
[00:56:51.460 --> 00:56:56.740]   About once a day, sometimes for a day, but well, it's a it's worth it and be it's the
[00:56:56.740 --> 00:57:01.100]   sort of it's the sort of thing where the time where you're worried what can happen to my
[00:57:01.100 --> 00:57:06.260]   data because I've just lost my phone or because I've been stopped at I've have business in
[00:57:06.260 --> 00:57:09.500]   China, I've been stopped at the border and now they've taken away my phone.
[00:57:09.500 --> 00:57:15.420]   I would much rather that at that point you wish you had a 15, 16, 17 digit unlock now.
[00:57:15.420 --> 00:57:20.420]   Now, so that's interesting because this device, it says sometimes could take as long as three
[00:57:20.420 --> 00:57:23.060]   days if it's a six digit passcode.
[00:57:23.060 --> 00:57:29.820]   Sounds like if you had a 14 digit passcode on this device, it wouldn't be as effective
[00:57:29.820 --> 00:57:32.900]   or as exponentially more time to it in time as you're out.
[00:57:32.900 --> 00:57:35.940]   I mean, maybe in the future they'll be able to break it faster, but for right now it's
[00:57:35.940 --> 00:57:36.940]   your it's your ally.
[00:57:36.940 --> 00:57:42.780]   So this this may be the you know, the straw that compels you to have a longer passcode
[00:57:42.780 --> 00:57:43.780]   on your phone.
[00:57:43.780 --> 00:57:48.740]   Well, and I know I actually know companies that that we work with that if if a home
[00:57:48.740 --> 00:57:52.020]   on security or whatever, for some reason, ask for their computer number one is they're
[00:57:52.020 --> 00:57:56.620]   instructed to take it all off their computer, put it in the cloud before they enter reenter
[00:57:56.620 --> 00:57:57.620]   the country.
[00:57:57.620 --> 00:58:02.580]   If if they're asked for their computer and they and they take it away from them, they're
[00:58:02.580 --> 00:58:04.580]   instructed to not take it back.
[00:58:04.580 --> 00:58:05.580]   >> Right.
[00:58:05.580 --> 00:58:07.900]   >> Like you don't know what's been done to it, right?
[00:58:07.900 --> 00:58:08.900]   >> You don't know what's been done for it.
[00:58:08.900 --> 00:58:14.420]   You don't know, you know, and it's just completely and then I travel with more than one phone
[00:58:14.420 --> 00:58:18.580]   and you know, and I have a phone that's my international phone.
[00:58:18.580 --> 00:58:25.660]   I just constantly am swapping Sims and I have, I have a account on it, but I don't have
[00:58:25.660 --> 00:58:28.140]   a ton of stuff and that's kind of the phone.
[00:58:28.140 --> 00:58:31.260]   If you're gonna ask me for a phone at the board, I may have ruined everything by saying
[00:58:31.260 --> 00:58:32.260]   it on the show.
[00:58:32.260 --> 00:58:35.860]   If you're gonna ask me on the boarder what phone to hand, you know, is I just hand you
[00:58:35.860 --> 00:58:38.340]   this little Samsung phone and like here, that's my phone.
[00:58:38.340 --> 00:58:40.500]   And you go through it and there's stuff on it, it doesn't look like it's empty, but it's
[00:58:40.500 --> 00:58:41.940]   not, it's not my phone.
[00:58:41.940 --> 00:58:44.140]   >> Extra nay stuff, he doesn't care.
[00:58:44.140 --> 00:58:48.940]   >> I am now changing my past code, Andy, you'll be glad to know to a 14 digit.
[00:58:48.940 --> 00:58:50.900]   I'm gonna really regret this, this is the middle of the-
[00:58:50.900 --> 00:58:55.700]   >> We can ask, we can ask you to get some of the type Andy is still secure.
[00:58:55.700 --> 00:58:56.700]   >> Andy is really smart.
[00:58:56.700 --> 00:58:58.700]   >> Andy is my hero.
[00:58:58.700 --> 00:59:00.500]   >> This is all Andy's fault.
[00:59:00.500 --> 00:59:04.780]   >> I hate you, Andy, I hate you, I hate you, I hate you.
[00:59:04.780 --> 00:59:10.380]   >> I will say that for some reason the touch ID on my iPad Pro has stopped working and
[00:59:10.380 --> 00:59:15.820]   so now I'm sort of regretting the super long password I've got because every time I unlock
[00:59:15.820 --> 00:59:20.060]   it now for some reason, I have to type in, I'm taking it to the Apple store next week
[00:59:20.060 --> 00:59:21.060]   to check it out.
[00:59:21.060 --> 00:59:23.300]   >> I know you were so clever, Andy, you took off your fingerprints and nobody could find
[00:59:23.300 --> 00:59:24.300]   you, but then-
[00:59:24.300 --> 00:59:28.500]   >> All right, it's taken a while.
[00:59:28.500 --> 00:59:34.300]   It's probably re-encrypting my whole phone here, but there, go ahead, gray lock, gray
[00:59:34.300 --> 00:59:36.500]   key, take my phone.
[00:59:36.500 --> 00:59:39.340]   >> Double dog dare you.
[00:59:39.340 --> 00:59:40.340]   >> Double dog dare you.
[00:59:40.340 --> 00:59:45.420]   >> No, I don't, I'm not daring anybody, I'm not bringing that up at all.
[00:59:45.420 --> 00:59:47.420]   Let's see.
[00:59:47.420 --> 00:59:54.980]   R&D Bonanza, I like this article, I might tie in a little bit to the event coming up next
[00:59:54.980 --> 00:59:55.980]   week.
[00:59:55.980 --> 01:00:00.940]   >> Apple's R&D expenditure has grown astronomically.
[01:00:00.940 --> 01:00:07.580]   In fact, last year Apple spent more on R&D than they'd spent in the two decades before,
[01:00:07.580 --> 01:00:13.980]   including the years they launched the iPod, the iPhone and the iPad.
[01:00:13.980 --> 01:00:15.980]   So- >> That's a bit of, I mean, Horace did, you
[01:00:15.980 --> 01:00:17.980]   had a fantastic explanation of this.
[01:00:17.980 --> 01:00:22.060]   When, you know, he sort of elaborated on the iPhone and the iPad were transformative products
[01:00:22.060 --> 01:00:25.580]   for Apple, but they didn't require a huge amount of R&D because they essentially used
[01:00:25.580 --> 01:00:26.780]   existing resources.
[01:00:26.780 --> 01:00:30.700]   It was the people that Apple had working on Mac hardware engineering, iPod hardware engineering
[01:00:30.700 --> 01:00:35.700]   on Mac OS development, or back then OS 10 development, and they just put those people
[01:00:35.700 --> 01:00:37.340]   to more efficient use.
[01:00:37.340 --> 01:00:41.980]   They retasked them, they did have to hire some people for antennas and things, but largely
[01:00:41.980 --> 01:00:43.660]   it was in-house expertise.
[01:00:43.660 --> 01:00:48.060]   And now that Apple has sort of expanded those product lines, it has to add external expertise.
[01:00:48.060 --> 01:00:51.740]   It has to do things like adding the audio engineers who made the AirPods and who made
[01:00:51.740 --> 01:00:52.740]   the HomePod.
[01:00:52.740 --> 01:00:56.220]   It's doing things like augmented reality glasses and autonomous vehicles.
[01:00:56.220 --> 01:01:00.100]   And it's got a page Jennifer Aniston, who knows how many millions of dollars for a TV
[01:01:00.100 --> 01:01:01.100]   show.
[01:01:01.100 --> 01:01:04.300]   And none of that is drawn from existing budget staff or resources.
[01:01:04.300 --> 01:01:06.380]   And that's when R&D starts to get expensive.
[01:01:06.380 --> 01:01:11.460]   >> So the content number, is that part of the R&D number, the billion dollars in content?
[01:01:11.460 --> 01:01:16.060]   >> I mean, I've seen people say that Apple is spending more on the content they ever spent
[01:01:16.060 --> 01:01:19.940]   on any of their devices, but it might be on the delivery network.
[01:01:19.940 --> 01:01:23.620]   >> According to, this is Neil Sybart's Above Avalon, which I love this blog.
[01:01:23.620 --> 01:01:27.180]   According to Neil, he says Apple's effort to launch a video streaming for service from
[01:01:27.180 --> 01:01:31.460]   scratch is likely being classified as R&D.
[01:01:31.460 --> 01:01:36.300]   So that would certainly explain a billion of the 14 billion they planned to spend in
[01:01:36.300 --> 01:01:37.300]   fiscal year 2018.
[01:01:37.300 --> 01:01:39.540]   >> I mean, perhaps it's an alone.
[01:01:39.540 --> 01:01:44.580]   >> Yeah, but I also think smart cars, I mean, you nailed it, Renee.
[01:01:44.580 --> 01:01:46.260]   But that's why this number is interesting.
[01:01:46.260 --> 01:01:51.620]   It's Apple spending a lot of money on new categories, probably, in all light.
[01:01:51.620 --> 01:01:52.900]   >> It's new categories.
[01:01:52.900 --> 01:01:56.980]   And I mean, and also really further developing the current categories.
[01:01:56.980 --> 01:02:00.820]   I mean, the challenge that Apple has is they're not trying to figure out how to make widgets
[01:02:00.820 --> 01:02:01.820]   cheaper.
[01:02:01.820 --> 01:02:05.460]   They're trying to make them significantly better than everyone around them.
[01:02:05.460 --> 01:02:06.460]   >> Right.
[01:02:06.460 --> 01:02:11.100]   >> And so the real difficulty there is that I think that the number that they had on 60
[01:02:11.100 --> 01:02:15.540]   minutes a couple years ago was 200 engineers working on the iPhone.
[01:02:15.540 --> 01:02:18.340]   I just on the phone, I mean, on the camera.
[01:02:18.340 --> 01:02:19.340]   >> The camera, yeah.
[01:02:19.340 --> 01:02:23.980]   >> So just the camera is 200 people that are just trying to get the most out of that little
[01:02:23.980 --> 01:02:25.340]   square that they get.
[01:02:25.340 --> 01:02:32.780]   And so it's just an enormous amount of, to get things like touch ID or face ID or all
[01:02:32.780 --> 01:02:38.540]   of those things is just, and it gets bigger and bigger because staying in front of Samsung
[01:02:38.540 --> 01:02:40.140]   is going to get more and more expensive.
[01:02:40.140 --> 01:02:42.940]   Staying in front of Google is going to be more expensive.
[01:02:42.940 --> 01:02:47.380]   And so I think that that's going to keep on seeing that number growing for quite some
[01:02:47.380 --> 01:02:51.660]   time and they do have to develop a new product line at some point.
[01:02:51.660 --> 01:02:52.660]   Or a couple.
[01:02:52.660 --> 01:02:57.100]   The watch has been very successful, I think, from, I think a lot of people didn't think
[01:02:57.100 --> 01:02:59.820]   it would be, but it threw heavy R&D.
[01:02:59.820 --> 01:03:01.740]   And I think Apple's gotten with most of their products.
[01:03:01.740 --> 01:03:04.820]   They're starting to get into this groove of every year you're going to get a new one.
[01:03:04.820 --> 01:03:06.420]   So we're going to see a new watch every year.
[01:03:06.420 --> 01:03:08.900]   We're going to see a new iPad maybe every year.
[01:03:08.900 --> 01:03:11.060]   Maybe we'll see not Mac mini every year.
[01:03:11.060 --> 01:03:12.060]   We can.
[01:03:12.060 --> 01:03:13.060]   Oh, you can pray.
[01:03:13.060 --> 01:03:14.060]   You can hope.
[01:03:14.060 --> 01:03:15.700]   You had me, you had me up until there.
[01:03:15.700 --> 01:03:16.700]   Yeah, exactly.
[01:03:16.700 --> 01:03:18.340]   Here's one thing there.
[01:03:18.340 --> 01:03:21.580]   According to Mark Gurman, they're spending it on at CES this year.
[01:03:21.580 --> 01:03:28.940]   Samsung showed something they call micro LED, which is actually a really interesting new
[01:03:28.940 --> 01:03:30.620]   screen technology.
[01:03:30.620 --> 01:03:35.060]   Samsung was able to show a very large, I can't remember what it was, but a hundred-sum
[01:03:35.060 --> 01:03:36.060]   inch screen.
[01:03:36.060 --> 01:03:38.460]   They're in a twenty inch because they couldn't get the pixels any smaller.
[01:03:38.460 --> 01:03:42.900]   And they had to show that because in order to get a 4K screen with those pixels, they
[01:03:42.900 --> 01:03:44.980]   had to be under twenty inches.
[01:03:44.980 --> 01:03:49.220]   But presumably with research, those pixels will be lower and lower.
[01:03:49.220 --> 01:03:55.060]   They're really direct view LEDs, red, green and blue for each pixel.
[01:03:55.060 --> 01:04:00.460]   So they're all individually backlit because OLED does that, but OLED has so many compromises.
[01:04:00.460 --> 01:04:02.620]   People joke about Apple just using a Samsung screen.
[01:04:02.620 --> 01:04:07.100]   Samsung has very mature OLED technology, but it is an incredible pain to get OLED to look
[01:04:07.100 --> 01:04:08.100]   good.
[01:04:08.100 --> 01:04:11.140]   And Apple had to do years of work to make the iPhone 10 display look the way they wanted
[01:04:11.140 --> 01:04:12.140]   it to work.
[01:04:12.140 --> 01:04:14.980]   And OLED has some advantages, but there's problems with brightness, there's problems
[01:04:14.980 --> 01:04:16.420]   with the blue sub pixel.
[01:04:16.420 --> 01:04:17.900]   There's just all these things you have to work around.
[01:04:17.900 --> 01:04:24.180]   The micro LED promises to have many of the advantages of OLED, but not any of those compromises,
[01:04:24.180 --> 01:04:26.100]   which would be huge for display technology.
[01:04:26.100 --> 01:04:30.580]   Plus, you can, with micro LED, you can have pretty much arbitrary screen shapes.
[01:04:30.580 --> 01:04:40.580]   And you can also conceivably do things like buy a 55 inch TV that's based on a micro LED
[01:04:40.580 --> 01:04:41.580]   panel.
[01:04:41.580 --> 01:04:44.980]   And then when you decide actually you want the full wall size screen, you could just
[01:04:44.980 --> 01:04:49.980]   simply buy additional panels and just mount them together in a new frame.
[01:04:49.980 --> 01:04:53.580]   And they will all behave as one single thing because each one is an individually addressable
[01:04:53.580 --> 01:04:54.580]   element.
[01:04:54.580 --> 01:05:00.100]   And the controller is what tells the controlling device what shape and size this screen is.
[01:05:00.100 --> 01:05:01.620]   So really exciting stuff.
[01:05:01.620 --> 01:05:07.340]   But yeah, when you have to demonstrate a huge TV because that's the only way to get a 4K
[01:05:07.340 --> 01:05:11.220]   display, you're not, you're safe buying a new TV right now.
[01:05:11.220 --> 01:05:13.540]   I don't think your next TV is going to be micro LED.
[01:05:13.540 --> 01:05:16.900]   Well, of course, I mean, they didn't used to make their own chip sets.
[01:05:16.900 --> 01:05:20.900]   The original iPhone had a Samsung, I think a Samsung set top box chipset and now Apple
[01:05:20.900 --> 01:05:23.460]   makes custom silicon, which takes more R&D.
[01:05:23.460 --> 01:05:26.980]   And they used to get LED from sharp or OLED from Samsung and now they're starting to
[01:05:26.980 --> 01:05:28.500]   work on their own micro LED.
[01:05:28.500 --> 01:05:29.500]   And that's more R&D.
[01:05:29.500 --> 01:05:31.860]   So just like the bill goes up and up.
[01:05:31.860 --> 01:05:35.460]   And I definitely think that Apple at some point, I mean, I think they feel like they
[01:05:35.460 --> 01:05:40.540]   are slowly moving towards this idea that they would be able to maybe have Foxconn build
[01:05:40.540 --> 01:05:44.140]   everything for them, but you know, less, you know, more and more of stuff that they
[01:05:44.140 --> 01:05:46.660]   own, more and more of making it.
[01:05:46.660 --> 01:05:50.100]   That's the real thrust of the story is not micro LED, but that for the first time in
[01:05:50.100 --> 01:05:53.220]   many years, Apple's actually working on its own screens.
[01:05:53.220 --> 01:05:54.220]   Oh, there's Mark.
[01:05:54.220 --> 01:05:56.780]   There's Mark himself.
[01:05:56.780 --> 01:06:02.860]   According to Mark, the these micro LED screens were micro LED screens are so hard to make.
[01:06:02.860 --> 01:06:06.500]   The company almost killed the project a year or so ago.
[01:06:06.500 --> 01:06:12.060]   Engineers has since been making progress and the technology is now at an advanced stage,
[01:06:12.060 --> 01:06:15.580]   although consumers will probably have to wait a few years before seeing any results.
[01:06:15.580 --> 01:06:19.140]   These are, I presume, not cinema displays, but for iPhones and iPads.
[01:06:19.140 --> 01:06:20.140]   Yeah, Apple Watch first.
[01:06:20.140 --> 01:06:21.140]   You know, Apple Watch first.
[01:06:21.140 --> 01:06:22.140]   Apple Watch first.
[01:06:22.140 --> 01:06:23.140]   Yeah.
[01:06:23.140 --> 01:06:24.620]   And you can't have 120 inch Apple.
[01:06:24.620 --> 01:06:30.100]   Well, yeah, I mean, I'd have to do the math, but if it took 120 inches to do 4K, it's going
[01:06:30.100 --> 01:06:33.140]   to take, you know, 12 inches to do an Apple Watch resolution.
[01:06:33.140 --> 01:06:37.220]   So that's like a Wonder Woman shield at that.
[01:06:37.220 --> 01:06:44.860]   By the way, they've killed shares of Universal Display Corporation, which makes OLEDs.
[01:06:44.860 --> 01:06:45.860]   Isn't that interesting?
[01:06:45.860 --> 01:06:48.580]   Because like they killed imagination, they started making custom GPU.
[01:06:48.580 --> 01:06:50.980]   And now it's just amazing how that works.
[01:06:50.980 --> 01:06:56.180]   Even a whisper now that Apple's going to replace their current suppliers with their own products
[01:06:56.180 --> 01:06:59.500]   is enough to depress the stock.
[01:06:59.500 --> 01:07:03.020]   But you know, it's clearly that's, that's, that was always Steve Jobs' goal.
[01:07:03.020 --> 01:07:04.300]   Tim Cook's following right along.
[01:07:04.300 --> 01:07:06.140]   They want to make it all make the chips.
[01:07:06.140 --> 01:07:08.900]   They want to control the technologies that are essential to the products that they're
[01:07:08.900 --> 01:07:10.900]   making so they can offer their differentiated experience.
[01:07:10.900 --> 01:07:14.180]   It's like the, what does Horace call it, the Tim Cook mandate?
[01:07:14.180 --> 01:07:15.180]   Right.
[01:07:15.180 --> 01:07:16.180]   The Tim Cook.
[01:07:16.180 --> 01:07:17.180]   Also, yeah.
[01:07:17.180 --> 01:07:20.820]   He is, nobody else is doing it except Samsung.
[01:07:20.820 --> 01:07:25.740]   And Ray Sennara from DisplayMate said, this is a golden opportunity for Apple.
[01:07:25.740 --> 01:07:28.060]   Everybody can buy an OLED or LCD screen these days.
[01:07:28.060 --> 01:07:34.980]   But if Apple could own micro LED, that would be a very clear difference here.
[01:07:34.980 --> 01:07:35.980]   Yeah.
[01:07:35.980 --> 01:07:37.620]   They're not going to achieve that though.
[01:07:37.620 --> 01:07:39.500]   It's too important to too many technologies.
[01:07:39.500 --> 01:07:43.420]   It's a real, it's the next thing for display companies and display companies don't want
[01:07:43.420 --> 01:07:44.420]   to lose money to Apple.
[01:07:44.420 --> 01:07:46.660]   It's as much as anybody else does.
[01:07:46.660 --> 01:07:53.100]   I hope that Apple, I'm sure they do, but I really hope that Apple understands that it's
[01:07:53.100 --> 01:07:56.140]   okay if you let someone else make your key switches.
[01:07:56.140 --> 01:08:00.820]   It's okay if you let someone else manufacture certain components that they are very, very,
[01:08:00.820 --> 01:08:06.220]   very, very, very good and experienced at developing because there are so many instances in which
[01:08:06.220 --> 01:08:10.740]   integration on this level has benefited, particularly the iPad, particularly the iPhone
[01:08:10.740 --> 01:08:13.100]   to the nth degree.
[01:08:13.100 --> 01:08:18.100]   But there's also such thing as at some point an off the shelf component that is completely
[01:08:18.100 --> 01:08:23.380]   reliable, completely useful that people already understand how to build at quantity and to
[01:08:23.380 --> 01:08:27.940]   support is going to cost way less than the equivalent Apple thing.
[01:08:27.940 --> 01:08:33.020]   And the Apple is going to, I think, sometime in the next five years really start butting
[01:08:33.020 --> 01:08:39.380]   its head against the problem of we are selling things that cost 20%, 30%, 40%, then in many
[01:08:39.380 --> 01:08:46.860]   cases an equivalent piece of hardware and we're not showing the user any front facing
[01:08:46.860 --> 01:08:52.500]   benefit outside of an Apple logo and they can't just simply whip out the checkbook for the
[01:08:52.500 --> 01:08:57.020]   very, very best or the very, very most interesting or innovative or in-house solution when they
[01:08:57.020 --> 01:09:02.020]   could simply buy something out of the digikey catalog for one millionth of a cent.
[01:09:02.020 --> 01:09:12.020]   German says this is a secret to the initiative code named T159.
[01:09:12.020 --> 01:09:14.020]   They have a 62,000 square foot manufacturing facility in Santa Clara, 300 engineers they're
[01:09:14.020 --> 01:09:18.900]   designing and producing micro LED screens for use in future products.
[01:09:18.900 --> 01:09:23.780]   The facility also has a special area for the intricate process of producing LEDs.
[01:09:23.780 --> 01:09:29.020]   Another facility nearby houses technology that handles so called LED transfers.
[01:09:29.020 --> 01:09:33.460]   It's the process of placing the individual pixels onto the micro LED screen.
[01:09:33.460 --> 01:09:35.180]   Apple sounds like they're all in.
[01:09:35.180 --> 01:09:40.340]   Some of this they got the IP for when they bought the company Luxview in 2014.
[01:09:40.340 --> 01:09:44.860]   I think some of this though, to address any point, I think some of this is so that they
[01:09:44.860 --> 01:09:46.460]   feel like they're hitting a lot of constraints now.
[01:09:46.460 --> 01:09:50.020]   For example, they just can't get enough OLED and they can't get it from multiple suppliers.
[01:09:50.020 --> 01:09:53.140]   So they're at the mercy of Samsung where if they develop this technology, they typically
[01:09:53.140 --> 01:09:57.100]   don't manufacture their own stuff but when they design Silicon they'll get Taiwan, Sami
[01:09:57.100 --> 01:10:01.180]   conductor or Samsung or someone to make it and they have the option of multi sourcing
[01:10:01.180 --> 01:10:03.660]   and are moving back and forth between manufacturers.
[01:10:03.660 --> 01:10:07.540]   My guess would be that they would multi source the micro LED so that they have a huge supply
[01:10:07.540 --> 01:10:10.900]   and it listens it back to those companies and they don't have to worry about the constraints
[01:10:10.900 --> 01:10:16.060]   for having Samsung be the sole owner of their NAND flash RAM or the sole owner of the OLED
[01:10:16.060 --> 01:10:17.900]   technology that they're using.
[01:10:17.900 --> 01:10:24.620]   German says that they opened a center in Taiwan a year after buying Luxview in 2015 to see
[01:10:24.620 --> 01:10:26.620]   if they could do it in house.
[01:10:26.620 --> 01:10:33.140]   So I mean, this is not an insignificant effort that sounds like they may want to make these,
[01:10:33.140 --> 01:10:36.140]   not merely develop the technology and license them.
[01:10:36.140 --> 01:10:37.220]   I don't think they will.
[01:10:37.220 --> 01:10:42.020]   I think they will and I think that one of the things that we've talked about before
[01:10:42.020 --> 01:10:48.700]   is that for all of these companies, an Apple probably more than most, the goal is you commoditize
[01:10:48.700 --> 01:10:54.220]   anything that you want that is not differentiating you, you want to make that as cheap as possible
[01:10:54.220 --> 01:10:56.300]   and you control everything else.
[01:10:56.300 --> 01:11:01.180]   So to get to Andy's point, for the chips that are not distinguishing the phone from something
[01:11:01.180 --> 01:11:03.020]   else, yeah, you buy that off the shelf.
[01:11:03.020 --> 01:11:10.340]   For anything that you are going to promote as different or distinct from Samsung, you
[01:11:10.340 --> 01:11:11.820]   are, you want to control that.
[01:11:11.820 --> 01:11:14.500]   You don't want them to be able to provide it to Samsung or Samsung.
[01:11:14.500 --> 01:11:16.460]   You don't want to keep on paying Samsung to get better at it.
[01:11:16.460 --> 01:11:21.740]   You want to be doing all of that stuff yourself and really being able to totally control that,
[01:11:21.740 --> 01:11:22.740]   totally understand it.
[01:11:22.740 --> 01:11:29.260]   And think about it if you're trying to create differentiated products in wearables, in phones,
[01:11:29.260 --> 01:11:33.380]   having a technology to yourself that you're, you know, apparently there are Apple Watch
[01:11:33.380 --> 01:11:34.380]   LED prototypes.
[01:11:34.380 --> 01:11:36.940]   The first ones were produced late last year.
[01:11:36.940 --> 01:11:38.700]   They're not really working yet.
[01:11:38.700 --> 01:11:41.060]   They're connected to a computer.
[01:11:41.060 --> 01:11:45.820]   But Gurman says the screens are notably brighter than the current OLED watch displays and engineers
[01:11:45.820 --> 01:11:51.380]   have a final, final level of control over individual colors.
[01:11:51.380 --> 01:11:53.860]   And that's what all early prototypes look like, just to, you know, they have a screen
[01:11:53.860 --> 01:11:54.860]   now.
[01:11:54.860 --> 01:11:57.060]   You've got to sit with so much of wires coming.
[01:11:57.060 --> 01:12:00.780]   Executives recently approved continued development for the next two years with the aim of shipping
[01:12:00.780 --> 01:12:03.380]   micro-LED screens and products.
[01:12:03.380 --> 01:12:08.220]   He says his sources tell him iPhone is three to five years off.
[01:12:08.220 --> 01:12:12.020]   But I think that this is the kind of thing I can totally see a company doing saying, look,
[01:12:12.020 --> 01:12:17.660]   if we can develop this and own this technology, that the iPhone three years from now will really
[01:12:17.660 --> 01:12:22.060]   be a, have a huge selling point that no one else can compete with.
[01:12:22.060 --> 01:12:24.260]   I'm sure, I mean, I know Sony's working on this.
[01:12:24.260 --> 01:12:25.300]   Samsung already has it.
[01:12:25.300 --> 01:12:27.580]   I bet you LG is working as fast as they can.
[01:12:27.580 --> 01:12:28.780]   But it's like Face ID.
[01:12:28.780 --> 01:12:32.300]   The companies are about two years out from having us this, whether you think Face ID is
[01:12:32.300 --> 01:12:36.460]   great or not, Apple's about two years ahead of other companies and in fielding that technology.
[01:12:36.460 --> 01:12:37.460]   Right.
[01:12:37.460 --> 01:12:38.460]   Right.
[01:12:38.460 --> 01:12:42.980]   There's the Apple grabs two year lead in 3D sensing race.
[01:12:42.980 --> 01:12:45.020]   That's the face recognition stuff.
[01:12:45.020 --> 01:12:46.540]   And it's super, I mean, it doesn't seem super important.
[01:12:46.540 --> 01:12:49.980]   Like a company like Google will say that we can, we don't need two cameras or we don't
[01:12:49.980 --> 01:12:52.340]   need, but they're not actually grabbing 3D data.
[01:12:52.340 --> 01:12:55.900]   They're extrapolating pixel data on the back of the, of the, of the, the phase adjust
[01:12:55.900 --> 01:12:57.620]   focus system on the back camera.
[01:12:57.620 --> 01:13:00.460]   But all they're doing is a segmentation mask on the front camera.
[01:13:00.460 --> 01:13:01.460]   And that's fine.
[01:13:01.460 --> 01:13:02.940]   They do an amazing job at portrait mode.
[01:13:02.940 --> 01:13:07.580]   But there are all sorts of people and all sorts of industries from special effects to
[01:13:07.580 --> 01:13:12.380]   science to computing technology that use the depth data provided by that, by that API
[01:13:12.380 --> 01:13:14.220]   for totally different things.
[01:13:14.220 --> 01:13:16.460]   And if you don't have that data, I mean, they're not interested in the least.
[01:13:16.460 --> 01:13:20.540]   In portrait mode, but they're interested in getting sensing data about the world around
[01:13:20.540 --> 01:13:21.540]   them.
[01:13:21.540 --> 01:13:23.340]   And that's where that sort of stuff becomes valuable.
[01:13:23.340 --> 01:13:26.220]   Well, it's, it's, that's a niche at best.
[01:13:26.220 --> 01:13:33.460]   Um, this, this, I love in the constant like yin yang between Google and Apple.
[01:13:33.460 --> 01:13:40.300]   It's like Google, Apple is often rolling out wonderful hardware that does miraculous things.
[01:13:40.300 --> 01:13:43.380]   And the argument is that you see, you really, you really need to do it.
[01:13:43.380 --> 01:13:47.700]   You can do it lamely without this great hardware, but not only do you need this great hardware,
[01:13:47.700 --> 01:13:53.300]   you need a tight level of, of level of integration between that, the component, the overall device,
[01:13:53.300 --> 01:13:56.900]   the operating system and the software, which only Apple can provide.
[01:13:56.900 --> 01:13:59.100]   And the Google side is that's really, really cool.
[01:13:59.100 --> 01:14:01.700]   By the way, we just figure out how to do it just with electrons.
[01:14:01.700 --> 01:14:05.220]   And so we don't have to release, we can, we just really, we just released that feature
[01:14:05.220 --> 01:14:06.660]   as a software update.
[01:14:06.660 --> 01:14:09.300]   You know, we're not asking a thousand dollars for this mode.
[01:14:09.300 --> 01:14:13.940]   We're asking for maybe 51 megabytes of download time.
[01:14:13.940 --> 01:14:18.020]   And so it's, there's always a shifting when it, when it becomes something like portrait
[01:14:18.020 --> 01:14:25.540]   mode, which works well on, in my opinion, works very, very well on the iPhone 10, but
[01:14:25.540 --> 01:14:30.900]   isn't a game changer of that's something that it doesn't really matter.
[01:14:30.900 --> 01:14:36.500]   When it comes to someone figuring out how to use the actual 3D 2 camera face scanning
[01:14:36.500 --> 01:14:40.980]   to do extremely interesting things, things that have nothing to do with just making a,
[01:14:40.980 --> 01:14:42.420]   making a portrait look better.
[01:14:42.420 --> 01:14:44.340]   That's when it gets really, really cool.
[01:14:44.340 --> 01:14:50.900]   That's when, when you have the ability to capture at some useful resolution, a static object on a table,
[01:14:50.900 --> 01:14:55.380]   when you have the ability to actually take a picture of a room and then do something useful
[01:14:55.380 --> 01:14:58.900]   to the parametrics of that room, that's when it gets super interesting.
[01:14:58.900 --> 01:15:03.300]   It's when you get the one thing that this phone, when you have a world in which any
[01:15:03.300 --> 01:15:13.220]   $850 smartphone is at least 95% the same as any other $850 smartphone, when you get that one thing
[01:15:13.220 --> 01:15:18.100]   that elevates it to a level that was never at before, be it a retina display or whatever,
[01:15:18.100 --> 01:15:21.780]   that suddenly translates into a sale for your team and not a sale for the other team.
[01:15:21.780 --> 01:15:24.660]   Our show today brought to you by WordPress.
[01:15:24.660 --> 01:15:29.620]   If you're a blogger, a business, if you've got an online brand, you know that the website
[01:15:29.620 --> 01:15:31.300]   is the most important thing.
[01:15:31.300 --> 01:15:35.940]   For one of the reasons I don't mind getting rid of Facebook is I've got leolaport.com.
[01:15:35.940 --> 01:15:41.380]   You ought to have your company.com and I'll tell you, I've been running a WordPress
[01:15:41.380 --> 01:15:47.860]   blog, a WordPress site since the early 2000s, WordPress.com makes it so easy.
[01:15:47.860 --> 01:15:49.700]   They maintain the software.
[01:15:49.700 --> 01:15:50.660]   They keep it up to date.
[01:15:50.660 --> 01:15:51.860]   They add the plugins.
[01:15:51.860 --> 01:15:56.340]   They give you the best support 24/7 Monday through Friday, weekends too.
[01:15:57.060 --> 01:16:02.420]   They've got all the design tools you need, all the templates and you don't have to be an expert
[01:16:02.420 --> 01:16:05.220]   in web design or HTML or computers or anything.
[01:16:05.220 --> 01:16:09.460]   They do all the technical stuff behind the scenes.
[01:16:09.460 --> 01:16:13.700]   All you do is create a great looking website and your website has the features you need.
[01:16:13.700 --> 01:16:19.540]   Things like built-in SEO and social sharing, specialized plugins to meet your needs.
[01:16:19.540 --> 01:16:24.580]   The social sharing is I think really great because you don't have to, you should have,
[01:16:24.580 --> 01:16:29.220]   of course, if you have a business, I guess you should have a Facebook page and a Twitter account.
[01:16:29.220 --> 01:16:31.620]   But you also need that website that's yours and yours alone.
[01:16:31.620 --> 01:16:36.740]   When you have social sharing on that site, then people can read your stuff and say,
[01:16:36.740 --> 01:16:39.940]   "This is great," or buy something from you and say, "Boy, I want this.
[01:16:39.940 --> 01:16:44.020]   This is fabulous." They all share that on their Facebook page and on their Twitter account.
[01:16:44.020 --> 01:16:45.860]   And that really increases your reach.
[01:16:45.860 --> 01:16:49.220]   The built-in SEO means that Google can find you.
[01:16:49.220 --> 01:16:51.060]   Everybody can find you if Google can find you.
[01:16:51.060 --> 01:16:58.340]   No wonder 30% of all the websites in the world, 30%, 30% run on WordPress.
[01:16:58.340 --> 01:17:00.260]   Shouldn't you be running on WordPress too?
[01:17:00.260 --> 01:17:01.460]   They've got great e-commerce.
[01:17:01.460 --> 01:17:02.740]   This is something relatively new.
[01:17:02.740 --> 01:17:08.580]   It ranges from a simple and effective just buy me button to a complete online store.
[01:17:08.580 --> 01:17:11.060]   Really easy to set up and easy to use.
[01:17:11.060 --> 01:17:13.220]   Your WordPress site will make you money.
[01:17:13.220 --> 01:17:18.340]   And when sites start as little as $4 a month, that's not too difficult.
[01:17:18.340 --> 01:17:23.620]   Get started today with 15% off any new plan purchase at WordPress.com/MacBreak.
[01:17:23.620 --> 01:17:26.900]   That's WordPress.com/MacBreak.
[01:17:26.900 --> 01:17:34.260]   15% off your new website, WordPress.com/MacBreak.
[01:17:34.260 --> 01:17:41.620]   Here's a story I never thought I would see according to KGI, Ming Chi Quo's famous
[01:17:42.180 --> 01:17:50.340]   analyst's analytics company. Apple laptop sales. Beat iPhone and iPad in year over year growth.
[01:17:50.340 --> 01:17:54.900]   Or will, anyway, he says 15% growth in 2018.
[01:17:54.900 --> 01:18:01.460]   Mac laptop unit shipments will rise between 13% to 16% this year.
[01:18:01.460 --> 01:18:02.980]   I'm not sure.
[01:18:02.980 --> 01:18:06.900]   Obviously, again, we talk about this all the time.
[01:18:06.900 --> 01:18:12.740]   This is an analyst who's writing a memo to investors.
[01:18:12.740 --> 01:18:16.180]   And that's why he's talking about this year as opposed to last year.
[01:18:16.180 --> 01:18:18.260]   But do you think this is credible?
[01:18:18.260 --> 01:18:20.900]   Do you think that MacBook sales will go through the roof?
[01:18:20.900 --> 01:18:23.860]   Renee?
[01:18:23.860 --> 01:18:24.660]   Matter so far as that it...
[01:18:24.660 --> 01:18:26.100]   Oh, okay. Go ahead.
[01:18:26.100 --> 01:18:32.580]   Oh, no, it's going to say that the number for, like in order for iPhones to go up by
[01:18:32.580 --> 01:18:34.660]   10%, you'd have to sell another 100-gill.
[01:18:34.660 --> 01:18:36.020]   That's a good point.
[01:18:36.020 --> 01:18:42.660]   In order for MacBooks to go up by 100% you'd only have to sell like another 500,000.
[01:18:42.660 --> 01:18:45.780]   So they're not insignificant numbers, but the bar, that's why he's not like,
[01:18:45.780 --> 01:18:49.060]   this phone grew most of any other phone. Well, that phone was like the lowest.
[01:18:49.060 --> 01:18:52.980]   It's like my surface sales always grew faster than MacBooks because almost nobody was buying them.
[01:18:52.980 --> 01:18:57.620]   So any small shift with a disproportionately huge amount of acceleration change.
[01:18:57.620 --> 01:19:01.220]   And I gather he's basing this on his predictions that there'll be a more affordable MacBook
[01:19:01.220 --> 01:19:04.420]   care as we talked about a... Which would be really good sales.
[01:19:04.420 --> 01:19:07.700]   It sure would. 13-inch MacBook, Retina MacBook.
[01:19:07.700 --> 01:19:10.980]   That's what one inch bigger than the current MacBook.
[01:19:10.980 --> 01:19:16.900]   So maybe he's just saying based on my, based on my guesses that there are going to be new
[01:19:16.900 --> 01:19:20.020]   laptops, this should really goose laptop sales.
[01:19:20.020 --> 01:19:30.340]   Apple's going to open a Shinjuku store in Tokyo just in time for my visit April 7th.
[01:19:31.220 --> 01:19:33.380]   Is that a big deal? I don't know.
[01:19:33.380 --> 01:19:38.500]   You know, you know Japan pretty well, Alex. Do you have to have a store in Shinjuku?
[01:19:38.500 --> 01:19:44.100]   Shinjuku is a great place to have a store. I mean, the store, the only Apple store I've been to
[01:19:44.100 --> 01:19:48.660]   in Japan is Ginza, which is a really nice gorgeous.
[01:19:48.660 --> 01:19:50.660]   They have Ginza and Shibuya stores.
[01:19:50.660 --> 01:19:54.340]   I've been to Shibuya, the Shibuya store, but I've definitely been to the Ginza one.
[01:19:54.340 --> 01:19:58.580]   And I mean, Shinjuku would be, it's a massive shopping area.
[01:19:58.580 --> 01:20:01.300]   Isn't that where Akihabara is? Is it Shinjuku?
[01:20:01.300 --> 01:20:06.020]   I don't think so. I think that is in Chibu.
[01:20:06.020 --> 01:20:09.380]   Chibu, I think, or near it. I'd have to look back at it.
[01:20:09.380 --> 01:20:12.660]   That's the fun electronics district.
[01:20:12.660 --> 01:20:15.860]   It's in the Chiyota.
[01:20:15.860 --> 01:20:23.300]   Nevermind, then. I don't know anything about these names I'm throwing at.
[01:20:23.300 --> 01:20:28.100]   There's a lot of Hyand shopping right now.
[01:20:28.100 --> 01:20:33.300]   Dio Drive with Tokyo. I don't know if it's the Rodeo Drive, but there's a lot of incredible
[01:20:33.300 --> 01:20:36.420]   shops there, so I'm sure that that's part of the plan.
[01:20:36.420 --> 01:20:42.420]   This story from the Financial Times came out before yesterday's story about the Uber running down
[01:20:42.420 --> 01:20:47.700]   and killing a woman in Tempe, Arizona. Apple ramping up self-driving car testing.
[01:20:47.700 --> 01:20:51.460]   It has more permits in California than Tesla and Uber.
[01:20:51.460 --> 01:20:54.980]   If James Corden is driving, is that really self-driving?
[01:20:54.980 --> 01:20:59.220]   I hope that's not the self-driving car. He's not paying attention. He's focused on singing.
[01:20:59.220 --> 01:21:04.900]   He may as well be not driving. He looks like more like he's saying, "Tim, Tim, don't sing.
[01:21:04.900 --> 01:21:12.660]   Let Ferel do it. Tim, please, I beg of you." The big story out of Tempe, Arizona is that a
[01:21:12.660 --> 01:21:19.940]   self-driving Uber in autonomous mode, although it had a safety driver there. Last night ran down
[01:21:19.940 --> 01:21:26.980]   a pedestrian in Tempe, Arizona, killing her. Then now we've started to hear from the Tempe
[01:21:26.980 --> 01:21:31.220]   police who say it didn't look like there was anything they could do. They've reviewed video of
[01:21:31.220 --> 01:21:38.820]   it. It was 10 o'clock at night. She came out of shadows, and the car was going 40 miles an hour,
[01:21:38.820 --> 01:21:42.100]   and she wasn't at a crosswalk. She just walked into the street and
[01:21:42.100 --> 01:21:46.660]   tell you if that happens to a human or a self-driving car. My first reaction was it must be
[01:21:46.660 --> 01:21:51.620]   something like that. I feel like self-driving cars are not going to run into things if they
[01:21:51.620 --> 01:21:58.980]   can see them. Almost every self-driving accident so far has been some kind of human doing something
[01:21:58.980 --> 01:22:05.700]   crazy. Cars doing things in front of them that they can't calculate for, and there's not really
[01:22:05.700 --> 01:22:10.660]   any guarantee that a human would react any better in fact. Tempe police said they said that no human
[01:22:10.660 --> 01:22:15.380]   would have stopped that either. That there was unavoidable. The one thing you have is you have an
[01:22:15.380 --> 01:22:20.100]   enormous amount of footage. Yeah, you're certainly well documented.
[01:22:20.100 --> 01:22:26.100]   I think every accident that happens with an autonomous vehicle is going to end up with
[01:22:26.100 --> 01:22:33.060]   an enormous amount of data of exactly how it happened. We have to remember to put it in perspective.
[01:22:33.060 --> 01:22:40.500]   I mean, it's a horrible stat, but probably 70 to 80 people by a day to car accidents.
[01:22:40.500 --> 01:22:45.140]   And probably, ultimately, autonomous vehicles will save many lives, many more lives.
[01:22:45.140 --> 01:22:47.300]   It's a sentiment thing, though. We like to blame robots.
[01:22:47.300 --> 01:22:52.180]   That's right. You want to? Yeah. And Uber said, "Okay, that's it. We're not testing any more cars for a
[01:22:52.180 --> 01:22:58.100]   while here." Yeah, it's hard to get people to understand, to make an intellectual decision as
[01:22:58.100 --> 01:23:05.540]   opposed to an emotional decision, where yes, you can say that when it comes to self-driving cars,
[01:23:05.540 --> 01:23:11.700]   just to pick a number out of a hat. 100 people, it's like this. I'm going back up. It's almost like
[01:23:11.700 --> 01:23:18.100]   the seatbelt argument that had to be made like 30 or 40, 50 years ago. Because if we make everybody
[01:23:18.100 --> 01:23:24.980]   wear a seatbelt, it's then 100 people for every incident, every number of incidents, 100 people
[01:23:24.980 --> 01:23:30.900]   who would have died in that accident without that seatbelt will live. And out of 1000, maybe
[01:23:30.900 --> 01:23:34.740]   two people who would have survived without the seatbelt because they got tangled in it or whatever
[01:23:34.740 --> 01:23:40.740]   will die. And it's hard for people not to focus on the two people who were killed by this seatbelt
[01:23:40.740 --> 01:23:45.540]   as opposed to the lives that were saved by it. But it also points out to the biggest problem of
[01:23:45.540 --> 01:23:51.540]   self-driving technology. What you're describing is, I'm speaking as somebody who has been driving on
[01:23:51.540 --> 01:23:58.100]   Boston roads since he was 16 years old. And you don't talk about all the times where all the days
[01:23:58.100 --> 01:24:03.220]   you didn't get in an accident. It's when someone does something stupid and you manage to avoid
[01:24:03.220 --> 01:24:07.860]   killing yourself or that person. That's the time that you take to Twitter or Instagram to talk about
[01:24:07.860 --> 01:24:14.420]   what that idiot did. So it's going to be so hard to teach a to either teach self-driving cars.
[01:24:14.420 --> 01:24:19.700]   Yes, this total idiot decided to go from. You're missing the point though, Andy. A human would
[01:24:19.700 --> 01:24:26.100]   have done it too. There was no way to avoid hitting that person. So that's really the question is,
[01:24:26.100 --> 01:24:30.180]   did the self-driving car do worse than a human driver? And according to the 10 people,
[01:24:30.180 --> 01:24:36.100]   at least absolutely not. It was not enough. But they're going to be instance,
[01:24:36.100 --> 01:24:40.340]   there are going to be times where we're going to have to teach these cars how to identify
[01:24:40.340 --> 01:24:44.660]   something that shouldn't be there. That's how you avoid accidents. That there are some
[01:24:44.660 --> 01:24:49.700]   accidents that were totally avoidable. There are some that were totally avoidable because
[01:24:49.700 --> 01:24:54.740]   you simply decided what if this person that's kind of driving slightly erratically,
[01:24:54.740 --> 01:24:58.820]   it does the dumbest thing that could possibly do under the circumstance. I'm going to give
[01:24:58.820 --> 01:25:03.540]   that person not four car lengths, but eight car lengths so that when they almost get into that
[01:25:03.540 --> 01:25:07.940]   crash up ahead, they get they find a place to ditch. So I'm saying that that's going to be the
[01:25:07.940 --> 01:25:12.100]   level of sophistication that we're going to want from these cars. We're going to have to convince
[01:25:12.100 --> 01:25:17.460]   people that this is something we're going to have to expect. It will save far more lives than are
[01:25:17.460 --> 01:25:22.180]   taken. But we're going to have to be as the subtleties are going to get us. I think that's
[01:25:22.180 --> 01:25:26.420]   the biggest difficulty with this will be. Go ahead, Renee. I'll just say it really quickly. So I think
[01:25:26.420 --> 01:25:30.100]   the biggest challenge with this will be again will be sentiment. So for example, a human might
[01:25:30.100 --> 01:25:35.140]   swerve to avoid an animal on the street and kill another car. And we'll just all understand like
[01:25:35.140 --> 01:25:39.860]   you wanted to avoid that animal. But a computer might just run down that animal because it knows
[01:25:39.860 --> 01:25:43.220]   it's going to kill a human another car. And we'll just think what a horrible computer it killed
[01:25:43.220 --> 01:25:46.260]   an animal. And that's sort of the psychology we're going to have to overcome.
[01:25:46.260 --> 01:25:48.900]   Well, Spencer, what chips that Sam, so Alex, go ahead, Alex.
[01:25:48.900 --> 01:25:56.260]   Tesla, the Tesla already has had issues where I mean, not issues, but situations where the car
[01:25:56.260 --> 01:25:59.940]   actually started doing something different or started warning someone. Because another car
[01:25:59.940 --> 01:26:04.020]   in front of it was not doing what the car expected it to do. And it turned out that that person
[01:26:04.020 --> 01:26:08.260]   then got into an accident within minutes after the Tesla started to pull back.
[01:26:08.260 --> 01:26:15.780]   I think that the cars are over time are going to, I mean, Warren Buffett already put out a warning
[01:26:15.780 --> 01:26:23.540]   that Geico is in risk of having huge profit drops or revenue drops because of self-driving cars.
[01:26:23.540 --> 01:26:27.620]   He already knows the direction of how this is going to work. The cars are going to have
[01:26:27.620 --> 01:26:32.980]   so much more information than we do. They're going to have tons of information of knowing that
[01:26:32.980 --> 01:26:40.500]   this is a traffic stop where there's a higher risk of collision. This is a turn that has a higher
[01:26:40.500 --> 01:26:44.980]   risk of collision. Eventually, it could even get to a point where, I mean, this would,
[01:26:44.980 --> 01:26:49.380]   of course, worry a lot of people, but this person in front of you has a higher risk collision.
[01:26:49.380 --> 01:26:52.500]   You know, this is a, you know, like where it knows.
[01:26:52.500 --> 01:26:55.460]   >> We do. Humans do that. I already do that, don't you?
[01:26:55.460 --> 01:27:00.980]   >> What I'm saying is literally it could have the driver history of the person in front of you.
[01:27:00.980 --> 01:27:02.420]   >> Oh, yeah. It might know more than I do.
[01:27:02.420 --> 01:27:03.220]   >> Yeah.
[01:27:03.220 --> 01:27:06.820]   >> It doesn't give you the information personally, but it says statistically,
[01:27:06.820 --> 01:27:10.180]   you might want to give that car more room. And I think that,
[01:27:10.180 --> 01:27:15.860]   and there are built, you know, you talk, I was talking to an engineer that works on planes.
[01:27:15.860 --> 01:27:20.980]   And, you know, I asked, you know, when the, you know, when the, when he said, you know,
[01:27:20.980 --> 01:27:24.660]   all the planes can take off and land on their own. I mean, all the big airlines,
[01:27:24.660 --> 01:27:27.780]   those planes can take off and then they can fly and then they can land together.
[01:27:27.780 --> 01:27:31.540]   >> They have trouble. Well, they don't often do it. It's funny. He said,
[01:27:31.540 --> 01:27:35.060]   oh, they have to do it once a month to keep them certified. You know, you have to do it
[01:27:35.060 --> 01:27:38.980]   automated landing. And he said that the interesting thing was is that I said,
[01:27:38.980 --> 01:27:40.980]   so that's the one that's a little rocky and he goes, no, no, no.
[01:27:40.980 --> 01:27:41.700]   >> It's the other way around.
[01:27:41.700 --> 01:27:45.620]   >> When the plane lands, it's when it just barely feels like you're grounded.
[01:27:45.620 --> 01:27:46.580]   >> It's the perfect landing.
[01:27:46.580 --> 01:27:50.180]   >> And so, you know, that's the direction that I think that we're going to end up with automated,
[01:27:50.180 --> 01:27:53.540]   you know, with self-driving cars. In fact, I think that what's most likely is that we're
[01:27:53.540 --> 01:27:58.100]   going to go through this period where people, you know, that unfortunately are going to have,
[01:27:58.100 --> 01:28:03.380]   you know, interactions with the car that are lethal. But I think that you are going to get to a point,
[01:28:03.380 --> 01:28:10.900]   I think somewhere in the 2030s and the 2040s, where there are literally roads that you're not allowed to,
[01:28:10.900 --> 01:28:14.340]   like literally you can't get on the road if you're touching the wheel.
[01:28:14.340 --> 01:28:18.580]   You know, the wheel will pull into the car. You will not be allowed to touch it because the cars
[01:28:18.580 --> 01:28:22.020]   are moving too fast and they're expecting a very specific thing and they only interact with each
[01:28:22.020 --> 01:28:29.620]   other. And you're not able to, it gets beyond a human's ability to interact. And I think we're
[01:28:29.620 --> 01:28:33.380]   probably 20 or 30 years away from that. But I think that we're not going to a point where we're
[01:28:33.380 --> 01:28:37.540]   going to give up on it. I think we're going to go to a point where it's where we don't want to do
[01:28:37.540 --> 01:28:40.420]   it and we're not even allowed to. >> Well, that's what I'm finding interesting about this
[01:28:40.420 --> 01:28:45.460]   financial time story. Apple had three autonomous vehicles and testing a year ago.
[01:28:45.460 --> 01:28:53.780]   The number went to 27 just a few months ago and is now 45. So while we had said Apple might be
[01:28:53.780 --> 01:28:59.780]   losing interest in their autonomous vehicle research, it doesn't sound like they are.
[01:28:59.780 --> 01:29:04.660]   >> The manufacturing of the car, they shifted away from it, but they still have it on the software.
[01:29:04.660 --> 01:29:07.540]   >> So they're not going to build the car, but they want to sell the software.
[01:29:07.540 --> 01:29:10.820]   >> Well, I think they want to get the software to the point where it makes sense for them to make
[01:29:10.820 --> 01:29:15.140]   a car and not worry about making a car before they have the. >> So project tighten, which is their
[01:29:15.140 --> 01:29:22.500]   autonomous vehicle project isn't dead. >> No, but it's all more focused now on testing autonomous
[01:29:22.500 --> 01:29:25.860]   systems than it is. >> I mean, there's a whole, like this computer vision, there's ingestion,
[01:29:25.860 --> 01:29:30.580]   there's all sorts of technologies that you really have to make robust before you can even begin to
[01:29:30.580 --> 01:29:37.140]   feel the car in the project. >> Elon Musk is learning it might not be such a good idea to make cars.
[01:29:37.140 --> 01:29:40.260]   >> And Apple is, I mean, we're benefiting from the technology now and you look at what Apple's
[01:29:40.260 --> 01:29:43.220]   doing with computer vision and machine learning and everything. We're getting that, like the
[01:29:43.220 --> 01:29:46.980]   stuff that they're ingesting is a huge part of ARKit, it's a huge part of CoreML, it's a huge part
[01:29:46.980 --> 01:29:50.260]   of a lot of the technologies that we're enjoying now. So it makes a ton of sense for Apple to keep
[01:29:50.260 --> 01:29:58.340]   those projects going. >> Here's Apple's latest face unlock advertisement. Young woman in high
[01:29:58.340 --> 01:30:05.780]   school walking down the hall, her phone unlocks, and she gives you that look. >> Then her locker unlocks,
[01:30:06.660 --> 01:30:10.420]   and she gives another look. >> Then every locker is unlocking. >> See, it's failing.
[01:30:10.420 --> 01:30:15.060]   Faced by these not keeping her out of those students today. >> Yeah, I don't know why they're
[01:30:15.060 --> 01:30:20.020]   celebrating this. >> The wrong message. >> She's basically going down the hall,
[01:30:20.020 --> 01:30:25.140]   unlocking all the lockers, and now she's opened all the desks. She's really terrorizing the school.
[01:30:25.140 --> 01:30:32.100]   >> I was wondering if this was a promo for the next Marvel Avengers TV show. >> That is a super
[01:30:32.100 --> 01:30:36.020]   human ability. >> X-Men movie. >> She's a mutant that unlocks everything with her eyes.
[01:30:36.900 --> 01:30:41.860]   >> Even the science lab. >> It's actually a great ad. It's a fun ad, but yeah, I'm not sure really.
[01:30:41.860 --> 01:30:48.660]   >> This is what you want. >> So, well, now she's just being a jerk, frankly.
[01:30:48.660 --> 01:30:54.740]   >> I mean, look at the mess that one custodian that they saw is gonna have to clean up. Thank you
[01:30:54.740 --> 01:31:00.820]   very much, girl. >> The only question I have given the timing of this and the timing of the event
[01:31:00.820 --> 01:31:09.380]   next week is whether there's any kind of connection to Apple increasing security,
[01:31:09.380 --> 01:31:17.540]   where or access, where that phone or the Face ID lets you do other things within,
[01:31:17.540 --> 01:31:23.380]   maybe you open your locker with your- >> Fast phone. >> Which would cost millions.
[01:31:23.380 --> 01:31:27.940]   >> A locker. >> By the way, speaking of Tesla, you can now use Apple Pay for your Model 3 reservation.
[01:31:29.860 --> 01:31:35.140]   >> That makes it way too easy. I really don't want to tap to pay our company the $65,000.
[01:31:35.140 --> 01:31:35.780]   >> [LAUGH]
[01:31:35.780 --> 01:31:40.020]   >> I want there to be a sense of ceremony. I don't want to be better at the store.
[01:31:40.020 --> 01:31:44.900]   >> We had that story a year ago. Remember that Leo, the guy bought a Bentley or Rolls Royce on
[01:31:44.900 --> 01:31:49.380]   Apple Pay? >> Little too easy. >> I have bought stuff.
[01:31:49.380 --> 01:31:55.380]   I remember buying this 10.5 inch iPad. I was on the plane and I bought it. It was like, wow,
[01:31:55.380 --> 01:32:02.500]   that was way too easy. I have it. Wow. >> In an era where they're monitoring everything
[01:32:02.500 --> 01:32:06.660]   we do in real life using credit cards, using a token that's anonymized is not a bad idea.
[01:32:06.660 --> 01:32:09.540]   >> I agree. >> Facebook has no idea you bought that iPad.
[01:32:09.540 --> 01:32:12.900]   >> I guess you'd have to have a credit card with a high limit.
[01:32:12.900 --> 01:32:16.740]   Still using a credit card. >> Yeah, American Express, all right.
[01:32:16.740 --> 01:32:18.100]   >> All right. >> Preset limit. >> I know.
[01:32:18.100 --> 01:32:21.700]   >> I know. >> Don't attach your Apple ID to your American Express
[01:32:21.700 --> 01:32:25.300]   platform cards. >> I just bought a yacht.
[01:32:25.300 --> 01:32:31.700]   Let's take a break. Gentlemen, get your picks ready. I'm going to show everybody this.
[01:32:31.700 --> 01:32:40.660]   This is my lighthouse torn from its nice place up over on the mantelpiece because I wanted to
[01:32:40.660 --> 01:32:46.420]   show you. I love this. This is not just a security camera, my friends. This is my lighthouse. It has
[01:32:46.420 --> 01:32:54.660]   3D, LIDAR, time of flight LIDAR built in. There's the camera below it, infrared light. So it's a
[01:32:54.660 --> 01:33:02.500]   camera that works in the dark. And AI built in. It's the only camera that understands what it's
[01:33:02.500 --> 01:33:07.620]   seeing. You can't always be home, but if you have kids and pets and nanny, a dog walker,
[01:33:07.620 --> 01:33:14.100]   people in your house, it's really helpful to know who belongs, who doesn't, to not be surprised by
[01:33:14.740 --> 01:33:20.900]   pet pictures as burglars. Lighthouse understands that life is more than a bunch of motion alerts.
[01:33:20.900 --> 01:33:25.300]   It really isn't anything like the lighthouse. I love it. You can buy it now. It's finally out.
[01:33:25.300 --> 01:33:33.860]   $2.99 for the camera, $10 a month for the AI service. You get 3D sensing. The same stuff in
[01:33:33.860 --> 01:33:39.620]   those self-driving cars. It's time of flight LIDAR. It knows the difference because of that
[01:33:40.260 --> 01:33:48.740]   between people, pets, mylar balloons, 1080p HD automatic night vision. Oh, by the way,
[01:33:48.740 --> 01:33:55.780]   two-way talk too. So it's got microphone. It's got a speaker in it. Security siren. Set it up
[01:33:55.780 --> 01:34:01.460]   easily via Wi-Fi. One thing that's really important to us for privacy, Lisa said, we're not putting
[01:34:01.460 --> 01:34:06.260]   a camera in the house unless it turns off when I'm home. It turns off when we're home.
[01:34:07.620 --> 01:34:14.340]   Easily. It sees when I walk in the door and it says, "Okay, camera off." You also could turn it
[01:34:14.340 --> 01:34:20.580]   off via the app. I have the app on my iPhone right now. You can see my camera is off. It's in privacy
[01:34:20.580 --> 01:34:28.580]   mode, probably because somebody's home. But I love this part. Here's the siren. I won't do that now
[01:34:28.580 --> 01:34:33.460]   because somebody's home and scared the hell out of them. Two-way talk. But I also love this. You can
[01:34:33.460 --> 01:34:38.900]   see who, so you see Lisa and I are our owners. Oh, Lisa's home. So that's how we know.
[01:34:38.900 --> 01:34:43.140]   Lisa and I are owners. But you can see if other people, it sees when I've left, when I've arrived,
[01:34:43.140 --> 01:34:48.500]   when Lisa's arrived, when Lisa's left. And look at this. This is so cool. I've got 30 days of video
[01:34:48.500 --> 01:34:54.500]   and I can search it with natural language questions. It even knows who people are. So I can say,
[01:34:54.500 --> 01:34:59.860]   "Ping me when Lisa comes or goes. Show me a time lapse from yesterday. Let me know if Lisa
[01:34:59.860 --> 01:35:05.780]   leaves after 9 a.m. Monday. Let me know if the cats go in my office. What did the kids do
[01:35:05.780 --> 01:35:14.020]   while I was out? Show me a summary of Friday. It's truly awesome. You can ask about people,
[01:35:14.020 --> 01:35:19.140]   pets. Like, did you see the cat between three and five yesterday? We've been using this to track
[01:35:19.140 --> 01:35:24.420]   our new feral cat, Hamilton, who was a house cat, then just decided he liked life in the great
[01:35:24.420 --> 01:35:28.580]   outdoors. We hadn't seen him for a week. I said, "Wait a minute. We could check and see."
[01:35:29.380 --> 01:35:34.500]   And he's been coming in after we leave and leave him before we come home. He comes in,
[01:35:34.500 --> 01:35:38.980]   he eats, he sleeps, and then he leaves so we never see him. But the lighthouse sees him.
[01:35:38.980 --> 01:35:45.220]   I love this lighthouse. You also have this great feature. If the kid, if you have a younger kid
[01:35:45.220 --> 01:35:50.180]   doesn't have a cell phone, say, "Just go stand for the lighthouse and wave and I'll talk to you."
[01:35:50.180 --> 01:35:53.940]   And it pings me that somebody's standing in front of my lighthouse and waving and I say,
[01:35:53.940 --> 01:36:01.540]   "Yeah, what do you want?" Lighthouse is so cool. No more spurious alerts only when important things
[01:36:01.540 --> 01:36:08.260]   happen when you're away from home. Lighthouse is the website. Lighthouse. If you use a promo
[01:36:08.260 --> 01:36:13.700]   code, "Twit," you'll get 15% off your lighthouse camera and because you're an early customer,
[01:36:13.700 --> 01:36:20.740]   90 days of AI service free. I like that. Lighthouse, 15% off of the promo code,
[01:36:20.740 --> 01:36:27.060]   "Twit," and don't forget early customers get 90 days of AI service at no cost. Lighthouse.
[01:36:27.060 --> 01:36:32.260]   I've been waiting for this to come out. We had it early on when we could, they had a few
[01:36:32.260 --> 01:36:37.780]   hundred for Twit viewers and now it's out officially and available. And I have two of them. And I'll
[01:36:37.780 --> 01:36:43.300]   probably get more. I really love this. And it's, you know, I put it right there. It's nice. It's got a
[01:36:43.300 --> 01:36:48.420]   rotating base. Very easy. He eats a pair of googly eyes. I could do that. Well, you don't want to cover
[01:36:48.420 --> 01:36:51.620]   that. I may have to put them on top. Watch a go.
[01:36:51.620 --> 01:36:58.500]   Like, I'm looking for Leo. It's really, I don't want to show you the video because Lisa's, you know,
[01:36:58.500 --> 01:37:02.980]   she does no video of inside our house. I said, "Okay, I won't show you the video." But it really is
[01:37:02.980 --> 01:37:06.580]   great. You can really see what's going on. I always know when somebody's in there. It outlines
[01:37:06.580 --> 01:37:12.580]   the people in the pets. He says, "Look, see, there's what I'm talking about. Let's get your picks of
[01:37:12.580 --> 01:37:18.740]   the week. Alex Lindsay, I got my checkbook out." This one, this one's a big one. Yeah, this one's a
[01:37:18.740 --> 01:37:25.140]   big one. I feel like I've been playing nice for a while and I've just decided that. So we've been,
[01:37:25.140 --> 01:37:32.340]   we've been doing tests, you know, here in, in DC. And I just had, I had to show you this. So,
[01:37:32.340 --> 01:37:36.500]   so, you know, I think, Leo, you've seen me do this before, you know, with the water bottle,
[01:37:36.500 --> 01:37:40.740]   which is very hard to key out. But wait a minute, I could see through the bottle and see the picture
[01:37:40.740 --> 01:37:46.260]   behind you. That's not possible. And, well, and the interesting thing is, is that this is,
[01:37:46.260 --> 01:37:51.220]   now, in the past. Because you're not really in front of a real picture, you're in front of a
[01:37:51.220 --> 01:37:56.420]   green screen, right? Well, yeah, but I haven't been in the past, but I am here. You can see it there.
[01:37:56.420 --> 01:38:03.780]   You can see, see, there's the background. There's the mat. There's the-
[01:38:03.780 --> 01:38:07.460]   Leo, look at your hair, Matt. This is like the Jerry Todd show. I love this.
[01:38:08.340 --> 01:38:11.780]   Let me just see if I could do it. It's hard to do the hair, though. You see all those fine hairs?
[01:38:11.780 --> 01:38:16.660]   If you look at that hair, yeah. To do that live is, I've never seen it before, to be honest with
[01:38:16.660 --> 01:38:21.780]   you. Like, so, so this is the new ultimate, 12 from Blackmagic. They bought ultimate. Oh, yeah,
[01:38:21.780 --> 01:38:26.340]   because that was the brand name. That was the name for Matt. Yeah. So, so what's interesting is,
[01:38:26.340 --> 01:38:32.980]   is that they, they bought, they bought ultimate. And I, not that I'm bitter, but I did pay
[01:38:32.980 --> 01:38:40.820]   $33,000 myself for the ultimate 11. So, and this ultimate 12 is $10,000.
[01:38:40.820 --> 01:38:46.980]   This is a typical, now I bought it years ago, I paid it off. It's fine. But, but the,
[01:38:46.980 --> 01:38:51.540]   the main thing is, is that, is that, because it's got a 4K processor, because it's using,
[01:38:51.540 --> 01:38:56.820]   you know, without all the bits and pieces, they've dropped the price to, now I know the price is
[01:38:56.820 --> 01:39:02.180]   still really high for most of our viewers. So 4K, Matt? It's 4K. It's a 4K.
[01:39:03.060 --> 01:39:09.700]   Uh, key. And, and I literally, uh, the first, the first time we turned it on,
[01:39:09.700 --> 01:39:16.340]   because they also took some of this. And if you look at the 12G, their, or their 4ME or 12G,
[01:39:16.340 --> 01:39:21.380]   2ME switcher, they put some of the tools from Ultimat into that. So, when I saw that, we were
[01:39:21.380 --> 01:39:25.300]   testing that. I was testing it. I was like, this is great. This looks really good. Like, it was
[01:39:25.300 --> 01:39:31.220]   good enough to, to use. And then I, then I turned the Ultimat on and I was like, oh my goodness. This
[01:39:31.220 --> 01:39:36.980]   is the best key I have ever, I have ever seen. Best Live Key. And I would say that
[01:39:36.980 --> 01:39:40.820]   arrogantly, I always felt that the best Live Key before this was Conduit, which is something
[01:39:40.820 --> 01:39:44.740]   that we developed. Um, and, uh, and so I kept on going, well, it's not as good as Conduit,
[01:39:44.740 --> 01:39:49.060]   but it's okay. It's not as good as Conduit, but this is literally the best key I've ever had live.
[01:39:49.060 --> 01:39:52.180]   It's more than just a background, like the weather report, you know,
[01:39:52.180 --> 01:39:57.060]   map behind you, because you could put stuff in front too, which is really interesting.
[01:39:57.060 --> 01:40:01.300]   And you have, and you have garbage matting and you have a lot of other things. It is
[01:40:01.300 --> 01:40:09.620]   combined with a 4K camera we're using a, this camera is a black magic mini or so. The, the,
[01:40:09.620 --> 01:40:13.620]   the key is just, I mean, it's just stunning. And it was almost out of the box. Like literally,
[01:40:13.620 --> 01:40:16.740]   when I turned it on, it was amazing. And I tweaked it a little bit, but it was like,
[01:40:16.740 --> 01:40:20.420]   literally it came right out, turned right on and I was like, well, there you go. So, um,
[01:40:20.420 --> 01:40:25.540]   Look at this. Here's Goldfish. This is so cool. This is Goldfish on a, on a green screen, right?
[01:40:25.540 --> 01:40:30.100]   And I'll, I'll check this box. Now they're swimming in front of all this other stuff.
[01:40:30.100 --> 01:40:31.860]   They're in the, they're, I put them in the tank.
[01:40:31.860 --> 01:40:37.060]   That is, that is really, really amazing translucent fins and everything. That's, yeah.
[01:40:37.060 --> 01:40:40.340]   You can see, look at the back finger. You can see through it and you see the coral above through it.
[01:40:40.340 --> 01:40:46.020]   Wow. Yeah. It's, and that's live. I mean, you know, it's, it's, we do this kind of
[01:40:46.020 --> 01:40:51.460]   compositing in film all the time with, you know, new core, what we take. Shake and it takes a,
[01:40:51.460 --> 01:40:56.100]   well, it just takes a long time to process. You can't do it live. And this is, I mean,
[01:40:56.100 --> 01:40:59.300]   it's, it's incredible. You know, it is literally the best key that I have.
[01:40:59.300 --> 01:41:01.780]   Everyone should have one of these in their house. Yeah.
[01:41:01.780 --> 01:41:05.380]   You should. I, you know, I have to admit it's since it was sitting in front of me and I,
[01:41:05.380 --> 01:41:09.940]   and I just keep on going like this, like, Oh my goodness. Look at that's pretty cool.
[01:41:09.940 --> 01:41:14.100]   Hair. I mean, it's, it's, uh, you know, it really is doing a good job. I don't see any
[01:41:14.100 --> 01:41:18.740]   halo at all or anything or. No, well, and also if you look at the little hairs on my head that are
[01:41:18.740 --> 01:41:23.860]   kind of sticking out, they are, I mean, it's getting all these little like really, really
[01:41:23.860 --> 01:41:28.340]   single hairs that are coming out and there's not any spill. There's not any, you know, it is,
[01:41:28.340 --> 01:41:35.220]   it's truly, um, I, again, I, I usually, I've started to, I know I have a reputation for putting
[01:41:35.220 --> 01:41:39.860]   expensive stuff up here. I've, I've started to only 15, about 0.7. Alex, is it's not that bad?
[01:41:39.860 --> 01:41:45.540]   I've been, I've, I've, I think about it very hard if I'm going to do something that's,
[01:41:45.540 --> 01:41:51.460]   that's super expensive, but, or for, for over the above the, above the average, but, um, it's a
[01:41:51.460 --> 01:41:55.860]   really, it's a really, if you're in the market for this kind of thing, I mean, this is, and when you
[01:41:55.860 --> 01:41:59.460]   think about doing your own little broadcast or a, or not even that, but if you're looking at a
[01:41:59.460 --> 01:42:04.420]   digital set, we're looking at it for larger sets. Um, I'm just thinking when I retire in 10 years
[01:42:04.420 --> 01:42:10.340]   and move to Hawaii, no one will know. In fact, no, no, I might be retired now.
[01:42:11.300 --> 01:42:22.660]   You could, you could definitely do that. I'm not even wearing any clothes. This is all made up.
[01:42:22.660 --> 01:42:27.300]   You know, Leo, Leo, this is a lot less money than it would cost for me to like buy a much bigger
[01:42:27.300 --> 01:42:30.740]   and more expensive house. Exactly. I want to fake people into thinking that about freedom
[01:42:30.740 --> 01:42:36.980]   of living. Yeah. Exactly. Leo, Leo, we should, we should actually do your, you know, like, we
[01:42:36.980 --> 01:42:42.660]   could try it. You could do the, uh, you could do the avatars, the places where people's faces
[01:42:42.660 --> 01:42:46.420]   throw up and everything, right? You could key that. Well, we could, we could do that, but I'm
[01:42:46.420 --> 01:42:50.500]   saying like just with you, we could just take you out, put you in another room with green screen,
[01:42:50.500 --> 01:42:55.300]   and then just, it just has an idea of just like sticking you in there and then, uh, having a close
[01:42:55.300 --> 01:42:59.940]   up and a wide and you just kind of cut back and forth. We should try that. See if anyone notices.
[01:42:59.940 --> 01:43:06.660]   Black magic now owns ultimate and the new ultimate 12 is out for less, a lot less.
[01:43:06.820 --> 01:43:10.100]   Then Alex paid for his ultimate 11. Not that I'm bitter.
[01:43:10.100 --> 01:43:14.980]   Then he maybe just a little and he and I go your pick of the week, my friend.
[01:43:14.980 --> 01:43:21.460]   Mine is not very tech related, but boy, did it save my life. I'm in the last, finally, the last
[01:43:21.460 --> 01:43:27.540]   throes of moving out of the old house. Uh, this is, this is the point. This is the point at which
[01:43:27.540 --> 01:43:32.260]   the only thing that was as of last week, the only thing left inside the house were boxes and boxes
[01:43:32.260 --> 01:43:38.580]   of books and just furniture that what I wasn't taking and old, old CRTs. I had a 20,
[01:43:38.580 --> 01:43:45.140]   you remember the 24 inch power computing, like white screen CRT? Yeah, that give it this. And so
[01:43:45.140 --> 01:43:48.820]   I'm trying to figure out, Oh, damn it. How am I going to get rid of all this sort of stuff?
[01:43:48.820 --> 01:43:53.140]   And I'm, I've been planning on renting in dumpster and just having it all all the way.
[01:43:53.140 --> 01:43:58.420]   And then finally, I decided I'm just going to throw money at this problem. I called 1-800-GOT-JUNK.
[01:43:58.420 --> 01:44:01.940]   And they were on the website. And we see their trucks going around.
[01:44:01.940 --> 01:44:07.380]   And it was the best choice I ever made. You're pick as 1-800-GOT-JUNK.
[01:44:07.380 --> 01:44:14.500]   Because you're going to remember this at some time in the future. They will, if all you have is
[01:44:14.500 --> 01:44:20.740]   like a couple, like an old sofa or like in my case, I have this, my dad used to work at
[01:44:20.740 --> 01:44:25.860]   Westinghouse. And at some point, they upgraded like all their office furniture from this like
[01:44:25.860 --> 01:44:32.820]   1950s, 1960s, huge steel drafting desks to something more modern. And it was just, I remember going
[01:44:32.820 --> 01:44:37.540]   with him to like a warehouse at Westinghouse and picking up a couple of these big desks.
[01:44:37.540 --> 01:44:41.620]   They're beautiful, huge desks. But the thing is, they were going to be impossible for me to,
[01:44:41.620 --> 01:44:45.540]   I didn't need it anymore. I couldn't have a use for it. I needed to get rid of it. So
[01:44:45.540 --> 01:44:49.940]   anything from just like that, one huge desk that you have in the basement because you can't figure,
[01:44:49.940 --> 01:44:54.740]   you don't have a car big enough to take it to the trash or whatever, all the way to the entire
[01:44:54.740 --> 01:45:00.820]   house. And now is just filled with trash junk, old furniture, paint cans, stuff that would normally
[01:45:00.820 --> 01:45:07.300]   be like hazardous material pickup recycling. They will come with trucks. It costs you nothing to
[01:45:07.300 --> 01:45:11.140]   make an appointment. They will show up. They, you show them what you want them to do. They will give
[01:45:11.140 --> 01:45:15.540]   you an estimate. If you say, no, that's not too, that's too much for me to do. They will say goodbye.
[01:45:15.540 --> 01:45:20.580]   That thank you for calling us anyway. And it costs you nothing. Or they will just simply solve the
[01:45:20.580 --> 01:45:26.020]   problem for you. It's like, it's like being in the mob and calling, say, yeah, I got a problem,
[01:45:26.020 --> 01:45:30.660]   I need to take care of. They will simply take care of the problem. And they were really great.
[01:45:30.660 --> 01:45:38.900]   They were really there. They were super, super fast. And at the end of the the morning, they'd
[01:45:38.900 --> 01:45:44.180]   called away two huge trucks full of just stuff, stuff, stuff. The other good news is that they don't,
[01:45:44.180 --> 01:45:49.860]   they don't necessarily donate things to charity. But the thing is that big steel desk, if it's
[01:45:49.860 --> 01:45:55.060]   usable, they will find a place. It will be put back to use by someone somewhere.
[01:45:55.060 --> 01:46:00.180]   They will even if you get rid of a mattress that is still in good shape and could be recovered,
[01:46:00.180 --> 01:46:05.700]   they will recover the mattress and sell it or put it to you somewhere. So it's not as so you feel as
[01:46:05.700 --> 01:46:10.660]   we're asked most of the stuff would have gone to a landfill. A lot of the stuff that I'm getting
[01:46:10.660 --> 01:46:16.180]   rid of has gone to charity. But again, I don't have the ability to transport like a 20 year old sofa
[01:46:16.180 --> 01:46:21.860]   that's already kind of beaten up someplace. So it's not it was not certainly not as cheap as simply
[01:46:21.860 --> 01:46:26.340]   running a dumpster and hauling everything over there. But it was you make a phone call,
[01:46:26.340 --> 01:46:31.860]   you had a I had a problem at 9 a.m. by 1130 a.m. that same day, all of that problem was someone
[01:46:31.860 --> 01:46:36.340]   else's problem. So I'm just saying that if you're again, whether it's a whole house full of stuff,
[01:46:36.340 --> 01:46:41.540]   whether it's a storage locker full of stuff, whether it's again one huge steel drafting desk that
[01:46:41.540 --> 01:46:45.940]   took eight guys to move into your house eight years ago and has been stuck there ever since.
[01:46:45.940 --> 01:46:49.540]   They will do anything large and small. And I just I was very, very happy with their search.
[01:46:49.540 --> 01:46:52.900]   I think we have some video of them arriving at Andy's house. Yeah.
[01:46:52.900 --> 01:46:54.820]   Exactly that. Yeah.
[01:46:54.820 --> 01:46:59.860]   Collewool. Yeah. I call you had to call the wolf. I heard. Yeah, please.
[01:46:59.860 --> 01:47:06.340]   And called the wolf. It was it was very much like that. They showed up with a clipboard. They
[01:47:06.340 --> 01:47:12.580]   okay. What what what can we help you with today? The body. I mean, the problem is over there in
[01:47:12.580 --> 01:47:17.700]   the living room. Mr. Renee Richie, your pick of the week. Well, first I will second Andy's pick.
[01:47:17.700 --> 01:47:21.380]   I use them. I must use them two or three times when I moved and I just I always found stuff that
[01:47:21.380 --> 01:47:25.220]   I didn't want to deal with and they happily I think encountered at least they do donate. So it was
[01:47:25.220 --> 01:47:30.020]   just I didn't feel bad about doing it. And it got rid of all of my problems and I was so happy with
[01:47:30.020 --> 01:47:35.220]   it. So highly recommended. My pick of the week is an app that Jason Snow, I'm sure has picked
[01:47:35.220 --> 01:47:40.740]   previously and it's called Fairight. It's a podcasting editing app for the iPad. I use it on the iPad
[01:47:40.740 --> 01:47:47.140]   Pro. And I still do a lot of editing and logic, but it has this one feature that of course I heard
[01:47:47.140 --> 01:47:51.380]   about on Twitter when I was discussing an old program called the level leader with Jason Snell.
[01:47:51.380 --> 01:47:54.580]   And we were lamenting the fact that it wasn't continued anymore and nobody had really made it
[01:47:54.580 --> 01:47:59.620]   in a replacement. And yeah, you could do all the same things if you applied a lot of specific filters
[01:47:59.620 --> 01:48:04.980]   in in logic or if you used audition or something. But we just really it was just so simple. You
[01:48:04.980 --> 01:48:09.620]   would click it and it wouldn't be fantastic. And you know, maybe real sound editors would look at
[01:48:09.620 --> 01:48:13.620]   you like you smelled really, really bad. But it would just solve a lot of problems really quickly
[01:48:13.620 --> 01:48:18.980]   and really easily. And Canis, the developer behind Fairight said, Hey, you know, in his beautiful brogue,
[01:48:18.980 --> 01:48:24.020]   I've solved this problem for you. It is included in Fairight. It's buried a little bit in the settings
[01:48:24.020 --> 01:48:28.340]   because not that many people use it. But if you go in there and before you export, you can just
[01:48:28.340 --> 01:48:32.660]   press that button and it'll level everything for you. And like, you know, a couple weeks later,
[01:48:32.660 --> 01:48:37.380]   I had a problem, I had audio because once in a while, the whole podcasting machine breaks down
[01:48:37.380 --> 01:48:41.620]   somebody forgets or somebody forgets to record locally or they don't have they have a lot of
[01:48:41.620 --> 01:48:46.260]   leakage in their audio or you hear a lot of fan noise and you can't use it. But the Skype is too
[01:48:46.260 --> 01:48:50.580]   hot for one person, way too low for other people. And you just don't have time to go in and
[01:48:50.580 --> 01:48:55.380]   tediously edit every little, you know, in one way form, edit everybody to be normal. And it just,
[01:48:55.380 --> 01:49:00.900]   I imported it into Fairight on the iPad Pro. I pressed that one button, it exported it beautifully,
[01:49:00.900 --> 01:49:07.140]   and I could use the audio immediately. No knowledge or skill required on my end. And if there's
[01:49:07.140 --> 01:49:11.060]   one thing and you're Fairight, you can download it for free. There is a Pro Pack that I pay for,
[01:49:11.060 --> 01:49:15.780]   but if there's one thing that is more important to me than money and that's time, like I can
[01:49:15.780 --> 01:49:19.540]   just never make more of it. And when you have tools like this that do a really, really good job
[01:49:19.540 --> 01:49:24.100]   simply and easily. In this case, literally the touch of a button, they're just absolutely invaluable.
[01:49:24.100 --> 01:49:29.940]   So do you work vector onto this? I don't. I record, I record vector like,
[01:49:29.940 --> 01:49:34.580]   multi in multiple ways on multiple machines, because I'm so scared now of single points of failure.
[01:49:35.780 --> 01:49:39.940]   But you know, when I travel, I have used Fairight in the past, but I do use it for editing. I do use
[01:49:39.940 --> 01:49:44.260]   it, you know, to do different sound effects. And now whenever I really just need to level something
[01:49:44.260 --> 01:49:51.060]   and I need to do it super fast, I just pull it into Fairight and I go. Nice. F E R I T E. It's
[01:49:51.060 --> 01:49:55.940]   $20 if you want to get the whole thing, but free to try. Yeah. And Jason has a bunch of really
[01:49:55.940 --> 01:49:59.300]   great resources on six colors about how he edits with it, because he does do a lot of the editing
[01:49:59.300 --> 01:50:04.100]   in it. Nice. Oh, he does a lot of it. By the way, Leo, a follow up on our, on our, on our
[01:50:04.100 --> 01:50:10.740]   recommendations from last week, evidently the popping sold out. You should never have recommended
[01:50:10.740 --> 01:50:18.340]   those. There won't be any more until corn season. Didn't know what I was doing. I needed to
[01:50:18.340 --> 01:50:23.780]   preorder before I before I ordered it. Now I'm you're, you know, I'm just glad Lisa ordered a hundred
[01:50:23.780 --> 01:50:28.420]   of them. Okay, I'm going to talk to Lisa. I'm going to see if Lisa's got, oh, no, we're hard
[01:50:28.420 --> 01:50:35.220]   in our pop and cops. So I love this. It says, please check back in September.
[01:50:35.220 --> 01:50:43.220]   After the harvest, we should have more con cops. Thank you very much, Alex. That family was going
[01:50:43.220 --> 01:50:49.540]   to go on vacation to Disney World for Easter. Now they're filling orders because of you. You have
[01:50:49.540 --> 01:50:53.940]   to wonder when these things happen and they happen all the time. What these companies think like,
[01:50:53.940 --> 01:50:58.340]   good Lord, why did I sales triple last year last week? I mean, what happened?
[01:50:58.340 --> 01:51:05.300]   Lindsay happened Lindsay Josh Adam at pop and cops. We're sorry. We sorry to have done.
[01:51:05.300 --> 01:51:07.060]   Alex. You got Alex.
[01:51:07.060 --> 01:51:11.700]   And I'm just glad we got ours before you picked those.
[01:51:11.700 --> 01:51:14.100]   Dagnabbit. Ladies, gentlemen, thank you.
[01:51:14.100 --> 01:51:15.060]   I've only gotten my four.
[01:51:15.060 --> 01:51:22.340]   Join the mailing list. You can get a, get a, get in early and wipe them out before we get there.
[01:51:22.340 --> 01:51:23.860]   Lisa bought a hundred of them.
[01:51:23.860 --> 01:51:25.780]   Dang it.
[01:51:25.780 --> 01:51:31.540]   Secondary market Leo reselling steep markups. We've gone on the market on cops.
[01:51:31.540 --> 01:51:35.620]   You can get your house in the light just using those cops. You become a cop.
[01:51:35.620 --> 01:51:40.180]   Baron. You're so good. And you know, I would do that, but I don't want to. I love them.
[01:51:40.180 --> 01:51:44.500]   They're so good. All right. We are out of time. Thank you so much. Renee Richie.
[01:51:44.500 --> 01:51:48.180]   Follow the vector podcast and everything Renee does at imore.com.
[01:51:49.060 --> 01:51:53.140]   Great podcasts every day now with vector every day.
[01:51:53.140 --> 01:51:56.660]   Thank you. I'm doing three audio and two or three video a week.
[01:51:56.660 --> 01:51:59.460]   So I'm trying to mix the crazy crazy man.
[01:51:59.460 --> 01:52:02.420]   You're, you're, you're, you're, you just got to warn you.
[01:52:02.420 --> 01:52:04.420]   You're going to look like me in a couple of years.
[01:52:04.420 --> 01:52:07.140]   It keeps me off the streets. Oh, it's me and I should be so lucky.
[01:52:07.140 --> 01:52:10.340]   Old and worn out.
[01:52:10.340 --> 01:52:15.300]   Andy and I go Chicago Sun times his website C.W.O.B.com.
[01:52:15.300 --> 01:52:20.260]   He's on the Twitter at IHNATKO. Thank you, Andrew.
[01:52:20.260 --> 01:52:21.060]   I'm going, thank you.
[01:52:21.060 --> 01:52:24.900]   Mr. Alex Lindsey, he's, he's at the Pixel Core. You can now become a Pixel Core member
[01:52:24.900 --> 01:52:27.220]   through their Patreon page. Patreon.com.
[01:52:27.220 --> 01:52:27.780]   We had a great.
[01:52:27.780 --> 01:52:28.420]   Pixel Core.
[01:52:28.420 --> 01:52:33.140]   We had a great session with Fred. We had kind of a one hour version of what we did on,
[01:52:33.140 --> 01:52:37.300]   on the screen savers with Q&A and kind of really big news.
[01:52:37.300 --> 01:52:38.340]   Oh, the light archive.
[01:52:38.340 --> 01:52:42.020]   Boy, this guy was a former surveyor who's now become the light arch.
[01:52:42.020 --> 01:52:42.820]   Still a surveyor.
[01:52:43.380 --> 01:52:45.300]   Yeah, but he does it in 3D now.
[01:52:45.300 --> 01:52:46.500]   Awesome.
[01:52:46.500 --> 01:52:49.620]   Yeah, and we're going to do a Q&A this Friday, so that's going well.
[01:52:49.620 --> 01:52:52.900]   We're also, the sale has turned into a bit of a trickle because it takes us a while.
[01:52:52.900 --> 01:52:55.380]   So if you've signed up, we are sending out updates.
[01:52:55.380 --> 01:52:59.620]   You can go to pickscore.com/sale and sign up, even though it says old dates, I'll fix that.
[01:52:59.620 --> 01:53:03.060]   But we are, we're selling more gear.
[01:53:03.060 --> 01:53:05.380]   So we're going to keep on putting that stuff up on going late.
[01:53:05.380 --> 01:53:07.060]   The check is in the mail for the warehouse.
[01:53:07.060 --> 01:53:09.780]   For that camera. I'm sorry, I thought they had paid you.
[01:53:09.780 --> 01:53:10.420]   No, it's all good.
[01:53:10.420 --> 01:53:12.420]   I talk a camera. I forgot to give them any money.
[01:53:13.380 --> 01:53:14.260]   No, no, it's fine.
[01:53:14.260 --> 01:53:15.700]   Thank you, Alex.
[01:53:15.700 --> 01:53:17.060]   Thank you, everybody, for joining us.
[01:53:17.060 --> 01:53:19.220]   We do Mac Break Weekly right after iOS today.
[01:53:19.220 --> 01:53:22.420]   As I said, next week might be a little extra delayed because of Apple's event,
[01:53:22.420 --> 01:53:26.420]   but we will cover that event and tell you everything there is to know about it.
[01:53:26.420 --> 01:53:30.980]   Usually around 11 a.m. Pacific, that's 2 p.m. Eastern time, 1800 UTC.
[01:53:30.980 --> 01:53:33.140]   You can watch at twit.tv/live.
[01:53:33.140 --> 01:53:38.260]   If you do that, please go into the chat room and be part of the crowd in the back of the bus there
[01:53:38.260 --> 01:53:44.260]   at IRC.twit.tv. You can also listen on demand.
[01:53:44.260 --> 01:53:49.060]   We have on demand video and audio for all our shows at our website.
[01:53:49.060 --> 01:53:57.220]   In this case, twit.tv/mbw, twit.tv/mbw or subscribe in your favorite podcatcher.
[01:53:57.220 --> 01:53:58.980]   That way you won't miss an episode.
[01:53:58.980 --> 01:54:00.500]   If you'd like to be in studio, I don't know why.
[01:54:00.500 --> 01:54:02.340]   We have massive studio audience today.
[01:54:02.340 --> 01:54:05.460]   Some of them are probably here for security now,
[01:54:05.460 --> 01:54:06.980]   which is coming up in just a moment.
[01:54:06.980 --> 01:54:08.820]   Email tickets at twit.tv.
[01:54:08.820 --> 01:54:11.380]   We'll make sure there's a chair awaiting for you.
[01:54:11.380 --> 01:54:12.900]   Thanks for being here.
[01:54:12.900 --> 01:54:14.180]   I have some bad news though.
[01:54:14.180 --> 01:54:16.020]   It's time to get back to work because...
[01:54:16.020 --> 01:54:18.260]   Break time is over!
[01:54:18.260 --> 01:54:22.260]   Bye-bye everybody.
[01:54:22.260 --> 01:54:29.300]   [Music]

