;FFMETADATA1
title=Stop Sniffing My AirPods
artist=TWiT
album_artist=TWiT
album=MacBreak Weekly
track=697
genre=Podcast
comment=http://twit.tv/mbw
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2020
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.000]   It's time for Mac Break Weekly, Andy Renee, Alex, and Lori are all here.
[00:00:06.000 --> 00:00:09.000]   But you know what I'm glad we have a good panel because there's lots to talk about,
[00:00:09.000 --> 00:00:14.000]   including did Apple drop its plans for encrypting iCloud after the FBI complain?
[00:00:14.000 --> 00:00:18.000]   We're missing the point when we're saying we're banning face recognition
[00:00:18.000 --> 00:00:21.000]   and Apple's opportunity with health data and more.
[00:00:21.000 --> 00:00:25.000]   There is a lot to talk about. It's coming up next on Mac Break Weekly.
[00:00:25.000 --> 00:00:29.000]   Mac Break Weekly comes to you from the Twit LastPass Studios.
[00:00:29.000 --> 00:00:32.000]   You're focused on security, but are your employees?
[00:00:32.000 --> 00:00:36.000]   LastPass can ensure they are by making access and authentication seamless.
[00:00:36.000 --> 00:00:41.000]   Visit lasspass.com/twit to learn more.
[00:00:41.000 --> 00:00:46.000]   Podcasts you love from people you trust.
[00:00:46.000 --> 00:00:50.000]   This is Twit.
[00:00:50.000 --> 00:00:58.000]   This is Mac Break Weekly, Episode 697, recorded Tuesday, January 21, 2020.
[00:00:58.000 --> 00:01:01.000]   Stop sniffing my AirPods.
[00:01:01.000 --> 00:01:04.000]   Mac Break Weekly is brought to you by Hover.
[00:01:04.000 --> 00:01:08.000]   Use a domain name that truly represents you and your passion.
[00:01:08.000 --> 00:01:15.000]   Visit hover.com/twit to get 10% off your first purchase of any domain extension for the entire first year.
[00:01:15.000 --> 00:01:21.000]   It's time for Mac Break Weekly, the show we cover the latest news from Apple.
[00:01:21.000 --> 00:01:24.000]   Alex Lindsay is in studio with me from the Pixelcore.
[00:01:24.000 --> 00:01:25.000]   Hello, Alex.
[00:01:25.000 --> 00:01:29.000]   Good to see you once again. Rene Ritchie from iMore.com.
[00:01:29.000 --> 00:01:31.000]   Mr. Ritchie.
[00:01:31.000 --> 00:01:33.000]   Bonsoir.
[00:01:33.000 --> 00:01:35.000]   Bonsoir, Leo. It's great to see you again.
[00:01:35.000 --> 00:01:38.000]   You have behind you the Santa Monica Pier.
[00:01:38.000 --> 00:01:41.000]   I believe that is the Apple. Apple is responsible for that.
[00:01:41.000 --> 00:01:44.000]   I have no control over what they choose to put on my displays.
[00:01:44.000 --> 00:01:45.000]   I love this.
[00:01:45.000 --> 00:01:47.000]   That's going to be a controversy on the internet in three seconds now that I said that.
[00:01:47.000 --> 00:01:48.000]   Yeah.
[00:01:48.000 --> 00:01:51.000]   Actually, as if they can, I'm trying to figure out what it is.
[00:01:51.000 --> 00:01:52.000]   I love those screens.
[00:01:52.000 --> 00:01:54.000]   They add more all the time, right?
[00:01:54.000 --> 00:01:56.000]   Do you like the underwater or the Monica Pier?
[00:01:56.000 --> 00:01:57.000]   Yeah, it is.
[00:01:57.000 --> 00:02:01.000]   That's part of the game I play. What am I looking at?
[00:02:01.000 --> 00:02:02.000]   It's kind of fun.
[00:02:02.000 --> 00:02:03.000]   At least you can find out now.
[00:02:03.000 --> 00:02:05.000]   I think 90% of them end up being Dubai.
[00:02:05.000 --> 00:02:08.000]   You finally find a lot of Dubai shots.
[00:02:08.000 --> 00:02:09.000]   It's a beautiful city.
[00:02:09.000 --> 00:02:13.000]   Lori Gill also here, managing editor at iMore.com.
[00:02:13.000 --> 00:02:14.000]   Hello, Lori.
[00:02:14.000 --> 00:02:15.000]   Hello.
[00:02:15.000 --> 00:02:19.000]   I don't have a beautiful image of Santa Monica in my background, but I do have a new background.
[00:02:19.000 --> 00:02:20.000]   You got wood.
[00:02:20.000 --> 00:02:27.000]   I took a page from Andy's book and I'm trying to make my background a little more interesting,
[00:02:27.000 --> 00:02:29.000]   but as you can see, it's not perfect.
[00:02:29.000 --> 00:02:30.000]   This side is not perfect.
[00:02:30.000 --> 00:02:32.000]   I'm still showing too much.
[00:02:32.000 --> 00:02:33.000]   I'm working on it.
[00:02:33.000 --> 00:02:34.000]   I'm working on it.
[00:02:34.000 --> 00:02:35.000]   I'm working on it.
[00:02:35.000 --> 00:02:36.000]   I'm working on it.
[00:02:36.000 --> 00:02:37.000]   I'm working on it.
[00:02:37.000 --> 00:02:40.000]   I took a page from Andy's book and I'm trying to make my background a little more interesting,
[00:02:40.000 --> 00:02:42.000]   but as you can see, I still, it's not perfect.
[00:02:42.000 --> 00:02:43.000]   It's not perfect.
[00:02:43.000 --> 00:02:44.000]   I'm still showing too much.
[00:02:44.000 --> 00:02:45.000]   Yeah.
[00:02:45.000 --> 00:02:46.000]   There's a towel or something that's shining very bright.
[00:02:46.000 --> 00:02:47.000]   But it's all right.
[00:02:47.000 --> 00:02:48.000]   JJ Abrams would approve.
[00:02:48.000 --> 00:02:49.000]   It's a little specular highlight towel.
[00:02:49.000 --> 00:02:50.000]   I'm going to make a little bit of the sideburns.
[00:02:50.000 --> 00:02:53.000]   I learned something sad the other day.
[00:02:53.000 --> 00:02:56.000]   I don't know if you knew this.
[00:02:56.000 --> 00:03:01.000]   Isaac Asimov died from a blood transfusion and got HIV in 1984.
[00:03:01.000 --> 00:03:02.000]   That's how he died.
[00:03:02.000 --> 00:03:05.000]   It was covered up at the time.
[00:03:05.000 --> 00:03:07.000]   His family and doctors didn't want to say anything.
[00:03:07.000 --> 00:03:12.000]   It was until all the doctors involved had passed away that that information was revealed.
[00:03:12.000 --> 00:03:13.000]   Yeah.
[00:03:13.000 --> 00:03:18.920]   Actually, his family was advised to keep it quiet because at that time, it's hard to be
[00:03:18.920 --> 00:03:20.920]   able to move on alive during that day.
[00:03:20.920 --> 00:03:21.920]   Exactly.
[00:03:21.920 --> 00:03:27.320]   It was nothing was a nothing was understood about HIV and also it was coupled to really
[00:03:27.320 --> 00:03:30.640]   horrible, horrible bigotry in homophobia.
[00:03:30.640 --> 00:03:35.240]   And so basically, if anybody if anybody had the disease, they would definitely not want
[00:03:35.240 --> 00:03:41.280]   to tell anybody about it because it would be ostracization, ostracization at best and
[00:03:41.280 --> 00:03:42.280]   violence that works.
[00:03:42.280 --> 00:03:43.280]   That's sad.
[00:03:43.280 --> 00:03:50.040]   And of course, was after several years after that, that efforts were made to make sure
[00:03:50.040 --> 00:03:54.920]   that the blood supply was not contaminated and you couldn't get sick from the blood supply.
[00:03:54.920 --> 00:04:00.400]   But I only mention that because your sideburns always reminded me of Isaac Asimov's.
[00:04:00.400 --> 00:04:01.400]   Yeah.
[00:04:01.400 --> 00:04:02.400]   Yeah.
[00:04:02.400 --> 00:04:04.560]   I mean, it's I've also been changing.
[00:04:04.560 --> 00:04:06.840]   It's this is this is this is a winner.
[00:04:06.840 --> 00:04:12.360]   This is a queen of green in Asimov.
[00:04:12.360 --> 00:04:18.560]   Summer it's I trim them just basically to cut down on the echo, but winter sometimes winter,
[00:04:18.560 --> 00:04:21.080]   it's like it's dark at four.
[00:04:21.080 --> 00:04:23.600]   I can't get motivated to start any new project.
[00:04:23.600 --> 00:04:26.880]   Even something that's more complicated than brushing my teeth and basic shaving.
[00:04:26.880 --> 00:04:28.720]   It's like, well, what the hell it's for.
[00:04:28.720 --> 00:04:29.720]   It's dark outside.
[00:04:29.720 --> 00:04:33.080]   Anyway, the day's over and may as well just shut down the computer.
[00:04:33.080 --> 00:04:40.120]   Now Renee, you tweeted this top story a little bit because you remember when this happened.
[00:04:40.120 --> 00:04:47.760]   Reuters this morning reported that Apple had planned to encrypt iCloud backups in a way
[00:04:47.760 --> 00:04:53.040]   that even the FBI couldn't get them, couldn't see them, Apple couldn't see them, but they
[00:04:53.040 --> 00:05:00.040]   dropped those plans after according to Reuters, after the FBI said the move would harm investigations.
[00:05:00.040 --> 00:05:02.960]   Reuters said they have six sources on this.
[00:05:02.960 --> 00:05:05.120]   This was about two years ago.
[00:05:05.120 --> 00:05:10.760]   Just as background you should know iCloud is not is encrypted so that nobody but you
[00:05:10.760 --> 00:05:13.840]   and Apple can see what's on your iCloud backup.
[00:05:13.840 --> 00:05:17.880]   And Apple very famously has offered to provide those that backup.
[00:05:17.880 --> 00:05:21.480]   In fact, they did in the Pensacola Naval Air Station shooting.
[00:05:21.480 --> 00:05:26.840]   They provided that information from the iCloud backup on the shooter's phones to the FBI almost
[00:05:26.840 --> 00:05:28.880]   immediately.
[00:05:28.880 --> 00:05:35.280]   It's a double edged sword though because anytime there is a key, this underscores also the problem
[00:05:35.280 --> 00:05:41.280]   we have with AG Bar and others asking Apple to back off on encryption on the phone.
[00:05:41.280 --> 00:05:45.080]   Anytime that key is available there's always a risk and all you have to do is ask Scarlett
[00:05:45.080 --> 00:05:50.560]   Johansson how she feels about the fact that iCloud is not trust no one encrypted.
[00:05:50.560 --> 00:05:52.880]   Well, so there's several different things that work there.
[00:05:52.880 --> 00:05:57.960]   The Scarlett Johansson was part of and they're actually all fit together which is weird but
[00:05:57.960 --> 00:06:03.680]   what happened to Scarlett Johansson are other celebrities is that iCloud was encrypted but
[00:06:03.680 --> 00:06:05.720]   they had means for you to get into your account.
[00:06:05.720 --> 00:06:08.520]   One of them was your password and a lot of people reused.
[00:06:08.520 --> 00:06:14.000]   But before Apple had the key, if people used the repeated password and that password was
[00:06:14.000 --> 00:06:18.800]   compromised or if they were public figures and their security questions were publicly
[00:06:18.800 --> 00:06:23.720]   available people could break into their account or social engineer their way into the account.
[00:06:23.720 --> 00:06:28.080]   That let Apple deliver two-step encryption and two-step encryption was different because
[00:06:28.080 --> 00:06:30.600]   you had to print out a recovery key.
[00:06:30.600 --> 00:06:34.840]   But what that happened is people turned it on and they promptly lost their recovery key
[00:06:34.840 --> 00:06:38.560]   and they were shut out of their iCloud backup which is their kids' photos, their wedding
[00:06:38.560 --> 00:06:43.480]   photos, all those things and it became a huge issue for Apple because for most people it's
[00:06:43.480 --> 00:06:47.960]   not a hacker or someone stealing their data or even a subpoena that's the biggest worry
[00:06:47.960 --> 00:06:50.560]   it's them losing access to their data.
[00:06:50.560 --> 00:06:56.640]   So Apple deprecated the old two-step authentication and created a new two-factor authentication
[00:06:56.640 --> 00:07:01.520]   and with a new two-factor authentication Apple held a key so that if you could prove ownership
[00:07:01.520 --> 00:07:05.720]   they could unlock it for you and restore your access to iCloud because that was the biggest
[00:07:05.720 --> 00:07:10.760]   support issue they were facing, not the legal requests and not the hacking requests.
[00:07:10.760 --> 00:07:15.240]   And that was four or five years ago and we documented it a lot, had a lot of discussions
[00:07:15.240 --> 00:07:17.680]   at the time.
[00:07:17.680 --> 00:07:22.440]   Since then that has not changed, absolutely nothing has changed since iCloud went which
[00:07:22.440 --> 00:07:26.480]   is where the story is a little wrong because it's not that Apple had plans to go to encryption,
[00:07:26.480 --> 00:07:32.080]   they were fail secure and they switched to fail safe and Tim Cook mentioned in passing
[00:07:32.080 --> 00:07:35.640]   when pressed in a German interview, will you do fail secure?
[00:07:35.640 --> 00:07:38.640]   He said it's something we're always looking at and my understanding is they have these
[00:07:38.640 --> 00:07:42.560]   debates inside Apple constantly, what about people walking out, what about the government
[00:07:42.560 --> 00:07:44.600]   asking for stuff, what about all this?
[00:07:44.600 --> 00:07:46.760]   But it creates a level of complexity.
[00:07:46.760 --> 00:07:52.080]   It's the same one when they say why can't you have touch ID or face ID and your passcode,
[00:07:52.080 --> 00:07:55.240]   it's well then if you burn your finger or something else happens you need a recovery
[00:07:55.240 --> 00:07:59.120]   password and they're right back to people losing their recovery password.
[00:07:59.120 --> 00:08:03.520]   So I will leave it something that's an absolute discussion but they haven't implemented that
[00:08:03.520 --> 00:08:04.840]   as an option yet.
[00:08:04.840 --> 00:08:06.320]   Hold on a sec.
[00:08:06.320 --> 00:08:07.480]   Yes.
[00:08:07.480 --> 00:08:12.040]   Regardless of the two-factor two-step.
[00:08:12.040 --> 00:08:15.360]   Apple has the key to iCloud.
[00:08:15.360 --> 00:08:20.760]   Apple if asked by law enforcement can say here is unencrypted iCloud data.
[00:08:20.760 --> 00:08:21.760]   We know that.
[00:08:21.760 --> 00:08:22.760]   They just did.
[00:08:22.760 --> 00:08:23.760]   They do with the current system.
[00:08:23.760 --> 00:08:24.760]   Yes.
[00:08:24.760 --> 00:08:27.920]   And also were they considering a system where they would not have the key?
[00:08:27.920 --> 00:08:30.600]   That's what I got this writer's story from.
[00:08:30.600 --> 00:08:32.760]   They had a system where they didn't have the key.
[00:08:32.760 --> 00:08:33.760]   You had a recovery key.
[00:08:33.760 --> 00:08:36.120]   So you're saying to a system where they have the key?
[00:08:36.120 --> 00:08:39.120]   You're saying that the fail, what is it?
[00:08:39.120 --> 00:08:40.120]   Fail secure?
[00:08:40.120 --> 00:08:41.120]   Fail secure.
[00:08:41.120 --> 00:08:42.840]   They have them not having a key.
[00:08:42.840 --> 00:08:45.400]   Yeah you had to have that physical recovery key.
[00:08:45.400 --> 00:08:47.040]   Now how long was that?
[00:08:47.040 --> 00:08:48.800]   There was a theoretical period of time.
[00:08:48.800 --> 00:08:50.440]   2015 I believe.
[00:08:50.440 --> 00:08:55.200]   So at that time if law enforcement goes to Apple there is no way they can unencrypt.
[00:08:55.200 --> 00:08:58.680]   Yeah I mean some people if you're super technical that you could say that Apple could maybe
[00:08:58.680 --> 00:09:03.880]   do a person in the middle attack on their own servers which is what we said about iCloud
[00:09:03.880 --> 00:09:08.520]   messages because unless people manage their own certificates and do end user side encryption
[00:09:08.520 --> 00:09:13.600]   like PGP or like someone because almost every storage service online backup service the
[00:09:13.600 --> 00:09:15.040]   vendor also has a key.
[00:09:15.040 --> 00:09:16.600]   Yes Dropbox does it.
[00:09:16.600 --> 00:09:20.720]   One drive does it Google drive Amazon Amazon does it and there's a good reason for that
[00:09:20.720 --> 00:09:23.440]   besides the what do you call it?
[00:09:23.440 --> 00:09:26.120]   So fail secure and fail safe.
[00:09:26.120 --> 00:09:29.600]   Besides fail safe the other reason is there's lots of things they can do.
[00:09:29.600 --> 00:09:32.080]   If you want to view your data on a web page.
[00:09:32.080 --> 00:09:38.240]   If you want a D-dupe if Amazon wants to D-dupe data all of that the data has to be they have
[00:09:38.240 --> 00:09:44.360]   to be able to unencrypt the data without your say so without your you know so almost all
[00:09:44.360 --> 00:09:45.560]   cloud services work that way.
[00:09:45.560 --> 00:09:49.440]   If I Steve Gibson looked at all of them the only one he could find that was in his mind
[00:09:49.440 --> 00:09:55.760]   trust no one was a sink.com where it's encrypted to the point where they can't decrypt it only
[00:09:55.760 --> 00:10:01.920]   you can and it's like last words and only on device what you want is only on device decryption
[00:10:01.920 --> 00:10:06.320]   right so there's no man in the middle possible it's not being decrypted then streamed to you
[00:10:06.320 --> 00:10:09.000]   it is decrypted on your device.
[00:10:09.000 --> 00:10:14.080]   Last pass does that but no cloud service including iCloud does that and sink.
[00:10:14.080 --> 00:10:18.120]   And Apple's thing is that if you really want that like they believe that this is the best
[00:10:18.120 --> 00:10:22.480]   solution for 90 plus percent of people 90 plus percent of the time but if for some reason
[00:10:22.480 --> 00:10:27.480]   you don't want that you can turn off iCloud backup and do an iTunes or now a finder backup
[00:10:27.480 --> 00:10:32.560]   and there's an encrypted button there and then nobody can get to it or pre encrypt this
[00:10:32.560 --> 00:10:39.560]   wouldn't work with the iCloud phone backup.
[00:10:39.560 --> 00:10:40.560]   Yes.
[00:10:40.560 --> 00:10:44.640]   And just make sure nothing's in that folder unless you encrypt it with VeraCrypt or BGP
[00:10:44.640 --> 00:10:48.720]   before it gets put in that folder then your back services what you do that.
[00:10:48.720 --> 00:10:53.640]   There's definitely like two camps there's two worlds to live in there's one world where
[00:10:53.640 --> 00:10:58.880]   we can be totally encrypted but we can also lose everything and if you think that your
[00:10:58.880 --> 00:11:04.680]   children's photos getting lost forever are worth the security that you have then you
[00:11:04.680 --> 00:11:09.520]   do want to be feel secure and you want to do something like only backup on iTunes and
[00:11:09.520 --> 00:11:15.240]   only to a hard drive on your computer at home that's encrypted you know using encrypted
[00:11:15.240 --> 00:11:20.480]   backups but if things like the music that you listen to and your kids photos they're
[00:11:20.480 --> 00:11:24.120]   more important to you than whatever data you store on your phone then you do want to be
[00:11:24.120 --> 00:11:30.080]   feel safe and Apple I think in this case with our phones believes that an iCloud backup
[00:11:30.080 --> 00:11:35.800]   should be feel safe not feel secure because if my mom loses all of her photos of her
[00:11:35.800 --> 00:11:41.160]   kids she's going to be horrified she's not going to care if someone steals her phone
[00:11:41.160 --> 00:11:45.400]   because she knows that there's nothing on there for her that needs to be protected at
[00:11:45.400 --> 00:11:50.060]   that level she can just erase her phone be done with it everything's good so there's
[00:11:50.060 --> 00:11:57.180]   definitely the two camps and Renee back in 2007 did talk about how you know why not
[00:11:57.180 --> 00:12:04.780]   let those of us who feel experienced enough and feel reliable enough to keep our own key
[00:12:04.780 --> 00:12:10.460]   let us keep our own key and then everybody else you know Apple can keep that key it's
[00:12:10.460 --> 00:12:17.140]   also an education problem because your mom probably didn't see this story from last or
[00:12:17.140 --> 00:12:22.040]   two years ago about the 16 year old who hacked into Apple servers and stole 90 gigabytes
[00:12:22.040 --> 00:12:29.620]   of secure files he was able to do that because Apple was fail safe not fail secure and maybe
[00:12:29.620 --> 00:12:34.700]   your mom would be if she knew about this because I bet you anything she thinks your
[00:12:34.700 --> 00:12:38.860]   data is safe and encrypted although that wasn't like cloud that would have been machine
[00:12:38.860 --> 00:12:43.820]   okay that he hacked into yeah well but would he have had access to that if Apple had apparently
[00:12:43.820 --> 00:12:49.980]   not iCloud iCloud is pretty well protected okay but it's it's also it's always this
[00:12:49.980 --> 00:12:54.580]   tension between customers who want a service that always not only always works but always
[00:12:54.580 --> 00:12:59.100]   works the way that they expected to even if they're expecting to do something that is
[00:12:59.100 --> 00:13:06.500]   prohibited by the laws of laws of gravity physics and probability and having to actually
[00:13:06.500 --> 00:13:13.300]   make good decisions on the users behalf of just like you can't hit you the networking
[00:13:13.300 --> 00:13:19.340]   box that I have my NAS behind me is in many ways a pain in the butt to use because it'll
[00:13:19.340 --> 00:13:24.500]   get thrown offline or it'll stop like security certificate will go bad and needs to be renewed
[00:13:24.500 --> 00:13:29.740]   and I'd rather have this thing that just always always works but the reason why it fails
[00:13:29.740 --> 00:13:34.540]   is because it's not taking any shortcuts to make me happy it's assuming that I know how
[00:13:34.540 --> 00:13:40.580]   to secure a secure box that's available across the internet whereas when you have something
[00:13:40.580 --> 00:13:47.140]   like a ring doorbell or any other sort of IOT sort of device the consumer wants to plug
[00:13:47.140 --> 00:13:51.300]   it in give it a Wi-Fi password and it just simply works everywhere the only way to make
[00:13:51.300 --> 00:13:57.500]   that work is to have default passwords that are shared by every single device that this
[00:13:57.500 --> 00:14:05.620]   manufacturer makes and shared secrets that are shared known passwords and logins to central
[00:14:05.620 --> 00:14:09.940]   entities and that's the sort of stuff that gets immediately exploited by the bad people
[00:14:09.940 --> 00:14:16.820]   so it's it's a there's a lot of tension between those those movements and Apple it's I this
[00:14:16.820 --> 00:14:22.500]   there would be a wonderful series of like oral history books inside Apple about saying here's
[00:14:22.500 --> 00:14:28.060]   how many people are complaining because they've this was the little emails that customer service
[00:14:28.060 --> 00:14:32.940]   is sick of getting emails that begin with but this was like the last video the last Christmas
[00:14:32.940 --> 00:14:37.980]   video we had of mom and dad before the accident and we lost them forever and now you're telling
[00:14:37.980 --> 00:14:43.340]   me that I backed them up to iCloud and now I can't get them back and how do you cut down
[00:14:43.340 --> 00:14:48.500]   on that while also trying to assure people that bad people can't get access to your data
[00:14:48.500 --> 00:14:56.420]   and they also can't be they won't be victimized by let's say a future pivot of law enforcement
[00:14:56.420 --> 00:15:02.220]   to insist that that the government has 100% access to all of your cloud data at all times
[00:15:02.220 --> 00:15:08.140]   and and I think that a lot of times we always say Apple does not focuses on what 90% of
[00:15:08.140 --> 00:15:13.100]   people want to do 90% of the time this is what 99% of the people want 99% but I can't
[00:15:13.100 --> 00:15:17.860]   submit that's because they don't understand that also means their date is vulnerable but
[00:15:17.860 --> 00:15:22.340]   I think that they I know my parents and many other people would consider their day are
[00:15:22.340 --> 00:15:26.860]   vulnerable they know that they there's chances that there's no way to get back into it and
[00:15:26.860 --> 00:15:30.420]   I think that if you're doing something that you're concerned about you know in that in
[00:15:30.420 --> 00:15:35.740]   that sense you know then you should educate yourself on how to employ tradecraft and do
[00:15:35.740 --> 00:15:39.380]   things to do this one I think that education is an issue we didn't nobody cared about
[00:15:39.380 --> 00:15:43.740]   privacy ten years ago right all of a sudden we're learning more about what that means
[00:15:43.740 --> 00:15:50.700]   and so suddenly I would say a vast majority of Americans who in the past weren't worried
[00:15:50.700 --> 00:15:55.580]   about it are suddenly worried about it so I think maybe it's a question partly of security
[00:15:55.580 --> 00:16:00.580]   versus convenience but also partly of ignorance and not knowing fully just you know because
[00:16:00.580 --> 00:16:04.780]   Apple says encrypted encrypted encrypted I think that they're they're giving people
[00:16:04.780 --> 00:16:08.540]   the impression that it is very culture is saying that too like we've become incredibly
[00:16:08.540 --> 00:16:12.940]   info sex sensitive to the point where and the whole reason I started looking into this
[00:16:12.940 --> 00:16:16.700]   is that Lori told me you were going to write an article about encrypted backups and I went
[00:16:16.700 --> 00:16:20.540]   to you know friend of the show a Dave Nainian and said how do I encrypt super duper and
[00:16:20.540 --> 00:16:24.660]   he said I will tell you but please don't ever do it and please don't ever tell anybody
[00:16:24.660 --> 00:16:28.780]   to do it because the minute you do that we can no longer recover anything and I have had
[00:16:28.780 --> 00:16:33.580]   so many people tell me they lost their photos to fire to flood to they forgot the password
[00:16:33.580 --> 00:16:39.140]   all these things that and there is this tension between info sec people who say encrypt everything
[00:16:39.140 --> 00:16:43.380]   and back up people who say no please only encrypt stuff that really needs to be encrypted it shouldn't
[00:16:43.380 --> 00:16:47.500]   be a default state for you that I've tried to find a middle ground and I think there is
[00:16:47.500 --> 00:16:52.420]   and I think the education that's lacking is that it like encryption it by definition isn't
[00:16:52.420 --> 00:16:56.820]   always good and neither is the lack of encryption but you have to identify the stuff that would
[00:16:56.820 --> 00:17:03.420]   mean if this being stolen or subpoenaed is was worse than it being lost please encrypt it
[00:17:03.420 --> 00:17:08.220]   if it being lost or destroyed is worse than it being subpoenaed or stolen please do not
[00:17:08.220 --> 00:17:12.300]   encrypt it and we have to educate people about both those choices you also have to acknowledge
[00:17:12.300 --> 00:17:17.580]   that Apple is sensitive to law enforcement yeah I mean one of the reasons Apple would
[00:17:17.580 --> 00:17:23.580]   like to make this so that they can't decrypt it is not US law enforcement they're not worried
[00:17:23.580 --> 00:17:29.300]   about the FBI they're worried about Chinese Russian right and I think Australian and
[00:17:29.300 --> 00:17:35.540]   and data patronize repatriation yeah so I think Apple would love to be able to say sorry we
[00:17:35.540 --> 00:17:42.460]   can't hand that over but they don't and they can't and they also are sensitive to the
[00:17:42.460 --> 00:17:46.620]   criticism from the FBI there's that's a lot of negative headlines the FBI is putting out
[00:17:46.620 --> 00:17:51.020]   well it's also about them not helping not helping in the investigation against terrorism
[00:17:51.020 --> 00:17:56.500]   refusing to refusing to help is yeah that's bad PR that's bogus language well and I think
[00:17:56.500 --> 00:18:01.460]   that also the by the way when the president tweets it to that hurts well I think that
[00:18:01.460 --> 00:18:07.940]   infosec expert noted noted idiot yeah I understand but but but but remember you forget you forget
[00:18:07.940 --> 00:18:12.980]   running he has Giuliani on his side he's a noted he owns a cybersecurity company we're
[00:18:12.980 --> 00:18:17.660]   talking about public perception and that's also important exactly one and I think that
[00:18:17.660 --> 00:18:22.140]   also that when you talk about legislation which comes from public perception drawing
[00:18:22.140 --> 00:18:26.140]   that line too far out means that it puts them at risk of going people saying well that's
[00:18:26.140 --> 00:18:30.340]   not reasonable people have to be getting into it yeah I'm envy them they're in a tough
[00:18:30.340 --> 00:18:34.660]   position right so by they've drawn the line with everyone around the world including China
[00:18:34.660 --> 00:18:38.660]   including everyone else that the phone is the line right that's the line they do encrypt
[00:18:38.660 --> 00:18:43.020]   that right you can't take my phone and get into it unless you I mean believe the chance
[00:18:43.020 --> 00:18:48.460]   of it being lost is higher than a well and by the way I should mention the 95 Mac report
[00:18:48.460 --> 00:18:53.140]   that the police were able to unlock the Pensacola shooters phone using gray key there's celebrating
[00:18:53.140 --> 00:18:57.500]   gray key both of which have exploits that apparently work with an iPhone 11 but it was
[00:18:57.500 --> 00:19:00.460]   these weren't even iPhone 11's there are five and seven yeah yeah so this was but they
[00:19:00.460 --> 00:19:05.860]   have on the iPhone 11's exactly so so well they have checkmate right that goes back to
[00:19:05.860 --> 00:19:09.620]   almost all previous iPhones except not the 11 checkmate doesn't yeah they're all they
[00:19:09.620 --> 00:19:15.780]   can use checkmate against the Pensacola shooters phone and and gray key and and this is frustrating
[00:19:15.780 --> 00:19:20.780]   but gray key and celebrate as part of their business discover and keep secret exploits
[00:19:20.780 --> 00:19:26.320]   that allow FBI and other nation states how does it happen other nation states like the
[00:19:26.320 --> 00:19:31.060]   Saudis to decrypt phones after I just wonder why Apple doesn't spend like five billion
[00:19:31.060 --> 00:19:36.420]   dollars and just buy those companies call to that just buy the whole company is by both
[00:19:36.420 --> 00:19:44.320]   great he is celebrating ago that's a oh that's a let's say it's still like they like they're like why not here's your here's your here's your
[00:19:44.320 --> 00:19:47.980]   benefit to fitting words guys we don't want to make exploits even more value but then and
[00:19:47.980 --> 00:19:56.300]   they have by the way apples paying more and more and more for exploits than ever before right probably because of this right yes I think what is the last bug bounty was up to a million
[00:19:56.300 --> 00:20:04.140]   and a half dollars so even if Apple had to buy one company a year for two billion dollars it wouldn't matter all you know it would be the cost of being secure
[00:20:04.140 --> 00:20:13.020]   they can afford it but then they would do that they'd make a new company they get bought it in two years yeah monopoly of cracking phones yeah well you got all you just put the
[00:20:13.020 --> 00:20:23.960]   company that cracked phones it would probably be a problem for the FBI and the government there'd be a lot of problems with that yeah anybody who knew that you could form if you know of a good
[00:20:23.960 --> 00:20:37.260]   exploit or if you are a former employee of a company that's now owned by Apple that knew of an exploit if you can just simply file a file a corporation with Delaware and then three months later get a check for half a billion dollars this would be a quite quite an
[00:20:37.260 --> 00:20:38.780]   expensive apple
[00:20:38.780 --> 00:20:46.640]   this is not that many of them well not that many of them but this is your gonna check at some point why it's a big deal that they want ask back
[00:20:46.640 --> 00:21:15.640]   keep back doors or keys in escrow because you can't keep a lid on this stuff exploits or escrow you can't keep a lid on this stuff and so what governments I mean look at this is gonna be given and it'll happen in the United States governments will say you have to I don't care how you do it you have to provide me with unencrypted clear text whenever I ask for it and you figure out how it is trying that right yeah Russia's already done it I don't know what the state in China is but I would guess
[00:21:15.640 --> 00:21:31.640]   that every Chinese company can do it but I think but I believe that Apple's talking about walking out of Russia is you know I believe that when I see it but they're talking about like they're not the phone is not ever gonna walk out of China and Russia's not as high of the market yeah Russia was a big deal
[00:21:31.640 --> 00:22:01.280]   also also they have you have bigger problems in Russia in terms of human rights abuses than you have in China conceivably because you have one person needs to be caught in rid of arrest with China it's something that's a whole minority of Muslims yeah if you're if you're the north northwest of China it's pretty bad yeah yeah you know you don't want to be a weger and have an iPhone but there's no but I don't know where you go and this is the problem is that yeah you could say well as you just did all people need to get better at info
[00:22:01.280 --> 00:22:31.200]   sec well that's not well I'm not saying that I don't think that you can't expect Apple to do it for you I think you're exactly you know that's that's that's all they're not and they're not gonna because of governmental pressure among other things right I think they found a line that they're willing to defend and it's a defendable position and they're gonna step with that well because when the FBI says you didn't unlock the phone they can always say but we gave you the iCloud backups right we cooperated we gave you all the day a week and right now this is unlocking the phone from Apple's perspective it is
[00:22:31.200 --> 00:22:36.280]   literally building something new like that I think that's really an important distinction
[00:22:36.280 --> 00:23:00.200]   that's what they wanted to send Bernardino they wanted a new OS that they could push all of these things that would allow them to try many many many many keys all these all these access requests from the government now is not we want you to take something you already have and unlock it they're saying we would like you to create something new that burrows into our into your own system so it's like you have a very secure castle wall we would like you to knock a hole here and
[00:23:00.200 --> 00:23:29.200]   and then put a you know like put a hole here that goes right to the center and then we want you to give us the key and you know and we're gonna assume that nobody can open that door what they what they really want is the is the precedent they want the precedent on the books that says that if you if government demands data that you might have conceivably have had access to you have to give us the technology to fork it over and this starts with unusual cases like the like active shooter like these these shooting cases.
[00:23:29.200 --> 00:23:57.200]   >> In five or ten years though that for that present will say will basically expand to say that if you have a server that offers cloud services there has to be you have to have you have to have a government representative on on on site that has access to a keyboard and a screen that can access anything that's on that device or on that again we're talking about the fact that they already have all this person's metadata so they already know every person that person called with that phone everybody they texted everybody they have that they were they get into it and they got into it.
[00:23:57.200 --> 00:24:12.200]   >> And you know like when we talk about risks here if if we decided that it was the number of people dying to terrorists which is horrible but not a high number if we compare that to your pool your car.
[00:24:12.200 --> 00:24:21.200]   >> We would say no one should drive and no one should get no one should have a pool because those actually kill far more people I was wondering where you're going with the swimming pool okay.
[00:24:21.200 --> 00:24:35.200]   >> Yeah I mean that's my pool. Yeah you can't have a pool I mean like it you would literally make Americans more safe by statistically by taking away their pools then you can get that with my damn hot tub I can't get it more than 104 degrees anymore.
[00:24:35.200 --> 00:24:38.200]   >> You just can't sue me that one whole side of me.
[00:24:38.200 --> 00:24:42.200]   >> You know you can you can work you can that can be that can be sorted.
[00:24:42.200 --> 00:24:45.200]   >> No I have been trying to get my hot tub hacked for years.
[00:24:45.200 --> 00:24:47.200]   >> It's based on the thermometer.
[00:24:47.200 --> 00:24:49.200]   >> Yeah but it's a digital thermometer.
[00:24:49.200 --> 00:24:53.200]   >> That's what I thought is there has to be a way to change things.
[00:24:53.200 --> 00:24:57.200]   >> Remember that the heaters just a it's just being told when to turn on and off.
[00:24:57.200 --> 00:24:58.200]   >> Right.
[00:24:58.200 --> 00:24:59.200]   >> You can put another thing in my room.
[00:24:59.200 --> 00:25:00.200]   >> I could trip the heater.
[00:25:00.200 --> 00:25:10.200]   >> I just wish that Apple Apple's been able to like buy huge billboards saying that hey we are the company that was on your iPhone.
[00:25:10.200 --> 00:25:11.200]   >> On your iPhone stays on your iPhone.
[00:25:11.200 --> 00:25:18.200]   >> Yeah but what I wish they would do is increase the table stakes of of knowledge for anybody who's using a phone these days.
[00:25:18.200 --> 00:25:23.200]   >> Like the tape for if you own a car you don't have to know how fuel injection systems work.
[00:25:23.200 --> 00:25:26.200]   >> You don't have to know like what the O2 sensor does.
[00:25:26.200 --> 00:25:33.200]   >> You do have to know that see this hole right here you need to check the oil and you need to replace you need to keep adding oil and do oil changes.
[00:25:33.200 --> 00:25:39.200]   >> And if you live in the Northeast you need to like get an under get the underbrought body sprayed or else it will rest out.
[00:25:39.200 --> 00:25:44.200]   >> They're like only you only have to know like three or four things to keep a car running for 150,000 miles.
[00:25:44.200 --> 00:25:46.200]   >> Those are the table stakes.
[00:25:46.200 --> 00:25:54.200]   >> And I think that we need to understand what the table stakes are for basic literacy of owning a phone or on your computer.
[00:25:54.200 --> 00:26:06.200]   >> If people understood how encryption worked what the trade off and what the balance point is between we want to make sure you always can get access to your phones, to your photos and your videos and your information.
[00:26:06.200 --> 00:26:10.200]   >> If your phone gets lost or stolen we also want to make sure that your phone is lost or stolen.
[00:26:10.200 --> 00:26:13.200]   >> Those photos and videos don't wind up in other people's hands.
[00:26:13.200 --> 00:26:26.200]   >> And if there were a better education campaign people would have a better appreciation and a more realistic understanding of A, here's what Canon cannot be done if you are careless and you lose your phone.
[00:26:26.200 --> 00:26:40.200]   >> But also here is why when the government says that Apple is refusing and trying to create the refusing to help an investigation and trying to create the false impression that Apple has this big Manila folder in a both fashionable,
[00:26:40.200 --> 00:26:50.200]   and full-timey steel filing cabinet of all this terrorist data that they just simply refused to hand over they will know that we understand that that's not how encryption works.
[00:26:50.200 --> 00:26:57.200]   So the table stakes of information and understanding has to be increased and Apple is in a good position to do that I think.
[00:26:57.200 --> 00:27:01.200]   >> It's a really interesting conundrum.
[00:27:01.200 --> 00:27:02.200]   >> Yeah.
[00:27:02.200 --> 00:27:03.200]   >> I think Apple's--
[00:27:03.200 --> 00:27:04.200]   >> The conversation is good.
[00:27:04.200 --> 00:27:05.200]   >> Yeah, it is.
[00:27:05.200 --> 00:27:06.200]   It's important.
[00:27:06.200 --> 00:27:07.200]   And I think education is important.
[00:27:07.200 --> 00:27:11.200]   And I think Apple's default is, look, if you want to encrypt it, that's on you.
[00:27:11.200 --> 00:27:12.200]   >> Right.
[00:27:12.200 --> 00:27:17.200]   >> If you want trust no one encryption because there are negative side effects, that's on you.
[00:27:17.200 --> 00:27:24.200]   And, you know, it'd be nice if they didn't pretend that everything was secure and safe.
[00:27:24.200 --> 00:27:26.200]   That's an advertising slogan.
[00:27:26.200 --> 00:27:27.200]   >> Don't worry about it.
[00:27:27.200 --> 00:27:28.200]   Don't think about it.
[00:27:28.200 --> 00:27:29.200]   We're doing the thinking for you, but that's what--
[00:27:29.200 --> 00:27:36.200]   >> It'd be better if everybody knew and then it's our job, by the way, to educate people on how to keep yourself safe and safe.
[00:27:36.200 --> 00:27:38.200]   We're going to do more of that.
[00:27:38.200 --> 00:27:50.200]   I think that that's an important responsibility of tech journalism these days is to explain what is going on, what is safe and private, what is it, and how you can make it safe and private.
[00:27:50.200 --> 00:27:52.200]   For instance, stop using email.
[00:27:52.200 --> 00:27:53.200]   Start using signal.
[00:27:53.200 --> 00:27:54.200]   Things like that.
[00:27:54.200 --> 00:27:55.200]   >> Yeah.
[00:27:55.200 --> 00:27:56.200]   >> You know, very simple.
[00:27:56.200 --> 00:28:00.200]   But if you want secure communications, it's not eye messages.
[00:28:00.200 --> 00:28:03.200]   It's not-- it's not-- it's not email.
[00:28:03.200 --> 00:28:04.200]   It's signal.
[00:28:04.200 --> 00:28:07.200]   By the way, I want to ask Renee real quickly on this Reuters article.
[00:28:07.200 --> 00:28:08.200]   I presume you read the whole thing.
[00:28:08.200 --> 00:28:09.200]   >> Yep.
[00:28:09.200 --> 00:28:10.200]   >> They did make some other assertions.
[00:28:10.200 --> 00:28:15.200]   I just want to check with you because you're going to be the fact checker on Reuters.
[00:28:15.200 --> 00:28:17.200]   >> I like Leo.
[00:28:17.200 --> 00:28:22.200]   >> It did say, had it proceeded with its plan and we wouldn't disagree with this, Apple would not have been able to turn over any readable data.
[00:28:22.200 --> 00:28:25.200]   You only disagree with the motive, not the fact.
[00:28:25.200 --> 00:28:35.200]   >> No, well, the way that they're phrasing it, so my read of this is that they had one person who was not very looped in at Apple and then a bunch of people on the FBI side.
[00:28:35.200 --> 00:28:37.200]   And we have no way of any house sophisticated--
[00:28:37.200 --> 00:28:40.200]   >> Yeah, those six sources may have been all within the FBI.
[00:28:40.200 --> 00:28:41.200]   >> I believe they were.
[00:28:41.200 --> 00:28:42.200]   >> To put that out.
[00:28:42.200 --> 00:28:48.200]   >> One of them was former Apple and that person was self-admittedly not in the loop on this.
[00:28:48.200 --> 00:28:55.200]   But my understanding from people who are in the loop on this is that it has been a constant topic of discussion inside Apple.
[00:28:55.200 --> 00:29:00.200]   And at first, they were hardcore on this has got to be secure and fail secure and all of that.
[00:29:00.200 --> 00:29:04.200]   And they realized that they were a bunch of scientists in a room and their parents were screaming at them.
[00:29:04.200 --> 00:29:06.200]   And so they made the current system.
[00:29:06.200 --> 00:29:09.200]   But now they're still like, no, we've got-- and Tim Cook mentioned it.
[00:29:09.200 --> 00:29:16.200]   They're having this huge debate about how to do it and how to set it up because the online is just less error prone.
[00:29:16.200 --> 00:29:18.200]   Like, I shouldn't say that.
[00:29:18.200 --> 00:29:24.200]   You're more likely to have a recent backup if it's online because not everybody's going to sink continuously to iTunes or the Finder.
[00:29:24.200 --> 00:29:26.200]   So it does have definite benefits.
[00:29:26.200 --> 00:29:34.200]   But it's just architecting a system that doesn't compromise either end and making it available to billions-- potentially billions of people.
[00:29:34.200 --> 00:29:36.200]   So my understanding is that's the hang up.
[00:29:36.200 --> 00:29:38.200]   It's just they haven't-- >> Well, here's what gets to it.
[00:29:38.200 --> 00:29:41.200]   >> It's on the list, but it hasn't been shipped yet. >> Clarify that.
[00:29:41.200 --> 00:29:49.200]   Instead of protecting all of iCloud with end-to-end encryption writers' rights, Apple has shifted to focus on protecting some of the most sensitive user information.
[00:29:49.200 --> 00:29:51.200]   >> Yes. >> Such as saved passwords and health data.
[00:29:51.200 --> 00:29:55.200]   Is that because they put it in the secure enclave or it stays on device?
[00:29:55.200 --> 00:30:01.200]   >> Well, it's not in the-- there's different-- so the secure iCloud, which is where all the stuff is stored and there's different iCloud.
[00:30:01.200 --> 00:30:04.200]   >> Oh, okay. >> They don't have access to secure iCloud.
[00:30:04.200 --> 00:30:14.200]   >> So they don't have-- they don't keep a key for a bunch of different things that they consider to-- the things that they consider are better for you to lose than them to have access to, which includes your health data and your password.
[00:30:14.200 --> 00:30:16.200]   >> Perfect. >> It's like that. >> Okay.
[00:30:16.200 --> 00:30:25.200]   But-- Reuters goes on to say, "Backed up contact information, texts from iMessage, WhatsApp, if you back up WhatsApp to iCloud, you don't have to."
[00:30:25.200 --> 00:30:30.200]   >> Yes. >> And other encrypted services remain available to Apple employees and authorities, yes?
[00:30:30.200 --> 00:30:37.200]   >> And iMessage is a little complicated because when they shipped iCloud Sync, sorry iMessage Sync, it was a little bit messy.
[00:30:37.200 --> 00:30:42.200]   And so currently, if you turn on iMessage Sync, which is called messages in the cloud-- >> You have to turn that on.
[00:30:42.200 --> 00:30:44.200]   >> Yeah. >> You don't have-- yeah.
[00:30:44.200 --> 00:30:48.200]   Well, I'm not sure-- because I haven't started off a new phone without that.
[00:30:48.200 --> 00:30:49.200]   >> Well, this is what we always have said.
[00:30:49.200 --> 00:30:58.200]   If you can turn messages in the cloud on and all of your messages now go to the new phone unencrypted, then clearly Apple decrypted them in the back.
[00:30:58.200 --> 00:31:04.200]   >> Well, no, so there's a difference. When you turn on messages in the cloud, the key is on your device, not on Apple servers.
[00:31:04.200 --> 00:31:09.200]   If you turn it off, it defaults back to being iCloud backup, and then it is on Apple servers.
[00:31:09.200 --> 00:31:15.200]   >> Oh, that's very confusing. So if you want to get the secure turn on messages in the cloud.
[00:31:15.200 --> 00:31:17.200]   But then when it's-- >> Yes.
[00:31:17.200 --> 00:31:24.200]   >> But no, that doesn't make sense, Renee, because how-- so I have it-- messages on my old iPhone XS.
[00:31:24.200 --> 00:31:29.200]   I turn on messages in the cloud, I get my new iPhone 11 without doing anything else.
[00:31:29.200 --> 00:31:32.200]   Those messages propagate to my iPhone 11 unencrypted.
[00:31:32.200 --> 00:31:37.200]   >> Yes. >> That's the only difference is Apple doesn't-- so it works like the health data, because it'll sync back your health data.
[00:31:37.200 --> 00:31:43.200]   It's just-- it's stored in encrypted iCloud secure iCloud enclosure that Apple does not keep the key for.
[00:31:43.200 --> 00:31:47.200]   >> Okay. So if you turn on messages in the cloud-- >> So if you lose all your devices, they cannot help you.
[00:31:47.200 --> 00:31:49.200]   >> Okay. So what's happening is-- >> It's key is on your device.
[00:31:49.200 --> 00:31:56.200]   >> It's key going from my iPhone XS into the cloud decrypting and then coming the unencrypted message down into my phone.
[00:31:56.200 --> 00:32:01.200]   >> That's my understanding, yes. Because if you lose all those devices, if those devices all somehow get wiped, you are starting fresh.
[00:32:01.200 --> 00:32:04.200]   >> No, one of them tell people about this is freaking confusing.
[00:32:04.200 --> 00:32:08.200]   >> All right. >> It is. It is. And I think the normalizer eventually.
[00:32:08.200 --> 00:32:15.200]   But like, they still haven't rolled out, I messes sync for the Apple watch yet. So it was very much-- and it was one of those things that was delayed several months after it was announced.
[00:32:15.200 --> 00:32:20.200]   So it's my guess that they're still working feverishly to make it-- they made it work. Now they have to make it work well.
[00:32:20.200 --> 00:32:24.200]   >> So let me-- okay, I'm sorry. I'm an idiot. Let me clarify.
[00:32:24.200 --> 00:32:26.200]   >> No. >> I want to see if I understand this.
[00:32:26.200 --> 00:32:31.200]   There is general iCloud that Apple has access to, authorities have access to.
[00:32:31.200 --> 00:32:38.200]   There is a secure iCloud that only you have access to. Apple can't share with the authorities.
[00:32:38.200 --> 00:32:46.200]   In that are your keychain passwords, right? That's where keychain goes. In that is your health data.
[00:32:46.200 --> 00:32:52.200]   Because I have tied everything into Apple Health, so it knows my blood pressure, my blood type, my medications.
[00:32:52.200 --> 00:32:56.200]   But that's secure. Authorities can't get my blood type from that, right?
[00:32:56.200 --> 00:33:00.200]   >> So the better way to think about it is only iCloud backup is not secure.
[00:33:00.200 --> 00:33:03.200]   iCloud backup is the only thing Apple can get into. >> Oh, interesting.
[00:33:03.200 --> 00:33:05.200]   >> So everything else is secure. >> Yeah.
[00:33:05.200 --> 00:33:08.200]   >> But the problem is it's not clear when you're using iCloud backup.
[00:33:08.200 --> 00:33:11.200]   For instance, it sounds like messages in the cloud is iCloud backup. It's not.
[00:33:11.200 --> 00:33:15.200]   >> Yes. It's not unless you turn it off and then it becomes iCloud backup.
[00:33:15.200 --> 00:33:20.200]   >> Jeez, Louise. Okay. In October, Reuters goes on. In October--
[00:33:20.200 --> 00:33:22.200]   >> I'll do an explainer to video for this tomorrow.
[00:33:22.200 --> 00:33:24.200]   >> Would you do a vector for this? >> Yes.
[00:33:24.200 --> 00:33:26.200]   >> Yes. >> This is not-- and look it.
[00:33:26.200 --> 00:33:30.200]   If I don't know this, I guarantee you very few people know this.
[00:33:30.200 --> 00:33:34.200]   Because I thought I was keeping up on this story. >> Yeah. >> And, clearly, I'm not.
[00:33:34.200 --> 00:33:38.200]   >> In October-- let me just finish the Reuters story. In October 2018,
[00:33:38.200 --> 00:33:44.200]   Alphabet announced a similar system to Apple's dropped plan for secure backups.
[00:33:44.200 --> 00:33:46.200]   It sounds like maybe Reuters got this wrong too.
[00:33:46.200 --> 00:33:52.200]   The maker of Android software said users could back up their data to its own cloud,
[00:33:52.200 --> 00:33:55.200]   to the Google Cloud without trusting the company with the key.
[00:33:55.200 --> 00:33:59.200]   Two people familiar with the project said Google gave no advance notice to governments
[00:33:59.200 --> 00:34:03.200]   and picked a time to announce it when encryption was not in the news.
[00:34:03.200 --> 00:34:08.200]   >> Dude, you're sneaking in under the wire. The company continues to offer the service
[00:34:08.200 --> 00:34:11.200]   but declined to comment on how many users have taken up the option.
[00:34:11.200 --> 00:34:14.200]   I don't even know where that check mark is. >> I don't know either.
[00:34:14.200 --> 00:34:18.200]   And I've never seen it. I'm sure it exists. I'm sure Jerry Hildenbrand knows where it is,
[00:34:18.200 --> 00:34:20.200]   but I've not found it. >> So this isn't the Android show.
[00:34:20.200 --> 00:34:22.200]   I'll have to get Jason Howell to tell us. >> Yeah.
[00:34:22.200 --> 00:34:26.200]   >> But weird. The FBI did not respond to a request for comment on Google's service
[00:34:26.200 --> 00:34:29.200]   or the agency's approach to it. >> But Android, that makes sense because
[00:34:29.200 --> 00:34:32.200]   Android is also run way more like a computer. You can do all sorts of computer-y things on it,
[00:34:32.200 --> 00:34:36.200]   and you may well get a little utility that lets you device-side encrypted
[00:34:36.200 --> 00:34:39.200]   and then send it all to Google. I don't know. >> Yeah.
[00:34:39.200 --> 00:34:43.200]   >> Okay. So to recap. >> [LAUGH]
[00:34:43.200 --> 00:34:46.200]   >> I don't know what the hell's going on. I don't know what else does.
[00:34:46.200 --> 00:34:49.200]   >> There are a bunch of support articles and a whole white paper on this,
[00:34:49.200 --> 00:34:52.200]   so it is documented, but it is just not that transparent.
[00:34:52.200 --> 00:34:56.200]   >> What should, if you want to be the most private possible and you're using an Apple device,
[00:34:56.200 --> 00:34:59.200]   what should you do? >> Turn off iCloud backup and backup only
[00:34:59.200 --> 00:35:04.200]   to an encrypted using iTunes or the Finder to an encrypted backup desk.
[00:35:04.200 --> 00:35:07.200]   >> So none of the-- >> Do any most secure, just don't--
[00:35:07.200 --> 00:35:12.200]   >> None of the water to backup is safe. Back up with a cable connected to your computer
[00:35:12.200 --> 00:35:16.200]   and iTunes and make sure you type in a password, good one, for the encryption.
[00:35:16.200 --> 00:35:19.200]   >> Yeah. >> Not one, two, three, four. >> And I would use a cable because you don't
[00:35:19.200 --> 00:35:22.200]   know if someone is sitting there stupid your wife like this. >> Yeah, right.
[00:35:22.200 --> 00:35:24.200]   >> So you want to be real paranoid. >> So, okay.
[00:35:24.200 --> 00:35:27.200]   >> A non-juice-jacked cable. Because it's the one for ever-meal.
[00:35:27.200 --> 00:35:31.200]   >> She's Louise. >> Don't backup. Alex is right.
[00:35:31.200 --> 00:35:33.200]   If you really want to be better, press this back up.
[00:35:33.200 --> 00:35:35.200]   >> Yeah, don't back up. >> If I turn off all backup on my phone,
[00:35:35.200 --> 00:35:38.200]   this is impenetrable. Oh, you also have to do it. >> If they crypto-break.
[00:35:38.200 --> 00:35:42.200]   >> And by the way, don't use a four or even a six.
[00:35:42.200 --> 00:35:48.200]   >> Mine's a key passcode because that's what great key and celebrate break into.
[00:35:48.200 --> 00:35:51.200]   >> Mine's-- I thought it was 26, but it was 24 characters.
[00:35:51.200 --> 00:35:54.200]   >> Few use a nice-- >> A regular password like you would use on a website,
[00:35:54.200 --> 00:35:58.200]   a long random password that's going to be a pain in the butt.
[00:35:58.200 --> 00:36:00.200]   >> And it doesn't even have to be-- it has to be--
[00:36:00.200 --> 00:36:02.200]   the best thing is a phrase that you can remember.
[00:36:02.200 --> 00:36:04.200]   >> Yeah. >> Then probably it's probably the case that
[00:36:04.200 --> 00:36:07.200]   even celebrate and breaky don't work. >> Well, they take years.
[00:36:07.200 --> 00:36:11.200]   >> They don't know it. >> I think they said it was like a day for six numbers,
[00:36:11.200 --> 00:36:13.200]   and then it goes up to a couple of years real fast.
[00:36:13.200 --> 00:36:15.200]   >> Yeah, I mean-- >> We're working a little bit in the dark
[00:36:15.200 --> 00:36:17.200]   because we don't know exactly what their exploit is,
[00:36:17.200 --> 00:36:20.200]   but it's presumed their exploit allows-- >> It turns off the time code
[00:36:20.200 --> 00:36:22.200]   and then allows brute force, yeah. >> Exactly.
[00:36:22.200 --> 00:36:24.200]   >> The time-- >> The time-- the time belief.
[00:36:24.200 --> 00:36:26.200]   >> So a good, strong pass code like Alex's.
[00:36:26.200 --> 00:36:28.200]   >> So when did you type it in every time?
[00:36:28.200 --> 00:36:30.200]   >> Oh, yeah. Oh, yeah. >> Mother McCree.
[00:36:30.200 --> 00:36:34.200]   >> If I-- if it goes dead in the car, I have to pull over.
[00:36:34.200 --> 00:36:37.200]   >> My problem is face ID doesn't-- do you still use face ID?
[00:36:37.200 --> 00:36:40.200]   >> Oh, yeah. >> I didn't do it until I had face ID
[00:36:40.200 --> 00:36:43.200]   because most of the time it just opens.
[00:36:43.200 --> 00:36:47.200]   >> Right. >> It's when I-- it's when like when it's restarted
[00:36:47.200 --> 00:36:49.200]   or it doesn't recognize me or whatever,
[00:36:49.200 --> 00:36:51.200]   because I wake up or whatever.
[00:36:51.200 --> 00:36:52.200]   >> When you're wearing the ninja suit.
[00:36:52.200 --> 00:36:54.200]   >> Yeah, when I'm wearing the ninja suit, I have to do it
[00:36:54.200 --> 00:36:58.200]   every single time, you know, and-- >> Honey, what is that clicking
[00:36:58.200 --> 00:37:00.200]   on the roof? >> Yeah, exactly.
[00:37:00.200 --> 00:37:02.200]   >> I don't know, but he did it 24 times.
[00:37:02.200 --> 00:37:03.200]   [laughter]
[00:37:03.200 --> 00:37:05.200]   >> It's quite happy for the honey.
[00:37:05.200 --> 00:37:08.200]   >> But yeah, so-- but it's-- but it's actually easier to remember
[00:37:08.200 --> 00:37:10.200]   than most of the other ones I've had because it's just a--
[00:37:10.200 --> 00:37:11.200]   >> Yeah, pass for you. >> It's a unique--
[00:37:11.200 --> 00:37:12.200]   >> It's best for you. >> It's best for you.
[00:37:12.200 --> 00:37:15.200]   >> It's best for us. >> All right, so that's actually easy
[00:37:15.200 --> 00:37:19.200]   to digest advice. If you don't want your data to ever be seen
[00:37:19.200 --> 00:37:21.200]   by anybody else, don't back up.
[00:37:21.200 --> 00:37:24.200]   >> Right. >> Don't do Google Photos, don't do iCloud Photos,
[00:37:24.200 --> 00:37:26.200]   don't do Messages in the cloud, don't do not Message by--
[00:37:26.200 --> 00:37:28.200]   >> Don't use Facebook, don't use Twitter, don't use TikTok,
[00:37:28.200 --> 00:37:30.200]   don't use Twitter, don't use-- >> TikTok, don't use Twitter,
[00:37:30.200 --> 00:37:32.200]   don't use-- >> Yeah, but make sure you really need to,
[00:37:32.200 --> 00:37:34.200]   like not just because you think you fancy yourself a badass,
[00:37:34.200 --> 00:37:36.200]   like you've got to really not need this stuff.
[00:37:36.200 --> 00:37:37.200]   >> No. >> We're willing to lose it.
[00:37:37.200 --> 00:37:38.200]   >> But I back up everything everywhere.
[00:37:38.200 --> 00:37:40.200]   I have copies of all of my data everywhere,
[00:37:40.200 --> 00:37:42.200]   because I don't want to lose it.
[00:37:42.200 --> 00:37:43.200]   I'm the fail-- >> Same.
[00:37:43.200 --> 00:37:46.200]   >> -- safe guy. >> I really want Apple to add an option
[00:37:46.200 --> 00:37:48.200]   to allow for encrypted backups, and I will then promptly
[00:37:48.200 --> 00:37:51.200]   never use it. >> Well, I have a second phone that I don't
[00:37:51.200 --> 00:37:54.200]   back up that I, you know, if I don't overseas, I might use--
[00:37:54.200 --> 00:37:56.200]   I often use that one as the one I'm walking around with,
[00:37:56.200 --> 00:37:58.200]   so if I don't have to think about it.
[00:37:58.200 --> 00:38:01.200]   >> Wow. >> So-- >> This was a good conversation.
[00:38:01.200 --> 00:38:04.200]   Thank you, everybody. >> Alex is obsecused, precise.
[00:38:04.200 --> 00:38:06.200]   >> Can I ask you-- >> Can I ask you--
[00:38:06.200 --> 00:38:08.200]   >> -- nothing to hide, so I don't know why he's doing that.
[00:38:08.200 --> 00:38:10.200]   He just does it because he can. >> He's so good,
[00:38:10.200 --> 00:38:11.200]   that's what we think, Leo. >> Oh.
[00:38:11.200 --> 00:38:13.200]   >> That's the true professional there.
[00:38:13.200 --> 00:38:15.200]   >> We have the evidence of the-- >> We have the evidence
[00:38:15.200 --> 00:38:16.200]   of the reprobates here, he hangs out with--
[00:38:16.200 --> 00:38:21.200]   >> The irony of this is, meanwhile, CIA operatives are using
[00:38:21.200 --> 00:38:24.200]   Strava to mind their runs, and-- >> Yeah.
[00:38:24.200 --> 00:38:27.200]   >> They're not-- they're obsecused, terrible.
[00:38:27.200 --> 00:38:29.200]   And our government leaders are all using--
[00:38:29.200 --> 00:38:31.200]   no longer using secure phones, they're just using WhatsApp,
[00:38:31.200 --> 00:38:32.200]   with little street-- >> Yeah.
[00:38:32.200 --> 00:38:35.200]   >> -- with a store-bought phone. >> Yeah.
[00:38:35.200 --> 00:38:38.200]   >> Well, it makes it easier to investigate them, but--
[00:38:38.200 --> 00:38:39.200]   >> Yeah. >> [laughs]
[00:38:39.200 --> 00:38:41.200]   >> Truth. >> What a world.
[00:38:41.200 --> 00:38:44.200]   >> Yeah. >> Wasn't there a story a while back about how
[00:38:44.200 --> 00:38:48.200]   Russia thought that there was some sort of like super top
[00:38:48.200 --> 00:38:51.200]   secret meeting room in the dead center of the Pentagon,
[00:38:51.200 --> 00:38:55.200]   the plaz in the middle of the Pentagon, because they were using
[00:38:55.200 --> 00:38:57.200]   like fitness trackers and like consumer devices,
[00:38:57.200 --> 00:39:00.200]   because all these really important people like are going to
[00:39:00.200 --> 00:39:02.200]   the same time, and they discover, well, no,
[00:39:02.200 --> 00:39:04.200]   that there's a coffee shop there. >> [laughs]
[00:39:04.200 --> 00:39:06.200]   >> That's where they're going to get coffee.
[00:39:06.200 --> 00:39:09.200]   >> Yeah. >> However, I would put an operative in the coffee shop
[00:39:09.200 --> 00:39:10.200]   just to finish. >> Yeah, I'd be good.
[00:39:10.200 --> 00:39:12.200]   >> But just one-- well, no, that's the worst place,
[00:39:12.200 --> 00:39:14.200]   because I'll never get the person's name right.
[00:39:14.200 --> 00:39:16.200]   Anyway. >> [laughs]
[00:39:16.200 --> 00:39:19.200]   >> I have a coffee for Allison.
[00:39:19.200 --> 00:39:21.200]   Allison, anybody? Oh, it's Annie. >> Yeah.
[00:39:21.200 --> 00:39:24.200]   >> A doppie trap. Doppie trap. >> Doppie trap.
[00:39:24.200 --> 00:39:27.200]   >> Wasn't there an industrial espionage thing a while back,
[00:39:27.200 --> 00:39:29.200]   where they found out that someone was just hanging out at a bar
[00:39:29.200 --> 00:39:32.200]   with a bunch of drunken techno people? >> Yeah, that's the real
[00:39:32.200 --> 00:39:34.200]   truth. >> They just go there after hours, and that's the real truth.
[00:39:34.200 --> 00:39:36.200]   >> Yeah. >> The only thing I wanted to point out is that
[00:39:36.200 --> 00:39:40.200]   sometimes you find that you have to acquire new skills and new habits,
[00:39:40.200 --> 00:39:45.200]   and that there is one way to deal with all of the entities
[00:39:45.200 --> 00:39:48.200]   they're trying to get access to your privacy and your data
[00:39:48.200 --> 00:39:51.200]   is to accept that some things can be private,
[00:39:51.200 --> 00:39:56.200]   some things can be a little bit less so awareness and ability
[00:39:56.200 --> 00:39:58.200]   to understand what the risks are.
[00:39:58.200 --> 00:40:02.200]   One other part of that is just realizing that there was a time
[00:40:02.200 --> 00:40:07.200]   where I would use the same simple password on any service
[00:40:07.200 --> 00:40:11.200]   that I didn't care about, because like, well, okay, this is just
[00:40:11.200 --> 00:40:14.200]   like some message border. This is just like where I'm backing up
[00:40:14.200 --> 00:40:17.200]   some pictures that I don't really care about. I'll just use this one,
[00:40:17.200 --> 00:40:20.200]   the simple six letter password I can't remember.
[00:40:20.200 --> 00:40:25.200]   And now, no matter what it is, I had to sign up for like four new
[00:40:25.200 --> 00:40:29.200]   music stores to download HD music from. I had to come up with a new
[00:40:29.200 --> 00:40:32.200]   like at least 16 letter password for each one of them because
[00:40:32.200 --> 00:40:36.200]   you realize that it stops becoming such a notable pain in the butt
[00:40:36.200 --> 00:40:40.200]   once you get into the habit of doing it all the time and once you
[00:40:40.200 --> 00:40:43.200]   accept that, no, I have to have complex passwords and they have to be
[00:40:43.200 --> 00:40:47.200]   unique per device. And when you, every time you get a new piece
[00:40:47.200 --> 00:40:51.200]   of hardware or subscribe to new service, your one of your first
[00:40:51.200 --> 00:40:54.200]   thoughts has to be what kind of data am I putting here and what am I
[00:40:54.200 --> 00:40:58.200]   putting at risk by putting that data there. So you, I keep coming
[00:40:58.200 --> 00:41:02.200]   back to table stakes. There used to be time where you just didn't even
[00:41:02.200 --> 00:41:05.200]   care about hitchhiking. And now you realize that, okay, that was
[00:41:05.200 --> 00:41:10.200]   that was nice in the 20s and 30s in universal madcap movies.
[00:41:10.200 --> 00:41:13.200]   But now you can't do that anymore. You have to think hard about
[00:41:13.200 --> 00:41:16.200]   the kind of strangers that you're getting into contact with.
[00:41:16.200 --> 00:41:18.200]   Well, that's the sort of stuff I'm talking about. I'm hoping that
[00:41:18.200 --> 00:41:23.200]   we see more of what Apple has been doing with the, you know,
[00:41:23.200 --> 00:41:27.200]   the anonymized email. I'm hoping we get to a point where Apple kind
[00:41:27.200 --> 00:41:32.200]   of enforces that more into more places. Like my biggest problem
[00:41:32.200 --> 00:41:36.200]   with Apple TV, for instance, is that you sign up for a new video
[00:41:36.200 --> 00:41:39.200]   service and now you have to put in your email and you have to put it.
[00:41:39.200 --> 00:41:42.200]   I'm like, look, I'm on a device. Just take the security from the
[00:41:42.200 --> 00:41:45.200]   device. You can do that. But you, but they don't do, you know, like
[00:41:45.200 --> 00:41:48.200]   Disney Plus once my, once my email and they want my thing, you know,
[00:41:48.200 --> 00:41:50.200]   and I get that they want all that stuff. I don't want to give it to
[00:41:50.200 --> 00:41:53.200]   them. You know, I want Apple to sign on as your, is your best friend.
[00:41:53.200 --> 00:41:57.200]   I want Apple to just like, slowly enforce on all of their devices
[00:41:57.200 --> 00:42:00.200]   that we'll sign in for you. We're not going to tell them anything.
[00:42:00.200 --> 00:42:03.200]   Yep. You know, and I think that that the anonymized email is their first
[00:42:03.200 --> 00:42:05.200]   little like, we're not going to scare you. So we're just going to add this
[00:42:05.200 --> 00:42:08.200]   one little thing. Don't confuse that though with what Google and Facebook
[00:42:08.200 --> 00:42:11.200]   and Twitter are doing with their single sign on because they are getting all
[00:42:11.200 --> 00:42:14.200]   the information. Right, right, right. So it's nice to have a trusted
[00:42:14.200 --> 00:42:18.200]   third party like Apple where you could say, and I use my Apple pay,
[00:42:18.200 --> 00:42:22.200]   my Apple credit card whenever possible because not because I'm trying to hide
[00:42:22.200 --> 00:42:25.200]   from the grocery store, my personal information, but just because it's
[00:42:25.200 --> 00:42:28.200]   you, it's getting the habit of it. It's great. And you're right. They
[00:42:28.200 --> 00:42:32.200]   do authentication that way. We have a very strong authenticated device with
[00:42:32.200 --> 00:42:37.200]   a secure enclave, a very good face recognition system. It would be a great
[00:42:37.200 --> 00:42:40.200]   thing if this would become my password. Of course, of course, if they do that,
[00:42:40.200 --> 00:42:43.200]   there's going to be people saying monopoly and they're going to be saying that
[00:42:43.200 --> 00:42:46.200]   Apple is, you know, doesn't shouldn't be allowed to do this to us as developers.
[00:42:46.200 --> 00:42:50.200]   But as users, it's to our advantage. Google's already on that train, but
[00:42:50.200 --> 00:42:53.200]   but you're giving Google your information. So it's not quite the same thing.
[00:42:53.200 --> 00:42:57.200]   They just announced that you can use Google Smart Lock and make your iPhone
[00:42:57.200 --> 00:43:02.200]   a trusted security hardware dongle. Yeah, but it's with Google.
[00:43:02.200 --> 00:43:05.200]   But also the really important thing to remember there is like myself included
[00:43:05.200 --> 00:43:08.200]   for a long time, we said that they would never sell our personal data because
[00:43:08.200 --> 00:43:12.200]   it's way more valuable to them. It's way too valuable to them. And then Facebook
[00:43:12.200 --> 00:43:15.200]   got caught selling data. You know, Google absolutely hasn't, but we can no longer
[00:43:15.200 --> 00:43:18.200]   just use that as a blanket excuse for like company would ever do this.
[00:43:18.200 --> 00:43:22.200]   We also have to, we also have to understand the power of these corporations like
[00:43:22.200 --> 00:43:26.200]   Verizon says, Hey, we really, security of searches is a great thing.
[00:43:26.200 --> 00:43:30.200]   Security of searches is important. So don't use Google. Don't use duck.go.
[00:43:30.200 --> 00:43:35.200]   Use our Verizon created portal that makes sure that your searches are secure.
[00:43:35.200 --> 00:43:39.200]   And this is the same company that got fined $1.5 million for having something
[00:43:39.200 --> 00:43:44.200]   that had to be called a super cookie for tracking Verizon users everywhere.
[00:43:44.200 --> 00:43:49.200]   So yeah, this is everybody's getting into the act. You know, we've been doing
[00:43:49.200 --> 00:43:52.200]   on the last few shows the story that Google Chrome, they announced they're
[00:43:52.200 --> 00:43:57.200]   going to be able to do a lot of things. And I thought, well, that's interesting
[00:43:57.200 --> 00:44:01.200]   because they are in the ad business. Why would they do that? And the more I think
[00:44:01.200 --> 00:44:04.200]   about it, the more I realize it's complete red herring. It's because everybody knows
[00:44:04.200 --> 00:44:08.200]   about third party cookies. They have other means. So they're going to look like
[00:44:08.200 --> 00:44:11.200]   the good guys by, Oh, yeah, no more third party cookies. But they're always the
[00:44:11.200 --> 00:44:15.200]   first party. But they're always the first party. I want to, I want to do
[00:44:15.200 --> 00:44:22.200]   this one, Bruce Schneier, who just wrote a very good piece. I think it was in the
[00:44:22.200 --> 00:44:26.200]   New York Times about face recognition saying, you know, we're doing all this
[00:44:26.200 --> 00:44:35.200]   focusing on face recognition. And, you know, making it illegal in, you know, San Francisco
[00:44:35.200 --> 00:44:39.200]   and other jurisdictions are starting to make it illegal to use it. He says face
[00:44:39.200 --> 00:44:46.200]   recognition isn't really the problem. We're banning face recognition. We're missing
[00:44:46.200 --> 00:44:51.200]   the point. The whole point of modern surveillance is to treat people differently.
[00:44:51.200 --> 00:44:55.200]   That is in other words, to know who you are so they can treat you in the way they
[00:44:55.200 --> 00:44:59.200]   want to treat you. Face recognition is a small part of that. And this is exactly
[00:44:59.200 --> 00:45:08.200]   what we're talking about. That there, there are multitude, a myriad of ways from
[00:45:08.200 --> 00:45:13.200]   heartbeat to gate to fingerprint to Iris. You can see, you can read somebody's
[00:45:13.200 --> 00:45:18.200]   Iris from meters away from the MAC addresses broadcast by our smartphones at
[00:45:18.200 --> 00:45:22.200]   all times. There are a myriad of ways for people to identify us for advertising
[00:45:22.200 --> 00:45:27.200]   companies and governments to advertise us. China is using it and using all of them
[00:45:27.200 --> 00:45:32.200]   to support a surveillance state. This is something. So it's not face recognition.
[00:45:32.200 --> 00:45:36.200]   And I think it would be nice to have a company that we could trust Apple for instance
[00:45:36.200 --> 00:45:40.200]   to minimize his MAC addresses on the iPhone. That is a little thing. No one
[00:45:40.200 --> 00:45:45.200]   understands. No one, you know, they don't put that on the side of a hotel in
[00:45:45.200 --> 00:45:51.200]   Vegas. But that is a really great idea. Right. Yeah. And if fingerprinting is
[00:45:51.200 --> 00:45:55.200]   stopping fingerprinting too. Yeah. So he says there's an entire industry of data
[00:45:55.200 --> 00:45:59.200]   brokers who make a living analyzing and augmenting data about who we are. Maybe
[00:45:59.200 --> 00:46:05.200]   Google says we don't sell that data. It's too valuable. Nevertheless, that data is
[00:46:05.200 --> 00:46:11.200]   being collected, aggregated and then sold by a many, he says regulate data
[00:46:11.200 --> 00:46:16.200]   brokers, regulate the companies, not face recognition, regulate the companies
[00:46:16.200 --> 00:46:22.200]   that are gathering this data and using it. And I think he's exactly right that if we
[00:46:22.200 --> 00:46:28.200]   care about all this stuff, this is not face recognition is the least of our
[00:46:28.200 --> 00:46:33.200]   worries. But I think we do need a company we can trust. Maybe this is the one and
[00:46:33.200 --> 00:46:37.200]   only company we can trust. Right now, I think that Apple is the only one really
[00:46:37.200 --> 00:46:42.200]   focused on whether they're not perfect as far as protecting our data. But they're
[00:46:42.200 --> 00:46:46.200]   the only ones that are slowly tying all these holes up. Well, and we, we, and I
[00:46:46.200 --> 00:46:51.200]   mean, you, everybody on this panel, but also as consumers, we need to really
[00:46:51.200 --> 00:46:55.200]   impress this on Apple. This is a valuable commodity. Oh, I think they're going to.
[00:46:55.200 --> 00:46:59.200]   If you do this right and you and you stick to your promises and you tell us the
[00:46:59.200 --> 00:47:04.200]   truth and you're very clear about it, you will gain hugely from this. So Tim,
[00:47:04.200 --> 00:47:08.200]   I think he knows that, but I think there's also pressure from governments and
[00:47:08.200 --> 00:47:14.200]   others. That's, that's something that could make you the, you know, even more
[00:47:14.200 --> 00:47:19.200]   success. And I think to remember is that the, that our cell phone security has
[00:47:19.200 --> 00:47:25.200]   proven to be not, you know, it's not as high a levels priority for our legislature,
[00:47:25.200 --> 00:47:30.200]   legislature as abortion or gun control. Well, they don't understand it.
[00:47:30.200 --> 00:47:35.200]   And it's not, it's important. Like when the FBI comes and does a little talk for
[00:47:35.200 --> 00:47:39.200]   them, they go, Oh yeah, that's, that's, that's, that's important. But if millions
[00:47:39.200 --> 00:47:43.200]   of people send them email and March and everything else, they're going to be like,
[00:47:43.200 --> 00:47:46.200]   Oh, no, no, no, what I'm saying is they're going to go, Oh, this isn't worth it.
[00:47:46.200 --> 00:47:48.200]   I mean, you look at, look at like that. That's why they don't touch gun control.
[00:47:48.200 --> 00:47:52.200]   They're like, right. It's not, I'm not going to die on this hill. Literally, you know,
[00:47:52.200 --> 00:47:56.200]   until, so the key is as users, we will never have to worry about this as long as
[00:47:56.200 --> 00:48:00.200]   NRA for privacy and security. Yes.
[00:48:00.200 --> 00:48:06.200]   I think I remember reading and I wish I remembered who produced this information,
[00:48:06.200 --> 00:48:11.200]   but I don't at that brand identity for, for young people, like people under the
[00:48:11.200 --> 00:48:17.200]   age of 18 are more likely to buy an iPhone than, than any, or any time before.
[00:48:17.200 --> 00:48:22.200]   Like, so right now the young people brand identity is with Apple. And if I always
[00:48:22.200 --> 00:48:27.200]   wondered what, what that why that was. And one thing that comes to mind is this
[00:48:27.200 --> 00:48:35.200]   idea that maybe knowing that Apple is putting like our data privacy in at the
[00:48:35.200 --> 00:48:40.200]   forefront of their, of what they, what they do, that might be, because it's the young
[00:48:40.200 --> 00:48:45.200]   people who are more active and more engaged in recognizing the fact that we
[00:48:45.200 --> 00:48:49.200]   shouldn't be just allowing all these companies to have access to our data.
[00:48:49.200 --> 00:48:53.200]   They get it a little bit faster than we do. They figured it out a little sooner
[00:48:53.200 --> 00:48:56.200]   than we did. And maybe that's why Apple has been such like a important
[00:48:56.200 --> 00:48:59.200]   brand for the young people's future. I don't know.
[00:48:59.200 --> 00:49:03.200]   I don't know. When I talk to young people, they don't seem to care at all.
[00:49:03.200 --> 00:49:07.200]   Like, like, when I talk to about security, they, they, they're like, yeah, you know,
[00:49:07.200 --> 00:49:11.200]   it's, it's all gone. Like, like, you know, when I talk to people that are not that
[00:49:11.200 --> 00:49:14.200]   much younger than me and everyone down below, it always seems like maybe they're
[00:49:14.200 --> 00:49:19.200]   also a little more practical. Maybe they understand that you don't need to have
[00:49:19.200 --> 00:49:23.200]   everything be 100% secure. Well, but I think that they're also just a little
[00:49:23.200 --> 00:49:27.200]   resigned to the fact that they just feel like everything is, you know, and, and, and
[00:49:27.200 --> 00:49:31.200]   again, while I talk about it, and I'm, and I'm, and I'm, I'm super careful about
[00:49:31.200 --> 00:49:34.200]   certain things because I don't, you know, I, I get concerned. One of the United
[00:49:34.200 --> 00:49:37.200]   States, I don't think about it that much. It's not like I'm constantly worried about
[00:49:37.200 --> 00:49:40.200]   security. I'm just worried about losing this, this little way of securing if needed.
[00:49:40.200 --> 00:49:45.200]   But when you go places like Zimbabwe, is it more of a concern for you? Yes.
[00:49:45.200 --> 00:49:48.200]   Yeah. Yeah. I mean, there's definitely a question when you cross orders. It's a concern.
[00:49:48.200 --> 00:49:56.200]   I mean, if, if I'm in, even, you know, India or Cambodia or Iraq or, you know, those kinds
[00:49:56.200 --> 00:50:02.200]   of countries, you want to make sure that it's difficult, you know, to, to sort that out.
[00:50:02.200 --> 00:50:11.200]   Yeah. All right. Good. Good. Good. Good talk. Thank you, everybody. Good talk.
[00:50:11.200 --> 00:50:19.200]   We are doing, speaking of privacy, there's a constant battle, by the way, to get
[00:50:19.200 --> 00:50:25.200]   podcasts to track you. I'm very happy to say we have no way to do that.
[00:50:25.200 --> 00:50:32.200]   Yeah. We have, because you, now, if you listen with one podcast app all the time, whether
[00:50:32.200 --> 00:50:37.080]   it's Apple's podcast, they do keep, you know, information about you or Spotify. If
[00:50:37.080 --> 00:50:40.680]   Spotify's already announced, they plan to do more of that. So when you see an exclusive
[00:50:40.680 --> 00:50:45.360]   podcast for one particular platform, I can guarantee you the reason that's happening
[00:50:45.360 --> 00:50:49.360]   is so that they can track you. We are, we want to be everywhere you go, right? We don't
[00:50:49.360 --> 00:50:54.120]   track you, but we do ask for some information only because whatever we can find out about
[00:50:54.120 --> 00:50:59.360]   our audience helps us two ways. One, make better programs for you, but two, to sell our advertising.
[00:50:59.360 --> 00:51:02.720]   Advertisers, you know, will say, well, I want to know everybody who's listening. We can't
[00:51:02.720 --> 00:51:07.680]   tell you that, but we could tell you that the, you know, it's a 53% mail and, you know,
[00:51:07.680 --> 00:51:11.400]   and so that information's valuable. The way we get that is completely voluntarily from
[00:51:11.400 --> 00:51:16.400]   you. And if you don't want to participate, cool, man. I dig it. But if you do, our annual
[00:51:16.400 --> 00:51:22.760]   surveys here, twitch.to/survey20, just a few questions, you'll see as soon as you look
[00:51:22.760 --> 00:51:26.440]   at it, what we're looking for, the kind of information, what's your, what's your, what
[00:51:26.440 --> 00:51:30.360]   areas your job and what your age is, that kind of thing. Don't answer any question you're
[00:51:30.360 --> 00:51:33.960]   uncomfortable with. That's fine. We don't keep track of who's answering. We just look
[00:51:33.960 --> 00:51:40.360]   at the idea is for us to know an aggregate and a little bit more about our audience.
[00:51:40.360 --> 00:51:43.600]   It's a big help. We do this once a year and anything you can do to help us out. We would
[00:51:43.600 --> 00:51:53.280]   appreciate it with that. T O slash survey 20. Thank you very much. Apple, according to
[00:51:53.280 --> 00:51:58.800]   strategy analytics, which is a, see Apple doesn't say, but strategy analytics has somehow
[00:51:58.800 --> 00:52:03.200]   figured out that Apple has sold, because they got data, they got data, they have data from
[00:52:03.200 --> 00:52:08.860]   us and they do, they have credit card info and stuff that Apple has sold 60 million air
[00:52:08.860 --> 00:52:15.300]   pods in 2019 make it 60 million and one. I finally broke down. I finally broke, well,
[00:52:15.300 --> 00:52:18.940]   you know what I want to do? I want to see your pods pro. I want to see, yeah, I want
[00:52:18.940 --> 00:52:23.420]   to see if these really do smell like blueberries. Oh God. Oh, you're not going to sniff them.
[00:52:23.420 --> 00:52:27.740]   You can have Micah, sorry to check for you. Well, these are a fresh, unworn pair. I want
[00:52:27.740 --> 00:52:31.780]   to get, I want to get a green pair. So it reminds me of plants versus zombies. Doesn't
[00:52:31.780 --> 00:52:35.580]   it doesn't remind you of one of the plants and plants versus zombies? It is kind of like
[00:52:35.580 --> 00:52:41.260]   a little like organic shooter, a little book shooter. Remember how we mocked the air pod
[00:52:41.260 --> 00:52:46.140]   and how that little dongle and everybody, it's, oh, it looks so hard. That's all you
[00:52:46.140 --> 00:52:52.300]   see anywhere. I'm watching the NFL playoffs, all the athletes coming to the stadium.
[00:52:52.300 --> 00:52:57.540]   They'll put point of logic. People who are using more modern ones, you don't see anything.
[00:52:57.540 --> 00:53:01.020]   So you don't know they're wearing very even better. You're even better.
[00:53:01.020 --> 00:53:05.580]   Do the AirPods pro are a little bit less obvious, right? My wife.
[00:53:05.580 --> 00:53:10.860]   No, they're just as obvious. Just stick out. She's wearing them. Yeah. So you're saying,
[00:53:10.860 --> 00:53:14.500]   when you say a little more modern, you mean anything but Apple? Is that what you mean,
[00:53:14.500 --> 00:53:20.100]   Andy? No, no, I'm saying that just because you can't see like, doesn't mean they're
[00:53:20.100 --> 00:53:23.620]   different between, they might be wearing a competitor that fits all the way of the year.
[00:53:23.620 --> 00:53:27.380]   But I don't, I've never thought that that was a big deal anyway, given that particularly
[00:53:27.380 --> 00:53:32.180]   at the time, if you have to have two little tiny little popsicle sticks like grouping
[00:53:32.180 --> 00:53:37.140]   down from the ears, but it actually like keeps in contact with my phone without breaking
[00:53:37.140 --> 00:53:40.260]   up while I'm listening to music, I'm good. Yeah.
[00:53:40.260 --> 00:53:45.540]   So a couple of things about the AirPods. You can now have the poop emoji on engraved
[00:53:45.540 --> 00:53:48.500]   Did you do it? Did you do that?
[00:53:48.500 --> 00:53:56.460]   I put my name and my phone number on it hoping that, hoping against all odds that somebody
[00:53:56.460 --> 00:53:59.300]   would find these and return them to me instead of just saying, Oh, good. They smell like
[00:53:59.300 --> 00:54:05.780]   blueberries. Let me, okay. Never been opened. Seal direct from the factory. And I'm going
[00:54:05.780 --> 00:54:12.660]   to let you do it to Alex as an independent nasal tester. He's getting my other. Oh, okay.
[00:54:12.660 --> 00:54:18.940]   Just tell me, what do they smell like? Just don't know, no preconceived notions. Blueberries.
[00:54:18.940 --> 00:54:24.860]   They smell just like never. And not the second time. It's like, I had to close it when I
[00:54:24.860 --> 00:54:28.940]   open it. I go blueberries. But but of course it's not really blueberries. It's a highly
[00:54:28.940 --> 00:54:35.700]   carcinogenic chemical off gas, but that smells man, does it smell like blueberries? It's not
[00:54:35.700 --> 00:54:40.260]   like it's not like it's a neuro toxin that comes from a dementia that makes it smell
[00:54:40.260 --> 00:54:42.020]   like you're dead. It was good.
[00:54:42.020 --> 00:54:46.140]   It's nothing else for Orlando. I noticed your wife ran screaming. Dan, tell me you got
[00:54:46.140 --> 00:54:52.700]   to close it first. Oh, yeah. Let it build up. It's very independent studio audience member.
[00:54:52.700 --> 00:55:00.660]   Yeah. My goodness. Yeah. It's everyone's face. I can tell you.
[00:55:00.660 --> 00:55:06.820]   It's not just that it smells like a blueberry smell. It's like it is like a juicy ready to
[00:55:06.820 --> 00:55:13.820]   eat blueberry. They may not have recycled strawberry shortcake dolls. Alex, sir, you have never
[00:55:13.820 --> 00:55:19.660]   been to the to the state of Maine during blueberry season. I would never say anything like that.
[00:55:19.660 --> 00:55:26.660]   Probably not. I don't know. Have you smelled those air pods pro yet? Maybe they do smell
[00:55:26.660 --> 00:55:31.820]   like that. No, but I've had blueberry pancakes and blueberry pie in the same meal at a diner
[00:55:31.820 --> 00:55:35.740]   in Maine at the height of blueberry season. Wow. That is the definitive marker for all
[00:55:35.740 --> 00:55:40.020]   blueberry nests. You are Mr. Blueberry. Well, as I say, I grew up picking blueberries from
[00:55:40.020 --> 00:55:45.260]   the trees. Was it in Bush's name or is it one of those fake blueberry patches where they
[00:55:45.260 --> 00:55:51.820]   just plant them for the tourists? They were simply in the backyard. Oh, the best guy.
[00:55:51.820 --> 00:55:58.940]   I love black garden blueberry. These aren't wire. Why are they charging, right? Or are
[00:55:58.940 --> 00:56:04.460]   they? I don't know. They are. Oh, look. Drop them on your pad. My Lenovo has a little
[00:56:04.460 --> 00:56:10.260]   Qi charge pad. If I just put it right here, it'll just light up and it does. Look at
[00:56:10.260 --> 00:56:14.700]   that. That is really cool. All right. I can put my snazberry. Don't forget. Don't forget
[00:56:14.700 --> 00:56:20.100]   those are there. Yeah, really. It's a lot to spend for headphones that are that easy
[00:56:20.100 --> 00:56:24.820]   to lose. I got to say, I wasn't going to do that. I resisted it. Are you? Are you the
[00:56:24.820 --> 00:56:29.860]   kind of person that easily loses things? I consider myself somebody that, you know, I
[00:56:29.860 --> 00:56:34.540]   keep track of my stuff and I don't lose things. So that's never been a problem or concern for
[00:56:34.540 --> 00:56:42.700]   me. But I've read so many articles about people losing multiple headphones. And actually, Russell
[00:56:42.700 --> 00:56:48.140]   Holly of Imore and and Android Central and all the other sites that are a part of the
[00:56:48.140 --> 00:56:55.420]   future family. Like he just lost his AirPods Pro right before CES. So it does happen, but
[00:56:55.420 --> 00:57:02.420]   I never lost your iPod AirPods. I've never. I've left them at home. I have to knock on
[00:57:02.420 --> 00:57:11.180]   one right now, but I've never lost anything important. I am on my fourth pair of AirPods.
[00:57:11.180 --> 00:57:19.260]   Okay, I get at some at some point, that's just carelessness. I'm saying that at some
[00:57:19.260 --> 00:57:24.580]   point you need to say that the nanny part of your brain needs to say, no, we're going
[00:57:24.580 --> 00:57:29.780]   to be using wired earphones, Leo. And let me explain why, because we gave you three opportunities
[00:57:29.780 --> 00:57:34.460]   to have wireless earphones and you lost them three times. No, I think that maybe that will
[00:57:34.460 --> 00:57:39.620]   teach you responsibility. And I'm saying that as someone who at the app, as I speak right
[00:57:39.620 --> 00:57:45.700]   now, I know that I have a pair of air for AirPods somewhere in the office, but I don't
[00:57:45.700 --> 00:57:49.700]   know where they are. Oh, yeah, well, it depends on your definition of lost. I know they're
[00:57:49.700 --> 00:57:55.980]   missing. I just miss I'm not I'm not Maria Conda Condoed enough to realize that I would
[00:57:55.980 --> 00:58:01.260]   have to actually take everything down to bare wooden walls to definitely one pair. There
[00:58:01.260 --> 00:58:04.900]   might be something I bought another pair. And then I found the other pair under the bed
[00:58:04.900 --> 00:58:09.500]   because I was wearing it on bed and they fell under the bed. Oh, yeah. So that's yeah, by
[00:58:09.500 --> 00:58:14.860]   the way, how much of this cost 250 bucks? You think I get some Apple stickers? You get
[00:58:14.860 --> 00:58:21.180]   stickers? Where's my Oh, man, get to buy the Mac Pro for the stickers, Leo, stickers,
[00:58:21.180 --> 00:58:29.100]   I guess you're below the price and how much do the wheels for those? What no, they have
[00:58:29.100 --> 00:58:35.900]   racks now, Andy, your rack mounted. There is a pamphlet on how to use them. I will say
[00:58:35.900 --> 00:58:41.660]   that my my untetical wife actually just talked about was just talking this morning about how
[00:58:41.660 --> 00:58:45.580]   much she loves her AirPods. Like it was just like a very she lost them for a couple months.
[00:58:45.580 --> 00:58:50.380]   Yeah, so she misplaced them. She's wandering around trying to find them and finally found
[00:58:50.380 --> 00:58:54.220]   them. And it was like a couple days later, she's like, Oh my goodness, I don't know how I live
[00:58:54.220 --> 00:59:00.140]   without these before. Like that's. And she had like, I don't know how many pairs of Bluetooth
[00:59:00.140 --> 00:59:04.700]   headphones and wired headphones and all kinds of other things. And it was always this constant
[00:59:04.700 --> 00:59:08.380]   frustration and the idea that you can pull them out. They're generally charged. We probably
[00:59:08.380 --> 00:59:12.220]   mean, I always, it always seems random. Like I try to keep charging them, but I always feel like
[00:59:12.220 --> 00:59:16.940]   I'm using them for a long time before I have to be able to just kind of keep my feeling like
[00:59:16.940 --> 00:59:20.220]   you're putting them in the case and you're like, this should run out. Can you hear that sound?
[00:59:20.220 --> 00:59:23.660]   Like doo doo doo doo doo doo. You're like, what? That still happens. Yeah, exactly. Leo,
[00:59:23.660 --> 00:59:28.940]   do you still have your instructions for the AirPods Pro? I want to know whether or not Apple took away
[00:59:28.940 --> 00:59:36.620]   or removed the the reference to airpower finally on the on the on the instructions when I bought.
[00:59:36.620 --> 00:59:43.180]   I've heard you lost the instruction. Where'd they go? How could I have lost them already?
[00:59:43.740 --> 00:59:48.140]   I mean, they probably have taken them off. I threw them in the air. You're like a magician.
[00:59:48.140 --> 00:59:54.140]   Oh, here they are. Wait a minute. Okay, AirPods Pro. So, so I'm looking for an air power.
[00:59:54.140 --> 00:59:59.900]   I think it was one of the last pages about charging. I think it says something about
[00:59:59.900 --> 01:00:08.380]   air power. Charged wirelessly. AirPods Pro charging the case, place case with status light facing up
[01:00:08.380 --> 01:00:15.180]   on a cheat shirtify. Shr, our sure. Cheat, that's what I'm saying. Cheat, shell, cheat, shell,
[01:00:15.180 --> 01:00:21.100]   she sell cheese shells on the cheat charger on a cheese certified. It's up sniffing my AirPods or
[01:00:21.100 --> 01:00:29.020]   using the light. It does not. I think he really likes that blueberry odor. I'm telling you,
[01:00:29.020 --> 01:00:33.980]   it's carcinogenic. Again, it's a neurotoxin. What the side is is making things smell like blueberry
[01:00:33.980 --> 01:00:40.060]   for God's sake. It does not want $30,000 for a Mac Pro. It's not too much to spend.
[01:00:40.060 --> 01:00:45.500]   You know what? They did a good run on the pamphlets after 50 million. They said we better
[01:00:45.500 --> 01:00:50.940]   print some more pamphlets because they took it out of the pamphlets. Anyway, congratulations,
[01:00:50.940 --> 01:01:06.460]   Apple. I'm making a product that everybody has to have several of several of my
[01:01:07.500 --> 01:01:15.740]   roly poly. There we go. I can only see the top four stories.
[01:01:15.740 --> 01:01:25.420]   So we're done. Thank you, everybody, for joining me on the show today. Okay, let's see what else.
[01:01:25.420 --> 01:01:33.260]   It's something wrong with the scroly. I have a stuck scroly.
[01:01:34.140 --> 01:01:37.580]   How could that? I wonder if it's the AirPods or screwing up my mouse.
[01:01:37.580 --> 01:01:43.820]   You know what? I'm putting these way over here. I bet you that's it. It's setting it.
[01:01:43.820 --> 01:01:49.020]   No, active jamming features to interfere with all non-apple blue places.
[01:01:49.020 --> 01:01:51.820]   Sorry for blaming the airpods. They're crying in the corner.
[01:01:51.820 --> 01:01:56.380]   They blamed the AirPods and threw them. Thank you. Give me your iPad. At least that's the worst.
[01:01:56.380 --> 01:01:59.340]   Apple is daring the Justice Department to file a suit against them.
[01:01:59.340 --> 01:02:06.940]   Oh, this made me really mad. This made me really mad. Congress had hearings in Boulder, Colorado.
[01:02:06.940 --> 01:02:10.140]   That's not what made me mad, by the way.
[01:02:10.140 --> 01:02:19.260]   Bouldersprick. I live there. I love DHH. DHH. And the more than DHH, the CEO of Tyle,
[01:02:19.260 --> 01:02:26.780]   who said Apple acted anti-competitively by putting fine my iPhone in the iPhone.
[01:02:26.780 --> 01:02:32.140]   Tyle makes a Bluetooth tracker. It's really big. You can't stick a tile to your iPhone and then
[01:02:32.140 --> 01:02:39.180]   expect to use it. I don't know if I agree with her contention that by putting it. I think there is
[01:02:39.180 --> 01:02:46.220]   merit in the overall complaint that Apple will can do stuff with iPhone apps that other companies
[01:02:46.220 --> 01:02:54.060]   can't do. Tyle does not have all of the location features that Apple's iPhone could allow.
[01:02:54.060 --> 01:02:59.820]   Apple has monopoly in that respect. She was there testifying in front of Congress,
[01:02:59.820 --> 01:03:06.940]   were a few other CEOs including, and then we're just testifying about Apple, the guys who do,
[01:03:06.940 --> 01:03:11.740]   this is probably why they were in Boulder. Those pop sockets that go on the back of your phone,
[01:03:11.740 --> 01:03:14.060]   that's from Boulder. I know that because my son-
[01:03:14.060 --> 01:03:15.980]   And now I dock it for the pop socket and you can rock it.
[01:03:15.980 --> 01:03:20.940]   You can dock the rocket of the pocket of the socket. This is a very Danny K moment.
[01:03:21.580 --> 01:03:26.700]   The pop socket guy said, "We have to sell through Amazon. We tried not selling through Amazon.
[01:03:26.700 --> 01:03:33.740]   I cost us $10 million, but Amazon basically stole our design and started making their own."
[01:03:33.740 --> 01:03:36.860]   I guess that's what he was saying. I think he just said it was hard. I don't think he even
[01:03:36.860 --> 01:03:42.140]   said it. He didn't sell that. I think he says that there was restrictions or regulations on selling.
[01:03:42.140 --> 01:03:45.900]   This whole thing. And then David Hanomier Hanson,
[01:03:47.180 --> 01:03:52.140]   who is the founder of Basecamp, and I've talked to many times. He invented Ruby on rails. He's
[01:03:52.140 --> 01:03:58.300]   kind of legendary in our business despite his propensity for buying very, very expensive automobiles.
[01:03:58.300 --> 01:04:02.460]   But what could possibly be wrong with that?
[01:04:02.460 --> 01:04:05.580]   In a bit of a Twitter drama magnet.
[01:04:05.580 --> 01:04:09.980]   He's a drama magnet. He's the guy who said my wife got a different credit
[01:04:11.260 --> 01:04:17.580]   limit on the Apple card. Outrage would never be, would never press
[01:04:17.580 --> 01:04:19.820]   age investigation. The investigation should always come first.
[01:04:19.820 --> 01:04:24.940]   Well, that's what Twitter is. We're in this world now where Twitter and the outrage engine
[01:04:24.940 --> 01:04:32.940]   precedes everything. In any event, he said that these companies, what did he say? He had a great
[01:04:32.940 --> 01:04:38.300]   line and I can't read it because I can't scroll. You're our only hope. Help us. Help us, Congress.
[01:04:38.300 --> 01:04:41.580]   You're our only hope. He complained about with Ruby on rail. He said that, yeah, he said the
[01:04:41.580 --> 01:04:47.020]   digital ecosystem been quote colonized by a handful of big tech companies. Help us, Congress.
[01:04:47.020 --> 01:04:53.260]   You're our only hope. Yeah, that's, that's, that's, that's, that's not a great way to leave for
[01:04:53.260 --> 01:04:58.060]   that. That's not customary for a tech company to say, please Congress, legislate and interfere
[01:04:58.060 --> 01:05:02.140]   with our business because that's not that great. It's a long standing tradition. If you go to the
[01:05:02.140 --> 01:05:10.620]   EU, for instance, a lot of the EU's regulations and privacy things, that whole, you know, what
[01:05:10.620 --> 01:05:15.580]   Jeff Jarvis calls the like, so much, you strike this whole snippet thing came because publishers,
[01:05:15.580 --> 01:05:21.580]   magazine and newspaper publishers went to the courts in EU and said, these guys, this Google,
[01:05:21.580 --> 01:05:26.860]   they're really making it hard for us. And, and so I think it's not unusual for companies that can't
[01:05:26.860 --> 01:05:30.300]   compete in the marketplace to go to legislators and say fix it.
[01:05:31.100 --> 01:05:38.860]   I educated legislators by large parts. Yeah. Right. I made a, I, I, I, we sold a green screen
[01:05:38.860 --> 01:05:45.420]   plugin called DB Matt for, yes, for a long time. Yes. You know, and we made a lot of money on it.
[01:05:45.420 --> 01:05:52.860]   I mean, it was, you know, it was a solid $30,000 a month, you know, that comes in on 128k unprotected
[01:05:52.860 --> 01:05:57.660]   plugin. Like there was no, there was no password, there was no nothing to just installed. And it
[01:05:57.660 --> 01:06:04.540]   just kind of came in and, and I knew, everybody knew that as soon as Apple fixes the green screen,
[01:06:04.540 --> 01:06:09.180]   because their green screen plugin and final cut was so bad that the reason that everyone was buying
[01:06:09.180 --> 01:06:13.100]   ours is because it was the only way to make it work that is that the money, the, the, you know,
[01:06:13.100 --> 01:06:17.100]   the meal train would be over. You'd be sure locked eventually. I knew I was gonna be sure
[01:06:17.100 --> 01:06:21.340]   locked eventually. Like it was like not a, you know, it, it was like that was coming, you know,
[01:06:21.340 --> 01:06:26.540]   and, and I wasn't. So when Apple came out with, with Final Cut 10 and they had a great,
[01:06:26.540 --> 01:06:30.220]   uh, green screen plugin in there, I was like, well, we're done there. We're gonna move on to
[01:06:30.220 --> 01:06:33.820]   something else. You know, no, and, and the thing is, is that if he had done it two weeks after you
[01:06:33.820 --> 01:06:37.740]   came out with DV map, me, that was bad timing. That would have been bad timing. Yeah. You know,
[01:06:37.740 --> 01:06:41.180]   the thing is, is that they're not, and the thing is, is that you, you could see that it was there,
[01:06:41.180 --> 01:06:45.260]   and definitely when you're working with any of these big companies, you know, I mean,
[01:06:45.260 --> 01:06:48.300]   I've worked with a lot of the big companies, you know that they could squash you in a second.
[01:06:48.300 --> 01:06:52.780]   Yeah. And you have to go to Congress and say, you know, there's way too many podcasts.
[01:06:52.780 --> 01:06:58.620]   You've just got to limit the number of shows available. But the, the other thing though,
[01:06:58.620 --> 01:07:05.420]   is that, is that it is, um, these companies have made it easier to sell than it ever has been.
[01:07:05.420 --> 01:07:09.740]   Like when we look at this, I mean, the idea, they do have a monopsony in the Apple store,
[01:07:09.740 --> 01:07:13.340]   though, right? But it's less than, by the way, it's going to go to the Supreme Court. That is,
[01:07:13.340 --> 01:07:17.900]   the Apple store though, that's their store. Like they have an opally on their store for,
[01:07:17.900 --> 01:07:22.780]   for less than far less than half of the world's population, and definitely less than half of the
[01:07:22.780 --> 01:07:27.020]   US. But it's also, there's also an argument, go to Android. There's also an argument to be made
[01:07:27.020 --> 01:07:32.060]   that Apple has proprietary access to private APIs that give them the ability to compete in a way
[01:07:32.060 --> 01:07:37.340]   that no third party developer can. But that goes right back to our previous argument, because
[01:07:37.340 --> 01:07:42.140]   would you trust any random person with access to the APIs? There's good reason. Location and secure.
[01:07:42.140 --> 01:07:46.460]   Yeah. So you have to have some sort of, like in my perfect world, there would be a vetting system
[01:07:46.460 --> 01:07:52.380]   where certain other trusted companies will do it. It's incredibly gray and fuzzy line to try to walk.
[01:07:52.380 --> 01:07:56.140]   But I do think that they're, they're, they will, not only does there have to be, but they will
[01:07:56.140 --> 01:08:01.820]   eventually be regulated to be some sort of bigger system than we have right now at the App Store.
[01:08:01.820 --> 01:08:05.420]   But I really thought that part of the conversation, if we're talking about Apple,
[01:08:05.420 --> 01:08:12.780]   that they would be talking about borderline abuse that Apple reeks upon developers
[01:08:12.780 --> 01:08:17.900]   through controlling all of their apps through required APIs. Like if you have, like,
[01:08:17.900 --> 01:08:26.780]   if Apple suddenly decides that a new cloud service is absolutely critical to the future of the company
[01:08:26.780 --> 01:08:33.900]   and the future of the cloud service will be boosted by forcing every single developer to support it,
[01:08:33.900 --> 01:08:40.700]   whether that API is finished usable or sensible for the product that the developer is making,
[01:08:40.700 --> 01:08:45.820]   they can inflict that demand, that change upon the developer. The number of stories that you
[01:08:45.820 --> 01:08:50.860]   get from developers saying that we have all these plans for 2019 to spend most of our,
[01:08:50.860 --> 01:08:55.500]   most of the money and time that we spend enhancing our product to finally bringing this
[01:08:55.500 --> 01:08:59.260]   wonderful feature that all of our users have been demanding. Unfortunately, we had to spend
[01:08:59.260 --> 01:09:05.420]   that time instead supporting an API that we don't use that makes no sense to us, that Apple is not
[01:09:05.420 --> 01:09:10.460]   so new that Apple hasn't really documented, but Apple will not allow our app to be updated in the
[01:09:10.460 --> 01:09:15.740]   app store unless it supports this new API. So Apple has a lot to answer for, but
[01:09:15.740 --> 01:09:21.100]   not being able to track a tile tracker through the Find My app, probably to me is not one of them.
[01:09:21.100 --> 01:09:25.180]   Well, and I just want to say, like, as someone who has sold a lot of software,
[01:09:25.180 --> 01:09:31.980]   I just have to say that in the old days, I have to now figure out how I'm going to distribute that,
[01:09:31.980 --> 01:09:35.260]   how am I going to secure it, how am I going to do the e-commerce for it, how am I going to do
[01:09:35.260 --> 01:09:40.140]   the promotion for it, how am I going to do all these things that the app store took away and made it
[01:09:40.140 --> 01:09:45.500]   simpler, I would even consider doing it on my own anymore. I mean, I would absolutely put all,
[01:09:45.500 --> 01:09:51.420]   if I develop software again, I would absolutely put it on the app store. I wouldn't complain and
[01:09:51.420 --> 01:09:58.220]   I just move on because it has never been better than what it is now. I mean, as someone who went
[01:09:58.220 --> 01:10:04.620]   through the trouble of doing it and all the stuff that I had to do, of being able to distribute
[01:10:04.620 --> 01:10:07.980]   updates and know that everybody's going to get, I mean, the idea that you could distribute updates
[01:10:07.980 --> 01:10:11.980]   and everybody actually knows that there's an update and it's super simple for them to update and
[01:10:11.980 --> 01:10:16.460]   do their thing is amazing. It is amazing as someone who did this, I mean, like, we'd update and,
[01:10:16.460 --> 01:10:20.220]   you know, 10% of the people would even know it was there and it was free, you know, and then they'd
[01:10:20.220 --> 01:10:24.540]   come back and complain and they were talking, "Oh, this doesn't work." I'm like, "That's like three
[01:10:24.540 --> 01:10:28.860]   versions ago," you know, and so the idea that you can keep everybody up, up to date on it, that you
[01:10:28.860 --> 01:10:32.220]   can move those things down the other hand, it's not perfect, it's definitely not better, but
[01:10:32.220 --> 01:10:39.420]   holy smokes, is it better than it used to be? Well, okay. I mean, it was so painful.
[01:10:39.420 --> 01:10:44.860]   So, there's two things. There's the APIs and, by the way, that's historically been a problem that,
[01:10:44.860 --> 01:10:48.780]   for years, Microsoft was accused of having secret APIs in Windows. They were even books.
[01:10:48.780 --> 01:10:53.100]   Well, they did. They did. They did. And any company that took,
[01:10:53.100 --> 01:11:00.140]   they found them and used them was sadly upset when they got deprecated, but that's the price you
[01:11:00.140 --> 01:11:02.620]   payed. Although, to their credit, and credit is the wrong word, I mean,
[01:11:02.620 --> 01:11:07.260]   Cydia and other jailbroken app stores existed for years and they only stopped when they became
[01:11:07.260 --> 01:11:13.820]   noneconomically viable. Apple never executed evil sinister plans to destroy all the jail broke
[01:11:13.820 --> 01:11:17.660]   out stores. Well, they just kind of, every time there's a jailbreak, they turned it off.
[01:11:17.660 --> 01:11:20.940]   But they could have done a lot of things to make their lives really miserable. They never went out.
[01:11:20.940 --> 01:11:26.060]   They never like put DMCA takedowns on, as far as I know, they never personally sued Sarek, for example.
[01:11:26.060 --> 01:11:30.620]   I think, you know, Apple is a little bit of it, speaking of nanny states, a little bit of a nanny
[01:11:30.620 --> 01:11:36.700]   in what it allows in the app store. It doesn't allow some political comment in the app store.
[01:11:36.700 --> 01:11:42.300]   It doesn't allow adult content in the app store. And so, it isn't monopoly. You know,
[01:11:42.300 --> 01:11:46.780]   it's actually technically a monopoly. And that's what the Supreme Court's going to be ruling on.
[01:11:46.780 --> 01:11:53.820]   The monopoly is if you have, if you're dominant in sales, a monopoly is if you're dominant in buying.
[01:11:54.380 --> 01:11:59.340]   It's also like one of the things that the Apple is a buyer of apps that it then resells,
[01:11:59.340 --> 01:12:04.620]   which is kind of a stretch. Like an aggregator. An aggregator. But they don't actually say that,
[01:12:04.620 --> 01:12:09.980]   though. They straight up say, we're not the owners of these apps. All we are is like a store where
[01:12:09.980 --> 01:12:13.020]   people shall be. Of course, that's what they say. But that's what the Supreme Court's going to rule
[01:12:13.020 --> 01:12:18.220]   on because in effect, they are because they're the only place you can buy apps. You have to.
[01:12:18.220 --> 01:12:23.660]   So it's as if you went to a fruit store, if you will. And the only place you could buy apples
[01:12:24.300 --> 01:12:28.220]   was from that particular fruit store. They would be or Walmart said that they were all independent
[01:12:28.220 --> 01:12:31.820]   contractors. Everyone on the shelf. But the hard part, like the thing for me that the
[01:12:31.820 --> 01:12:36.620]   in general, the internet is really bad at multiple truths. And it is possible that
[01:12:36.620 --> 01:12:41.100]   human. When the app store launched, they were like, I remember that with Palm Info Center,
[01:12:41.100 --> 01:12:45.180]   and I was paying a fortune and getting terrible support. And it was the app market for mobile
[01:12:45.180 --> 01:12:50.380]   was terrible. And for developers, they were being charged way more than 30%, 55%, 70%,
[01:12:51.020 --> 01:12:55.580]   more than that. And it's possible that the app store was an incredibly good model in order to get
[01:12:55.580 --> 01:13:01.900]   the modern app economy, the pop app economy, actually going for people to trust software again on
[01:13:01.900 --> 01:13:06.300]   mobile devices and to get buy in from all the developers and the millions and millions of
[01:13:06.300 --> 01:13:10.860]   customers that followed. But it's also possible. That's a platform. Yeah, but it's also possible
[01:13:10.860 --> 01:13:15.500]   that over the last 10 years, we and the market and the customer base has matured to a point
[01:13:15.500 --> 01:13:21.820]   where the app store model and the agency model are no longer the one size fits all solutions that
[01:13:21.820 --> 01:13:27.020]   really helped get them going in the beginning. And we have to evolve them to a more shared and
[01:13:27.020 --> 01:13:31.820]   flexible model. If I were going to write an amicus brief for Apple, I would say when you go to the
[01:13:31.820 --> 01:13:38.620]   Supreme Court, you point out that progressive web apps, PWAs, can be written and in no way limited
[01:13:38.620 --> 01:13:44.860]   by Apple. And so anybody can write an app that's downloaded from the web directly onto an iPhone.
[01:13:44.860 --> 01:13:50.220]   And what Apple should do is make sure that there's first class support for progressive web apps.
[01:13:50.220 --> 01:13:53.420]   And then it's over because you don't have a monopoly anymore. That's what they said to
[01:13:53.420 --> 01:13:57.580]   Playboy, remember? They said you guys can make a web app and they did. And honestly, that's a
[01:13:57.580 --> 01:14:02.140]   reasonable argument. As long as web apps can be as good as native apps from the app store,
[01:14:02.140 --> 01:14:07.500]   that's then it's you're done. And if somebody wants adult content, they do that. I mean,
[01:14:07.500 --> 01:14:12.780]   I guess they kind of argue that, well, that's what the browser is for, you know. But make PWAs a
[01:14:12.780 --> 01:14:17.740]   first class citizen on the iPhone. And I don't think you have a case.
[01:14:17.740 --> 01:14:23.500]   And I think with WebGL, and I'm guessing pure guess that Apple is going to have a web metal
[01:14:23.500 --> 01:14:28.060]   ready fairly soon. Once all that stuff is in place, those are going to be much more
[01:14:28.060 --> 01:14:32.060]   performant than they are today. And then just like on the Macintosh, because that's what happens
[01:14:32.060 --> 01:14:37.580]   on the Macintosh, you have an app store. But if you're a developer like Rich Segal, and you say,
[01:14:37.580 --> 01:14:42.140]   well, why BB Ed, it needs to do more, you don't have to sell it on the app store. And if you could
[01:14:42.140 --> 01:14:47.420]   do that on the iPhone, it's over. Then you say, well, look, if you want safe, secure apps that we
[01:14:47.420 --> 01:14:51.580]   vet, get them from the app store. But if you're willing to take a chance, download your apps from
[01:14:51.580 --> 01:14:58.860]   a website as a progressive web app, I think that's a great compromise. Our TWIT forums, the TWIT.community,
[01:14:58.860 --> 01:15:04.140]   is in a program called Discourse that actually does a very nice PWA version. And if you compare
[01:15:04.140 --> 01:15:09.180]   it to the app store version, I prefer the new way better. Yeah, it's way better. So as long as
[01:15:09.180 --> 01:15:15.420]   Apple doesn't start pulling back, now they won't because of this pull back on Safari's PWA capabilities,
[01:15:15.420 --> 01:15:20.220]   I think. Yeah, that makes sense. That's a great solution. You know, it has to be app-like.
[01:15:20.220 --> 01:15:26.940]   But you know, I think that Twitter's PWA is a very good app on the experience on the iPhone.
[01:15:26.940 --> 01:15:29.820]   And then fixed Daniel Jalka's API is everyone will be happy again.
[01:15:29.820 --> 01:15:35.020]   Well, see, that's the thing. And you talk to developers and you understand there is reasonable
[01:15:35.020 --> 01:15:38.780]   friction from developers about the app store. There's things they're not happy with.
[01:15:39.100 --> 01:15:43.180]   Well, it's a lack of consistency. It's like Jalka, that app was approved every day for 10 years,
[01:15:43.180 --> 01:15:47.980]   and all of a sudden it's like, oh, we'd realize that this is not really the API you should be using.
[01:15:47.980 --> 01:15:50.700]   He's like, I've filed this right eight times, guys, seriously. Yeah.
[01:15:50.700 --> 01:15:57.420]   Tim Cook is in Ireland. Mr. Cook is on his way to Davos in Switzerland, where he's going to have
[01:15:57.420 --> 01:16:02.380]   lunch with the president in a bunch of big shots. Ireland's probably pretty cool and damp right now,
[01:16:02.380 --> 01:16:06.700]   too. He's... I was quote as Davos. He's gonna know how you're not missing. But there's better skiing
[01:16:06.700 --> 01:16:12.780]   in Davos. He was speaking as he got an award from IDA Ireland, the state agency responsible for
[01:16:12.780 --> 01:16:18.060]   foreign direct investment, celebrating Apple's 40 year history in Ireland, where, of course,
[01:16:18.060 --> 01:16:25.500]   the Irish wrap around in the Dutch sandwich began. By the way, Google has announced they are no
[01:16:25.500 --> 01:16:29.340]   longer going to use that, but they don't have to because they get a tax break anyway. So there's
[01:16:29.340 --> 01:16:34.300]   no point in Amazon gets tax breaks in the US. So there's really, you don't need the Irish wrap
[01:16:34.300 --> 01:16:39.180]   around. It's the double Irish and that... What does it call this? The double Irish...
[01:16:39.180 --> 01:16:44.780]   Dutch sandwich? Double Irish Dutch sandwich? I think that's what it's called. I can never.
[01:16:44.780 --> 01:16:46.780]   Also the most dangerous cheerleading move.
[01:16:46.780 --> 01:16:54.460]   Dan Edk Cook said, talked about AR. He's talking about AR again, kids.
[01:16:54.460 --> 01:16:58.620]   For a while, that's all he could talk about. And then he stopped and I thought, oh, oh.
[01:16:58.620 --> 01:17:03.500]   But now he says, I'm excited about AR. My view is it's the next big thing. It will
[01:17:03.500 --> 01:17:10.620]   pervade our entire lives. He visited a Dublin development firm called War Ducks.
[01:17:10.620 --> 01:17:19.660]   I don't know. He says, I guess they're a game developer, War Ducks. He also said...
[01:17:19.660 --> 01:17:23.260]   I'm super excited about AR. I'm not excited about games that they are. I haven't played
[01:17:23.260 --> 01:17:26.300]   any that I thought were. No, no. VR for games. AR for life.
[01:17:26.300 --> 01:17:31.020]   But there's so many cool things with AR. I've been playing with it that are just awesome.
[01:17:31.580 --> 01:17:37.820]   Outside of AR, which Tim couldn't stop talking about, he says he's extremely excited about the
[01:17:37.820 --> 01:17:43.260]   role technology to play in healthcare. He says, this intersection has not yet been explored very
[01:17:43.260 --> 01:17:46.700]   well. I think you could take the simple idea of having preventative things and find many more
[01:17:46.700 --> 01:17:51.580]   areas where technology intersects healthcare. And I think all of our lives are probably better
[01:17:51.580 --> 01:17:55.420]   off for it. And there's a ton of money to be made. You didn't mention that, but I did.
[01:17:55.420 --> 01:17:59.980]   Most of the money in healthcare goes to the cases that weren't identified early enough.
[01:17:59.980 --> 01:18:04.700]   Yeah, you're dying now, so let's fix you. It will take some time, but the things we are doing now
[01:18:04.700 --> 01:18:10.780]   that I'm not going to talk about today, those give me a lot of close for hope. Diagnostic stuff,
[01:18:10.780 --> 01:18:16.060]   early warning stuff. Well, again, I think that there's a cross-section when we trust the company
[01:18:16.060 --> 01:18:21.980]   to hold on to our data. If I have an optimized data and I know all this health information and I
[01:18:21.980 --> 01:18:28.940]   have DNA information, I can start modeling things that we could never model without huge samples.
[01:18:28.940 --> 01:18:33.580]   So the idea that I can see things in your... The problem is the place you go to get your DNA
[01:18:33.580 --> 01:18:39.100]   sample does not have to... Until Apple does DNA testing. Or they integrate with that.
[01:18:39.100 --> 01:18:43.100]   They do deal with 23andMe or something that says, look, only we will get this information.
[01:18:43.100 --> 01:18:48.460]   Or some kind of thing, but when you connect that and you connect all that health data,
[01:18:48.460 --> 01:18:52.620]   you're going to be able to see things oftentimes months or even years before.
[01:18:52.620 --> 01:18:56.620]   They're going to start... And it's things that they don't even know to look for yet. If they're
[01:18:56.620 --> 01:19:00.300]   just looking at all the data and you're cutting through that data, you're going to start seeing,
[01:19:00.300 --> 01:19:04.940]   okay, this person ended up having this or this person ends up having this and then you can start
[01:19:04.940 --> 01:19:10.060]   tracking back and looking at all the things that happen to them and then compare those things.
[01:19:10.060 --> 01:19:14.940]   And it's going to be groundbreaking when it happens. It's just keeping private as the issue.
[01:19:14.940 --> 01:19:16.940]   There could be some bumpy road before that.
[01:19:16.940 --> 01:19:19.020]   It's just history.
[01:19:19.020 --> 01:19:24.220]   Well, I'm talking about probably more than a decade out because the history of tech and
[01:19:24.220 --> 01:19:28.620]   medicine is littered with a lot of things that were... A lot of interesting ideas that were
[01:19:28.620 --> 01:19:33.500]   productized, for lack of a better word, well before the science was in on it.
[01:19:33.500 --> 01:19:40.300]   And the difficulty is that you have these... Once you turn a lot of these digital health services
[01:19:40.300 --> 01:19:44.060]   into just like these commercials, I say, ask your doctor about this pill.
[01:19:44.060 --> 01:19:48.140]   Even though your doctor probably would have brought it up himself, if you thought it were
[01:19:48.140 --> 01:19:52.540]   appropriate for your current condition. When people come in saying, hey, when people say,
[01:19:52.540 --> 01:19:57.020]   hey, for $400, I'm going to go get this full body scan. And then they find that, oh, well,
[01:19:57.020 --> 01:20:04.220]   there's a thickening in this artery near your heart that is unusual for someone like you.
[01:20:04.220 --> 01:20:08.220]   And now suddenly someone has to go through a whole bunch of diagnostic tests, even though
[01:20:08.220 --> 01:20:12.700]   it's likely that if there's no symptoms and there's no family history of heart disease,
[01:20:12.700 --> 01:20:18.300]   this is just one of the aberrations that is normal for the human body. So the problem that we
[01:20:18.300 --> 01:20:23.020]   don't want to face is that, well, we did see that you have a genetic marker that is associated with
[01:20:23.020 --> 01:20:27.340]   that is associated with this disease. So we're going to put you through a very,
[01:20:27.340 --> 01:20:32.140]   very expensive set of diagnostic tests just to make sure you rule, we ruled that out.
[01:20:32.140 --> 01:20:38.860]   So I'm just... I'll be... It'll be amazing if the only thing that technology and medicine does
[01:20:38.860 --> 01:20:44.220]   is automate paperwork and automate all these... Everything that a doctor's office has to be able
[01:20:44.220 --> 01:20:51.740]   to provide in records and forms to insurers and certifications and take that load off of the
[01:20:51.740 --> 01:20:57.820]   actual practitioner. And also just the simple things like universally, here are all the prescriptions
[01:20:57.820 --> 01:21:02.380]   that this person is taking. You're about to add a new one to it. Here are some things that you
[01:21:02.380 --> 01:21:07.580]   should know about that before you decide to write this script. I'm still amazed that when I was
[01:21:07.580 --> 01:21:13.660]   taking care of somebody that... It was the first time that anybody had like actually gone through
[01:21:13.660 --> 01:21:19.820]   and said, "Okay, here are the eight pills that this person is taking per day." And a simple idiot's
[01:21:19.820 --> 01:21:25.500]   search said that maybe you shouldn't be taking this with this. And this also has the same effect
[01:21:25.500 --> 01:21:31.260]   as this other pill. And once you just simply not saying, "Hey, I googled this doc and you're wrong."
[01:21:31.260 --> 01:21:35.420]   Once you come back to the next appointment and saying, "Here are all of the pills that mom is
[01:21:35.420 --> 01:21:40.140]   taking. Could you take a look at it and see if perhaps any of these are superfluous?" And that
[01:21:40.140 --> 01:21:46.140]   load got knocked down to just like four or five. That's the sort of stuff that is of immediate benefit
[01:21:46.140 --> 01:21:52.780]   that isn't theoretical at all. Although I do agree with you that once a huge data set of health data
[01:21:52.780 --> 01:21:59.500]   becomes anonymized and available to researchers, that could be quite revolutionary. Either for
[01:21:59.500 --> 01:22:03.500]   enhancing the health of people or enhancing the number of reasons that insurance companies have
[01:22:03.500 --> 01:22:08.460]   to deny you coverage, we will not know. But it'll be interesting the next 34 years.
[01:22:08.460 --> 01:22:12.620]   It's not all bad. One time when I was in high school, I got a painkiller and a
[01:22:12.620 --> 01:22:17.420]   thing that keeps your balance after hitting my head really hard. And football is great.
[01:22:17.420 --> 01:22:23.100]   And they were two different doctors that didn't know the other ones were doing it. It was
[01:22:23.100 --> 01:22:29.660]   best week ever. That was awesome. Ever since then, everything smells like blueberries. No,
[01:22:29.660 --> 01:22:36.700]   it was when the letters were peeling off the blackboard. In class, I was like, "Whoa, and I
[01:22:36.700 --> 01:22:39.820]   realize that no one else is seeing this. I'm not going to react. I'm not going to react."
[01:22:39.820 --> 01:22:45.340]   They're not going to fall over. I have to say they've already done a lot of the benefit. I already
[01:22:45.340 --> 01:22:50.460]   see it. For instance, now because Kaiser, my health care provider, allows health records to
[01:22:50.460 --> 01:22:55.420]   be stored in iPhone. I think that's fantastic. I've got all my health records here. In a way,
[01:22:55.420 --> 01:22:59.020]   by the way, that's a lot better than the way I was doing it when I went to the Kaiser website.
[01:22:59.020 --> 01:23:05.180]   And I just was looking at the health things. I can see what my heart rate is right now,
[01:23:05.180 --> 01:23:11.020]   which is an amazing 59 beats per minute. I am. If it were 300, I'd be in trouble.
[01:23:11.020 --> 01:23:16.540]   But here's one that's new and I love environmental sound levels. I can now see it says,
[01:23:16.540 --> 01:23:21.020]   "I have not been exposed over the last seven days to any dangerous sound pressure levels,
[01:23:21.020 --> 01:23:27.820]   and I can even get a graph of all the different." That is fantastic. Now, admittedly, that's not
[01:23:27.820 --> 01:23:33.980]   saving my life. But as you incrementally add more and more features like that, I think we're
[01:23:33.980 --> 01:23:37.580]   getting close to something like that. I just feel like this is fantastic.
[01:23:37.580 --> 01:23:44.620]   We just have to remind ourselves that access to more data is not necessarily therapeutically useful.
[01:23:44.620 --> 01:23:54.940]   And that typically the moment in which it's time to start actively treating a problem with your health
[01:23:54.940 --> 01:23:59.980]   is when a symptom presents itself. That shows you that something is actually cool.
[01:23:59.980 --> 01:24:06.380]   Well, that's traditionally when doctors kind of move in. There is such a thing as preventative
[01:24:06.380 --> 01:24:11.980]   medicine. I'm not denying that. But if someone says, "Here are your numbers for this," they're not
[01:24:11.980 --> 01:24:18.780]   saying. They don't wait until it's too late. Right? Yeah. Right. No, but so I'm not sure I
[01:24:18.780 --> 01:24:24.300]   agree, Andy, with you. And here's why. Some of the things that I've read about the Apple Watch
[01:24:24.300 --> 01:24:29.820]   Saving People's Lives has to do with the ECG monitoring where there's been a number of people
[01:24:29.820 --> 01:24:35.420]   who didn't know that they had an issue with our heart. Their Apple Watch just kind of said,
[01:24:35.420 --> 01:24:39.340]   "You know, we noticed that you've had some weird things happen in with your heart. Maybe you
[01:24:39.340 --> 01:24:42.540]   should go talk to your doctor." And they went talk to their doctor and their doctor said,
[01:24:42.540 --> 01:24:46.780]   "It's probably nothing, but we'll check it out anyway." And it turned out it was something. And
[01:24:46.780 --> 01:24:53.260]   these are people who, if they let they went unchecked, may have actually had a really bad
[01:24:53.260 --> 01:24:57.660]   thing happen to them, a heart attack or something. How many times have you heard of young people who
[01:24:57.660 --> 01:25:03.100]   are way too young to have had a heart attack out of the blue die from a heart attack? This is one
[01:25:03.100 --> 01:25:08.620]   of those circumstances under which that may have been caught and treated before it turned into a
[01:25:08.620 --> 01:25:14.300]   life-threatening incident. And that to me, that's not waiting until the symptom comes up.
[01:25:14.300 --> 01:25:20.300]   That's just having something on your body that's keeping track of you on a regular basis. At some
[01:25:20.300 --> 01:25:26.460]   point, if there is something unusual, that sensor says to you, "Hey, we noticed that something is a
[01:25:26.460 --> 01:25:29.260]   little bit unusual. Maybe you should talk to your doctor about it."
[01:25:29.260 --> 01:25:35.980]   You're absolutely right. But a bad EKG is it is. I would consider that an actual symptom.
[01:25:35.980 --> 01:25:42.060]   As opposed to, "Here is a pile of data. Let's sift through it to look for anything that's
[01:25:42.060 --> 01:25:48.780]   aberrant or that is outside of the bell curve for someone of your age." And once you start going
[01:25:48.780 --> 01:25:54.620]   on a phishing expedition, trying to look for a problem that may or may not exist. Again,
[01:25:55.420 --> 01:26:00.140]   EKG monitoring, that is actually real. That is, your heart should not be doing that. That's not
[01:26:00.140 --> 01:26:09.100]   just simply... It's not just like your heart was racing for 10 minutes last week and hasn't
[01:26:09.100 --> 01:26:14.220]   recurred before or since. That's more like you don't know exactly what stresses are inside your
[01:26:14.220 --> 01:26:18.780]   body. You don't have enough sensors to understand how that was going. But you're absolutely right.
[01:26:18.780 --> 01:26:24.620]   But I'm just saying that I believe that an ESAG monitor, that is describing an actual symptom
[01:26:24.620 --> 01:26:28.860]   that is presenting itself. I also think that the issue is, I don't think anyone's going to do
[01:26:28.860 --> 01:26:31.900]   anything with the data for quite some time. I think that what's going to happen is we're going to
[01:26:31.900 --> 01:26:36.060]   start seeing data and then they're going to start tying it in. When you connect it to the health
[01:26:36.060 --> 01:26:40.300]   records and when people are willing to share those into the system, you're going to start seeing
[01:26:40.300 --> 01:26:45.980]   things that people are dying and this marker happens six months before all the time.
[01:26:45.980 --> 01:26:52.460]   It gets to a point where the correlation rate is high enough that you do decide that that
[01:26:52.460 --> 01:26:57.660]   person should probably get a test when they get that because it's more than 50% of the time
[01:26:57.660 --> 01:27:01.660]   they got six months. I think we're going to start seeing that when we start seeing the data.
[01:27:01.660 --> 01:27:07.100]   I think that a lot of that stuff also, when we get back to prescriptions, it's going to be like
[01:27:07.100 --> 01:27:12.300]   we're going to see the data that's going to show us that that you're not reacting well to that
[01:27:12.300 --> 01:27:17.180]   prescription drug. You don't have to tell the doctor that. The doctor can get an alert or
[01:27:17.180 --> 01:27:22.380]   if you decide to do that or whatever. That's there. I think that's where we're going
[01:27:22.380 --> 01:27:26.780]   and I think it's going to take a long time. We've been working on it for a long time too.
[01:27:26.780 --> 01:27:37.660]   I want to say happy birthday hover.com. It's 11 years old today. Hover is our preferred domain
[01:27:37.660 --> 01:27:44.780]   registrar. We love Hover. I know a lot of you have watched us talk about Hover in the past and
[01:27:44.780 --> 01:27:49.900]   many people, including Steve Gibson, who I think came to it independently. He was shocked when I
[01:27:49.900 --> 01:27:54.860]   said, "Oh, yeah, there are sponsors." He said, "Oh." Because I was looking, he was with one of the
[01:27:54.860 --> 01:28:01.100]   older best, better known registrars and was just really sick and tired of their poor service. They're
[01:28:01.100 --> 01:28:08.700]   upselling. He asked on Twitter, "Who's the best domain registrar?" It was universal. Everybody agreed.
[01:28:08.700 --> 01:28:16.780]   Hover.com. We love Hover. I have many, many domain names registered. That's why I'm logging in right
[01:28:16.780 --> 01:28:21.580]   now. I've got two factor on it. That's going to take me just a second because I want to show you.
[01:28:21.580 --> 01:28:28.940]   I've got a lot of domain names. Right now is a good time to get a domain from Hover.com. Hover is
[01:28:28.940 --> 01:28:37.980]   turning 11, which means if you click the link, you'll see some big sales.com, $11 a year. Design,
[01:28:37.980 --> 01:28:45.900]   dot design, $1, dot art, $1. We were talking on iOS today just a couple hours ago about getting a
[01:28:45.900 --> 01:28:52.140]   wiki. I convinced Micah he should get a wiki. He registered Sergeant Da wiki for $1.
[01:28:52.140 --> 01:29:00.140]   $1. I have my own wiki address. I have my own email address. Having a domain is great. For instance,
[01:29:00.140 --> 01:29:07.260]   email is a really good example. You shouldn't be really using a Gmail or a Hotmail or a Yahoo
[01:29:07.260 --> 01:29:12.620]   or an Outlook address. You can use those services, but you should have your own custom domain,
[01:29:12.620 --> 01:29:18.940]   Lindsay's email or whatever. I think I have 30 or 40 URLs on Hover.
[01:29:18.940 --> 01:29:25.260]   Oh, I know I do. I haven't counted, but I know it's a ton of them, including an email domain.
[01:29:25.260 --> 01:29:31.580]   That way you can move around whenever you've got, it's a bit of a list. Whatever you...
[01:29:31.580 --> 01:29:37.820]   You never know what you're going to need. You never know. Well, like when I wanted to set up
[01:29:37.820 --> 01:29:42.860]   our Twit community, I happened to have Twit.community that I had registered. Perfect for our forms.
[01:29:42.860 --> 01:29:51.420]   When I set up our mastodon, I had Twit.social set up. We just want to maybe a dozen Twit.domains.
[01:29:51.420 --> 01:29:56.380]   I have Hover has of course.com.net, all the big ones, but they have lots of very nice
[01:29:56.380 --> 01:30:03.340]   custom domains at a very affordable price. With every domain you get, who is privacy protection
[01:30:03.900 --> 01:30:10.780]   free, let's find a domain. Let's say a domain for... What should we say? Say, Carsten.
[01:30:10.780 --> 01:30:16.380]   Let's look at some Carsten domains. I'm just going to enter in Carsten, and it's going to suggest
[01:30:16.380 --> 01:30:26.380]   a bunch of TLDs searching 446 different registries. Carsten.art, Carsten.ink, Carsten.code.club.app.
[01:30:26.380 --> 01:30:30.540]   Carsten, now you're going to have to go register those really quickly, because everyone sees them.
[01:30:30.540 --> 01:30:37.660]   Carsten.cloud. I use Laport.cloud for my Synology. Carsten.computer.host.
[01:30:37.660 --> 01:30:41.580]   That's good. If you ever want to host a show, Carsten owning Carsten.host wouldn't hurt.
[01:30:41.580 --> 01:30:51.020]   Carsten software, Carsten solutions, Carsten boutique, Carsten LLC, Carsten.actor. You want
[01:30:51.020 --> 01:30:56.620]   to be an actor? .band.camera. I use Leo.camera. That links to my smug mug. That's where all my
[01:30:56.620 --> 01:31:03.020]   best pictures are. If somebody hadn't taken Leo.photo or Leo.pictures, I could use that, but I like the
[01:31:03.020 --> 01:31:11.100]   Leo.camera. I am a camera. It goes on. Carsten.dental. Carsten, I know you secretly want to be a
[01:31:11.100 --> 01:31:19.660]   dentist. It's not too late to get completely available. These are all at Hover.com. Very easy to navigate,
[01:31:19.660 --> 01:31:24.140]   not all that upselling. Let me go back to my domains, because I want to show you how easy
[01:31:24.860 --> 01:31:32.540]   DNS is. When I set up my wiki, for instance, I went to Laport.Wiki and I just changed the
[01:31:32.540 --> 01:31:38.860]   beautiful, I've got Laport report. That would have been a good one. It's easy to modify the DNS,
[01:31:38.860 --> 01:31:44.300]   either send the name servers to somebody else or use the DNS settings, built right into Hover.com.
[01:31:44.300 --> 01:31:49.740]   Look how clean and simple that is. That's what I love about Hover. Lots of sales.
[01:31:50.940 --> 01:31:55.020]   If you didn't get there in time for the 11th anniversary, there are always sales on top level
[01:31:55.020 --> 01:31:59.820]   domains. You should always check those out. It is a great way to get an email address that's
[01:31:59.820 --> 01:32:04.620]   custom that's unique to you. It's a great way to set up your next website, your business. By the
[01:32:04.620 --> 01:32:08.540]   way, I wouldn't even name a business without going to Hover.com first and getting the domain
[01:32:08.540 --> 01:32:12.940]   names associated with it, right? These days you got to have that. We open this probably every
[01:32:12.940 --> 01:32:16.380]   week or two where you're thinking about something, "Oh, I want to do this thing and I need a URL."
[01:32:17.180 --> 01:32:20.220]   It's part of the decision process. Can I get that URL?
[01:32:20.220 --> 01:32:26.060]   I immediately got my kids names. In fact, if you know somebody's having a baby or you're having a
[01:32:26.060 --> 01:32:32.460]   baby, get the kids names registered now. I have abiloport.com and henryoport.com. I registered those
[01:32:32.460 --> 01:32:39.580]   20 when they were born for 20 years. I've recently renewed Abi through 2030. Henry,
[01:32:39.580 --> 01:32:46.620]   whoa, he's going to come up next year. I'll get another 10 years on him. It's just a fantastic
[01:32:46.620 --> 01:32:54.380]   place to create a domain name. Easy to use, nice people, great tech support. It's just the place to
[01:32:54.380 --> 01:32:57.740]   go. If you're starting a business, if you want to do email, if you want to do a family wiki,
[01:32:57.740 --> 01:33:03.180]   I just set up my family wiki. You can't get in, so don't try it. Laport.wicki. In fact,
[01:33:03.180 --> 01:33:11.580]   you see it's warning. Security risk ahead. That's all right. I accept the risk. Accept the risk
[01:33:11.580 --> 01:33:20.700]   and continue. My wiki, by the way, I named it the 4 AM wiki because I set it up at 4 AM Saturday night.
[01:33:20.700 --> 01:33:28.700]   Very scientific. Very scientific. It's a family wiki. That's where I put stuff that we want to
[01:33:28.700 --> 01:33:34.220]   share. It's easy for everybody to do. You'll find a thousand ways to use it. 10% off your domain
[01:33:34.220 --> 01:33:41.580]   extension for a full year at hover.com/twit. Get a domain name that represents you, your passion,
[01:33:41.580 --> 01:33:53.740]   your interests. Get hover. Hover.com/twit. It is a great place to register all your things,
[01:33:53.740 --> 01:34:00.540]   all the things. Hover.com/twit. Thank you, Hover, for your support. Thank you for supporting us
[01:34:01.100 --> 01:34:09.260]   by using that special domain name, hover.com/twit. Now, time for the picks of the week. Let me start
[01:34:09.260 --> 01:34:16.460]   with a guy on the far right, Mr. Renee Ritchie. Okay, so this just was announced yesterday and I
[01:34:16.460 --> 01:34:22.060]   watched every video I could find on it. It's from Edelkrone, which is sort of a higher-end video
[01:34:22.060 --> 01:34:26.620]   gear company, but they make a lot of things that were previously incredibly high-end,
[01:34:26.620 --> 01:34:31.660]   sort of quasi-affordable for the prosumer market. For a while now, they've had a set of
[01:34:31.660 --> 01:34:38.700]   robotic camera accessories. They've had a head that we need this. It's a mini-gib. A little baby-gib.
[01:34:38.700 --> 01:34:44.380]   But it's also fully automated. They have a modular system. You can get sliders,
[01:34:44.380 --> 01:34:49.340]   you can get heads, you can get basic heads or 3D heads at pan and tilt. You can connect them
[01:34:49.340 --> 01:34:56.300]   all together. The cool thing about this, though, is that you mount the
[01:34:56.620 --> 01:35:00.220]   jib, then you mount one of their heads on it. If you want to, you don't have to, but if you want
[01:35:00.220 --> 01:35:05.420]   to, then you can place the camera where you want it, press a button, place it up to six other ways,
[01:35:05.420 --> 01:35:12.780]   press those buttons, and then it will move with your desired level of ease between any and all
[01:35:12.780 --> 01:35:18.540]   of those camera positions and even just loop back and forth if you so choose. You can remote-control
[01:35:18.540 --> 01:35:24.460]   it if you want, but ideally, you set where you want it to start. You set where you want it to go through
[01:35:24.460 --> 01:35:29.980]   and you set where you want it to end. And then it just does it for you both sideways, vertically
[01:35:29.980 --> 01:35:35.020]   horizontally and diagonally. So you can get some amazing shots even if you're by yourself,
[01:35:35.020 --> 01:35:41.260]   which is what really sold me on it. Oh, for any YouTube or, you know, when you're doing vector,
[01:35:41.260 --> 01:35:47.580]   for us, we're, I just just joined a by a couple because we're looking, this is exactly what we
[01:35:47.580 --> 01:35:54.780]   were looking for mere hours ago is a way to do easy jib shots, smooth, easy jib shots. Look at that
[01:35:54.780 --> 01:36:00.620]   influencer. She is golden. I think that's just for the head module. You got to get ahead too,
[01:36:00.620 --> 01:36:05.500]   which is probably the expensive part. Yeah. Yeah. It's about a three grand prat. If you get
[01:36:05.500 --> 01:36:09.180]   both, it's about 1500 for the jib, about 1500. If you get that, they're having a sale to the
[01:36:09.180 --> 01:36:13.420]   end of the month, but still you need the head because it's one thing to have the jib move. You
[01:36:13.420 --> 01:36:17.900]   got to also have the look at that. This is where they also have the little robotic legs.
[01:36:17.900 --> 01:36:23.100]   If you want to try to make it walk, no, not that well, you can modular it all together. So you
[01:36:23.100 --> 01:36:28.300]   could put this on top of one of their, their motorized leg units and it'll drive around and get the
[01:36:28.300 --> 01:36:34.220]   shots that you want to. All thing is amazing. That is beautiful. And there's sliders are good.
[01:36:34.220 --> 01:36:38.540]   They've heard nothing but good things. Pulling focus to it looks like it's pulling focus. It has a
[01:36:38.540 --> 01:36:42.540]   focus, an optional focus module that you can buy for your camera and it'll stick on the lens and
[01:36:42.540 --> 01:36:47.740]   then do the focus for you as well. Oh, so you see that module on the right of the on the left of the
[01:36:47.740 --> 01:36:51.740]   camera? Yeah, they're using it with the SLR. So clearly it has a pretty hefty,
[01:36:51.740 --> 01:36:57.260]   I mean, I have heard some people put C 200s on it, but you can't do the full extension. That's
[01:36:57.260 --> 01:37:01.980]   over the legal limit. So you have to extend it less to make up for the weight. But I see a lot
[01:37:01.980 --> 01:37:06.780]   of them have big counterbalance weights on them. A B and B, a black magic 6k, it'd probably be
[01:37:06.780 --> 01:37:11.500]   in the in the weight range. And that new SGF, forget what it's called, but they have a new
[01:37:11.500 --> 01:37:16.540]   camera that's 6k, but just just literally just a body. It's so small. So I mean, it's getting really,
[01:37:16.540 --> 01:37:22.300]   really good. I love it that you're all over video now, Renee, like you're becoming like a mini
[01:37:22.300 --> 01:37:30.620]   Alex. I like a mini Alex, you know, a mini budget Alex. I give you a full authority to buy that and
[01:37:30.620 --> 01:37:35.020]   ahead. And you know what, John, get that with the Canon E, what is it, the E 200? We're going to get
[01:37:35.020 --> 01:37:41.900]   a little, a little mirrorless Canon. I got the EOS R, which is really good. M 200. This is a little,
[01:37:41.900 --> 01:37:49.420]   it's APS see. Yeah. And Canon's pushing it for streamers because it'll, it'll do live streaming.
[01:37:49.420 --> 01:37:56.540]   And you can change lenses. It uses does, I don't remember what lens system it uses, but fairly
[01:37:56.540 --> 01:38:01.500]   inexpensive. And I think that sitting on that, see if they have a focus pull module.
[01:38:02.300 --> 01:38:08.700]   Yeah. The Canon's got good color too, which is why I like them. Yeah. Yeah. Wow. Good pick.
[01:38:08.700 --> 01:38:13.980]   Thank you, Renee. You just cost me three grand. That's amazing. It's already pushed out to like,
[01:38:13.980 --> 01:38:17.260]   I think two weeks or three weeks for delivery. So I wanted to make sure we got it out quickly.
[01:38:17.260 --> 01:38:21.580]   I'm still waiting for my mini A-TEM that Alex got me to buy. Me too. Yeah.
[01:38:21.580 --> 01:38:27.820]   We could still back work to make it as fast as they can. Yeah. Lori Gill, what else should I buy?
[01:38:28.620 --> 01:38:34.700]   Well, you did just get yourself a pair of iPods Pro. So maybe you should get yourself a beautiful
[01:38:34.700 --> 01:38:41.660]   leather case, the Air Snap Pro from 12 South. Oh, oh, sold. Look how pretty that is. Yeah. It's
[01:38:41.660 --> 01:38:46.940]   really good looking in that they've said they've done a minor update to to this particular model.
[01:38:46.940 --> 01:38:53.500]   This is why you don't lose your hair. I keep them hooked to my fanny pack all the time. It's true.
[01:38:54.540 --> 01:39:00.220]   So they've added an S-clip instead of the standard clip that they use. So this is a removable clip,
[01:39:00.220 --> 01:39:04.940]   which is really nice for people who don't necessarily want to use it. And it comes with a wrist strap.
[01:39:04.940 --> 01:39:10.140]   So I don't see how that is ever going to be useful, but there's someone out there who's going to
[01:39:10.140 --> 01:39:15.340]   think that a wrist strap is useful and it does come with it. And you can charge it with the case on.
[01:39:15.340 --> 01:39:21.900]   It's got a little. Yep. Yeah. It has the charging. Did you buy baby blue, black, or contact color.
[01:39:21.900 --> 01:39:25.020]   It looks so good. Really? Yeah.
[01:39:25.020 --> 01:39:28.620]   See, Lisa always says, oh, you'll always buy the brown Leo.
[01:39:28.620 --> 01:39:36.380]   Brown. She hates brown. Maybe I'll get the black and throw her an S curve. Really nice. AirPods.
[01:39:36.380 --> 01:39:43.660]   It's the Air Snap Pro from 12 South, which does great stuff. All right, Annie, it's your turn.
[01:39:43.660 --> 01:39:51.740]   I'm just going to make some changes here. If I'm sick of tired of all the really great video,
[01:39:51.900 --> 01:39:54.700]   that quality, we have a hardware that Renee has put against.
[01:39:54.700 --> 01:40:02.780]   The cring cam 2000 has reemerged. I'm going to intentionally tank my video settings because
[01:40:02.780 --> 01:40:09.500]   now trying to compete with it with him is just useful. So now then. Okay, that's better.
[01:40:09.500 --> 01:40:11.260]   Much. Much.
[01:40:11.260 --> 01:40:19.660]   Actually, this is a camera based recommendation. I often say that I'll be reviewing something
[01:40:19.660 --> 01:40:27.100]   or taking a look at something and saying, wow, for like for $600, this is really not very good,
[01:40:27.100 --> 01:40:33.660]   but for $110. This is really incredible. And so I'm going to pick up the week as the DX one
[01:40:33.660 --> 01:40:42.540]   iPhone camera. This came out about four years ago for $600 as a it has a pop out lightning port.
[01:40:42.540 --> 01:40:47.900]   And so basically you click it onto the end of your iPhone. You run you run an app that controls it.
[01:40:47.900 --> 01:40:54.140]   It has a 20 megapixel Sony sensor. It's built like a nice like an old fashioned cigarette lighter,
[01:40:54.140 --> 01:41:00.540]   has a sliding cover, all these nice things. And the problem was that for $600, it really didn't
[01:41:00.540 --> 01:41:07.020]   work very well. It wasn't very the app kept losing contact with the with the camera.
[01:41:07.020 --> 01:41:13.980]   And when you talk about this is a conventional camera, meaning that the impetus is usually on you,
[01:41:13.980 --> 01:41:20.220]   the camera's job as a conventional camera is to record as much data as possible so that
[01:41:20.220 --> 01:41:25.180]   you can then dump it into a photo editor and create that those really nice details, that nice
[01:41:25.180 --> 01:41:31.180]   color and lighting that you want. And it really the weird thing is the $600 camera could not
[01:41:31.180 --> 01:41:35.900]   compete with the photos that an iPhone was generating because it was doing all that Photoshop, so to
[01:41:35.900 --> 01:41:41.980]   speak, on its own. Now, so DxO that was kind of a failure. And now they're being clear and out.
[01:41:41.980 --> 01:41:49.420]   Amazon has them for $127 plus a $15 coupon if you click a button down to 110.
[01:41:49.420 --> 01:41:57.660]   And I haven't so and for $110, this is a really interesting camera. It's like this pebble size
[01:41:57.660 --> 01:42:02.620]   20 megapixel camera you can have in your pocket at all times to take pictures with
[01:42:02.620 --> 01:42:08.380]   when for whatever reason your phone isn't going to do or whether you're in sort of a more of a
[01:42:08.380 --> 01:42:16.460]   20 megapixel raw photo taking sort of mood. And the best way to use it, I found was it'll actually
[01:42:16.460 --> 01:42:21.900]   work independently if you if you if you don't have it connected up to to an iPhone, it has its own
[01:42:21.900 --> 01:42:28.300]   shutter button. And also after like it's initial release, they added a little update to its little
[01:42:28.300 --> 01:42:38.060]   LCD LCD screen on the back to make it into a kind of viewfinder. It's one it's two pixels deep,
[01:42:38.060 --> 01:42:42.060]   so you're not going to get like all the tonality. But if you want to line up a shot, it'll work
[01:42:42.060 --> 01:42:46.540]   just fine. And the big deal is that if you if you forget, this is an iPhone camera, you're talking
[01:42:46.540 --> 01:42:53.100]   about a super super super compact as in never leave the house without it camera with its own
[01:42:53.100 --> 01:42:59.500]   rechargeable battery, a 20 megapixel really good Sony sensor, which records directly onto SD cards,
[01:43:00.140 --> 01:43:08.540]   recharges via micro SD, and its native raw file format is just Adobe DNG. So you don't need the
[01:43:08.540 --> 01:43:14.460]   support from DXO, you're not going to get any. You can just use this as this little like snapshot
[01:43:14.460 --> 01:43:19.900]   camera that will take these beautiful raws that you can then dump into Photoshop and do amazing
[01:43:19.900 --> 01:43:29.100]   things with so like for $600 no for $300 no for $200 you have my attention for $110 you have me
[01:43:29.100 --> 01:43:36.060]   like trying to remember if I just if they made me an offer to buy it and keep it or if I actually
[01:43:36.060 --> 01:43:41.500]   send it back because if I don't have one in the office for $110 I might just buy one it's
[01:43:41.500 --> 01:43:45.260]   worth taking a look at it's a very very interesting object. I've got a couple
[01:43:45.260 --> 01:43:52.940]   lying around. Yeah, you gave me one. Yeah. And I we actually did the initial video for them.
[01:43:52.940 --> 01:43:58.380]   Yeah, for the for the product we made the video for them. I still have mine, but I don't I feel
[01:43:58.380 --> 01:44:02.220]   like the iPhone camera is so good. Well, so the the only thing I want to
[01:44:02.220 --> 01:44:06.380]   sense it right? It's a one inch since it looks way better. It looks way better than the iPhone.
[01:44:06.380 --> 01:44:10.620]   Like it's it's not like a little better like I when I take pictures of them I I was going through
[01:44:10.620 --> 01:44:15.180]   something the other day through my iPhoto. Suddenly the photos got really good and then they went back
[01:44:15.180 --> 01:44:20.780]   to being good but not and then I realized oh that's because that's my that was a show that I shot
[01:44:20.780 --> 01:44:27.580]   some on DX on my DXO on my iPhone. So you shoot video with it? Well this was stills but you can't
[01:44:27.580 --> 01:44:29.660]   - And shoot video and shoot the 1080p video.
[01:44:29.660 --> 01:44:30.500]   - Nice.
[01:44:30.500 --> 01:44:31.380]   - And again, it looks good.
[01:44:31.380 --> 01:44:33.440]   And what's really nice about it is the swivel.
[01:44:33.440 --> 01:44:36.180]   So what you can do is you can swivel it
[01:44:36.180 --> 01:44:37.480]   if you want to shoot like a high shot.
[01:44:37.480 --> 01:44:41.160]   I have to admit, I wasn't thinking about this,
[01:44:41.160 --> 01:44:43.300]   but when Andy started talking about it,
[01:44:43.300 --> 01:44:45.360]   I realized, oh I should pull it out for cooking videos,
[01:44:45.360 --> 01:44:47.740]   I'm working on these little cooking videos right now,
[01:44:47.740 --> 01:44:51.040]   and it would be really easy to turn it down
[01:44:51.040 --> 01:44:53.160]   and get a great, great image with something
[01:44:53.160 --> 01:44:54.580]   that's relatively compact.
[01:44:54.580 --> 01:44:57.060]   - This is such an expensive show for me.
[01:44:57.060 --> 01:44:57.900]   - Nice.
[01:44:57.900 --> 01:44:59.660]   - It's really good.
[01:44:59.660 --> 01:45:01.180]   - I've already bought everything.
[01:45:01.180 --> 01:45:02.460]   - Yeah, so you already have this one.
[01:45:02.460 --> 01:45:03.300]   - Oh, but you have this,
[01:45:03.300 --> 01:45:04.820]   I don't have the newest one though, do I?
[01:45:04.820 --> 01:45:06.460]   - No, I think it didn't change the same.
[01:45:06.460 --> 01:45:07.300]   - It didn't change the same.
[01:45:07.300 --> 01:45:10.540]   - Yeah, yeah, so at $110, it's a steal.
[01:45:10.540 --> 01:45:11.380]   - Oh, it's a must have.
[01:45:11.380 --> 01:45:12.460]   - Yeah, it's a must have.
[01:45:12.460 --> 01:45:15.660]   - And I reiterate that you can,
[01:45:15.660 --> 01:45:17.740]   I'm recommending it not as an iPhone camera,
[01:45:17.740 --> 01:45:20.100]   although you can certainly do that.
[01:45:20.100 --> 01:45:22.140]   The apps, you don't know how long those apps
[01:45:22.140 --> 01:45:23.980]   are gonna be compatible or upgraded.
[01:45:23.980 --> 01:45:25.100]   You can control the whole thing
[01:45:25.100 --> 01:45:26.580]   through that little touchscreen in the back.
[01:45:26.580 --> 01:45:28.460]   - Oh, so you just use a little baby camera.
[01:45:28.460 --> 01:45:31.340]   - You totally feel like a spy when you take a picture with it.
[01:45:31.340 --> 01:45:32.980]   - I found, no, I did find myself,
[01:45:32.980 --> 01:45:34.260]   I don't know where it is right now,
[01:45:34.260 --> 01:45:36.820]   I took one less look for it before,
[01:45:36.820 --> 01:45:37.740]   during the commercial.
[01:45:37.740 --> 01:45:39.540]   But yeah, I found myself carrying it in my pocket
[01:45:39.540 --> 01:45:41.780]   just as part of my EDC because again,
[01:45:41.780 --> 01:45:44.380]   it has a sliding lens cover that's kind of sad.
[01:45:44.380 --> 01:45:47.060]   It's almost like an AirPod case.
[01:45:47.060 --> 01:45:48.380]   - Oh, so just say that it's a camera.
[01:45:48.380 --> 01:45:49.220]   - So satisfying the flip one.
[01:45:49.220 --> 01:45:50.060]   - You can't, yeah, exactly.
[01:45:50.060 --> 01:45:52.020]   - It totally, that's how I was using it for.
[01:45:52.020 --> 01:45:53.500]   - It totally feels like, there'd be times
[01:45:53.500 --> 01:45:55.740]   where I'd be taking a snapshot with the iPhone
[01:45:55.740 --> 01:45:57.300]   and it's nice to realize that,
[01:45:57.300 --> 01:45:59.620]   ooh, you know, the colors and those flowers,
[01:45:59.620 --> 01:46:01.380]   like every once in a while,
[01:46:01.380 --> 01:46:03.700]   for some reason, the Boston Park's Department
[01:46:03.700 --> 01:46:05.220]   gets it in their ear too.
[01:46:05.220 --> 01:46:07.700]   Oh, we have to plant like amazing amounts of tulips
[01:46:07.700 --> 01:46:09.180]   and they'll all be stolen in two weeks,
[01:46:09.180 --> 01:46:11.020]   but for two weeks, we'll have tulips.
[01:46:11.020 --> 01:46:13.140]   And if I'm there, it's like, oh, you know what?
[01:46:13.140 --> 01:46:15.660]   The colors are so good and the sky is so blue,
[01:46:15.660 --> 01:46:20.660]   I actually want a real 20 megapixel raw file of this
[01:46:20.660 --> 01:46:23.060]   and so that I can tweak it as much as I want in Photoshop.
[01:46:23.060 --> 01:46:25.820]   And that was when I would not, it was in a situation
[01:46:25.820 --> 01:46:27.140]   where I would have brought a camera with me,
[01:46:27.140 --> 01:46:29.540]   but since I had this little like pebble sized camera,
[01:46:29.540 --> 01:46:30.780]   I don't have to worry about the app then
[01:46:30.780 --> 01:46:31.700]   to do all those settings.
[01:46:31.700 --> 01:46:36.100]   - No, no, it worked, again, it even saves in DNG format.
[01:46:36.100 --> 01:46:37.300]   So you can draw everything.
[01:46:37.300 --> 01:46:38.500]   - If you want to do a rumor and put it on your phone,
[01:46:38.500 --> 01:46:40.980]   you can, but all the hip cats are using it by itself.
[01:46:40.980 --> 01:46:43.620]   - And a little public service announcement, please people,
[01:46:43.620 --> 01:46:46.020]   stop stealing Boston's tulips.
[01:46:46.020 --> 01:46:47.860]   Okay, that's just not okay.
[01:46:47.860 --> 01:46:50.140]   - Or before Andy gets there
[01:46:50.140 --> 01:46:52.300]   and manages to get his backpack filled.
[01:46:52.300 --> 01:46:54.740]   - What kind of low life would steal tulips
[01:46:54.740 --> 01:46:56.340]   from Boston public parks?
[01:46:56.340 --> 01:46:58.260]   - Oh man, I don't know.
[01:46:58.260 --> 01:47:02.060]   - We're sure it's not Dutch Reclamation.
[01:47:02.060 --> 01:47:02.980]   It's a Dutch Reclamation.
[01:47:02.980 --> 01:47:05.300]   Or a little gopher, maybe one of those squirrels
[01:47:05.300 --> 01:47:07.060]   with Andy gets there. - Oh, it's one of Andy's squirrels.
[01:47:07.060 --> 01:47:07.900]   - Yeah.
[01:47:07.900 --> 01:47:09.740]   - Alex, what's it called?
[01:47:09.740 --> 01:47:10.900]   - Yeah, what's it called?
[01:47:10.900 --> 01:47:13.100]   Just breaking, to go with our subject today,
[01:47:13.100 --> 01:47:16.340]   the Guardian just reported that they think Jeff Bezos'
[01:47:16.340 --> 01:47:19.340]   phone was hacked via WhatsApp before all of the recent
[01:47:19.340 --> 01:47:21.060]   Saudi Arabian stuff went on.
[01:47:21.060 --> 01:47:23.620]   - Yes, see, there was some thought that it was his
[01:47:23.620 --> 01:47:28.060]   girlfriend's estranged brother that did that,
[01:47:28.060 --> 01:47:30.260]   but now they think it was a hack, huh?
[01:47:30.260 --> 01:47:31.780]   - Yeah, via WhatsApp from the Saudi--
[01:47:31.780 --> 01:47:32.620]   - Saudis, yeah.
[01:47:32.620 --> 01:47:33.460]   - Asian state, yeah.
[01:47:33.460 --> 01:47:34.700]   - There you go.
[01:47:34.700 --> 01:47:36.900]   - How do they, what has he got against the Saudis?
[01:47:36.900 --> 01:47:38.500]   What is he doing with the--
[01:47:38.500 --> 01:47:41.100]   - No, the Saudis are doing the president's bidding.
[01:47:41.100 --> 01:47:42.660]   So it's all up, the Washington close.
[01:47:42.660 --> 01:47:47.140]   It's all transactional, that's how this all works, right?
[01:47:47.140 --> 01:47:49.780]   You scratch my back, I scratch yours, right?
[01:47:49.780 --> 01:47:52.340]   So, and remember they were using the,
[01:47:52.340 --> 01:47:54.340]   they used the enquirer, another one of the
[01:47:54.340 --> 01:47:58.140]   president's crony to publish that.
[01:47:58.140 --> 01:47:58.980]   - How'd it have yet?
[01:47:58.980 --> 01:48:03.380]   - Yeah, so the whole, it's a wonderful circle of friends.
[01:48:03.380 --> 01:48:04.220]   - Happiness.
[01:48:04.220 --> 01:48:06.260]   - Of scratching each other's backs.
[01:48:06.260 --> 01:48:08.260]   - Yep, the circle of bling.
[01:48:08.260 --> 01:48:11.420]   - Loud lab, what is this?
[01:48:11.420 --> 01:48:13.340]   - Okay, so Alex, this is Rick.
[01:48:13.340 --> 01:48:16.460]   - For one of the things that when we're doing live streams
[01:48:16.460 --> 01:48:19.260]   for clients, we pay a lot of attention to the audio.
[01:48:19.260 --> 01:48:23.940]   Audio turns out in most shows to be more than 50% of the show.
[01:48:23.940 --> 01:48:26.620]   It is really 80 to 90% of the show.
[01:48:26.620 --> 01:48:28.980]   And the quality of your audio is really, really important.
[01:48:28.980 --> 01:48:33.620]   People can look through a lot of rough video,
[01:48:33.620 --> 01:48:35.740]   but if they get a lot of crackles and all kinds of stuff
[01:48:35.740 --> 01:48:38.660]   falling apart on their audio, they'll immediately tune away.
[01:48:38.660 --> 01:48:40.900]   So, we wanna know if it's too loud,
[01:48:40.900 --> 01:48:42.420]   we wanna know if it's out of phase,
[01:48:42.420 --> 01:48:44.020]   we wanna know if it's, or you know,
[01:48:44.020 --> 01:48:46.860]   all of these things are things that are important to us.
[01:48:46.860 --> 01:48:50.180]   And so, anyway, so this is a,
[01:48:50.180 --> 01:48:51.340]   what we used to use a Spectre,
[01:48:51.340 --> 01:48:54.260]   and Spectre went out of, they disappeared.
[01:48:54.260 --> 01:48:56.220]   So anyway, so we can't get Spectre anymore,
[01:48:56.220 --> 01:48:58.020]   and it was a little, getting a little unstable.
[01:48:58.020 --> 01:48:59.620]   This is called Sonic Atom,
[01:48:59.620 --> 01:49:01.900]   and Sonic Atom is made by Loud Lab,
[01:49:01.900 --> 01:49:03.780]   and they have a couple different apps.
[01:49:03.780 --> 01:49:06.300]   This is the one that I've been playing with the most,
[01:49:06.300 --> 01:49:07.500]   and I'm really happy with it.
[01:49:07.500 --> 01:49:12.300]   And what it does is it gives you a whole series of graphs
[01:49:12.300 --> 01:49:14.980]   and ways to analyze your audio.
[01:49:14.980 --> 01:49:16.460]   Audio, because you can't see it,
[01:49:16.460 --> 01:49:18.780]   you really need a lot of different ways to measure it.
[01:49:18.780 --> 01:49:21.740]   So, a lot of the things that we use are a spectrograph
[01:49:21.740 --> 01:49:22.580]   and spectogram.
[01:49:22.580 --> 01:49:26.900]   These are showing you across the spectrum frequencies,
[01:49:26.900 --> 01:49:28.140]   across the entire spectrum,
[01:49:28.140 --> 01:49:29.820]   this is, you know, what's happening.
[01:49:29.820 --> 01:49:31.540]   The spectogram shows it in color,
[01:49:31.540 --> 01:49:32.580]   so you can kind of see it there,
[01:49:32.580 --> 01:49:35.380]   and the spectrograph is kind of a cross-section
[01:49:35.380 --> 01:49:36.980]   of that spectogram in a lot of ways.
[01:49:36.980 --> 01:49:40.500]   And so, what we see a lot of times in those,
[01:49:40.500 --> 01:49:42.700]   if I have a ground loop in my audio,
[01:49:42.700 --> 01:49:43.660]   and I have a little buzz,
[01:49:43.660 --> 01:49:46.420]   that you hear a lot of times when you see politics,
[01:49:46.420 --> 01:49:48.260]   is you get this little,
[01:49:48.260 --> 01:49:49.940]   and we can look up and see a spectrogram,
[01:49:49.940 --> 01:49:51.460]   and I can see a line going across it,
[01:49:51.460 --> 01:49:53.140]   and I go, oh, I need to go over and look at that.
[01:49:53.140 --> 01:49:56.380]   So, a lot of times I'm dealing with multiple streams.
[01:49:56.380 --> 01:49:58.380]   Some of the stuff we've done is up to 20 different rooms,
[01:49:58.380 --> 01:49:59.540]   being streamed at the same time.
[01:49:59.540 --> 01:50:00.580]   I can't listen to all of them,
[01:50:00.580 --> 01:50:02.820]   I need to just look at the graph and just see,
[01:50:02.820 --> 01:50:04.060]   oh, that's what's happening over there.
[01:50:04.060 --> 01:50:04.900]   And so--
[01:50:04.900 --> 01:50:06.340]   - Definitely don't wanna cross the streams.
[01:50:06.340 --> 01:50:07.980]   - Yeah, definitely don't wanna cross the streams.
[01:50:07.980 --> 01:50:10.980]   So, looking at things like, and then the,
[01:50:10.980 --> 01:50:12.020]   and I don't know how to say this,
[01:50:12.020 --> 01:50:13.940]   because we, I just got good at saying,
[01:50:13.940 --> 01:50:17.380]   "Eligia Zoo meter," and they've changed it to,
[01:50:17.380 --> 01:50:20.420]   "Gonio, Gonio meter."
[01:50:20.420 --> 01:50:22.260]   I've never known how to pronounce that world,
[01:50:22.260 --> 01:50:23.260]   where I've been seeing it since I was a kid.
[01:50:23.260 --> 01:50:24.660]   - I think it's Lisa Hous.
[01:50:24.660 --> 01:50:25.560]   - Yeah, is it?
[01:50:25.560 --> 01:50:28.060]   - Yeah, but I don't know what the new term is.
[01:50:28.060 --> 01:50:30.060]   - Anyway, so, but that allows me--
[01:50:30.060 --> 01:50:32.300]   - 'Cause Lisa Hous' curve is a certain kind of curve.
[01:50:32.300 --> 01:50:33.220]   - Yeah, yeah, yeah.
[01:50:33.220 --> 01:50:38.020]   - So anyway, so, and there's obviously multi-meter loudness,
[01:50:38.020 --> 01:50:39.940]   a lot of other, you can even do tuners,
[01:50:39.940 --> 01:50:40.700]   so you can play things,
[01:50:40.700 --> 01:50:42.260]   and it'll just tell you what final--
[01:50:42.260 --> 01:50:44.340]   - Something I don't have to buy.
[01:50:44.340 --> 01:50:45.900]   - A oscilloscope.
[01:50:45.900 --> 01:50:48.180]   Anyway, so, a lot of this stuff is stuff we use all the time.
[01:50:48.180 --> 01:50:50.180]   We literally have walls of these,
[01:50:50.180 --> 01:50:51.340]   and what's nice about using these,
[01:50:51.340 --> 01:50:52.980]   there's hardware that does this,
[01:50:52.980 --> 01:50:55.100]   but number one, is this is a lot less expensive.
[01:50:55.100 --> 01:50:57.980]   Number two is that you can design how it's gonna look.
[01:50:57.980 --> 01:50:59.300]   So, each one of these windows,
[01:50:59.300 --> 01:51:01.340]   you can scale, you can move it around,
[01:51:01.340 --> 01:51:02.860]   you can change the settings in a way
[01:51:02.860 --> 01:51:04.900]   that you can't do with the hardware analyzers.
[01:51:04.900 --> 01:51:09.340]   And so, we really like to use software analyzers,
[01:51:09.340 --> 01:51:10.580]   because when it's coming in,
[01:51:10.580 --> 01:51:13.020]   especially on an STI signal, it's gonna be the same.
[01:51:13.020 --> 01:51:15.260]   - It's gonio. - It's gonna be embedded.
[01:51:15.260 --> 01:51:17.060]   - It's gonna be pronounced gonio-meter.
[01:51:17.060 --> 01:51:18.060]   - That's what I think, yeah.
[01:51:18.060 --> 01:51:20.300]   - That's like, "Hondio-meter."
[01:51:20.300 --> 01:51:23.740]   - Exactly, so anyway, so it's a great,
[01:51:23.740 --> 01:51:26.300]   this is a great, especially if you were using Spectre before
[01:51:26.300 --> 01:51:28.940]   and you're trying to find a solution that solves it,
[01:51:28.940 --> 01:51:30.780]   this solves it, there's a couple things
[01:51:30.780 --> 01:51:32.740]   that Spectre had that this doesn't have,
[01:51:32.740 --> 01:51:35.420]   but this has other things that Spectre didn't,
[01:51:35.420 --> 01:51:39.580]   and I think it's much more stable, it looks nicer,
[01:51:39.580 --> 01:51:44.340]   it is, and it's, we had trouble with stability.
[01:51:44.340 --> 01:51:45.900]   - By the way, that is pronounced,
[01:51:45.900 --> 01:51:48.060]   you were right, Lisa Hu, he's French.
[01:51:48.060 --> 01:51:50.540]   Julantuan Lisa Hu.
[01:51:50.540 --> 01:51:52.780]   But you could also call it a boditch curve,
[01:51:52.780 --> 01:51:54.380]   and then it'd be in English.
[01:51:54.380 --> 01:51:56.220]   - Yeah, exactly, so.
[01:51:56.220 --> 01:51:58.660]   - I've always said Lisa Hu's, but it's a Lisa Hu curve.
[01:51:58.660 --> 01:51:59.660]   - Yeah, it's a hoo.
[01:51:59.660 --> 01:52:02.940]   - Or it was very complicated.
[01:52:02.940 --> 01:52:04.540]   It took us a long time to figure it out,
[01:52:04.540 --> 01:52:05.380]   and then they changed the name one.
[01:52:05.380 --> 01:52:06.580]   - It's a closed curve.
[01:52:06.580 --> 01:52:08.580]   - Anyway, if you're really looking for,
[01:52:08.580 --> 01:52:10.540]   if you're doing something where you really need
[01:52:10.540 --> 01:52:12.260]   to analyze it and you're looking for a Mac solution
[01:52:12.260 --> 01:52:13.820]   that does this really well,
[01:52:13.820 --> 01:52:18.660]   this is a great affordable solution to do that.
[01:52:18.660 --> 01:52:20.660]   There's obviously crazy solutions
[01:52:20.660 --> 01:52:23.100]   that let you measure the room atmosphere.
[01:52:23.100 --> 01:52:24.020]   - Do we need this, John?
[01:52:24.020 --> 01:52:25.180]   Should we buy this too?
[01:52:25.180 --> 01:52:26.020]   No.
[01:52:26.020 --> 01:52:27.900]   (laughing)
[01:52:27.900 --> 01:52:28.900]   That was my guess.
[01:52:28.900 --> 01:52:32.140]   Although, I have to say, I'm looking at the other things
[01:52:32.140 --> 01:52:34.380]   that LoudLab does, and they do have some other
[01:52:34.380 --> 01:52:36.900]   interesting products, including a sound mixer
[01:52:36.900 --> 01:52:38.180]   and a very simple audio editor.
[01:52:38.180 --> 01:52:39.220]   I'm really interested in it.
[01:52:39.220 --> 01:52:40.620]   I haven't gotten to play with these yet.
[01:52:40.620 --> 01:52:45.260]   The audio editor, Fleckdis, and their sound mixer
[01:52:45.260 --> 01:52:46.660]   is something I'm super interested in.
[01:52:46.660 --> 01:52:49.460]   I just haven't looked at it enough to recommend it yet,
[01:52:49.460 --> 01:52:51.020]   but stay tuned.
[01:52:51.020 --> 01:52:52.860]   There may be recommendations in the future
[01:52:52.860 --> 01:52:53.740]   that are related to those.
[01:52:53.740 --> 01:52:56.820]   - This looks like it's such a clean, easy editor.
[01:52:56.820 --> 01:52:58.740]   It would be perfect for doing the kind of editing I do,
[01:52:58.740 --> 01:52:59.900]   which is voice editing.
[01:52:59.900 --> 01:53:00.740]   - Right.
[01:53:00.740 --> 01:53:05.940]   - And boy, that's Mac and Windows on this one.
[01:53:05.940 --> 01:53:08.020]   Let me see how much it is just out of curiosity.
[01:53:08.020 --> 01:53:10.580]   - I use, it feels very, like so far,
[01:53:10.580 --> 01:53:12.060]   what I feel like these are--
[01:53:12.060 --> 01:53:12.900]   - Oh, it's 40 bucks.
[01:53:12.900 --> 01:53:15.340]   - They're really affordable, and they're,
[01:53:15.340 --> 01:53:17.700]   it's a very modern interface,
[01:53:17.700 --> 01:53:19.580]   so you just really feel like it's responsive,
[01:53:19.580 --> 01:53:20.860]   it's doing what it needs to do,
[01:53:20.860 --> 01:53:23.340]   and it doesn't feel like you're dealing with
[01:53:23.340 --> 01:53:24.500]   a lot of old code.
[01:53:24.500 --> 01:53:26.620]   - Who is this LoudLab?
[01:53:26.620 --> 01:53:27.460]   Who are these people?
[01:53:27.460 --> 01:53:28.860]   - No, no, but I'm gonna bet that there's some serious--
[01:53:28.860 --> 01:53:30.420]   - I like the companies too.
[01:53:30.420 --> 01:53:32.220]   - Where did they come from?
[01:53:32.220 --> 01:53:33.420]   - Yeah, so.
[01:53:33.420 --> 01:53:36.420]   - They, all their names of their stuff are kind of,
[01:53:37.540 --> 01:53:41.820]   like Fluctus, but that's an interesting name.
[01:53:41.820 --> 01:53:43.260]   It's not quite Lisa who,
[01:53:43.260 --> 01:53:46.540]   by the way, here is a Lisa who curve on a oscilloscope
[01:53:46.540 --> 01:53:49.380]   that was later adapted to be the logo
[01:53:49.380 --> 01:53:51.660]   of the Australian Broadcasting Corporation,
[01:53:51.660 --> 01:53:54.500]   so our friends down under are familiar
[01:53:54.500 --> 01:53:55.500]   with the Lisa who's--
[01:53:55.500 --> 01:53:57.380]   - Is that what Tony Stark was using?
[01:53:57.380 --> 01:53:58.380]   - Yes, probably.
[01:53:58.380 --> 01:53:59.780]   (laughing)
[01:53:59.780 --> 01:54:01.080]   - Lisa who's suit.
[01:54:01.080 --> 01:54:03.740]   Friends, we've come to the end of this show,
[01:54:03.740 --> 01:54:05.140]   what a fun show.
[01:54:05.140 --> 01:54:06.940]   I thought we had a great conversation,
[01:54:06.940 --> 01:54:10.100]   it always is, but this was particularly enjoyable.
[01:54:10.100 --> 01:54:12.180]   Thanks to our panel, not me, I just,
[01:54:12.180 --> 01:54:16.540]   I'm here to hold the, keep the seat warm.
[01:54:16.540 --> 01:54:18.900]   Mr. Renee Ritchie is at imore.com
[01:54:18.900 --> 01:54:21.420]   and he is going to do a vector, he said he promised
[01:54:21.420 --> 01:54:24.980]   he'll do a vector explaining all of this encryption
[01:54:24.980 --> 01:54:29.260]   complicated, turn your messages in the cloud off
[01:54:29.260 --> 01:54:33.380]   to get security kind of a weird world we live in.
[01:54:33.380 --> 01:54:34.220]   - Yes, sir.
[01:54:34.220 --> 01:54:36.920]   - Thank you, imore.com/vector for the vector shows.
[01:54:37.240 --> 01:54:39.760]   Did you buy, are you using a jib one now?
[01:54:39.760 --> 01:54:42.440]   - I ordered one, but all the videos were up by the time
[01:54:42.440 --> 01:54:43.680]   I ordered it, so it's gonna be several weeks
[01:54:43.680 --> 01:54:44.520]   before it gets here.
[01:54:44.520 --> 01:54:45.440]   - That's so cool.
[01:54:45.440 --> 01:54:48.080]   - Okay, we're gonna have dueling jib shots.
[01:54:48.080 --> 01:54:49.480]   - Oh, it took great.
[01:54:49.480 --> 01:54:50.920]   Especially when I get the Alex mixer
[01:54:50.920 --> 01:54:53.160]   and then I can actually switch them in
[01:54:53.160 --> 01:54:54.000]   as the show starts.
[01:54:54.000 --> 01:54:54.840]   - Oh, yes.
[01:54:54.840 --> 01:54:57.720]   - Duh, duh, duh, duh, duh, duh, duh, duh, duh, duh,
[01:54:57.720 --> 01:55:01.000]   then you need a big 3D logo, a trumpet section.
[01:55:01.000 --> 01:55:02.520]   - You know what you can, what I bet you can do
[01:55:02.520 --> 01:55:05.480]   with the API inside of Blackmagic is you can set it up
[01:55:05.480 --> 01:55:07.920]   so that when you cut to that shot, it doesn't automatically.
[01:55:07.920 --> 01:55:09.680]   - It cues the jib.
[01:55:09.680 --> 01:55:10.800]   - We could do that with a Tri-Cast,
[01:55:10.800 --> 01:55:12.160]   I bet you could do it with the API.
[01:55:12.160 --> 01:55:13.000]   - Oh, I know you can.
[01:55:13.000 --> 01:55:13.840]   - Yeah.
[01:55:13.840 --> 01:55:14.680]   - That's Alex Lindsey.
[01:55:14.680 --> 01:55:17.000]   He gives us lots of expensive ideas.
[01:55:17.000 --> 01:55:20.040]   @AlexLindsay on the Twitter, A-L-E-X-L-I-N-D.
[01:55:20.040 --> 01:55:21.880]   - S-A-Y.
[01:55:21.880 --> 01:55:23.240]   My kids are now on TikTok.
[01:55:23.240 --> 01:55:27.680]   - You've been pushing TikTok like crazy.
[01:55:27.680 --> 01:55:28.520]   - I'm just telling you.
[01:55:28.520 --> 01:55:30.400]   - You wanna share their account with your?
[01:55:30.400 --> 01:55:31.240]   - Sure, that's sure.
[01:55:31.240 --> 01:55:32.560]   It's called the McDubbies.
[01:55:32.560 --> 01:55:33.400]   It's their idea.
[01:55:33.400 --> 01:55:34.240]   - The McDubbies?
[01:55:34.240 --> 01:55:38.120]   - If you go to Twitter, they're pretty funny actually.
[01:55:38.120 --> 01:55:41.880]   - M-C-D-U-B-B-I-E-S?
[01:55:41.880 --> 01:55:43.160]   - No, it's Y-S.
[01:55:43.160 --> 01:55:44.000]   - Why?
[01:55:44.000 --> 01:55:46.280]   - There's a big discussion about I-E-S and Y-S,
[01:55:46.280 --> 01:55:47.880]   but we decided if it was a name.
[01:55:47.880 --> 01:55:52.040]   - M-I-M-C-D-U-B-B.
[01:55:52.040 --> 01:55:55.280]   Well, you know what I should just go to Twitter.com/AlexLindsay?
[01:55:55.280 --> 01:55:56.840]   - It's like the last thing I tweeted, I think,
[01:55:56.840 --> 01:55:58.520]   so it'll be easy to find.
[01:55:58.520 --> 01:55:59.360]   - That's easy.
[01:55:59.360 --> 01:56:02.520]   - He's now flogging his kids TikTok.
[01:56:02.520 --> 01:56:04.160]   - You know, I just want them to be--
[01:56:04.160 --> 01:56:05.320]   - You're a tire on it.
[01:56:05.320 --> 01:56:07.640]   - It's blunt that I just wanted to have some followers.
[01:56:07.640 --> 01:56:08.640]   I want them to be encouraged.
[01:56:08.640 --> 01:56:11.000]   - Let's everybody follow the McDubbies.
[01:56:11.000 --> 01:56:12.520]   - Everybody, there they are.
[01:56:12.520 --> 01:56:13.720]   No, it's right there.
[01:56:13.720 --> 01:56:14.920]   Right there, just click on that.
[01:56:14.920 --> 01:56:17.280]   - The McDub, oh, it's the McDubbies.
[01:56:17.280 --> 01:56:19.720]   You forgot the the.
[01:56:19.720 --> 01:56:21.360]   - And they're gonna have the best special thing they are.
[01:56:21.360 --> 01:56:22.520]   - Oh, I'm gonna have to get this set up.
[01:56:22.520 --> 01:56:24.080]   I gotta hear what she's saying.
[01:56:24.080 --> 01:56:26.480]   - Oh, well, they're doing lip sync.
[01:56:26.480 --> 01:56:28.720]   - Yeah, but oh, yeah, we can't play the music
[01:56:28.720 --> 01:56:29.840]   'cause we'll get it take the D.
[01:56:29.840 --> 01:56:30.680]   - No, no, I'm me.
[01:56:30.680 --> 01:56:31.760]   - You're a stuff that's part of the--
[01:56:31.760 --> 01:56:33.080]   - You're a two.
[01:56:33.080 --> 01:56:33.920]   - Yeah, but we'll try it here.
[01:56:33.920 --> 01:56:34.760]   Wait, go to the.
[01:56:34.760 --> 01:56:38.520]   Go ahead, turn it up.
[01:56:38.520 --> 01:56:39.360]   - You can play this.
[01:56:39.360 --> 01:56:40.200]   - No, no.
[01:56:40.200 --> 01:56:41.040]   - This stuff?
[01:56:41.040 --> 01:56:41.880]   - Nope, they're saying we'll be taking that.
[01:56:41.880 --> 01:56:42.720]   - TikTok stuff?
[01:56:42.720 --> 01:56:44.240]   - You can screw you, YouTube.
[01:56:44.240 --> 01:56:45.600]   Just screw you.
[01:56:45.600 --> 01:56:47.120]   Take it down, YouTube.
[01:56:47.120 --> 01:56:49.240]   - If you play the one, play that second one,
[01:56:49.240 --> 01:56:50.680]   it's definitely not copy written.
[01:56:50.680 --> 01:56:51.720]   - All right, it's not amusing.
[01:56:51.720 --> 01:56:52.560]   It's not amusing.
[01:56:52.560 --> 01:56:53.880]   - It's not even there.
[01:56:53.880 --> 01:56:54.960]   - It's some sort of--
[01:56:54.960 --> 01:56:59.520]   - Could it be possibly my like 15 missing cookies
[01:56:59.520 --> 01:57:01.680]   that just finished out of nowhere?
[01:57:01.680 --> 01:57:03.400]   I didn't eat them.
[01:57:03.400 --> 01:57:05.800]   - Yeah, well, where are they Parker?
[01:57:05.800 --> 01:57:06.760]   - So anyway.
[01:57:06.760 --> 01:57:08.120]   - That is hysterical.
[01:57:08.120 --> 01:57:10.360]   Excellent job on the lip syncing.
[01:57:10.360 --> 01:57:12.600]   - Yeah, it's become, you know,
[01:57:12.600 --> 01:57:13.440]   wait a minute, wait a minute.
[01:57:13.440 --> 01:57:14.760]   Is daddy behind the camera?
[01:57:14.760 --> 01:57:15.680]   - Yeah, I'm the camera operator.
[01:57:15.680 --> 01:57:16.680]   - Okay, that's what I thought.
[01:57:16.680 --> 01:57:18.440]   You need to get one of those Jib ones.
[01:57:18.440 --> 01:57:19.440]   - I know, I know.
[01:57:19.440 --> 01:57:20.840]   - Well, the TikTok operation.
[01:57:20.840 --> 01:57:22.120]   - And put them on a green screen.
[01:57:22.120 --> 01:57:23.600]   - Yeah, green screen.
[01:57:23.600 --> 01:57:24.880]   It's already, it's coming.
[01:57:24.880 --> 01:57:25.720]   - Oh, I bet it is.
[01:57:25.720 --> 01:57:28.840]   This is gonna be the best-produced TikTok channel ever.
[01:57:28.840 --> 01:57:30.840]   - It's gonna be ILM quality TikTok.
[01:57:30.840 --> 01:57:33.200]   - It gets a little absurd because, you know,
[01:57:33.200 --> 01:57:36.440]   there's production meetings, there's a database.
[01:57:36.440 --> 01:57:37.520]   - Oh my God.
[01:57:37.520 --> 01:57:38.840]   - I go to the database in file language
[01:57:38.840 --> 01:57:42.000]   that keeps track of all the TikToks and the sounds
[01:57:42.000 --> 01:57:45.600]   and the notes and the storyboards and then we meet
[01:57:45.600 --> 01:57:46.600]   and we talk about what the ones are getting done.
[01:57:46.600 --> 01:57:48.520]   - Yeah, I was gonna ask, are there animatics?
[01:57:48.520 --> 01:57:51.440]   - Yeah, there's not animatics, but that's going too far.
[01:57:51.440 --> 01:57:52.360]   (laughing)
[01:57:52.360 --> 01:57:53.200]   - But--
[01:57:53.200 --> 01:57:54.040]   - That's a storyboard.
[01:57:54.040 --> 01:57:54.880]   - There's storyboards.
[01:57:54.880 --> 01:57:55.720]   - All the other things are not--
[01:57:55.720 --> 01:57:57.080]   - Some of them may require it.
[01:57:57.080 --> 01:58:02.080]   - If you're on TikTok, the McDubbies, T-H-E-M-C-D-U-B-B-Y-S.
[01:58:02.080 --> 01:58:04.440]   That's a very TikTok name.
[01:58:04.440 --> 01:58:05.600]   You had TikTok, you wouldn't do I, yes.
[01:58:05.600 --> 01:58:08.560]   - It took a lot of work to get that name.
[01:58:08.560 --> 01:58:10.320]   - Registry and TikTok turned out to be kind of a--
[01:58:10.320 --> 01:58:12.560]   - It's not them C-Dubbs either.
[01:58:12.560 --> 01:58:15.160]   It's the McDubbies, not them C-Dubbies.
[01:58:15.160 --> 01:58:16.240]   - I have no idea what it means.
[01:58:16.240 --> 01:58:17.440]   They came up with it.
[01:58:17.440 --> 01:58:18.280]   - It's great.
[01:58:18.280 --> 01:58:19.720]   - They were like, what do you want to call your channel
[01:58:19.720 --> 01:58:20.560]   and do it?
[01:58:20.560 --> 01:58:21.400]   - You're smart.
[01:58:21.400 --> 01:58:23.600]   - You should let their internet natives.
[01:58:23.600 --> 01:58:26.120]   You should let them live as an internet native.
[01:58:26.120 --> 01:58:27.640]   - And here's the thing is, I don't know,
[01:58:27.640 --> 01:58:29.400]   I mean, we're on a different,
[01:58:29.400 --> 01:58:31.280]   what we watch TikTok on is a different phone.
[01:58:31.280 --> 01:58:32.240]   We watch them together.
[01:58:32.240 --> 01:58:33.240]   There's not like, I feel like,
[01:58:33.240 --> 01:58:35.240]   I don't know how much risk there really is.
[01:58:35.240 --> 01:58:37.920]   - Oh, who cares if the Chinese know about new mass?
[01:58:37.920 --> 01:58:38.760]   - Exactly.
[01:58:38.760 --> 01:58:39.600]   - And I don't care.
[01:58:39.600 --> 01:58:41.240]   - And the funniest thing is,
[01:58:41.240 --> 01:58:44.440]   it's such a great way to teach them media production
[01:58:44.440 --> 01:58:46.040]   because we're doing everything,
[01:58:46.040 --> 01:58:47.800]   but it only lasts for 10 seconds.
[01:58:47.800 --> 01:58:50.240]   And so you sit there and you can plan all this stuff out
[01:58:50.240 --> 01:58:52.520]   and they get like a mini version where you're done with it
[01:58:52.520 --> 01:58:54.680]   and you hand it back and you people watch it.
[01:58:54.680 --> 01:58:56.760]   - Yeah, it's fun.
[01:58:56.760 --> 01:59:00.320]   - Only 18 followers as we speak, the McWish one up yesterday
[01:59:00.320 --> 01:59:01.880]   be a million followers.
[01:59:01.880 --> 01:59:02.720]   - That's our goal.
[01:59:02.720 --> 01:59:04.200]   - Everybody get out there and follow him.
[01:59:04.200 --> 01:59:07.240]   Andy Inako, follow him all the way to Boston Public Radio
[01:59:07.240 --> 01:59:10.520]   WGBH at, good lord.
[01:59:10.520 --> 01:59:14.320]   It's suddenly, it's the solarized Andy Inako.
[01:59:14.320 --> 01:59:17.840]   - Again, this is a strange game,
[01:59:17.840 --> 01:59:19.040]   this video won up from ship.
[01:59:19.040 --> 01:59:20.960]   The only way to win is not to play.
[01:59:20.960 --> 01:59:23.440]   - Shall we play a game?
[01:59:23.440 --> 01:59:25.320]   - Will you be on GBH?
[01:59:25.320 --> 01:59:28.120]   - No, I was just gonna smudge up the lens too.
[01:59:28.120 --> 01:59:29.120]   - Oh God.
[01:59:29.120 --> 01:59:29.960]   - Focus.
[01:59:29.960 --> 01:59:31.680]   (laughing)
[01:59:31.680 --> 01:59:34.840]   - Yeah, I was actually on yesterday.
[01:59:34.840 --> 01:59:38.160]   So you can go to wgbhnews.org to hear what I have to say
[01:59:38.160 --> 01:59:40.480]   above the FBI versus Apple and a couple other holiday
[01:59:40.480 --> 01:59:41.600]   flavorates.
[01:59:41.600 --> 01:59:43.800]   I think my next up, I'm back to Fridays.
[01:59:43.800 --> 01:59:46.600]   Next Friday, I think that'll be at the Boston Public Library
[01:59:46.600 --> 01:59:49.960]   at 1130 in the morning, I think.
[01:59:49.960 --> 01:59:52.520]   As usual, you can watch it live or later it's streamed
[01:59:52.520 --> 01:59:55.320]   on bgbhnews.org and if you go to my Twitter feed
[01:59:55.320 --> 01:59:57.560]   at an adgo, I usually tweet out like when I know
[01:59:57.560 --> 02:00:00.080]   the exact times, usually Fridays at around 1120,
[02:00:00.080 --> 02:00:03.400]   but we tend to, as presidents, keep getting impeached
[02:00:03.400 --> 02:00:06.760]   and stuff, the schedules kinda change.
[02:00:06.760 --> 02:00:09.440]   - We'll send you a jar of Vaseline for the next episode.
[02:00:09.440 --> 02:00:11.920]   - Yeah, please, give me the Barbra Streisand model.
[02:00:11.920 --> 02:00:15.120]   - It's in the budget, I believe.
[02:00:15.120 --> 02:00:17.880]   Lori Gill needs no Vaseline.
[02:00:17.880 --> 02:00:21.520]   She's fabulous managing editor at imore.com.
[02:00:21.520 --> 02:00:23.920]   Now with her new wood paneling.
[02:00:23.920 --> 02:00:26.520]   (laughing)
[02:00:26.520 --> 02:00:28.400]   - We're all stepping up our game.
[02:00:28.400 --> 02:00:30.640]   - Stepping up the game, man, I don't have to get to work here.
[02:00:30.640 --> 02:00:31.800]   Get that jib.
[02:00:31.800 --> 02:00:33.000]   Thank you, Lori.
[02:00:33.000 --> 02:00:35.160]   Anything you wanna plug?
[02:00:35.160 --> 02:00:36.400]   Everybody else, plug stuff.
[02:00:36.400 --> 02:00:38.040]   You got your new TikTok channel up?
[02:00:38.040 --> 02:00:38.880]   - I got not gonna plug.
[02:00:38.880 --> 02:00:40.800]   - No, I have not signed up for TikTok
[02:00:40.800 --> 02:00:44.120]   because I can look at TikTok without signing up for it.
[02:00:44.120 --> 02:00:46.080]   So that's how I'm gonna stay.
[02:00:46.080 --> 02:00:48.160]   - Lori made an awesome article on how to export
[02:00:48.160 --> 02:00:50.840]   your Apple card transactions.
[02:00:50.840 --> 02:00:55.280]   - New feature, you can finally drive in Lisa Crazy.
[02:00:55.280 --> 02:00:56.720]   She says, "Where's my statement?"
[02:00:56.720 --> 02:00:57.680]   I said, "Well, here's your statement."
[02:00:57.680 --> 02:01:01.000]   She says, "Well, how do I put this in the QuickBooks?"
[02:01:01.000 --> 02:01:02.120]   - Lori's got you covered.
[02:01:02.120 --> 02:01:03.720]   - I said, "You can't, until now.
[02:01:03.720 --> 02:01:05.520]   "This is a new feature of the Apple card."
[02:01:05.520 --> 02:01:08.040]   Thank you, Lori, we'll all look that up.
[02:01:08.040 --> 02:01:09.240]   Thank you all for being here.
[02:01:09.240 --> 02:01:11.840]   We do this show every Tuesday, right after iOS today.
[02:01:11.840 --> 02:01:14.320]   You can make Tuesday your Apple day,
[02:01:14.320 --> 02:01:16.880]   starting with iOS today around 9.30 Pacific
[02:01:16.880 --> 02:01:19.000]   and then Mac Break Weekly around 11 a.m. Pacific.
[02:01:19.000 --> 02:01:20.480]   That's 2 p.m. Eastern.
[02:01:20.480 --> 02:01:21.840]   That's 1,800 UTC.
[02:01:21.840 --> 02:01:24.800]   Tune in and watch the show.
[02:01:24.800 --> 02:01:27.360]   You could participate in the background
[02:01:27.360 --> 02:01:28.280]   if you're watching live.
[02:01:28.280 --> 02:01:31.520]   The live stream, audio or video is at twit.tv/live
[02:01:31.520 --> 02:01:36.520]   and the chat room that goes on is at irc.twit.tv.
[02:01:36.520 --> 02:01:40.160]   For those of you who listen on your own time,
[02:01:40.160 --> 02:01:44.240]   you can get copies of the show at twit.tv/mbw.
[02:01:44.240 --> 02:01:46.120]   We're also on YouTube, at least,
[02:01:46.120 --> 02:01:48.000]   until they take this episode down.
[02:01:48.480 --> 02:01:50.440]   (laughing)
[02:01:50.440 --> 02:01:51.440]   You can also--
[02:01:51.440 --> 02:01:52.440]   - Hey, Chew.
[02:01:52.440 --> 02:01:53.440]   - Don't miss it.
[02:01:53.440 --> 02:01:54.440]   - Stop it, stop it.
[02:01:54.440 --> 02:01:55.280]   Sorry, Chew.
[02:01:55.280 --> 02:01:56.840]   - I'm a troublemaker.
[02:01:56.840 --> 02:01:58.000]   - I'm a troublemaker.
[02:01:58.000 --> 02:02:00.240]   (laughing)
[02:02:00.240 --> 02:02:04.120]   That'd be interesting to see if content ID fingers
[02:02:04.120 --> 02:02:04.960]   that are not.
[02:02:04.960 --> 02:02:06.800]   That's gonna be interesting.
[02:02:06.800 --> 02:02:09.680]   You can subscribe in your favorite podcast application.
[02:02:09.680 --> 02:02:11.120]   Just look for Mac Break Weekly.
[02:02:11.120 --> 02:02:13.320]   And if you are listening, after the fact,
[02:02:13.320 --> 02:02:15.120]   we do have interactivity for you too.
[02:02:15.120 --> 02:02:17.600]   As I mentioned, twit.community are forums
[02:02:17.600 --> 02:02:20.840]   and our new Mastodon instance twit.social
[02:02:20.840 --> 02:02:23.520]   are all wide open and we love having you in there.
[02:02:23.520 --> 02:02:25.800]   I hang out there and we get our hosts to do that as well
[02:02:25.800 --> 02:02:27.360]   if possible.
[02:02:27.360 --> 02:02:31.040]   Again, thank you for participating.
[02:02:31.040 --> 02:02:34.000]   And I'm sorry to say it's time to get back to work though.
[02:02:34.000 --> 02:02:36.480]   'Cause it's, 'cause break time.
[02:02:36.480 --> 02:02:38.400]   How many years have I been doing this wrong?
[02:02:38.400 --> 02:02:40.520]   You made it up.
[02:02:40.520 --> 02:02:41.720]   Time to get back to work because--
[02:02:41.720 --> 02:02:43.320]   - Break time is over.
[02:02:43.320 --> 02:02:44.560]   - Thank you, Alec.
[02:02:44.560 --> 02:02:45.400]   (laughing)
[02:02:45.400 --> 02:02:46.560]   - See you next time.
[02:02:46.560 --> 02:02:49.140]   (upbeat music)
[02:02:49.140 --> 02:02:51.720]   (upbeat music)
[02:02:51.720 --> 02:02:54.300]   (upbeat music)
[02:02:54.300 --> 02:02:56.880]   (upbeat music)
[02:02:56.880 --> 02:02:59.460]   (upbeat music)

